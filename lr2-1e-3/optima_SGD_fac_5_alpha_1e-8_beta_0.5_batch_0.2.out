5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.861844539642334
Reconstruction Loss: -0.3792850971221924
Iteration 21:
Training Loss: 5.179971218109131
Reconstruction Loss: -0.3792850971221924
Iteration 41:
Training Loss: 5.685211658477783
Reconstruction Loss: -0.3792850971221924
Iteration 61:
Training Loss: 5.415765285491943
Reconstruction Loss: -0.37928515672683716
Iteration 81:
Training Loss: 5.324221611022949
Reconstruction Loss: -0.3792852461338043
Iteration 101:
Training Loss: 5.612210750579834
Reconstruction Loss: -0.37928542494773865
Iteration 121:
Training Loss: 5.587784290313721
Reconstruction Loss: -0.37928542494773865
Iteration 141:
Training Loss: 5.453563213348389
Reconstruction Loss: -0.37928542494773865
Iteration 161:
Training Loss: 5.494809627532959
Reconstruction Loss: -0.3792855143547058
Iteration 181:
Training Loss: 5.688539028167725
Reconstruction Loss: -0.3792855143547058
Iteration 201:
Training Loss: 5.739651203155518
Reconstruction Loss: -0.379285603761673
Iteration 221:
Training Loss: 5.4754767417907715
Reconstruction Loss: -0.37928569316864014
Iteration 241:
Training Loss: 5.639781475067139
Reconstruction Loss: -0.37928569316864014
Iteration 261:
Training Loss: 5.76561975479126
Reconstruction Loss: -0.37928587198257446
Iteration 281:
Training Loss: 5.5582451820373535
Reconstruction Loss: -0.3792859613895416
Iteration 301:
Training Loss: 6.031567573547363
Reconstruction Loss: -0.3792859613895416
Iteration 321:
Training Loss: 5.164552211761475
Reconstruction Loss: -0.3792860507965088
Iteration 341:
Training Loss: 5.727466106414795
Reconstruction Loss: -0.37928611040115356
Iteration 361:
Training Loss: 5.288304805755615
Reconstruction Loss: -0.37928611040115356
Iteration 381:
Training Loss: 5.7040696144104
Reconstruction Loss: -0.3792862892150879
Iteration 401:
Training Loss: 5.55072021484375
Reconstruction Loss: -0.3792862892150879
Iteration 421:
Training Loss: 5.3164801597595215
Reconstruction Loss: -0.3792862892150879
Iteration 441:
Training Loss: 5.5756940841674805
Reconstruction Loss: -0.3792864680290222
Iteration 461:
Training Loss: 5.205544471740723
Reconstruction Loss: -0.3792864680290222
Iteration 481:
Training Loss: 5.293570518493652
Reconstruction Loss: -0.37928664684295654
Iteration 501:
Training Loss: 5.550335884094238
Reconstruction Loss: -0.37928664684295654
Iteration 521:
Training Loss: 5.45892858505249
Reconstruction Loss: -0.3792867362499237
Iteration 541:
Training Loss: 5.6065497398376465
Reconstruction Loss: -0.37928682565689087
Iteration 561:
Training Loss: 5.3513569831848145
Reconstruction Loss: -0.37928682565689087
Iteration 581:
Training Loss: 5.3298797607421875
Reconstruction Loss: -0.37928709387779236
Iteration 601:
Training Loss: 5.629751682281494
Reconstruction Loss: -0.3792871832847595
Iteration 621:
Training Loss: 5.250828742980957
Reconstruction Loss: -0.3792872726917267
Iteration 641:
Training Loss: 5.538668155670166
Reconstruction Loss: -0.3792872726917267
Iteration 661:
Training Loss: 5.7174482345581055
Reconstruction Loss: -0.3792874217033386
Iteration 681:
Training Loss: 5.467619895935059
Reconstruction Loss: -0.3792875111103058
Iteration 701:
Training Loss: 5.664242267608643
Reconstruction Loss: -0.37928760051727295
Iteration 721:
Training Loss: 5.47395658493042
Reconstruction Loss: -0.3792876899242401
Iteration 741:
Training Loss: 5.5775465965271
Reconstruction Loss: -0.37928786873817444
Iteration 761:
Training Loss: 5.465006351470947
Reconstruction Loss: -0.3792879581451416
Iteration 781:
Training Loss: 5.572856426239014
Reconstruction Loss: -0.37928804755210876
Iteration 801:
Training Loss: 5.613118648529053
Reconstruction Loss: -0.3792881965637207
Iteration 821:
Training Loss: 5.652223587036133
Reconstruction Loss: -0.37928831577301025
Iteration 841:
Training Loss: 5.360861301422119
Reconstruction Loss: -0.3792884945869446
Iteration 861:
Training Loss: 5.449470520019531
Reconstruction Loss: -0.3792886435985565
Iteration 881:
Training Loss: 5.6474456787109375
Reconstruction Loss: -0.3792887330055237
Iteration 901:
Training Loss: 5.533815860748291
Reconstruction Loss: -0.379288911819458
Iteration 921:
Training Loss: 5.4945220947265625
Reconstruction Loss: -0.37928909063339233
Iteration 941:
Training Loss: 5.659725666046143
Reconstruction Loss: -0.37928926944732666
Iteration 961:
Training Loss: 5.446358680725098
Reconstruction Loss: -0.379289448261261
Iteration 981:
Training Loss: 5.449238300323486
Reconstruction Loss: -0.3792896270751953
Iteration 1001:
Training Loss: 5.529087543487549
Reconstruction Loss: -0.3792899549007416
Iteration 1021:
Training Loss: 5.883508682250977
Reconstruction Loss: -0.37929004430770874
Iteration 1041:
Training Loss: 5.642727851867676
Reconstruction Loss: -0.37929022312164307
Iteration 1061:
Training Loss: 5.4300031661987305
Reconstruction Loss: -0.3792904019355774
Iteration 1081:
Training Loss: 5.590158939361572
Reconstruction Loss: -0.37929075956344604
Iteration 1101:
Training Loss: 5.41884708404541
Reconstruction Loss: -0.37929099798202515
Iteration 1121:
Training Loss: 5.651281356811523
Reconstruction Loss: -0.37929126620292664
Iteration 1141:
Training Loss: 5.542043685913086
Reconstruction Loss: -0.3792915344238281
Iteration 1161:
Training Loss: 5.941962718963623
Reconstruction Loss: -0.37929195165634155
Iteration 1181:
Training Loss: 5.489038467407227
Reconstruction Loss: -0.37929221987724304
Iteration 1201:
Training Loss: 5.453402519226074
Reconstruction Loss: -0.37929266691207886
Iteration 1221:
Training Loss: 5.594083786010742
Reconstruction Loss: -0.3792930841445923
Iteration 1241:
Training Loss: 5.589669704437256
Reconstruction Loss: -0.37929344177246094
Iteration 1261:
Training Loss: 5.426431655883789
Reconstruction Loss: -0.37929385900497437
Iteration 1281:
Training Loss: 5.753981113433838
Reconstruction Loss: -0.37929439544677734
Iteration 1301:
Training Loss: 5.837200164794922
Reconstruction Loss: -0.3792949914932251
Iteration 1321:
Training Loss: 5.5873188972473145
Reconstruction Loss: -0.37929561734199524
Iteration 1341:
Training Loss: 5.8213210105896
Reconstruction Loss: -0.379296213388443
Iteration 1361:
Training Loss: 5.395206928253174
Reconstruction Loss: -0.37929701805114746
Iteration 1381:
Training Loss: 5.383983135223389
Reconstruction Loss: -0.3792976140975952
Iteration 1401:
Training Loss: 5.687379837036133
Reconstruction Loss: -0.3792986571788788
Iteration 1421:
Training Loss: 5.721315860748291
Reconstruction Loss: -0.3792996406555176
Iteration 1441:
Training Loss: 5.826566219329834
Reconstruction Loss: -0.3793007731437683
Iteration 1461:
Training Loss: 5.710648059844971
Reconstruction Loss: -0.3793019652366638
Iteration 1481:
Training Loss: 5.6575212478637695
Reconstruction Loss: -0.3793034553527832
Iteration 1501:
Training Loss: 5.414538383483887
Reconstruction Loss: -0.3793051242828369
Iteration 1521:
Training Loss: 5.776130676269531
Reconstruction Loss: -0.3793070316314697
Iteration 1541:
Training Loss: 5.4909491539001465
Reconstruction Loss: -0.37930920720100403
Iteration 1561:
Training Loss: 5.713113784790039
Reconstruction Loss: -0.37931182980537415
Iteration 1581:
Training Loss: 5.632787227630615
Reconstruction Loss: -0.37931495904922485
Iteration 1601:
Training Loss: 5.317399501800537
Reconstruction Loss: -0.3793187141418457
Iteration 1621:
Training Loss: 5.333532333374023
Reconstruction Loss: -0.3793233036994934
Iteration 1641:
Training Loss: 5.505642414093018
Reconstruction Loss: -0.3793289065361023
Iteration 1661:
Training Loss: 5.596325874328613
Reconstruction Loss: -0.37933602929115295
Iteration 1681:
Training Loss: 5.297224521636963
Reconstruction Loss: -0.3793453574180603
Iteration 1701:
Training Loss: 5.278937339782715
Reconstruction Loss: -0.3793577253818512
Iteration 1721:
Training Loss: 5.185248851776123
Reconstruction Loss: -0.379375159740448
Iteration 1741:
Training Loss: 5.4258928298950195
Reconstruction Loss: -0.3794001340866089
Iteration 1761:
Training Loss: 5.498624801635742
Reconstruction Loss: -0.3794388175010681
Iteration 1781:
Training Loss: 5.524811267852783
Reconstruction Loss: -0.3795042335987091
Iteration 1801:
Training Loss: 5.5590691566467285
Reconstruction Loss: -0.379629522562027
Iteration 1821:
Training Loss: 5.400293350219727
Reconstruction Loss: -0.37993162870407104
Iteration 1841:
Training Loss: 5.497549057006836
Reconstruction Loss: -0.3810901343822479
Iteration 1861:
Training Loss: 5.417396068572998
Reconstruction Loss: -0.41486239433288574
Iteration 1881:
Training Loss: 5.081884860992432
Reconstruction Loss: -0.5629808902740479
Iteration 1901:
Training Loss: 4.772087097167969
Reconstruction Loss: -0.5635108947753906
Iteration 1921:
Training Loss: 4.943119525909424
Reconstruction Loss: -0.5934929251670837
Iteration 1941:
Training Loss: 5.2616167068481445
Reconstruction Loss: -0.5800025463104248
Iteration 1961:
Training Loss: 5.060374736785889
Reconstruction Loss: -0.5954242944717407
Iteration 1981:
Training Loss: 4.8660664558410645
Reconstruction Loss: -0.5705921649932861
Iteration 2001:
Training Loss: 5.111889839172363
Reconstruction Loss: -0.5729418992996216
Iteration 2021:
Training Loss: 5.108852863311768
Reconstruction Loss: -0.5947285890579224
Iteration 2041:
Training Loss: 4.975187301635742
Reconstruction Loss: -0.5824052691459656
Iteration 2061:
Training Loss: 5.014192581176758
Reconstruction Loss: -0.5849255919456482
Iteration 2081:
Training Loss: 5.01627254486084
Reconstruction Loss: -0.5773108005523682
Iteration 2101:
Training Loss: 5.1078081130981445
Reconstruction Loss: -0.5795155167579651
Iteration 2121:
Training Loss: 4.769176959991455
Reconstruction Loss: -0.510747492313385
Iteration 2141:
Training Loss: 5.091498851776123
Reconstruction Loss: -0.5436626672744751
Iteration 2161:
Training Loss: 5.068214416503906
Reconstruction Loss: -0.5412827730178833
Iteration 2181:
Training Loss: 5.057931423187256
Reconstruction Loss: -0.5060960054397583
Iteration 2201:
Training Loss: 5.074638366699219
Reconstruction Loss: -0.5813086628913879
Iteration 2221:
Training Loss: 4.840812683105469
Reconstruction Loss: -0.5766029953956604
Iteration 2241:
Training Loss: 4.697322368621826
Reconstruction Loss: -0.5257860422134399
Iteration 2261:
Training Loss: 5.010318279266357
Reconstruction Loss: -0.595460057258606
Iteration 2281:
Training Loss: 4.797022342681885
Reconstruction Loss: -0.5607708096504211
Iteration 2301:
Training Loss: 5.2053608894348145
Reconstruction Loss: -0.5849674940109253
Iteration 2321:
Training Loss: 4.988136291503906
Reconstruction Loss: -0.5733248591423035
Iteration 2341:
Training Loss: 5.091241836547852
Reconstruction Loss: -0.5736600160598755
Iteration 2361:
Training Loss: 5.110732078552246
Reconstruction Loss: -0.6032788157463074
Iteration 2381:
Training Loss: 4.847448825836182
Reconstruction Loss: -0.5072849988937378
Iteration 2401:
Training Loss: 4.710813999176025
Reconstruction Loss: -0.5622941851615906
Iteration 2421:
Training Loss: 5.13687801361084
Reconstruction Loss: -0.5577800273895264
Iteration 2441:
Training Loss: 5.015437602996826
Reconstruction Loss: -0.575298011302948
Iteration 2461:
Training Loss: 4.8246331214904785
Reconstruction Loss: -0.5368601083755493
Iteration 2481:
Training Loss: 5.2210235595703125
Reconstruction Loss: -0.610205352306366
Iteration 2501:
Training Loss: 4.6784210205078125
Reconstruction Loss: -0.5201959609985352
Iteration 2521:
Training Loss: 5.029855251312256
Reconstruction Loss: -0.5543078184127808
Iteration 2541:
Training Loss: 5.10732364654541
Reconstruction Loss: -0.567544162273407
Iteration 2561:
Training Loss: 5.008743762969971
Reconstruction Loss: -0.5349957346916199
Iteration 2581:
Training Loss: 4.7010016441345215
Reconstruction Loss: -0.5533730387687683
Iteration 2601:
Training Loss: 4.902713775634766
Reconstruction Loss: -0.5419325828552246
Iteration 2621:
Training Loss: 5.166316509246826
Reconstruction Loss: -0.5424750447273254
Iteration 2641:
Training Loss: 5.141738414764404
Reconstruction Loss: -0.5968915820121765
Iteration 2661:
Training Loss: 5.270016193389893
Reconstruction Loss: -0.5940204858779907
Iteration 2681:
Training Loss: 4.969785213470459
Reconstruction Loss: -0.545081615447998
Iteration 2701:
Training Loss: 4.76519775390625
Reconstruction Loss: -0.5659112930297852
Iteration 2721:
Training Loss: 5.1012396812438965
Reconstruction Loss: -0.598841667175293
Iteration 2741:
Training Loss: 4.706658363342285
Reconstruction Loss: -0.5639192461967468
Iteration 2761:
Training Loss: 5.047673225402832
Reconstruction Loss: -0.5344655513763428
Iteration 2781:
Training Loss: 5.258803844451904
Reconstruction Loss: -0.514255166053772
Iteration 2801:
Training Loss: 4.917501449584961
Reconstruction Loss: -0.4998169243335724
Iteration 2821:
Training Loss: 5.039339542388916
Reconstruction Loss: -0.5546923279762268
Iteration 2841:
Training Loss: 4.970198154449463
Reconstruction Loss: -0.5764197707176208
Iteration 2861:
Training Loss: 4.803317070007324
Reconstruction Loss: -0.5578281283378601
Iteration 2881:
Training Loss: 5.026947021484375
Reconstruction Loss: -0.5788409113883972
Iteration 2901:
Training Loss: 5.025254249572754
Reconstruction Loss: -0.574788510799408
Iteration 2921:
Training Loss: 4.964571952819824
Reconstruction Loss: -0.5679945945739746
Iteration 2941:
Training Loss: 5.193424701690674
Reconstruction Loss: -0.5369449257850647
Iteration 2961:
Training Loss: 5.101700782775879
Reconstruction Loss: -0.5766032934188843
Iteration 2981:
Training Loss: 5.0750412940979
Reconstruction Loss: -0.5032596588134766
Iteration 3001:
Training Loss: 5.038910865783691
Reconstruction Loss: -0.5852658152580261
Iteration 3021:
Training Loss: 5.195577144622803
Reconstruction Loss: -0.6041613817214966
Iteration 3041:
Training Loss: 5.201899528503418
Reconstruction Loss: -0.5720683932304382
Iteration 3061:
Training Loss: 4.908646106719971
Reconstruction Loss: -0.577686071395874
Iteration 3081:
Training Loss: 4.834717750549316
Reconstruction Loss: -0.5875253081321716
Iteration 3101:
Training Loss: 4.821796894073486
Reconstruction Loss: -0.6028513312339783
Iteration 3121:
Training Loss: 5.003637790679932
Reconstruction Loss: -0.6005125641822815
Iteration 3141:
Training Loss: 5.074655532836914
Reconstruction Loss: -0.5736227631568909
Iteration 3161:
Training Loss: 5.066252708435059
Reconstruction Loss: -0.558441698551178
Iteration 3181:
Training Loss: 5.116804122924805
Reconstruction Loss: -0.5615386366844177
Iteration 3201:
Training Loss: 4.738178253173828
Reconstruction Loss: -0.5383404493331909
Iteration 3221:
Training Loss: 5.045894145965576
Reconstruction Loss: -0.5687397718429565
Iteration 3241:
Training Loss: 5.264129161834717
Reconstruction Loss: -0.5837638974189758
Iteration 3261:
Training Loss: 5.124298572540283
Reconstruction Loss: -0.4955444931983948
Iteration 3281:
Training Loss: 5.226988792419434
Reconstruction Loss: -0.6040384769439697
Iteration 3301:
Training Loss: 5.218523979187012
Reconstruction Loss: -0.5122847557067871
Iteration 3321:
Training Loss: 5.037164688110352
Reconstruction Loss: -0.5765599012374878
Iteration 3341:
Training Loss: 5.1050920486450195
Reconstruction Loss: -0.5880618691444397
Iteration 3361:
Training Loss: 5.205513000488281
Reconstruction Loss: -0.5525884628295898
Iteration 3381:
Training Loss: 4.64644718170166
Reconstruction Loss: -0.5888181924819946
Iteration 3401:
Training Loss: 4.799023151397705
Reconstruction Loss: -0.5186807513237
Iteration 3421:
Training Loss: 5.059391021728516
Reconstruction Loss: -0.5216355919837952
Iteration 3441:
Training Loss: 4.955348491668701
Reconstruction Loss: -0.5435224771499634
Iteration 3461:
Training Loss: 5.064921855926514
Reconstruction Loss: -0.5782654285430908
Iteration 3481:
Training Loss: 4.916644096374512
Reconstruction Loss: -0.565243124961853
Iteration 3501:
Training Loss: 4.875284194946289
Reconstruction Loss: -0.5093482732772827
Iteration 3521:
Training Loss: 4.926469802856445
Reconstruction Loss: -0.5855521559715271
Iteration 3541:
Training Loss: 5.135099411010742
Reconstruction Loss: -0.5794041156768799
Iteration 3561:
Training Loss: 4.818453788757324
Reconstruction Loss: -0.5481709837913513
Iteration 3581:
Training Loss: 4.886048316955566
Reconstruction Loss: -0.5712164640426636
Iteration 3601:
Training Loss: 4.892293453216553
Reconstruction Loss: -0.5870992541313171
Iteration 3621:
Training Loss: 4.809001922607422
Reconstruction Loss: -0.5353958606719971
Iteration 3641:
Training Loss: 5.022577285766602
Reconstruction Loss: -0.5699830055236816
Iteration 3661:
Training Loss: 4.784195423126221
Reconstruction Loss: -0.5386493802070618
Iteration 3681:
Training Loss: 4.976504802703857
Reconstruction Loss: -0.5903583765029907
Iteration 3701:
Training Loss: 5.28519344329834
Reconstruction Loss: -0.563201904296875
Iteration 3721:
Training Loss: 5.155352592468262
Reconstruction Loss: -0.578057050704956
Iteration 3741:
Training Loss: 5.089476108551025
Reconstruction Loss: -0.5595016479492188
Iteration 3761:
Training Loss: 4.915811061859131
Reconstruction Loss: -0.5910453200340271
Iteration 3781:
Training Loss: 4.941235065460205
Reconstruction Loss: -0.514373779296875
Iteration 3801:
Training Loss: 4.347779273986816
Reconstruction Loss: -0.809542715549469
Iteration 3821:
Training Loss: 4.5520172119140625
Reconstruction Loss: -0.784152090549469
Iteration 3841:
Training Loss: 4.457829475402832
Reconstruction Loss: -0.8018503785133362
Iteration 3861:
Training Loss: 4.30570125579834
Reconstruction Loss: -0.8453590273857117
Iteration 3881:
Training Loss: 4.364133358001709
Reconstruction Loss: -0.8024057745933533
Iteration 3901:
Training Loss: 4.586482048034668
Reconstruction Loss: -0.7804570198059082
Iteration 3921:
Training Loss: 4.349802017211914
Reconstruction Loss: -0.831689715385437
Iteration 3941:
Training Loss: 4.538010120391846
Reconstruction Loss: -0.8353949785232544
Iteration 3961:
Training Loss: 4.258707046508789
Reconstruction Loss: -0.8176688551902771
Iteration 3981:
Training Loss: 4.461339950561523
Reconstruction Loss: -0.8225852251052856
Iteration 4001:
Training Loss: 4.38289737701416
Reconstruction Loss: -0.8597187995910645
Iteration 4021:
Training Loss: 4.561643123626709
Reconstruction Loss: -0.8072842359542847
Iteration 4041:
Training Loss: 4.103569984436035
Reconstruction Loss: -0.8482444286346436
Iteration 4061:
Training Loss: 4.4586615562438965
Reconstruction Loss: -0.8566635251045227
Iteration 4081:
Training Loss: 4.214858531951904
Reconstruction Loss: -0.7840537428855896
Iteration 4101:
Training Loss: 4.443112373352051
Reconstruction Loss: -0.842727780342102
Iteration 4121:
Training Loss: 4.390666961669922
Reconstruction Loss: -0.8543038368225098
Iteration 4141:
Training Loss: 4.457239151000977
Reconstruction Loss: -0.8215603828430176
Iteration 4161:
Training Loss: 4.638876438140869
Reconstruction Loss: -0.7896425724029541
Iteration 4181:
Training Loss: 4.535764217376709
Reconstruction Loss: -0.7605502605438232
Iteration 4201:
Training Loss: 4.528040409088135
Reconstruction Loss: -0.8221026659011841
Iteration 4221:
Training Loss: 4.60694694519043
Reconstruction Loss: -0.8184340596199036
Iteration 4241:
Training Loss: 4.508158206939697
Reconstruction Loss: -0.8241490721702576
Iteration 4261:
Training Loss: 4.479633331298828
Reconstruction Loss: -0.8135882019996643
Iteration 4281:
Training Loss: 4.536930561065674
Reconstruction Loss: -0.803242027759552
Iteration 4301:
Training Loss: 4.418950080871582
Reconstruction Loss: -0.8445010781288147
Iteration 4321:
Training Loss: 4.350625514984131
Reconstruction Loss: -0.8036622405052185
Iteration 4341:
Training Loss: 4.5076985359191895
Reconstruction Loss: -0.8327212929725647
Iteration 4361:
Training Loss: 4.3603620529174805
Reconstruction Loss: -0.8068719506263733
Iteration 4381:
Training Loss: 4.373978614807129
Reconstruction Loss: -0.8537058234214783
Iteration 4401:
Training Loss: 4.252834320068359
Reconstruction Loss: -0.7822141051292419
Iteration 4421:
Training Loss: 4.261834621429443
Reconstruction Loss: -0.8280899524688721
Iteration 4441:
Training Loss: 4.536215782165527
Reconstruction Loss: -0.8106146454811096
Iteration 4461:
Training Loss: 4.11228084564209
Reconstruction Loss: -0.8293535709381104
Iteration 4481:
Training Loss: 4.397887229919434
Reconstruction Loss: -0.8430054187774658
Iteration 4501:
Training Loss: 4.468479156494141
Reconstruction Loss: -0.8240002989768982
Iteration 4521:
Training Loss: 4.317758083343506
Reconstruction Loss: -0.8211817741394043
Iteration 4541:
Training Loss: 4.489283084869385
Reconstruction Loss: -0.8225190043449402
Iteration 4561:
Training Loss: 4.7305521965026855
Reconstruction Loss: -0.7575792670249939
Iteration 4581:
Training Loss: 4.179737091064453
Reconstruction Loss: -0.7420047521591187
Iteration 4601:
Training Loss: 4.47148323059082
Reconstruction Loss: -0.8190627098083496
Iteration 4621:
Training Loss: 4.634405612945557
Reconstruction Loss: -0.8272325992584229
Iteration 4641:
Training Loss: 4.442155838012695
Reconstruction Loss: -0.7948688268661499
Iteration 4661:
Training Loss: 4.507900238037109
Reconstruction Loss: -0.8242431282997131
Iteration 4681:
Training Loss: 4.6126017570495605
Reconstruction Loss: -0.8449240922927856
Iteration 4701:
Training Loss: 4.202003002166748
Reconstruction Loss: -0.8227622509002686
Iteration 4721:
Training Loss: 4.445504188537598
Reconstruction Loss: -0.8375223875045776
Iteration 4741:
Training Loss: 4.297438621520996
Reconstruction Loss: -0.818579912185669
Iteration 4761:
Training Loss: 4.224868297576904
Reconstruction Loss: -0.8019217252731323
Iteration 4781:
Training Loss: 4.492063522338867
Reconstruction Loss: -0.8269791007041931
Iteration 4801:
Training Loss: 4.5837297439575195
Reconstruction Loss: -0.847831130027771
Iteration 4821:
Training Loss: 4.156725883483887
Reconstruction Loss: -0.8081722855567932
Iteration 4841:
Training Loss: 4.36783504486084
Reconstruction Loss: -0.8151747584342957
Iteration 4861:
Training Loss: 4.53307580947876
Reconstruction Loss: -0.8411888480186462
Iteration 4881:
Training Loss: 4.198599815368652
Reconstruction Loss: -0.8036932349205017
Iteration 4901:
Training Loss: 4.6314616203308105
Reconstruction Loss: -0.7890629768371582
Iteration 4921:
Training Loss: 4.680136203765869
Reconstruction Loss: -0.8249781727790833
Iteration 4941:
Training Loss: 4.068136692047119
Reconstruction Loss: -0.8168815970420837
Iteration 4961:
Training Loss: 4.323642730712891
Reconstruction Loss: -0.8410147428512573
Iteration 4981:
Training Loss: 4.296083927154541
Reconstruction Loss: -0.7757500410079956
Iteration 5001:
Training Loss: 4.3947062492370605
Reconstruction Loss: -0.8115091919898987
Iteration 5021:
Training Loss: 4.39042329788208
Reconstruction Loss: -0.8518572449684143
Iteration 5041:
Training Loss: 4.163712978363037
Reconstruction Loss: -0.8247919082641602
Iteration 5061:
Training Loss: 4.502995014190674
Reconstruction Loss: -0.8347285985946655
Iteration 5081:
Training Loss: 4.494375705718994
Reconstruction Loss: -0.8160935640335083
Iteration 5101:
Training Loss: 4.774013996124268
Reconstruction Loss: -0.8150212168693542
Iteration 5121:
Training Loss: 4.553746223449707
Reconstruction Loss: -0.8595376014709473
Iteration 5141:
Training Loss: 4.562490463256836
Reconstruction Loss: -0.8095563650131226
Iteration 5161:
Training Loss: 4.415448188781738
Reconstruction Loss: -0.829531729221344
Iteration 5181:
Training Loss: 4.216827392578125
Reconstruction Loss: -0.7954304814338684
Iteration 5201:
Training Loss: 4.119385242462158
Reconstruction Loss: -0.8307231664657593
Iteration 5221:
Training Loss: 4.540199279785156
Reconstruction Loss: -0.8004577159881592
Iteration 5241:
Training Loss: 4.512761116027832
Reconstruction Loss: -0.8121762275695801
Iteration 5261:
Training Loss: 4.474369525909424
Reconstruction Loss: -0.832457423210144
Iteration 5281:
Training Loss: 4.246572971343994
Reconstruction Loss: -0.869184136390686
Iteration 5301:
Training Loss: 4.206155300140381
Reconstruction Loss: -0.9679834842681885
Iteration 5321:
Training Loss: 3.67474365234375
Reconstruction Loss: -1.1275169849395752
Iteration 5341:
Training Loss: 3.7723963260650635
Reconstruction Loss: -1.0994093418121338
Iteration 5361:
Training Loss: 3.685971736907959
Reconstruction Loss: -1.0578656196594238
Iteration 5381:
Training Loss: 3.7148449420928955
Reconstruction Loss: -1.0850050449371338
Iteration 5401:
Training Loss: 3.7910492420196533
Reconstruction Loss: -1.1234327554702759
Iteration 5421:
Training Loss: 3.684264659881592
Reconstruction Loss: -1.0765469074249268
Iteration 5441:
Training Loss: 3.4807302951812744
Reconstruction Loss: -1.0587170124053955
Iteration 5461:
Training Loss: 3.8181045055389404
Reconstruction Loss: -1.056348204612732
Iteration 5481:
Training Loss: 3.462707757949829
Reconstruction Loss: -1.0786060094833374
Iteration 5501:
Training Loss: 3.9658467769622803
Reconstruction Loss: -1.0564430952072144
Iteration 5521:
Training Loss: 3.655808448791504
Reconstruction Loss: -1.0520786046981812
Iteration 5541:
Training Loss: 3.866537570953369
Reconstruction Loss: -1.0559515953063965
Iteration 5561:
Training Loss: 3.622596502304077
Reconstruction Loss: -1.0555706024169922
Iteration 5581:
Training Loss: 3.443887710571289
Reconstruction Loss: -1.068429708480835
Iteration 5601:
Training Loss: 3.8469743728637695
Reconstruction Loss: -1.0459198951721191
Iteration 5621:
Training Loss: 3.874035120010376
Reconstruction Loss: -1.0549883842468262
Iteration 5641:
Training Loss: 3.4723401069641113
Reconstruction Loss: -1.0679774284362793
Iteration 5661:
Training Loss: 3.723062038421631
Reconstruction Loss: -1.0663915872573853
Iteration 5681:
Training Loss: 3.859731912612915
Reconstruction Loss: -1.059441328048706
Iteration 5701:
Training Loss: 3.6058340072631836
Reconstruction Loss: -1.071704626083374
Iteration 5721:
Training Loss: 3.3587589263916016
Reconstruction Loss: -1.0617643594741821
Iteration 5741:
Training Loss: 3.4751062393188477
Reconstruction Loss: -1.0710103511810303
Iteration 5761:
Training Loss: 3.614370584487915
Reconstruction Loss: -1.0523829460144043
Iteration 5781:
Training Loss: 3.3836617469787598
Reconstruction Loss: -1.0366334915161133
Iteration 5801:
Training Loss: 3.5755932331085205
Reconstruction Loss: -1.0643854141235352
Iteration 5821:
Training Loss: 3.7712807655334473
Reconstruction Loss: -1.0523467063903809
Iteration 5841:
Training Loss: 3.703131914138794
Reconstruction Loss: -1.0616705417633057
Iteration 5861:
Training Loss: 3.5964250564575195
Reconstruction Loss: -1.0560193061828613
Iteration 5881:
Training Loss: 3.6100943088531494
Reconstruction Loss: -1.0550823211669922
Iteration 5901:
Training Loss: 3.7326745986938477
Reconstruction Loss: -1.0413837432861328
Iteration 5921:
Training Loss: 3.591580390930176
Reconstruction Loss: -1.0743021965026855
Iteration 5941:
Training Loss: 3.526407480239868
Reconstruction Loss: -1.0712625980377197
Iteration 5961:
Training Loss: 3.6297380924224854
Reconstruction Loss: -1.0455430746078491
Iteration 5981:
Training Loss: 3.80562162399292
Reconstruction Loss: -1.0654504299163818
Iteration 6001:
Training Loss: 3.8242759704589844
Reconstruction Loss: -1.060286521911621
Iteration 6021:
Training Loss: 3.691464900970459
Reconstruction Loss: -1.0680389404296875
Iteration 6041:
Training Loss: 3.772622585296631
Reconstruction Loss: -1.0585377216339111
Iteration 6061:
Training Loss: 3.6465864181518555
Reconstruction Loss: -1.058337926864624
Iteration 6081:
Training Loss: 3.3882315158843994
Reconstruction Loss: -1.0720593929290771
Iteration 6101:
Training Loss: 3.536339044570923
Reconstruction Loss: -1.0592695474624634
Iteration 6121:
Training Loss: 3.77496600151062
Reconstruction Loss: -1.0606834888458252
Iteration 6141:
Training Loss: 3.7066636085510254
Reconstruction Loss: -1.066924810409546
Iteration 6161:
Training Loss: 3.389373302459717
Reconstruction Loss: -1.043603777885437
Iteration 6181:
Training Loss: 3.4470484256744385
Reconstruction Loss: -1.0605368614196777
Iteration 6201:
Training Loss: 3.555692195892334
Reconstruction Loss: -1.0611374378204346
Iteration 6221:
Training Loss: 3.688196897506714
Reconstruction Loss: -1.0327346324920654
Iteration 6241:
Training Loss: 3.611607313156128
Reconstruction Loss: -1.0451877117156982
Iteration 6261:
Training Loss: 3.9628446102142334
Reconstruction Loss: -1.0439651012420654
Iteration 6281:
Training Loss: 3.5275964736938477
Reconstruction Loss: -1.051645040512085
Iteration 6301:
Training Loss: 3.6256253719329834
Reconstruction Loss: -1.1078860759735107
Iteration 6321:
Training Loss: 3.9391045570373535
Reconstruction Loss: -1.0587825775146484
Iteration 6341:
Training Loss: 3.708385705947876
Reconstruction Loss: -1.0588899850845337
Iteration 6361:
Training Loss: 3.6772985458374023
Reconstruction Loss: -1.0681087970733643
Iteration 6381:
Training Loss: 3.4240920543670654
Reconstruction Loss: -1.0533034801483154
Iteration 6401:
Training Loss: 3.7749135494232178
Reconstruction Loss: -1.0212459564208984
Iteration 6421:
Training Loss: 3.9311652183532715
Reconstruction Loss: -1.0736124515533447
Iteration 6441:
Training Loss: 3.4473366737365723
Reconstruction Loss: -1.0566065311431885
Iteration 6461:
Training Loss: 3.6377243995666504
Reconstruction Loss: -1.0816200971603394
Iteration 6481:
Training Loss: 3.8641624450683594
Reconstruction Loss: -1.0988281965255737
Iteration 6501:
Training Loss: 3.8380446434020996
Reconstruction Loss: -1.0604287385940552
Iteration 6521:
Training Loss: 3.6931097507476807
Reconstruction Loss: -1.063114881515503
Iteration 6541:
Training Loss: 3.9845588207244873
Reconstruction Loss: -1.0576286315917969
Iteration 6561:
Training Loss: 3.794123649597168
Reconstruction Loss: -1.0529475212097168
Iteration 6581:
Training Loss: 3.679427146911621
Reconstruction Loss: -1.0485793352127075
Iteration 6601:
Training Loss: 3.589573860168457
Reconstruction Loss: -1.0591163635253906
Iteration 6621:
Training Loss: 3.466071605682373
Reconstruction Loss: -1.0682287216186523
Iteration 6641:
Training Loss: 3.519145965576172
Reconstruction Loss: -1.0599799156188965
Iteration 6661:
Training Loss: 3.8069114685058594
Reconstruction Loss: -1.0366833209991455
Iteration 6681:
Training Loss: 3.836714267730713
Reconstruction Loss: -1.0540872812271118
Iteration 6701:
Training Loss: 3.425333023071289
Reconstruction Loss: -1.0486936569213867
Iteration 6721:
Training Loss: 3.6573445796966553
Reconstruction Loss: -1.06187903881073
Iteration 6741:
Training Loss: 3.7492334842681885
Reconstruction Loss: -1.0484062433242798
Iteration 6761:
Training Loss: 3.77569842338562
Reconstruction Loss: -1.0554707050323486
Iteration 6781:
Training Loss: 3.7671313285827637
Reconstruction Loss: -1.024981141090393
Iteration 6801:
Training Loss: 3.7854843139648438
Reconstruction Loss: -1.0285367965698242
Iteration 6821:
Training Loss: 3.6120879650115967
Reconstruction Loss: -1.0617002248764038
Iteration 6841:
Training Loss: 3.629387140274048
Reconstruction Loss: -1.089288592338562
Iteration 6861:
Training Loss: 3.3454270362854004
Reconstruction Loss: -1.0482487678527832
Iteration 6881:
Training Loss: 3.6785781383514404
Reconstruction Loss: -1.0650444030761719
Iteration 6901:
Training Loss: 3.4142026901245117
Reconstruction Loss: -1.0614672899246216
Iteration 6921:
Training Loss: 3.5844624042510986
Reconstruction Loss: -1.071221947669983
Iteration 6941:
Training Loss: 3.642390727996826
Reconstruction Loss: -1.0347729921340942
Iteration 6961:
Training Loss: 3.636255979537964
Reconstruction Loss: -1.0262258052825928
Iteration 6981:
Training Loss: 3.9138567447662354
Reconstruction Loss: -1.028350591659546
Iteration 7001:
Training Loss: 3.412487268447876
Reconstruction Loss: -1.0605041980743408
Iteration 7021:
Training Loss: 3.532599925994873
Reconstruction Loss: -1.0708831548690796
Iteration 7041:
Training Loss: 3.6308040618896484
Reconstruction Loss: -1.059150218963623
Iteration 7061:
Training Loss: 3.4639012813568115
Reconstruction Loss: -1.0667004585266113
Iteration 7081:
Training Loss: 3.686617851257324
Reconstruction Loss: -1.0471761226654053
Iteration 7101:
Training Loss: 3.654431104660034
Reconstruction Loss: -1.066349983215332
Iteration 7121:
Training Loss: 3.6283118724823
Reconstruction Loss: -1.0385247468948364
Iteration 7141:
Training Loss: 3.5779571533203125
Reconstruction Loss: -1.0833059549331665
Iteration 7161:
Training Loss: 3.5157034397125244
Reconstruction Loss: -1.090048909187317
Iteration 7181:
Training Loss: 3.2784981727600098
Reconstruction Loss: -1.4056442975997925
Iteration 7201:
Training Loss: 3.0239932537078857
Reconstruction Loss: -1.612916111946106
Iteration 7221:
Training Loss: 3.2558753490448
Reconstruction Loss: -1.6846389770507812
Iteration 7241:
Training Loss: 2.6170945167541504
Reconstruction Loss: -1.669448733329773
Iteration 7261:
Training Loss: 2.95242977142334
Reconstruction Loss: -1.692094087600708
Iteration 7281:
Training Loss: 2.6436920166015625
Reconstruction Loss: -1.6864253282546997
Iteration 7301:
Training Loss: 2.858473062515259
Reconstruction Loss: -1.7161383628845215
Iteration 7321:
Training Loss: 3.3296470642089844
Reconstruction Loss: -1.6494934558868408
Iteration 7341:
Training Loss: 2.7444941997528076
Reconstruction Loss: -1.7049561738967896
Iteration 7361:
Training Loss: 2.9314799308776855
Reconstruction Loss: -1.6925214529037476
Iteration 7381:
Training Loss: 3.0430657863616943
Reconstruction Loss: -1.698822021484375
Iteration 7401:
Training Loss: 3.030200481414795
Reconstruction Loss: -1.6946191787719727
Iteration 7421:
Training Loss: 2.986842393875122
Reconstruction Loss: -1.6824069023132324
Iteration 7441:
Training Loss: 2.9727730751037598
Reconstruction Loss: -1.6954548358917236
Iteration 7461:
Training Loss: 2.939918041229248
Reconstruction Loss: -1.690190315246582
Iteration 7481:
Training Loss: 3.0143637657165527
Reconstruction Loss: -1.6653246879577637
Iteration 7501:
Training Loss: 2.823774576187134
Reconstruction Loss: -1.699349045753479
Iteration 7521:
Training Loss: 2.8898208141326904
Reconstruction Loss: -1.6736266613006592
Iteration 7541:
Training Loss: 3.108870267868042
Reconstruction Loss: -1.6802972555160522
Iteration 7561:
Training Loss: 3.2691452503204346
Reconstruction Loss: -1.7112038135528564
Iteration 7581:
Training Loss: 3.1945550441741943
Reconstruction Loss: -1.6586053371429443
Iteration 7601:
Training Loss: 3.0912575721740723
Reconstruction Loss: -1.6899453401565552
Iteration 7621:
Training Loss: 2.8288846015930176
Reconstruction Loss: -1.6799767017364502
Iteration 7641:
Training Loss: 2.796332597732544
Reconstruction Loss: -1.6835217475891113
Iteration 7661:
Training Loss: 3.0357565879821777
Reconstruction Loss: -1.6775671243667603
Iteration 7681:
Training Loss: 2.788661241531372
Reconstruction Loss: -1.7000782489776611
Iteration 7701:
Training Loss: 2.782743215560913
Reconstruction Loss: -1.6691398620605469
Iteration 7721:
Training Loss: 3.1879096031188965
Reconstruction Loss: -1.6732457876205444
Iteration 7741:
Training Loss: 3.189608097076416
Reconstruction Loss: -1.6641631126403809
Iteration 7761:
Training Loss: 2.900095224380493
Reconstruction Loss: -1.6543705463409424
Iteration 7781:
Training Loss: 2.9515652656555176
Reconstruction Loss: -1.6713541746139526
Iteration 7801:
Training Loss: 2.8951256275177
Reconstruction Loss: -1.6740946769714355
Iteration 7821:
Training Loss: 2.7287425994873047
Reconstruction Loss: -1.6824407577514648
Iteration 7841:
Training Loss: 2.993817090988159
Reconstruction Loss: -1.6619839668273926
Iteration 7861:
Training Loss: 2.919384241104126
Reconstruction Loss: -1.6796191930770874
Iteration 7881:
Training Loss: 3.032681941986084
Reconstruction Loss: -1.7062772512435913
Iteration 7901:
Training Loss: 2.880044460296631
Reconstruction Loss: -1.6916545629501343
Iteration 7921:
Training Loss: 3.117658853530884
Reconstruction Loss: -1.6582218408584595
Iteration 7941:
Training Loss: 3.2723326683044434
Reconstruction Loss: -1.691416621208191
Iteration 7961:
Training Loss: 3.198774576187134
Reconstruction Loss: -1.6948413848876953
Iteration 7981:
Training Loss: 3.042701482772827
Reconstruction Loss: -1.7190532684326172
Iteration 8001:
Training Loss: 3.2974352836608887
Reconstruction Loss: -1.691422939300537
Iteration 8021:
Training Loss: 2.7915849685668945
Reconstruction Loss: -1.6454843282699585
Iteration 8041:
Training Loss: 3.13608717918396
Reconstruction Loss: -1.699472427368164
Iteration 8061:
Training Loss: 2.9071619510650635
Reconstruction Loss: -1.683843970298767
Iteration 8081:
Training Loss: 3.080887794494629
Reconstruction Loss: -1.687434196472168
Iteration 8101:
Training Loss: 2.739344596862793
Reconstruction Loss: -1.666966438293457
Iteration 8121:
Training Loss: 3.0362138748168945
Reconstruction Loss: -1.6965954303741455
Iteration 8141:
Training Loss: 3.1059532165527344
Reconstruction Loss: -1.7184089422225952
Iteration 8161:
Training Loss: 3.0951008796691895
Reconstruction Loss: -1.6763832569122314
Iteration 8181:
Training Loss: 2.9154343605041504
Reconstruction Loss: -1.6862287521362305
Iteration 8201:
Training Loss: 2.9631989002227783
Reconstruction Loss: -1.663900375366211
Iteration 8221:
Training Loss: 3.2604174613952637
Reconstruction Loss: -1.664602279663086
Iteration 8241:
Training Loss: 2.9812235832214355
Reconstruction Loss: -1.6964764595031738
Iteration 8261:
Training Loss: 3.148732900619507
Reconstruction Loss: -1.6670936346054077
Iteration 8281:
Training Loss: 2.916332483291626
Reconstruction Loss: -1.6920182704925537
Iteration 8301:
Training Loss: 3.2343146800994873
Reconstruction Loss: -1.672446608543396
Iteration 8321:
Training Loss: 2.9486522674560547
Reconstruction Loss: -1.6645643711090088
Iteration 8341:
Training Loss: 3.1831231117248535
Reconstruction Loss: -1.6554135084152222
Iteration 8361:
Training Loss: 2.9880247116088867
Reconstruction Loss: -1.684614896774292
Iteration 8381:
Training Loss: 2.8774187564849854
Reconstruction Loss: -1.6811461448669434
Iteration 8401:
Training Loss: 2.830320358276367
Reconstruction Loss: -1.6908471584320068
Iteration 8421:
Training Loss: 3.154343366622925
Reconstruction Loss: -1.6818873882293701
Iteration 8441:
Training Loss: 3.1326937675476074
Reconstruction Loss: -1.6817216873168945
Iteration 8461:
Training Loss: 2.6278162002563477
Reconstruction Loss: -1.6913800239562988
Iteration 8481:
Training Loss: 2.811065673828125
Reconstruction Loss: -1.6901582479476929
Iteration 8501:
Training Loss: 2.8297688961029053
Reconstruction Loss: -1.6921769380569458
Iteration 8521:
Training Loss: 2.7678046226501465
Reconstruction Loss: -1.651315689086914
Iteration 8541:
Training Loss: 2.904728412628174
Reconstruction Loss: -1.6911180019378662
Iteration 8561:
Training Loss: 3.018054723739624
Reconstruction Loss: -1.6534446477890015
Iteration 8581:
Training Loss: 3.0026261806488037
Reconstruction Loss: -1.6474335193634033
Iteration 8601:
Training Loss: 3.2829647064208984
Reconstruction Loss: -1.6797161102294922
Iteration 8621:
Training Loss: 2.661367893218994
Reconstruction Loss: -1.6892573833465576
Iteration 8641:
Training Loss: 3.221245527267456
Reconstruction Loss: -1.6817474365234375
Iteration 8661:
Training Loss: 3.1913352012634277
Reconstruction Loss: -1.6976869106292725
Iteration 8681:
Training Loss: 3.141298532485962
Reconstruction Loss: -1.688889503479004
Iteration 8701:
Training Loss: 3.1913743019104004
Reconstruction Loss: -1.6739082336425781
Iteration 8721:
Training Loss: 2.8568363189697266
Reconstruction Loss: -1.6844134330749512
Iteration 8741:
Training Loss: 3.4296200275421143
Reconstruction Loss: -1.7012646198272705
Iteration 8761:
Training Loss: 3.010490894317627
Reconstruction Loss: -1.6852229833602905
Iteration 8781:
Training Loss: 2.8928654193878174
Reconstruction Loss: -1.6840425729751587
Iteration 8801:
Training Loss: 3.2396438121795654
Reconstruction Loss: -1.6936837434768677
Iteration 8821:
Training Loss: 3.066401481628418
Reconstruction Loss: -1.6935272216796875
Iteration 8841:
Training Loss: 2.8269548416137695
Reconstruction Loss: -1.6977945566177368
Iteration 8861:
Training Loss: 2.7084152698516846
Reconstruction Loss: -1.7041640281677246
Iteration 8881:
Training Loss: 2.9225761890411377
Reconstruction Loss: -1.6887189149856567
Iteration 8901:
Training Loss: 3.045938491821289
Reconstruction Loss: -1.6895182132720947
Iteration 8921:
Training Loss: 2.739778995513916
Reconstruction Loss: -1.6574668884277344
Iteration 8941:
Training Loss: 2.7113397121429443
Reconstruction Loss: -1.6896193027496338
Iteration 8961:
Training Loss: 2.8392176628112793
Reconstruction Loss: -1.699043869972229
Iteration 8981:
Training Loss: 2.9727892875671387
Reconstruction Loss: -1.6812055110931396
Iteration 9001:
Training Loss: 2.993696928024292
Reconstruction Loss: -1.673508882522583
Iteration 9021:
Training Loss: 3.157095432281494
Reconstruction Loss: -1.7141480445861816
Iteration 9041:
Training Loss: 2.8843650817871094
Reconstruction Loss: -1.6727871894836426
Iteration 9061:
Training Loss: 2.7306392192840576
Reconstruction Loss: -1.6820560693740845
Iteration 9081:
Training Loss: 2.8760006427764893
Reconstruction Loss: -1.6829379796981812
Iteration 9101:
Training Loss: 3.2109150886535645
Reconstruction Loss: -1.6879775524139404
Iteration 9121:
Training Loss: 3.085587501525879
Reconstruction Loss: -1.6847621202468872
Iteration 9141:
Training Loss: 2.8033993244171143
Reconstruction Loss: -1.6856733560562134
Iteration 9161:
Training Loss: 2.840297222137451
Reconstruction Loss: -1.699246883392334
Iteration 9181:
Training Loss: 3.2648885250091553
Reconstruction Loss: -1.6589932441711426
Iteration 9201:
Training Loss: 2.7740213871002197
Reconstruction Loss: -1.6623376607894897
Iteration 9221:
Training Loss: 2.788508653640747
Reconstruction Loss: -1.6552813053131104
Iteration 9241:
Training Loss: 3.2155542373657227
Reconstruction Loss: -1.6688820123672485
Iteration 9261:
Training Loss: 2.8232946395874023
Reconstruction Loss: -1.651956558227539
Iteration 9281:
Training Loss: 2.380476236343384
Reconstruction Loss: -1.6998478174209595
Iteration 9301:
Training Loss: 2.8600454330444336
Reconstruction Loss: -1.6890532970428467
Iteration 9321:
Training Loss: 3.0380635261535645
Reconstruction Loss: -1.688159465789795
Iteration 9341:
Training Loss: 3.0445501804351807
Reconstruction Loss: -1.7007358074188232
Iteration 9361:
Training Loss: 3.0380847454071045
Reconstruction Loss: -1.6735241413116455
Iteration 9381:
Training Loss: 2.978114604949951
Reconstruction Loss: -1.663336157798767
Iteration 9401:
Training Loss: 3.0459988117218018
Reconstruction Loss: -1.6875556707382202
Iteration 9421:
Training Loss: 3.239438533782959
Reconstruction Loss: -1.635603666305542
Iteration 9441:
Training Loss: 3.0935873985290527
Reconstruction Loss: -1.6792608499526978
Iteration 9461:
Training Loss: 2.7474405765533447
Reconstruction Loss: -1.6919161081314087
Iteration 9481:
Training Loss: 3.146172285079956
Reconstruction Loss: -1.6724183559417725
Iteration 9501:
Training Loss: 3.0211575031280518
Reconstruction Loss: -1.6982381343841553
Iteration 9521:
Training Loss: 3.146003484725952
Reconstruction Loss: -1.6766858100891113
Iteration 9541:
Training Loss: 3.167142868041992
Reconstruction Loss: -1.6584690809249878
Iteration 9561:
Training Loss: 3.1776039600372314
Reconstruction Loss: -1.6842265129089355
Iteration 9581:
Training Loss: 3.2379982471466064
Reconstruction Loss: -1.6941149234771729
Iteration 9601:
Training Loss: 2.9951276779174805
Reconstruction Loss: -1.661238670349121
Iteration 9621:
Training Loss: 2.7417516708374023
Reconstruction Loss: -1.6818510293960571
Iteration 9641:
Training Loss: 2.9763691425323486
Reconstruction Loss: -1.7060942649841309
Iteration 9661:
Training Loss: 3.016164541244507
Reconstruction Loss: -1.6598522663116455
Iteration 9681:
Training Loss: 3.2324554920196533
Reconstruction Loss: -1.698011040687561
Iteration 9701:
Training Loss: 2.7554144859313965
Reconstruction Loss: -1.6874712705612183
Iteration 9721:
Training Loss: 2.5481057167053223
Reconstruction Loss: -1.6681666374206543
Iteration 9741:
Training Loss: 2.78250789642334
Reconstruction Loss: -1.6728906631469727
Iteration 9761:
Training Loss: 3.0708210468292236
Reconstruction Loss: -1.6747500896453857
Iteration 9781:
Training Loss: 3.1384894847869873
Reconstruction Loss: -1.6449131965637207
Iteration 9801:
Training Loss: 3.0544075965881348
Reconstruction Loss: -1.6729329824447632
Iteration 9821:
Training Loss: 3.209921360015869
Reconstruction Loss: -1.6357883214950562
Iteration 9841:
Training Loss: 3.171374559402466
Reconstruction Loss: -1.7036685943603516
Iteration 9861:
Training Loss: 3.1343271732330322
Reconstruction Loss: -1.6756675243377686
Iteration 9881:
Training Loss: 2.6390464305877686
Reconstruction Loss: -1.6730750799179077
Iteration 9901:
Training Loss: 2.9372057914733887
Reconstruction Loss: -1.710182785987854
Iteration 9921:
Training Loss: 3.058450937271118
Reconstruction Loss: -1.6850799322128296
Iteration 9941:
Training Loss: 2.8618061542510986
Reconstruction Loss: -1.6867880821228027
Iteration 9961:
Training Loss: 2.850796699523926
Reconstruction Loss: -1.6715271472930908
Iteration 9981:
Training Loss: 3.124115228652954
Reconstruction Loss: -1.6642361879348755
