5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 101:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 201:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 301:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 401:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 501:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 601:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 701:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 801:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 901:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 1001:
Training Loss: 5.612546920776367
Reconstruction Loss: -0.3344772458076477
Iteration 1101:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 1201:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 1301:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344772458076477
Iteration 1401:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 1501:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 1601:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 1701:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 1801:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 1901:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 2001:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 2101:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 2201:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 2301:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.33447757363319397
Iteration 2401:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 2501:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 2601:
Training Loss: 5.612546443939209
Reconstruction Loss: -0.3344774842262268
Iteration 2701:
Training Loss: 5.612545967102051
Reconstruction Loss: -0.33447757363319397
Iteration 2801:
Training Loss: 5.612545967102051
Reconstruction Loss: -0.33447757363319397
Iteration 2901:
Training Loss: 5.612545967102051
Reconstruction Loss: -0.33447757363319397
Iteration 3001:
Training Loss: 5.612545967102051
Reconstruction Loss: -0.33447757363319397
Iteration 3101:
Training Loss: 5.612545967102051
Reconstruction Loss: -0.33447757363319397
Iteration 3201:
Training Loss: 5.612545967102051
Reconstruction Loss: -0.33447757363319397
Iteration 3301:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447757363319397
Iteration 3401:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447757363319397
Iteration 3501:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 3601:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 3701:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 3801:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 3901:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 4001:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 4101:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 4201:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 4301:
Training Loss: 5.612545490264893
Reconstruction Loss: -0.33447766304016113
Iteration 4401:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.3344777524471283
Iteration 4501:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.3344777524471283
Iteration 4601:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.3344777524471283
Iteration 4701:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.33447790145874023
Iteration 4801:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.33447790145874023
Iteration 4901:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.33447790145874023
Iteration 5001:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.33447790145874023
Iteration 5101:
Training Loss: 5.612545013427734
Reconstruction Loss: -0.33447790145874023
Iteration 5201:
Training Loss: 5.612544536590576
Reconstruction Loss: -0.33447790145874023
Iteration 5301:
Training Loss: 5.612544536590576
Reconstruction Loss: -0.33447790145874023
Iteration 5401:
Training Loss: 5.612544536590576
Reconstruction Loss: -0.3344779908657074
Iteration 5501:
Training Loss: 5.612544536590576
Reconstruction Loss: -0.3344779908657074
Iteration 5601:
Training Loss: 5.612544536590576
Reconstruction Loss: -0.33447808027267456
Iteration 5701:
Training Loss: 5.612544536590576
Reconstruction Loss: -0.33447808027267456
Iteration 5801:
Training Loss: 5.612544536590576
Reconstruction Loss: -0.3344782590866089
Iteration 5901:
Training Loss: 5.612544059753418
Reconstruction Loss: -0.3344782590866089
Iteration 6001:
Training Loss: 5.612544059753418
Reconstruction Loss: -0.3344782590866089
Iteration 6101:
Training Loss: 5.612544059753418
Reconstruction Loss: -0.33447831869125366
Iteration 6201:
Training Loss: 5.612544059753418
Reconstruction Loss: -0.33447831869125366
Iteration 6301:
Training Loss: 5.61254358291626
Reconstruction Loss: -0.33447831869125366
Iteration 6401:
Training Loss: 5.61254358291626
Reconstruction Loss: -0.33447831869125366
Iteration 6501:
Training Loss: 5.61254358291626
Reconstruction Loss: -0.33447831869125366
Iteration 6601:
Training Loss: 5.61254358291626
Reconstruction Loss: -0.3344784080982208
Iteration 6701:
Training Loss: 5.612543106079102
Reconstruction Loss: -0.3344784080982208
Iteration 6801:
Training Loss: 5.612543106079102
Reconstruction Loss: -0.334478497505188
Iteration 6901:
Training Loss: 5.612543106079102
Reconstruction Loss: -0.334478497505188
Iteration 7001:
Training Loss: 5.612543106079102
Reconstruction Loss: -0.334478497505188
Iteration 7101:
Training Loss: 5.612542629241943
Reconstruction Loss: -0.3344786763191223
Iteration 7201:
Training Loss: 5.612542629241943
Reconstruction Loss: -0.3344787359237671
Iteration 7301:
Training Loss: 5.612542629241943
Reconstruction Loss: -0.33447882533073425
Iteration 7401:
Training Loss: 5.612542629241943
Reconstruction Loss: -0.3344790041446686
Iteration 7501:
Training Loss: 5.612542152404785
Reconstruction Loss: -0.3344790041446686
Iteration 7601:
Training Loss: 5.612542152404785
Reconstruction Loss: -0.33447906374931335
Iteration 7701:
Training Loss: 5.612541675567627
Reconstruction Loss: -0.33447906374931335
Iteration 7801:
Training Loss: 5.612541675567627
Reconstruction Loss: -0.3344791531562805
Iteration 7901:
Training Loss: 5.612541198730469
Reconstruction Loss: -0.3344792425632477
Iteration 8001:
Training Loss: 5.612541198730469
Reconstruction Loss: -0.334479421377182
Iteration 8101:
Training Loss: 5.6125407218933105
Reconstruction Loss: -0.3344794809818268
Iteration 8201:
Training Loss: 5.6125407218933105
Reconstruction Loss: -0.3344794809818268
Iteration 8301:
Training Loss: 5.612540245056152
Reconstruction Loss: -0.3344796597957611
Iteration 8401:
Training Loss: 5.612539768218994
Reconstruction Loss: -0.33447983860969543
Iteration 8501:
Training Loss: 5.612539768218994
Reconstruction Loss: -0.3344799876213074
Iteration 8601:
Training Loss: 5.612539291381836
Reconstruction Loss: -0.3344801664352417
Iteration 8701:
Training Loss: 5.612538814544678
Reconstruction Loss: -0.3344801664352417
Iteration 8801:
Training Loss: 5.6125383377075195
Reconstruction Loss: -0.3344804048538208
Iteration 8901:
Training Loss: 5.612537860870361
Reconstruction Loss: -0.3344805836677551
Iteration 9001:
Training Loss: 5.612537384033203
Reconstruction Loss: -0.33448073267936707
Iteration 9101:
Training Loss: 5.612537384033203
Reconstruction Loss: -0.3344810903072357
Iteration 9201:
Training Loss: 5.612536430358887
Reconstruction Loss: -0.3344811499118805
Iteration 9301:
Training Loss: 5.6125359535217285
Reconstruction Loss: -0.33448150753974915
Iteration 9401:
Training Loss: 5.612534999847412
Reconstruction Loss: -0.3344818353652954
Iteration 9501:
Training Loss: 5.612534523010254
Reconstruction Loss: -0.3344821631908417
Iteration 9601:
Training Loss: 5.6125335693359375
Reconstruction Loss: -0.3344823122024536
Iteration 9701:
Training Loss: 5.612532615661621
Reconstruction Loss: -0.33448290824890137
Iteration 9801:
Training Loss: 5.612532138824463
Reconstruction Loss: -0.3344833254814148
Iteration 9901:
Training Loss: 5.612530708312988
Reconstruction Loss: -0.3344838321208954
Iteration 10001:
Training Loss: 5.612529277801514
Reconstruction Loss: -0.3344844877719879
Iteration 10101:
Training Loss: 5.612527847290039
Reconstruction Loss: -0.3344849944114685
Iteration 10201:
Training Loss: 5.6125264167785645
Reconstruction Loss: -0.3344857394695282
Iteration 10301:
Training Loss: 5.612524509429932
Reconstruction Loss: -0.3344866633415222
Iteration 10401:
Training Loss: 5.612522125244141
Reconstruction Loss: -0.33448755741119385
Iteration 10501:
Training Loss: 5.61251974105835
Reconstruction Loss: -0.33448898792266846
Iteration 10601:
Training Loss: 5.6125168800354
Reconstruction Loss: -0.3344903290271759
Iteration 10701:
Training Loss: 5.612513065338135
Reconstruction Loss: -0.33449214696884155
Iteration 10801:
Training Loss: 5.612508773803711
Reconstruction Loss: -0.3344942331314087
Iteration 10901:
Training Loss: 5.6125030517578125
Reconstruction Loss: -0.33449697494506836
Iteration 11001:
Training Loss: 5.6124958992004395
Reconstruction Loss: -0.3345004916191101
Iteration 11101:
Training Loss: 5.612487316131592
Reconstruction Loss: -0.33450499176979065
Iteration 11201:
Training Loss: 5.612475395202637
Reconstruction Loss: -0.3345109820365906
Iteration 11301:
Training Loss: 5.612459182739258
Reconstruction Loss: -0.3345192074775696
Iteration 11401:
Training Loss: 5.612436294555664
Reconstruction Loss: -0.3345312178134918
Iteration 11501:
Training Loss: 5.612401008605957
Reconstruction Loss: -0.33454927802085876
Iteration 11601:
Training Loss: 5.612344741821289
Reconstruction Loss: -0.3345789313316345
Iteration 11701:
Training Loss: 5.612244129180908
Reconstruction Loss: -0.3346324861049652
Iteration 11801:
Training Loss: 5.612034797668457
Reconstruction Loss: -0.3347446024417877
Iteration 11901:
Training Loss: 5.611474514007568
Reconstruction Loss: -0.3350474536418915
Iteration 12001:
Training Loss: 5.608895778656006
Reconstruction Loss: -0.336448609828949
Iteration 12101:
Training Loss: 5.4940996170043945
Reconstruction Loss: -0.39404532313346863
Iteration 12201:
Training Loss: 5.133393287658691
Reconstruction Loss: -0.41937455534935
Iteration 12301:
Training Loss: 5.08915376663208
Reconstruction Loss: -0.4143657088279724
Iteration 12401:
Training Loss: 5.065665245056152
Reconstruction Loss: -0.4081554710865021
Iteration 12501:
Training Loss: 5.060667037963867
Reconstruction Loss: -0.40532875061035156
Iteration 12601:
Training Loss: 5.060105323791504
Reconstruction Loss: -0.40468984842300415
Iteration 12701:
Training Loss: 5.0600481033325195
Reconstruction Loss: -0.4044763743877411
Iteration 12801:
Training Loss: 5.060037612915039
Reconstruction Loss: -0.40437570214271545
Iteration 12901:
Training Loss: 5.060030937194824
Reconstruction Loss: -0.40433087944984436
Iteration 13001:
Training Loss: 5.060023307800293
Reconstruction Loss: -0.40431416034698486
Iteration 13101:
Training Loss: 5.060013771057129
Reconstruction Loss: -0.40431079268455505
Iteration 13201:
Training Loss: 5.060001850128174
Reconstruction Loss: -0.4043133854866028
Iteration 13301:
Training Loss: 5.0599870681762695
Reconstruction Loss: -0.40431952476501465
Iteration 13401:
Training Loss: 5.059967517852783
Reconstruction Loss: -0.4043288230895996
Iteration 13501:
Training Loss: 5.05994176864624
Reconstruction Loss: -0.4043413996696472
Iteration 13601:
Training Loss: 5.059906005859375
Reconstruction Loss: -0.4043591022491455
Iteration 13701:
Training Loss: 5.059854030609131
Reconstruction Loss: -0.4043847322463989
Iteration 13801:
Training Loss: 5.059776306152344
Reconstruction Loss: -0.4044235050678253
Iteration 13901:
Training Loss: 5.059649467468262
Reconstruction Loss: -0.40448635816574097
Iteration 14001:
Training Loss: 5.059422969818115
Reconstruction Loss: -0.40459883213043213
Iteration 14101:
Training Loss: 5.058957099914551
Reconstruction Loss: -0.4048303961753845
Iteration 14201:
Training Loss: 5.057740211486816
Reconstruction Loss: -0.40543484687805176
Iteration 14301:
Training Loss: 5.052563667297363
Reconstruction Loss: -0.4079815745353699
Iteration 14401:
Training Loss: 4.944840431213379
Reconstruction Loss: -0.45653146505355835
Iteration 14501:
Training Loss: 4.648741722106934
Reconstruction Loss: -0.5416300296783447
Iteration 14601:
Training Loss: 4.610729694366455
Reconstruction Loss: -0.5683283805847168
Iteration 14701:
Training Loss: 4.599854946136475
Reconstruction Loss: -0.581609308719635
Iteration 14801:
Training Loss: 4.596754550933838
Reconstruction Loss: -0.5857794284820557
Iteration 14901:
Training Loss: 4.59564733505249
Reconstruction Loss: -0.5862488746643066
