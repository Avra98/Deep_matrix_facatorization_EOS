5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.3186235427856445
Reconstruction Loss: -0.5484212636947632
Iteration 11:
Training Loss: 4.205770492553711
Reconstruction Loss: -0.8294455409049988
Iteration 21:
Training Loss: 3.1267588138580322
Reconstruction Loss: -1.3715466260910034
Iteration 31:
Training Loss: 2.5529682636260986
Reconstruction Loss: -1.7079631090164185
Iteration 41:
Training Loss: 1.9505826234817505
Reconstruction Loss: -2.1262166500091553
Iteration 51:
Training Loss: 1.1337175369262695
Reconstruction Loss: -2.5032899379730225
Iteration 61:
Training Loss: 0.6794555187225342
Reconstruction Loss: -2.8279526233673096
Iteration 71:
Training Loss: 0.12959839403629303
Reconstruction Loss: -3.1002566814422607
Iteration 81:
Training Loss: -0.16809596121311188
Reconstruction Loss: -3.3371503353118896
Iteration 91:
Training Loss: -0.3852296471595764
Reconstruction Loss: -3.5460546016693115
Iteration 101:
Training Loss: -0.4922647774219513
Reconstruction Loss: -3.725879669189453
Iteration 111:
Training Loss: -1.0334721803665161
Reconstruction Loss: -3.8828272819519043
Iteration 121:
Training Loss: -1.0859854221343994
Reconstruction Loss: -4.027017116546631
Iteration 131:
Training Loss: -1.4324253797531128
Reconstruction Loss: -4.15104866027832
Iteration 141:
Training Loss: -1.2361019849777222
Reconstruction Loss: -4.262976169586182
Iteration 151:
Training Loss: -1.706169605255127
Reconstruction Loss: -4.35922908782959
Iteration 161:
Training Loss: -1.4512642621994019
Reconstruction Loss: -4.4541850090026855
Iteration 171:
Training Loss: -1.6807445287704468
Reconstruction Loss: -4.533868789672852
Iteration 181:
Training Loss: -1.8140466213226318
Reconstruction Loss: -4.612467288970947
Iteration 191:
Training Loss: -1.8340859413146973
Reconstruction Loss: -4.678560256958008
Iteration 201:
Training Loss: -2.343087911605835
Reconstruction Loss: -4.745576858520508
Iteration 211:
Training Loss: -1.9874894618988037
Reconstruction Loss: -4.802004814147949
Iteration 221:
Training Loss: -2.3101251125335693
Reconstruction Loss: -4.858530521392822
Iteration 231:
Training Loss: -1.97897207736969
Reconstruction Loss: -4.910074710845947
Iteration 241:
Training Loss: -2.460148334503174
Reconstruction Loss: -4.960259914398193
Iteration 251:
Training Loss: -2.49147367477417
Reconstruction Loss: -5.004755973815918
Iteration 261:
Training Loss: -2.404425621032715
Reconstruction Loss: -5.045437335968018
Iteration 271:
Training Loss: -2.350306987762451
Reconstruction Loss: -5.087861061096191
Iteration 281:
Training Loss: -2.6275832653045654
Reconstruction Loss: -5.128766059875488
Iteration 291:
Training Loss: -2.3910088539123535
Reconstruction Loss: -5.164349555969238
Iteration 301:
Training Loss: -2.457000255584717
Reconstruction Loss: -5.20015811920166
Iteration 311:
Training Loss: -2.6774096488952637
Reconstruction Loss: -5.238656044006348
Iteration 321:
Training Loss: -2.753725290298462
Reconstruction Loss: -5.267737865447998
Iteration 331:
Training Loss: -2.6287686824798584
Reconstruction Loss: -5.300496578216553
Iteration 341:
Training Loss: -2.9550492763519287
Reconstruction Loss: -5.327857971191406
Iteration 351:
Training Loss: -3.022977113723755
Reconstruction Loss: -5.3600754737854
Iteration 361:
Training Loss: -2.893982410430908
Reconstruction Loss: -5.38671350479126
Iteration 371:
Training Loss: -3.040479898452759
Reconstruction Loss: -5.412889003753662
Iteration 381:
Training Loss: -2.777866840362549
Reconstruction Loss: -5.4403886795043945
Iteration 391:
Training Loss: -3.0257182121276855
Reconstruction Loss: -5.470870494842529
Iteration 401:
Training Loss: -2.9613189697265625
Reconstruction Loss: -5.493078708648682
Iteration 411:
Training Loss: -3.3094139099121094
Reconstruction Loss: -5.517503261566162
Iteration 421:
Training Loss: -3.2860352993011475
Reconstruction Loss: -5.541195392608643
Iteration 431:
Training Loss: -3.3409948348999023
Reconstruction Loss: -5.563263893127441
Iteration 441:
Training Loss: -3.2606911659240723
Reconstruction Loss: -5.584870338439941
Iteration 451:
Training Loss: -3.512101173400879
Reconstruction Loss: -5.607431888580322
Iteration 461:
Training Loss: -3.397001266479492
Reconstruction Loss: -5.627978801727295
Iteration 471:
Training Loss: -3.2186896800994873
Reconstruction Loss: -5.648964881896973
Iteration 481:
Training Loss: -3.74322772026062
Reconstruction Loss: -5.670984745025635
Iteration 491:
Training Loss: -3.6102395057678223
Reconstruction Loss: -5.688060760498047
Iteration 501:
Training Loss: -3.130042552947998
Reconstruction Loss: -5.707704544067383
Iteration 511:
Training Loss: -3.343529462814331
Reconstruction Loss: -5.7284650802612305
Iteration 521:
Training Loss: -3.0092523097991943
Reconstruction Loss: -5.746682643890381
Iteration 531:
Training Loss: -3.720259189605713
Reconstruction Loss: -5.765185356140137
Iteration 541:
Training Loss: -3.429366111755371
Reconstruction Loss: -5.783136367797852
Iteration 551:
Training Loss: -3.5106544494628906
Reconstruction Loss: -5.798213481903076
Iteration 561:
Training Loss: -3.1812453269958496
Reconstruction Loss: -5.813702583312988
Iteration 571:
Training Loss: -3.9008846282958984
Reconstruction Loss: -5.83317756652832
Iteration 581:
Training Loss: -3.7947611808776855
Reconstruction Loss: -5.848623275756836
Iteration 591:
Training Loss: -3.437206506729126
Reconstruction Loss: -5.864963531494141
Iteration 601:
Training Loss: -3.604623556137085
Reconstruction Loss: -5.87959623336792
Iteration 611:
Training Loss: -3.526675224304199
Reconstruction Loss: -5.896387100219727
Iteration 621:
Training Loss: -3.979682683944702
Reconstruction Loss: -5.910309791564941
Iteration 631:
Training Loss: -4.548475742340088
Reconstruction Loss: -5.924133777618408
Iteration 641:
Training Loss: -3.608433961868286
Reconstruction Loss: -5.939661979675293
Iteration 651:
Training Loss: -4.1147027015686035
Reconstruction Loss: -5.952088356018066
Iteration 661:
Training Loss: -3.5661418437957764
Reconstruction Loss: -5.9649553298950195
Iteration 671:
Training Loss: -3.933098793029785
Reconstruction Loss: -5.982520580291748
Iteration 681:
Training Loss: -4.267369270324707
Reconstruction Loss: -5.9947919845581055
Iteration 691:
Training Loss: -3.4343008995056152
Reconstruction Loss: -6.0050883293151855
Iteration 701:
Training Loss: -3.767859697341919
Reconstruction Loss: -6.020106792449951
Iteration 711:
Training Loss: -4.051278114318848
Reconstruction Loss: -6.033314228057861
Iteration 721:
Training Loss: -4.174153804779053
Reconstruction Loss: -6.046647071838379
Iteration 731:
Training Loss: -3.7668697834014893
Reconstruction Loss: -6.055636405944824
Iteration 741:
Training Loss: -3.928969621658325
Reconstruction Loss: -6.069954872131348
Iteration 751:
Training Loss: -4.086936950683594
Reconstruction Loss: -6.0790605545043945
Iteration 761:
Training Loss: -3.840090751647949
Reconstruction Loss: -6.093110084533691
Iteration 771:
Training Loss: -3.741882801055908
Reconstruction Loss: -6.105093955993652
Iteration 781:
Training Loss: -4.181983947753906
Reconstruction Loss: -6.1163177490234375
Iteration 791:
Training Loss: -3.951477527618408
Reconstruction Loss: -6.125706672668457
Iteration 801:
Training Loss: -4.185299396514893
Reconstruction Loss: -6.139981746673584
Iteration 811:
Training Loss: -4.479500770568848
Reconstruction Loss: -6.1499552726745605
Iteration 821:
Training Loss: -4.2956342697143555
Reconstruction Loss: -6.16282320022583
Iteration 831:
Training Loss: -4.219198226928711
Reconstruction Loss: -6.172582149505615
Iteration 841:
Training Loss: -4.24237585067749
Reconstruction Loss: -6.182976245880127
Iteration 851:
Training Loss: -4.256446361541748
Reconstruction Loss: -6.191033840179443
Iteration 861:
Training Loss: -4.2728657722473145
Reconstruction Loss: -6.203518390655518
Iteration 871:
Training Loss: -4.1397857666015625
Reconstruction Loss: -6.2127156257629395
Iteration 881:
Training Loss: -3.9792444705963135
Reconstruction Loss: -6.227334976196289
Iteration 891:
Training Loss: -4.335238456726074
Reconstruction Loss: -6.229910850524902
Iteration 901:
Training Loss: -4.510456085205078
Reconstruction Loss: -6.2420878410339355
Iteration 911:
Training Loss: -4.27910041809082
Reconstruction Loss: -6.2522406578063965
Iteration 921:
Training Loss: -4.031708240509033
Reconstruction Loss: -6.260837078094482
Iteration 931:
Training Loss: -4.317660331726074
Reconstruction Loss: -6.271468162536621
Iteration 941:
Training Loss: -4.352265357971191
Reconstruction Loss: -6.280612945556641
Iteration 951:
Training Loss: -3.964521646499634
Reconstruction Loss: -6.2866668701171875
Iteration 961:
Training Loss: -4.339144229888916
Reconstruction Loss: -6.298901557922363
Iteration 971:
Training Loss: -4.608912467956543
Reconstruction Loss: -6.3072004318237305
Iteration 981:
Training Loss: -4.55155086517334
Reconstruction Loss: -6.318439483642578
Iteration 991:
Training Loss: -4.437614440917969
Reconstruction Loss: -6.324516773223877
Iteration 1001:
Training Loss: -4.373324871063232
Reconstruction Loss: -6.334554195404053
Iteration 1011:
Training Loss: -4.443472862243652
Reconstruction Loss: -6.342106342315674
Iteration 1021:
Training Loss: -4.059446334838867
Reconstruction Loss: -6.349788188934326
Iteration 1031:
Training Loss: -4.344285011291504
Reconstruction Loss: -6.357188701629639
Iteration 1041:
Training Loss: -4.122426986694336
Reconstruction Loss: -6.366374492645264
Iteration 1051:
Training Loss: -4.288351058959961
Reconstruction Loss: -6.3765130043029785
Iteration 1061:
Training Loss: -4.809608459472656
Reconstruction Loss: -6.38130521774292
Iteration 1071:
Training Loss: -4.08350133895874
Reconstruction Loss: -6.390590190887451
Iteration 1081:
Training Loss: -4.676599979400635
Reconstruction Loss: -6.400017738342285
Iteration 1091:
Training Loss: -4.072128772735596
Reconstruction Loss: -6.406737327575684
Iteration 1101:
Training Loss: -4.554082870483398
Reconstruction Loss: -6.4150190353393555
Iteration 1111:
Training Loss: -4.726592063903809
Reconstruction Loss: -6.420637130737305
Iteration 1121:
Training Loss: -4.610701084136963
Reconstruction Loss: -6.430297374725342
Iteration 1131:
Training Loss: -4.369594097137451
Reconstruction Loss: -6.4388885498046875
Iteration 1141:
Training Loss: -4.412008762359619
Reconstruction Loss: -6.4437408447265625
Iteration 1151:
Training Loss: -4.551922798156738
Reconstruction Loss: -6.4514312744140625
Iteration 1161:
Training Loss: -4.5441412925720215
Reconstruction Loss: -6.4597296714782715
Iteration 1171:
Training Loss: -4.721681118011475
Reconstruction Loss: -6.4672932624816895
Iteration 1181:
Training Loss: -4.40894079208374
Reconstruction Loss: -6.472175121307373
Iteration 1191:
Training Loss: -4.35269832611084
Reconstruction Loss: -6.478809833526611
Iteration 1201:
Training Loss: -4.3997602462768555
Reconstruction Loss: -6.486053466796875
Iteration 1211:
Training Loss: -4.490067958831787
Reconstruction Loss: -6.492093086242676
Iteration 1221:
Training Loss: -4.769197463989258
Reconstruction Loss: -6.501402854919434
Iteration 1231:
Training Loss: -4.939418315887451
Reconstruction Loss: -6.5073933601379395
Iteration 1241:
Training Loss: -5.347644329071045
Reconstruction Loss: -6.514354705810547
Iteration 1251:
Training Loss: -4.650157928466797
Reconstruction Loss: -6.52154016494751
Iteration 1261:
Training Loss: -4.656295299530029
Reconstruction Loss: -6.527853488922119
Iteration 1271:
Training Loss: -4.511104583740234
Reconstruction Loss: -6.533946514129639
Iteration 1281:
Training Loss: -4.75221586227417
Reconstruction Loss: -6.540773868560791
Iteration 1291:
Training Loss: -5.030802249908447
Reconstruction Loss: -6.547773838043213
Iteration 1301:
Training Loss: -4.55636739730835
Reconstruction Loss: -6.552921772003174
Iteration 1311:
Training Loss: -5.272354602813721
Reconstruction Loss: -6.560075759887695
Iteration 1321:
Training Loss: -4.686854839324951
Reconstruction Loss: -6.566671848297119
Iteration 1331:
Training Loss: -4.8567585945129395
Reconstruction Loss: -6.56893253326416
Iteration 1341:
Training Loss: -4.941709518432617
Reconstruction Loss: -6.578664779663086
Iteration 1351:
Training Loss: -4.985026836395264
Reconstruction Loss: -6.583621978759766
Iteration 1361:
Training Loss: -4.6661176681518555
Reconstruction Loss: -6.590206623077393
Iteration 1371:
Training Loss: -4.822308540344238
Reconstruction Loss: -6.597208023071289
Iteration 1381:
Training Loss: -4.948535919189453
Reconstruction Loss: -6.60053014755249
Iteration 1391:
Training Loss: -4.794971942901611
Reconstruction Loss: -6.608225345611572
Iteration 1401:
Training Loss: -4.870633125305176
Reconstruction Loss: -6.615057945251465
Iteration 1411:
Training Loss: -4.579903602600098
Reconstruction Loss: -6.617722034454346
Iteration 1421:
Training Loss: -4.902220249176025
Reconstruction Loss: -6.627236366271973
Iteration 1431:
Training Loss: -4.652151107788086
Reconstruction Loss: -6.628701686859131
Iteration 1441:
Training Loss: -4.92652702331543
Reconstruction Loss: -6.635268211364746
Iteration 1451:
Training Loss: -4.993978023529053
Reconstruction Loss: -6.640659332275391
Iteration 1461:
Training Loss: -4.728359222412109
Reconstruction Loss: -6.648321628570557
Iteration 1471:
Training Loss: -4.951613903045654
Reconstruction Loss: -6.654204368591309
Iteration 1481:
Training Loss: -5.431541919708252
Reconstruction Loss: -6.658483028411865
Iteration 1491:
Training Loss: -5.115579605102539
Reconstruction Loss: -6.661967754364014
