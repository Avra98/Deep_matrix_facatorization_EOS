5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.616255760192871
Reconstruction Loss: -0.4407932758331299
Iteration 101:
Training Loss: 5.599193572998047
Reconstruction Loss: -0.4482303559780121
Iteration 201:
Training Loss: 5.075366497039795
Reconstruction Loss: -0.4674794673919678
Iteration 301:
Training Loss: 4.472832202911377
Reconstruction Loss: -0.723667323589325
Iteration 401:
Training Loss: 3.948183298110962
Reconstruction Loss: -1.000456690788269
Iteration 501:
Training Loss: 3.804455280303955
Reconstruction Loss: -1.0604557991027832
Iteration 601:
Training Loss: 3.0850374698638916
Reconstruction Loss: -1.5284165143966675
Iteration 701:
Training Loss: 2.6922085285186768
Reconstruction Loss: -1.762876033782959
Iteration 801:
Training Loss: 1.9352401494979858
Reconstruction Loss: -2.1932058334350586
Iteration 901:
Training Loss: 0.9372161030769348
Reconstruction Loss: -2.976938247680664
Iteration 1001:
Training Loss: -0.27800002694129944
Reconstruction Loss: -3.873192071914673
Iteration 1101:
Training Loss: -1.254913091659546
Reconstruction Loss: -4.645321846008301
Iteration 1201:
Training Loss: -2.0349504947662354
Reconstruction Loss: -5.3314619064331055
Iteration 1301:
Training Loss: -2.7315897941589355
Reconstruction Loss: -5.975890159606934
Iteration 1401:
Training Loss: -3.3835182189941406
Reconstruction Loss: -6.593324661254883
Iteration 1501:
Training Loss: -4.000465393066406
Reconstruction Loss: -7.187264919281006
Iteration 1601:
Training Loss: -4.580790042877197
Reconstruction Loss: -7.756551742553711
Iteration 1701:
Training Loss: -5.115968227386475
Reconstruction Loss: -8.29703426361084
Iteration 1801:
Training Loss: -5.592453479766846
Reconstruction Loss: -8.801878929138184
Iteration 1901:
Training Loss: -5.995064735412598
Reconstruction Loss: -9.261923789978027
Iteration 2001:
Training Loss: -6.313164234161377
Reconstruction Loss: -9.66698932647705
Iteration 2101:
Training Loss: -6.546638011932373
Reconstruction Loss: -10.008556365966797
Iteration 2201:
Training Loss: -6.706731796264648
Reconstruction Loss: -10.282914161682129
Iteration 2301:
Training Loss: -6.810966968536377
Reconstruction Loss: -10.493037223815918
Iteration 2401:
Training Loss: -6.876783847808838
Reconstruction Loss: -10.647666931152344
Iteration 2501:
Training Loss: -6.918013095855713
Reconstruction Loss: -10.758359909057617
Iteration 2601:
Training Loss: -6.944201946258545
Reconstruction Loss: -10.8364839553833
Iteration 2701:
Training Loss: -6.961391448974609
Reconstruction Loss: -10.89156436920166
Iteration 2801:
Training Loss: -6.973309516906738
Reconstruction Loss: -10.930758476257324
Iteration 2901:
Training Loss: -6.982161045074463
Reconstruction Loss: -10.959083557128906
Iteration 3001:
Training Loss: -6.989194393157959
Reconstruction Loss: -10.98004150390625
Iteration 3101:
Training Loss: -6.995217323303223
Reconstruction Loss: -10.99592113494873
Iteration 3201:
Training Loss: -7.000587463378906
Reconstruction Loss: -11.008272171020508
Iteration 3301:
Training Loss: -7.00562047958374
Reconstruction Loss: -11.018173217773438
Iteration 3401:
Training Loss: -7.010436058044434
Reconstruction Loss: -11.026338577270508
Iteration 3501:
Training Loss: -7.015114784240723
Reconstruction Loss: -11.033246040344238
Iteration 3601:
Training Loss: -7.019692420959473
Reconstruction Loss: -11.039260864257812
Iteration 3701:
Training Loss: -7.02421760559082
Reconstruction Loss: -11.04460620880127
Iteration 3801:
Training Loss: -7.028697490692139
Reconstruction Loss: -11.049430847167969
Iteration 3901:
Training Loss: -7.033154487609863
Reconstruction Loss: -11.053885459899902
Iteration 4001:
Training Loss: -7.037588119506836
Reconstruction Loss: -11.058058738708496
Iteration 4101:
Training Loss: -7.04197883605957
Reconstruction Loss: -11.062002182006836
Iteration 4201:
Training Loss: -7.046365261077881
Reconstruction Loss: -11.065779685974121
Iteration 4301:
Training Loss: -7.050746440887451
Reconstruction Loss: -11.069437980651855
Iteration 4401:
Training Loss: -7.055097579956055
Reconstruction Loss: -11.072999000549316
Iteration 4501:
Training Loss: -7.05943489074707
Reconstruction Loss: -11.076462745666504
Iteration 4601:
Training Loss: -7.063728332519531
Reconstruction Loss: -11.07985782623291
Iteration 4701:
Training Loss: -7.068027496337891
Reconstruction Loss: -11.083209037780762
Iteration 4801:
Training Loss: -7.072306156158447
Reconstruction Loss: -11.086516380310059
Iteration 4901:
Training Loss: -7.076564788818359
Reconstruction Loss: -11.089792251586914
Iteration 5001:
Training Loss: -7.080803394317627
Reconstruction Loss: -11.093036651611328
Iteration 5101:
Training Loss: -7.085054874420166
Reconstruction Loss: -11.096260070800781
Iteration 5201:
Training Loss: -7.08926248550415
Reconstruction Loss: -11.099459648132324
Iteration 5301:
Training Loss: -7.093432903289795
Reconstruction Loss: -11.102653503417969
Iteration 5401:
Training Loss: -7.0976033210754395
Reconstruction Loss: -11.10581111907959
Iteration 5501:
Training Loss: -7.101777076721191
Reconstruction Loss: -11.108963012695312
Iteration 5601:
Training Loss: -7.105926990509033
Reconstruction Loss: -11.112089157104492
Iteration 5701:
Training Loss: -7.110044956207275
Reconstruction Loss: -11.115181922912598
Iteration 5801:
Training Loss: -7.114177227020264
Reconstruction Loss: -11.118268013000488
Iteration 5901:
Training Loss: -7.118256092071533
Reconstruction Loss: -11.121328353881836
Iteration 6001:
Training Loss: -7.1223297119140625
Reconstruction Loss: -11.124371528625488
Iteration 6101:
Training Loss: -7.126397609710693
Reconstruction Loss: -11.12741470336914
Iteration 6201:
Training Loss: -7.130455493927002
Reconstruction Loss: -11.130439758300781
Iteration 6301:
Training Loss: -7.1345014572143555
Reconstruction Loss: -11.13346004486084
Iteration 6401:
Training Loss: -7.138530254364014
Reconstruction Loss: -11.136466026306152
Iteration 6501:
Training Loss: -7.142531871795654
Reconstruction Loss: -11.139471054077148
Iteration 6601:
Training Loss: -7.146515369415283
Reconstruction Loss: -11.142449378967285
Iteration 6701:
Training Loss: -7.150495529174805
Reconstruction Loss: -11.145418167114258
Iteration 6801:
Training Loss: -7.154449462890625
Reconstruction Loss: -11.148381233215332
Iteration 6901:
Training Loss: -7.158393383026123
Reconstruction Loss: -11.151322364807129
Iteration 7001:
Training Loss: -7.162337779998779
Reconstruction Loss: -11.154248237609863
Iteration 7101:
Training Loss: -7.166261196136475
Reconstruction Loss: -11.157164573669434
Iteration 7201:
Training Loss: -7.170151710510254
Reconstruction Loss: -11.160069465637207
Iteration 7301:
Training Loss: -7.174046516418457
Reconstruction Loss: -11.162946701049805
Iteration 7401:
Training Loss: -7.177946090698242
Reconstruction Loss: -11.165825843811035
Iteration 7501:
Training Loss: -7.181787967681885
Reconstruction Loss: -11.16869068145752
Iteration 7601:
Training Loss: -7.185640335083008
Reconstruction Loss: -11.171538352966309
Iteration 7701:
Training Loss: -7.18948221206665
Reconstruction Loss: -11.17437744140625
Iteration 7801:
Training Loss: -7.193327903747559
Reconstruction Loss: -11.177206993103027
Iteration 7901:
Training Loss: -7.197134971618652
Reconstruction Loss: -11.180021286010742
Iteration 8001:
Training Loss: -7.200915336608887
Reconstruction Loss: -11.18282699584961
Iteration 8101:
Training Loss: -7.204720973968506
Reconstruction Loss: -11.185624122619629
Iteration 8201:
Training Loss: -7.208491325378418
Reconstruction Loss: -11.188414573669434
Iteration 8301:
Training Loss: -7.212240219116211
Reconstruction Loss: -11.191187858581543
Iteration 8401:
Training Loss: -7.2159810066223145
Reconstruction Loss: -11.193947792053223
Iteration 8501:
Training Loss: -7.219721794128418
Reconstruction Loss: -11.196694374084473
Iteration 8601:
Training Loss: -7.223424434661865
Reconstruction Loss: -11.199426651000977
Iteration 8701:
Training Loss: -7.227137088775635
Reconstruction Loss: -11.202159881591797
Iteration 8801:
Training Loss: -7.230832576751709
Reconstruction Loss: -11.204877853393555
Iteration 8901:
Training Loss: -7.234495639801025
Reconstruction Loss: -11.20759105682373
Iteration 9001:
Training Loss: -7.238171100616455
Reconstruction Loss: -11.210301399230957
Iteration 9101:
Training Loss: -7.241847515106201
Reconstruction Loss: -11.212992668151855
Iteration 9201:
Training Loss: -7.245492935180664
Reconstruction Loss: -11.215681076049805
Iteration 9301:
Training Loss: -7.249111175537109
Reconstruction Loss: -11.218344688415527
Iteration 9401:
Training Loss: -7.252735137939453
Reconstruction Loss: -11.221007347106934
Iteration 9501:
Training Loss: -7.256351470947266
Reconstruction Loss: -11.223651885986328
Iteration 9601:
Training Loss: -7.259942054748535
Reconstruction Loss: -11.22629165649414
Iteration 9701:
Training Loss: -7.263521194458008
Reconstruction Loss: -11.228922843933105
Iteration 9801:
Training Loss: -7.267104148864746
Reconstruction Loss: -11.23155403137207
Iteration 9901:
Training Loss: -7.270661354064941
Reconstruction Loss: -11.234176635742188
Iteration 10001:
Training Loss: -7.274212837219238
Reconstruction Loss: -11.23679256439209
Iteration 10101:
Training Loss: -7.277759075164795
Reconstruction Loss: -11.239397048950195
Iteration 10201:
Training Loss: -7.281304836273193
Reconstruction Loss: -11.242005348205566
Iteration 10301:
Training Loss: -7.284825325012207
Reconstruction Loss: -11.244590759277344
Iteration 10401:
Training Loss: -7.288326263427734
Reconstruction Loss: -11.247166633605957
Iteration 10501:
Training Loss: -7.2918171882629395
Reconstruction Loss: -11.249728202819824
Iteration 10601:
Training Loss: -7.295305252075195
Reconstruction Loss: -11.252281188964844
Iteration 10701:
Training Loss: -7.2987775802612305
Reconstruction Loss: -11.2548189163208
Iteration 10801:
Training Loss: -7.302255153656006
Reconstruction Loss: -11.257359504699707
Iteration 10901:
Training Loss: -7.305686950683594
Reconstruction Loss: -11.259878158569336
Iteration 11001:
Training Loss: -7.309146881103516
Reconstruction Loss: -11.2623929977417
Iteration 11101:
Training Loss: -7.312572479248047
Reconstruction Loss: -11.264885902404785
Iteration 11201:
Training Loss: -7.315988540649414
Reconstruction Loss: -11.26738166809082
Iteration 11301:
Training Loss: -7.319385051727295
Reconstruction Loss: -11.269855499267578
Iteration 11401:
Training Loss: -7.322793483734131
Reconstruction Loss: -11.272332191467285
Iteration 11501:
Training Loss: -7.326172828674316
Reconstruction Loss: -11.274791717529297
Iteration 11601:
Training Loss: -7.32956600189209
Reconstruction Loss: -11.277243614196777
Iteration 11701:
Training Loss: -7.332924842834473
Reconstruction Loss: -11.279677391052246
Iteration 11801:
Training Loss: -7.336274147033691
Reconstruction Loss: -11.282116889953613
Iteration 11901:
Training Loss: -7.339618682861328
Reconstruction Loss: -11.284533500671387
Iteration 12001:
Training Loss: -7.3429412841796875
Reconstruction Loss: -11.286937713623047
Iteration 12101:
Training Loss: -7.34627103805542
Reconstruction Loss: -11.289347648620605
Iteration 12201:
Training Loss: -7.3496012687683105
Reconstruction Loss: -11.291742324829102
Iteration 12301:
Training Loss: -7.352920055389404
Reconstruction Loss: -11.294139862060547
Iteration 12401:
Training Loss: -7.356213092803955
Reconstruction Loss: -11.296530723571777
Iteration 12501:
Training Loss: -7.3594818115234375
Reconstruction Loss: -11.298908233642578
Iteration 12601:
Training Loss: -7.362767696380615
Reconstruction Loss: -11.301281929016113
Iteration 12701:
Training Loss: -7.3660478591918945
Reconstruction Loss: -11.3036470413208
Iteration 12801:
Training Loss: -7.36929988861084
Reconstruction Loss: -11.306018829345703
Iteration 12901:
Training Loss: -7.372562885284424
Reconstruction Loss: -11.308369636535645
Iteration 13001:
Training Loss: -7.375808238983154
Reconstruction Loss: -11.310711860656738
Iteration 13101:
Training Loss: -7.379002094268799
Reconstruction Loss: -11.313053131103516
Iteration 13201:
Training Loss: -7.382238864898682
Reconstruction Loss: -11.315380096435547
Iteration 13301:
Training Loss: -7.38545036315918
Reconstruction Loss: -11.317693710327148
Iteration 13401:
Training Loss: -7.388655185699463
Reconstruction Loss: -11.320002555847168
Iteration 13501:
Training Loss: -7.391844272613525
Reconstruction Loss: -11.322309494018555
Iteration 13601:
Training Loss: -7.395008087158203
Reconstruction Loss: -11.324597358703613
Iteration 13701:
Training Loss: -7.398203372955322
Reconstruction Loss: -11.32689094543457
Iteration 13801:
Training Loss: -7.401359558105469
Reconstruction Loss: -11.329169273376465
Iteration 13901:
Training Loss: -7.40450382232666
Reconstruction Loss: -11.331440925598145
Iteration 14001:
Training Loss: -7.407651424407959
Reconstruction Loss: -11.333707809448242
Iteration 14101:
Training Loss: -7.410791873931885
Reconstruction Loss: -11.33596134185791
Iteration 14201:
Training Loss: -7.413919925689697
Reconstruction Loss: -11.338226318359375
Iteration 14301:
Training Loss: -7.4170355796813965
Reconstruction Loss: -11.340463638305664
Iteration 14401:
Training Loss: -7.42015266418457
Reconstruction Loss: -11.342705726623535
Iteration 14501:
Training Loss: -7.423253059387207
Reconstruction Loss: -11.344931602478027
Iteration 14601:
Training Loss: -7.426340103149414
Reconstruction Loss: -11.347164154052734
Iteration 14701:
Training Loss: -7.429444313049316
Reconstruction Loss: -11.349372863769531
Iteration 14801:
Training Loss: -7.432502746582031
Reconstruction Loss: -11.351593017578125
Iteration 14901:
Training Loss: -7.435592174530029
Reconstruction Loss: -11.353790283203125
Iteration 15001:
Training Loss: -7.438647270202637
Reconstruction Loss: -11.355978012084961
Iteration 15101:
Training Loss: -7.441703796386719
Reconstruction Loss: -11.35816764831543
Iteration 15201:
Training Loss: -7.444751739501953
Reconstruction Loss: -11.3603515625
Iteration 15301:
Training Loss: -7.44778299331665
Reconstruction Loss: -11.362527847290039
Iteration 15401:
Training Loss: -7.450803756713867
Reconstruction Loss: -11.36469554901123
Iteration 15501:
Training Loss: -7.453850746154785
Reconstruction Loss: -11.366859436035156
Iteration 15601:
Training Loss: -7.4568657875061035
Reconstruction Loss: -11.369011878967285
Iteration 15701:
Training Loss: -7.4598774909973145
Reconstruction Loss: -11.371159553527832
Iteration 15801:
Training Loss: -7.462870121002197
Reconstruction Loss: -11.373313903808594
Iteration 15901:
Training Loss: -7.465869903564453
Reconstruction Loss: -11.375450134277344
Iteration 16001:
Training Loss: -7.468852519989014
Reconstruction Loss: -11.377581596374512
Iteration 16101:
Training Loss: -7.471802234649658
Reconstruction Loss: -11.379711151123047
Iteration 16201:
Training Loss: -7.4747772216796875
Reconstruction Loss: -11.381829261779785
Iteration 16301:
Training Loss: -7.477738857269287
Reconstruction Loss: -11.383944511413574
Iteration 16401:
Training Loss: -7.480663776397705
Reconstruction Loss: -11.386052131652832
Iteration 16501:
Training Loss: -7.48361873626709
Reconstruction Loss: -11.388144493103027
Iteration 16601:
Training Loss: -7.4865546226501465
Reconstruction Loss: -11.390250205993652
Iteration 16701:
Training Loss: -7.489461898803711
Reconstruction Loss: -11.392330169677734
Iteration 16801:
Training Loss: -7.4923906326293945
Reconstruction Loss: -11.394428253173828
Iteration 16901:
Training Loss: -7.49530553817749
Reconstruction Loss: -11.396510124206543
Iteration 17001:
Training Loss: -7.498205184936523
Reconstruction Loss: -11.398580551147461
Iteration 17101:
Training Loss: -7.501102924346924
Reconstruction Loss: -11.400649070739746
Iteration 17201:
Training Loss: -7.503979206085205
Reconstruction Loss: -11.402724266052246
Iteration 17301:
Training Loss: -7.50687313079834
Reconstruction Loss: -11.40478229522705
Iteration 17401:
Training Loss: -7.509751319885254
Reconstruction Loss: -11.406839370727539
Iteration 17501:
Training Loss: -7.512598991394043
Reconstruction Loss: -11.40888500213623
Iteration 17601:
Training Loss: -7.515455722808838
Reconstruction Loss: -11.410928726196289
Iteration 17701:
Training Loss: -7.518316745758057
Reconstruction Loss: -11.412972450256348
Iteration 17801:
Training Loss: -7.521158218383789
Reconstruction Loss: -11.415000915527344
Iteration 17901:
Training Loss: -7.52399206161499
Reconstruction Loss: -11.417031288146973
Iteration 18001:
Training Loss: -7.526826858520508
Reconstruction Loss: -11.419046401977539
Iteration 18101:
Training Loss: -7.5296478271484375
Reconstruction Loss: -11.421051979064941
Iteration 18201:
Training Loss: -7.532451629638672
Reconstruction Loss: -11.42305850982666
Iteration 18301:
Training Loss: -7.535276889801025
Reconstruction Loss: -11.42506217956543
Iteration 18401:
Training Loss: -7.538089275360107
Reconstruction Loss: -11.427054405212402
Iteration 18501:
Training Loss: -7.540896415710449
Reconstruction Loss: -11.429047584533691
Iteration 18601:
Training Loss: -7.543673515319824
Reconstruction Loss: -11.431029319763184
Iteration 18701:
Training Loss: -7.546453475952148
Reconstruction Loss: -11.43301010131836
Iteration 18801:
Training Loss: -7.549246788024902
Reconstruction Loss: -11.434971809387207
Iteration 18901:
Training Loss: -7.552002429962158
Reconstruction Loss: -11.436944961547852
Iteration 19001:
Training Loss: -7.55478572845459
Reconstruction Loss: -11.438899993896484
Iteration 19101:
Training Loss: -7.5575408935546875
Reconstruction Loss: -11.440841674804688
Iteration 19201:
Training Loss: -7.560299396514893
Reconstruction Loss: -11.44279956817627
Iteration 19301:
Training Loss: -7.5630412101745605
Reconstruction Loss: -11.44473648071289
Iteration 19401:
Training Loss: -7.565759181976318
Reconstruction Loss: -11.446671485900879
Iteration 19501:
Training Loss: -7.568497657775879
Reconstruction Loss: -11.44859504699707
Iteration 19601:
Training Loss: -7.571238040924072
Reconstruction Loss: -11.450515747070312
Iteration 19701:
Training Loss: -7.573941230773926
Reconstruction Loss: -11.452439308166504
Iteration 19801:
Training Loss: -7.576653957366943
Reconstruction Loss: -11.454340934753418
Iteration 19901:
Training Loss: -7.579339504241943
Reconstruction Loss: -11.456245422363281
Iteration 20001:
Training Loss: -7.582037448883057
Reconstruction Loss: -11.458147048950195
Iteration 20101:
Training Loss: -7.584728240966797
Reconstruction Loss: -11.46005630493164
Iteration 20201:
Training Loss: -7.587427139282227
Reconstruction Loss: -11.46194076538086
Iteration 20301:
Training Loss: -7.590080738067627
Reconstruction Loss: -11.463824272155762
Iteration 20401:
Training Loss: -7.5927629470825195
Reconstruction Loss: -11.465689659118652
Iteration 20501:
Training Loss: -7.595426082611084
Reconstruction Loss: -11.467569351196289
Iteration 20601:
Training Loss: -7.598087787628174
Reconstruction Loss: -11.469443321228027
Iteration 20701:
Training Loss: -7.600758075714111
Reconstruction Loss: -11.47131061553955
Iteration 20801:
Training Loss: -7.6033711433410645
Reconstruction Loss: -11.473180770874023
Iteration 20901:
Training Loss: -7.606019020080566
Reconstruction Loss: -11.475035667419434
Iteration 21001:
Training Loss: -7.608635425567627
Reconstruction Loss: -11.476887702941895
Iteration 21101:
Training Loss: -7.611269474029541
Reconstruction Loss: -11.478728294372559
Iteration 21201:
Training Loss: -7.6139044761657715
Reconstruction Loss: -11.480567932128906
Iteration 21301:
Training Loss: -7.616487503051758
Reconstruction Loss: -11.482409477233887
Iteration 21401:
Training Loss: -7.619100093841553
Reconstruction Loss: -11.4842529296875
Iteration 21501:
Training Loss: -7.621708869934082
Reconstruction Loss: -11.486084938049316
Iteration 21601:
Training Loss: -7.624288558959961
Reconstruction Loss: -11.487911224365234
Iteration 21701:
Training Loss: -7.626870155334473
Reconstruction Loss: -11.489714622497559
Iteration 21801:
Training Loss: -7.6294684410095215
Reconstruction Loss: -11.491532325744629
Iteration 21901:
Training Loss: -7.6320414543151855
Reconstruction Loss: -11.493345260620117
Iteration 22001:
Training Loss: -7.634607315063477
Reconstruction Loss: -11.495136260986328
Iteration 22101:
Training Loss: -7.6371660232543945
Reconstruction Loss: -11.496935844421387
Iteration 22201:
Training Loss: -7.639729022979736
Reconstruction Loss: -11.498723030090332
Iteration 22301:
Training Loss: -7.642308712005615
Reconstruction Loss: -11.500513076782227
Iteration 22401:
Training Loss: -7.644841194152832
Reconstruction Loss: -11.502276420593262
Iteration 22501:
Training Loss: -7.647375583648682
Reconstruction Loss: -11.50405216217041
Iteration 22601:
Training Loss: -7.649911403656006
Reconstruction Loss: -11.505826950073242
Iteration 22701:
Training Loss: -7.6524529457092285
Reconstruction Loss: -11.507588386535645
Iteration 22801:
Training Loss: -7.654952049255371
Reconstruction Loss: -11.509344100952148
Iteration 22901:
Training Loss: -7.657492160797119
Reconstruction Loss: -11.51109504699707
Iteration 23001:
Training Loss: -7.660004138946533
Reconstruction Loss: -11.512853622436523
Iteration 23101:
Training Loss: -7.66250467300415
Reconstruction Loss: -11.514592170715332
Iteration 23201:
Training Loss: -7.665009021759033
Reconstruction Loss: -11.516327857971191
Iteration 23301:
Training Loss: -7.667492866516113
Reconstruction Loss: -11.518065452575684
Iteration 23401:
Training Loss: -7.669984817504883
Reconstruction Loss: -11.519798278808594
Iteration 23501:
Training Loss: -7.672469139099121
Reconstruction Loss: -11.521520614624023
Iteration 23601:
Training Loss: -7.674960613250732
Reconstruction Loss: -11.523253440856934
Iteration 23701:
Training Loss: -7.67741584777832
Reconstruction Loss: -11.52497673034668
Iteration 23801:
Training Loss: -7.679872989654541
Reconstruction Loss: -11.526692390441895
Iteration 23901:
Training Loss: -7.68235445022583
Reconstruction Loss: -11.528409957885742
Iteration 24001:
Training Loss: -7.6848039627075195
Reconstruction Loss: -11.530131340026855
Iteration 24101:
Training Loss: -7.687266826629639
Reconstruction Loss: -11.531840324401855
Iteration 24201:
Training Loss: -7.689721584320068
Reconstruction Loss: -11.533553123474121
Iteration 24301:
Training Loss: -7.692139148712158
Reconstruction Loss: -11.535252571105957
Iteration 24401:
Training Loss: -7.694590091705322
Reconstruction Loss: -11.536957740783691
Iteration 24501:
Training Loss: -7.697021484375
Reconstruction Loss: -11.538653373718262
Iteration 24601:
Training Loss: -7.699453830718994
Reconstruction Loss: -11.540336608886719
Iteration 24701:
Training Loss: -7.70187520980835
Reconstruction Loss: -11.542024612426758
Iteration 24801:
Training Loss: -7.704286575317383
Reconstruction Loss: -11.543696403503418
Iteration 24901:
Training Loss: -7.706705570220947
Reconstruction Loss: -11.545371055603027
Iteration 25001:
Training Loss: -7.7091145515441895
Reconstruction Loss: -11.547057151794434
Iteration 25101:
Training Loss: -7.711512088775635
Reconstruction Loss: -11.548720359802246
Iteration 25201:
Training Loss: -7.713932514190674
Reconstruction Loss: -11.55038833618164
Iteration 25301:
Training Loss: -7.716289520263672
Reconstruction Loss: -11.552045822143555
Iteration 25401:
Training Loss: -7.71868371963501
Reconstruction Loss: -11.55370807647705
Iteration 25501:
Training Loss: -7.721071243286133
Reconstruction Loss: -11.555362701416016
Iteration 25601:
Training Loss: -7.7234601974487305
Reconstruction Loss: -11.557019233703613
Iteration 25701:
Training Loss: -7.725831031799316
Reconstruction Loss: -11.558667182922363
Iteration 25801:
Training Loss: -7.728196620941162
Reconstruction Loss: -11.560319900512695
Iteration 25901:
Training Loss: -7.73054838180542
Reconstruction Loss: -11.561970710754395
Iteration 26001:
Training Loss: -7.73293399810791
Reconstruction Loss: -11.563607215881348
Iteration 26101:
Training Loss: -7.735267162322998
Reconstruction Loss: -11.5652494430542
Iteration 26201:
Training Loss: -7.737620830535889
Reconstruction Loss: -11.566877365112305
Iteration 26301:
Training Loss: -7.739935874938965
Reconstruction Loss: -11.56850814819336
Iteration 26401:
Training Loss: -7.74229097366333
Reconstruction Loss: -11.57012939453125
Iteration 26501:
Training Loss: -7.744622230529785
Reconstruction Loss: -11.571754455566406
Iteration 26601:
Training Loss: -7.746964931488037
Reconstruction Loss: -11.573357582092285
Iteration 26701:
Training Loss: -7.749294757843018
Reconstruction Loss: -11.574965476989746
Iteration 26801:
Training Loss: -7.751594543457031
Reconstruction Loss: -11.57657241821289
Iteration 26901:
Training Loss: -7.75393533706665
Reconstruction Loss: -11.578171730041504
Iteration 27001:
Training Loss: -7.756194591522217
Reconstruction Loss: -11.5797758102417
Iteration 27101:
Training Loss: -7.758512496948242
Reconstruction Loss: -11.581364631652832
Iteration 27201:
Training Loss: -7.760841369628906
Reconstruction Loss: -11.582962036132812
Iteration 27301:
Training Loss: -7.763125419616699
Reconstruction Loss: -11.584547996520996
Iteration 27401:
Training Loss: -7.765400409698486
Reconstruction Loss: -11.586129188537598
Iteration 27501:
Training Loss: -7.767683982849121
Reconstruction Loss: -11.587724685668945
Iteration 27601:
Training Loss: -7.76997709274292
Reconstruction Loss: -11.589305877685547
Iteration 27701:
Training Loss: -7.77227258682251
Reconstruction Loss: -11.590886116027832
Iteration 27801:
Training Loss: -7.774537563323975
Reconstruction Loss: -11.592453002929688
Iteration 27901:
Training Loss: -7.776805400848389
Reconstruction Loss: -11.594032287597656
Iteration 28001:
Training Loss: -7.779063701629639
Reconstruction Loss: -11.595601081848145
Iteration 28101:
Training Loss: -7.781321048736572
Reconstruction Loss: -11.597161293029785
Iteration 28201:
Training Loss: -7.783564567565918
Reconstruction Loss: -11.598729133605957
Iteration 28301:
Training Loss: -7.7858357429504395
Reconstruction Loss: -11.600287437438965
Iteration 28401:
Training Loss: -7.788079261779785
Reconstruction Loss: -11.601845741271973
Iteration 28501:
Training Loss: -7.790327548980713
Reconstruction Loss: -11.603384017944336
Iteration 28601:
Training Loss: -7.792571067810059
Reconstruction Loss: -11.604934692382812
Iteration 28701:
Training Loss: -7.794811725616455
Reconstruction Loss: -11.606473922729492
Iteration 28801:
Training Loss: -7.797037601470947
Reconstruction Loss: -11.608016014099121
Iteration 28901:
Training Loss: -7.799278736114502
Reconstruction Loss: -11.60954761505127
Iteration 29001:
Training Loss: -7.801495552062988
Reconstruction Loss: -11.611087799072266
Iteration 29101:
Training Loss: -7.803705215454102
Reconstruction Loss: -11.612618446350098
Iteration 29201:
Training Loss: -7.805929183959961
Reconstruction Loss: -11.614137649536133
Iteration 29301:
Training Loss: -7.808142185211182
Reconstruction Loss: -11.615656852722168
Iteration 29401:
Training Loss: -7.810357093811035
Reconstruction Loss: -11.617169380187988
Iteration 29501:
Training Loss: -7.812562465667725
Reconstruction Loss: -11.618687629699707
Iteration 29601:
Training Loss: -7.8147687911987305
Reconstruction Loss: -11.62019157409668
Iteration 29701:
Training Loss: -7.81694221496582
Reconstruction Loss: -11.621688842773438
Iteration 29801:
Training Loss: -7.81915283203125
Reconstruction Loss: -11.623194694519043
Iteration 29901:
Training Loss: -7.821334362030029
Reconstruction Loss: -11.624680519104004
Iteration 30001:
Training Loss: -7.8235273361206055
Reconstruction Loss: -11.626179695129395
Iteration 30101:
Training Loss: -7.825692176818848
Reconstruction Loss: -11.62767505645752
Iteration 30201:
Training Loss: -7.827840805053711
Reconstruction Loss: -11.629169464111328
Iteration 30301:
Training Loss: -7.830034255981445
Reconstruction Loss: -11.630661964416504
Iteration 30401:
Training Loss: -7.832178115844727
Reconstruction Loss: -11.632146835327148
Iteration 30501:
Training Loss: -7.834380149841309
Reconstruction Loss: -11.633636474609375
Iteration 30601:
Training Loss: -7.836532115936279
Reconstruction Loss: -11.635124206542969
Iteration 30701:
Training Loss: -7.838734149932861
Reconstruction Loss: -11.636600494384766
Iteration 30801:
Training Loss: -7.840862274169922
Reconstruction Loss: -11.63807487487793
Iteration 30901:
Training Loss: -7.843018054962158
Reconstruction Loss: -11.639545440673828
Iteration 31001:
Training Loss: -7.845183372497559
Reconstruction Loss: -11.641023635864258
Iteration 31101:
Training Loss: -7.84733247756958
Reconstruction Loss: -11.642486572265625
Iteration 31201:
Training Loss: -7.849485397338867
Reconstruction Loss: -11.643962860107422
Iteration 31301:
Training Loss: -7.851624965667725
Reconstruction Loss: -11.645406723022461
Iteration 31401:
Training Loss: -7.853744983673096
Reconstruction Loss: -11.646875381469727
Iteration 31501:
Training Loss: -7.855887413024902
Reconstruction Loss: -11.648334503173828
Iteration 31601:
Training Loss: -7.857985019683838
Reconstruction Loss: -11.649791717529297
Iteration 31701:
Training Loss: -7.860121726989746
Reconstruction Loss: -11.651237487792969
Iteration 31801:
Training Loss: -7.862260818481445
Reconstruction Loss: -11.652678489685059
Iteration 31901:
Training Loss: -7.864362716674805
Reconstruction Loss: -11.654131889343262
Iteration 32001:
Training Loss: -7.866488933563232
Reconstruction Loss: -11.65556526184082
Iteration 32101:
Training Loss: -7.868597507476807
Reconstruction Loss: -11.657010078430176
Iteration 32201:
Training Loss: -7.870697975158691
Reconstruction Loss: -11.658442497253418
Iteration 32301:
Training Loss: -7.87280797958374
Reconstruction Loss: -11.65986442565918
Iteration 32401:
Training Loss: -7.874874591827393
Reconstruction Loss: -11.661287307739258
Iteration 32501:
Training Loss: -7.876980781555176
Reconstruction Loss: -11.66270923614502
Iteration 32601:
Training Loss: -7.879059314727783
Reconstruction Loss: -11.664142608642578
Iteration 32701:
Training Loss: -7.881156921386719
Reconstruction Loss: -11.665570259094238
Iteration 32801:
Training Loss: -7.88324499130249
Reconstruction Loss: -11.666990280151367
Iteration 32901:
Training Loss: -7.885320663452148
Reconstruction Loss: -11.668404579162598
Iteration 33001:
Training Loss: -7.887401103973389
Reconstruction Loss: -11.669820785522461
Iteration 33101:
Training Loss: -7.889476776123047
Reconstruction Loss: -11.671233177185059
Iteration 33201:
Training Loss: -7.891523838043213
Reconstruction Loss: -11.672651290893555
Iteration 33301:
Training Loss: -7.893612861633301
Reconstruction Loss: -11.674063682556152
Iteration 33401:
Training Loss: -7.8956756591796875
Reconstruction Loss: -11.675466537475586
Iteration 33501:
Training Loss: -7.897721767425537
Reconstruction Loss: -11.676874160766602
Iteration 33601:
Training Loss: -7.899784564971924
Reconstruction Loss: -11.678281784057617
Iteration 33701:
Training Loss: -7.901834487915039
Reconstruction Loss: -11.679676055908203
Iteration 33801:
Training Loss: -7.903868675231934
Reconstruction Loss: -11.681069374084473
Iteration 33901:
Training Loss: -7.90592098236084
Reconstruction Loss: -11.682470321655273
Iteration 34001:
Training Loss: -7.907961368560791
Reconstruction Loss: -11.683855056762695
Iteration 34101:
Training Loss: -7.909989356994629
Reconstruction Loss: -11.685247421264648
Iteration 34201:
Training Loss: -7.912019729614258
Reconstruction Loss: -11.686634063720703
Iteration 34301:
Training Loss: -7.914041519165039
Reconstruction Loss: -11.688015937805176
Iteration 34401:
Training Loss: -7.916083812713623
Reconstruction Loss: -11.689408302307129
Iteration 34501:
Training Loss: -7.91811990737915
Reconstruction Loss: -11.690780639648438
Iteration 34601:
Training Loss: -7.920133590698242
Reconstruction Loss: -11.692160606384277
Iteration 34701:
Training Loss: -7.922131061553955
Reconstruction Loss: -11.693536758422852
Iteration 34801:
Training Loss: -7.924121379852295
Reconstruction Loss: -11.694910049438477
Iteration 34901:
Training Loss: -7.926171779632568
Reconstruction Loss: -11.69626522064209
Iteration 35001:
Training Loss: -7.928155899047852
Reconstruction Loss: -11.6976318359375
Iteration 35101:
Training Loss: -7.9301605224609375
Reconstruction Loss: -11.69900131225586
Iteration 35201:
Training Loss: -7.932156562805176
Reconstruction Loss: -11.700358390808105
Iteration 35301:
Training Loss: -7.934140205383301
Reconstruction Loss: -11.701714515686035
Iteration 35401:
Training Loss: -7.936160087585449
Reconstruction Loss: -11.703073501586914
Iteration 35501:
Training Loss: -7.938138961791992
Reconstruction Loss: -11.704426765441895
Iteration 35601:
Training Loss: -7.9401021003723145
Reconstruction Loss: -11.70577621459961
Iteration 35701:
Training Loss: -7.9420905113220215
Reconstruction Loss: -11.707122802734375
Iteration 35801:
Training Loss: -7.944090843200684
Reconstruction Loss: -11.708453178405762
Iteration 35901:
Training Loss: -7.946046352386475
Reconstruction Loss: -11.709793090820312
Iteration 36001:
Training Loss: -7.948028564453125
Reconstruction Loss: -11.71113395690918
Iteration 36101:
Training Loss: -7.949977874755859
Reconstruction Loss: -11.712481498718262
Iteration 36201:
Training Loss: -7.9519429206848145
Reconstruction Loss: -11.713807106018066
Iteration 36301:
Training Loss: -7.953907489776611
Reconstruction Loss: -11.715130805969238
Iteration 36401:
Training Loss: -7.955878734588623
Reconstruction Loss: -11.716469764709473
Iteration 36501:
Training Loss: -7.9578328132629395
Reconstruction Loss: -11.71779727935791
Iteration 36601:
Training Loss: -7.959776878356934
Reconstruction Loss: -11.719122886657715
Iteration 36701:
Training Loss: -7.961694240570068
Reconstruction Loss: -11.720446586608887
Iteration 36801:
Training Loss: -7.963662147521973
Reconstruction Loss: -11.721766471862793
Iteration 36901:
Training Loss: -7.965597629547119
Reconstruction Loss: -11.723084449768066
Iteration 37001:
Training Loss: -7.9675469398498535
Reconstruction Loss: -11.724401473999023
Iteration 37101:
Training Loss: -7.96949577331543
Reconstruction Loss: -11.72571849822998
Iteration 37201:
Training Loss: -7.971416473388672
Reconstruction Loss: -11.727031707763672
Iteration 37301:
Training Loss: -7.973339557647705
Reconstruction Loss: -11.728339195251465
Iteration 37401:
Training Loss: -7.9752912521362305
Reconstruction Loss: -11.729652404785156
Iteration 37501:
Training Loss: -7.977180004119873
Reconstruction Loss: -11.730953216552734
Iteration 37601:
Training Loss: -7.979118347167969
Reconstruction Loss: -11.732253074645996
Iteration 37701:
Training Loss: -7.981016635894775
Reconstruction Loss: -11.733549118041992
Iteration 37801:
Training Loss: -7.982935905456543
Reconstruction Loss: -11.734843254089355
Iteration 37901:
Training Loss: -7.984855651855469
Reconstruction Loss: -11.73613452911377
Iteration 38001:
Training Loss: -7.986782550811768
Reconstruction Loss: -11.737425804138184
Iteration 38101:
Training Loss: -7.988656520843506
Reconstruction Loss: -11.7387113571167
Iteration 38201:
Training Loss: -7.990550518035889
Reconstruction Loss: -11.739999771118164
Iteration 38301:
Training Loss: -7.992435455322266
Reconstruction Loss: -11.741284370422363
Iteration 38401:
Training Loss: -7.994363307952881
Reconstruction Loss: -11.742572784423828
Iteration 38501:
Training Loss: -7.99622917175293
Reconstruction Loss: -11.743847846984863
Iteration 38601:
Training Loss: -7.998150825500488
Reconstruction Loss: -11.74512767791748
Iteration 38701:
Training Loss: -7.999999046325684
Reconstruction Loss: -11.746408462524414
Iteration 38801:
Training Loss: -8.001893043518066
Reconstruction Loss: -11.747682571411133
Iteration 38901:
Training Loss: -8.003783226013184
Reconstruction Loss: -11.748948097229004
Iteration 39001:
Training Loss: -8.00564956665039
Reconstruction Loss: -11.750228881835938
Iteration 39101:
Training Loss: -8.00755500793457
Reconstruction Loss: -11.751494407653809
Iteration 39201:
Training Loss: -8.009416580200195
Reconstruction Loss: -11.75277328491211
Iteration 39301:
Training Loss: -8.011273384094238
Reconstruction Loss: -11.754032135009766
Iteration 39401:
Training Loss: -8.01310920715332
Reconstruction Loss: -11.755289077758789
Iteration 39501:
Training Loss: -8.015012741088867
Reconstruction Loss: -11.756548881530762
Iteration 39601:
Training Loss: -8.016878128051758
Reconstruction Loss: -11.75780963897705
Iteration 39701:
Training Loss: -8.018717765808105
Reconstruction Loss: -11.759061813354492
Iteration 39801:
Training Loss: -8.020576477050781
Reconstruction Loss: -11.760313987731934
Iteration 39901:
Training Loss: -8.022418022155762
Reconstruction Loss: -11.761580467224121
Iteration 40001:
Training Loss: -8.024274826049805
Reconstruction Loss: -11.762824058532715
Iteration 40101:
Training Loss: -8.02612018585205
Reconstruction Loss: -11.764078140258789
Iteration 40201:
Training Loss: -8.028003692626953
Reconstruction Loss: -11.765323638916016
Iteration 40301:
Training Loss: -8.029818534851074
Reconstruction Loss: -11.766566276550293
Iteration 40401:
Training Loss: -8.031670570373535
Reconstruction Loss: -11.767806053161621
Iteration 40501:
Training Loss: -8.033503532409668
Reconstruction Loss: -11.76904582977295
Iteration 40601:
Training Loss: -8.035313606262207
Reconstruction Loss: -11.770284652709961
Iteration 40701:
Training Loss: -8.037161827087402
Reconstruction Loss: -11.771515846252441
Iteration 40801:
Training Loss: -8.038989067077637
Reconstruction Loss: -11.77275276184082
Iteration 40901:
Training Loss: -8.040812492370605
Reconstruction Loss: -11.773975372314453
Iteration 41001:
Training Loss: -8.042622566223145
Reconstruction Loss: -11.7752103805542
Iteration 41101:
Training Loss: -8.044445991516113
Reconstruction Loss: -11.776433944702148
Iteration 41201:
Training Loss: -8.04627513885498
Reconstruction Loss: -11.777654647827148
Iteration 41301:
Training Loss: -8.048079490661621
Reconstruction Loss: -11.77888298034668
Iteration 41401:
Training Loss: -8.04989242553711
Reconstruction Loss: -11.780101776123047
Iteration 41501:
Training Loss: -8.051697731018066
Reconstruction Loss: -11.781325340270996
Iteration 41601:
Training Loss: -8.053539276123047
Reconstruction Loss: -11.782532691955566
Iteration 41701:
Training Loss: -8.055349349975586
Reconstruction Loss: -11.783761978149414
Iteration 41801:
Training Loss: -8.05718994140625
Reconstruction Loss: -11.784974098205566
Iteration 41901:
Training Loss: -8.058944702148438
Reconstruction Loss: -11.786176681518555
Iteration 42001:
Training Loss: -8.060741424560547
Reconstruction Loss: -11.787384986877441
Iteration 42101:
Training Loss: -8.062505722045898
Reconstruction Loss: -11.78859806060791
Iteration 42201:
Training Loss: -8.064330101013184
Reconstruction Loss: -11.789802551269531
Iteration 42301:
Training Loss: -8.066102027893066
Reconstruction Loss: -11.79100513458252
Iteration 42401:
Training Loss: -8.067889213562012
Reconstruction Loss: -11.792201042175293
Iteration 42501:
Training Loss: -8.069658279418945
Reconstruction Loss: -11.793381690979004
Iteration 42601:
Training Loss: -8.071465492248535
Reconstruction Loss: -11.794574737548828
Iteration 42701:
Training Loss: -8.073246955871582
Reconstruction Loss: -11.79576301574707
Iteration 42801:
Training Loss: -8.074993133544922
Reconstruction Loss: -11.796960830688477
Iteration 42901:
Training Loss: -8.07675552368164
Reconstruction Loss: -11.798145294189453
Iteration 43001:
Training Loss: -8.078530311584473
Reconstruction Loss: -11.799324989318848
Iteration 43101:
Training Loss: -8.080286026000977
Reconstruction Loss: -11.800506591796875
Iteration 43201:
Training Loss: -8.082052230834961
Reconstruction Loss: -11.801689147949219
Iteration 43301:
Training Loss: -8.0837984085083
Reconstruction Loss: -11.802870750427246
Iteration 43401:
Training Loss: -8.085553169250488
Reconstruction Loss: -11.804037094116211
Iteration 43501:
Training Loss: -8.087311744689941
Reconstruction Loss: -11.805213928222656
Iteration 43601:
Training Loss: -8.0890531539917
Reconstruction Loss: -11.806378364562988
Iteration 43701:
Training Loss: -8.090828895568848
Reconstruction Loss: -11.807552337646484
Iteration 43801:
Training Loss: -8.092561721801758
Reconstruction Loss: -11.808721542358398
Iteration 43901:
Training Loss: -8.09430980682373
Reconstruction Loss: -11.809880256652832
Iteration 44001:
Training Loss: -8.096048355102539
Reconstruction Loss: -11.811052322387695
Iteration 44101:
Training Loss: -8.097796440124512
Reconstruction Loss: -11.812210083007812
Iteration 44201:
Training Loss: -8.099509239196777
Reconstruction Loss: -11.813369750976562
Iteration 44301:
Training Loss: -8.101245880126953
Reconstruction Loss: -11.814523696899414
Iteration 44401:
Training Loss: -8.10299015045166
Reconstruction Loss: -11.81567668914795
Iteration 44501:
Training Loss: -8.104693412780762
Reconstruction Loss: -11.8168306350708
Iteration 44601:
Training Loss: -8.106419563293457
Reconstruction Loss: -11.817977905273438
Iteration 44701:
Training Loss: -8.108139038085938
Reconstruction Loss: -11.819125175476074
Iteration 44801:
Training Loss: -8.109889030456543
Reconstruction Loss: -11.820270538330078
Iteration 44901:
Training Loss: -8.111574172973633
Reconstruction Loss: -11.821416854858398
Iteration 45001:
Training Loss: -8.11331844329834
Reconstruction Loss: -11.822561264038086
Iteration 45101:
Training Loss: -8.115017890930176
Reconstruction Loss: -11.823714256286621
Iteration 45201:
Training Loss: -8.116732597351074
Reconstruction Loss: -11.824845314025879
Iteration 45301:
Training Loss: -8.118432998657227
Reconstruction Loss: -11.825994491577148
Iteration 45401:
Training Loss: -8.120116233825684
Reconstruction Loss: -11.827134132385254
Iteration 45501:
Training Loss: -8.121816635131836
Reconstruction Loss: -11.828266143798828
Iteration 45601:
Training Loss: -8.123516082763672
Reconstruction Loss: -11.829412460327148
Iteration 45701:
Training Loss: -8.125238418579102
Reconstruction Loss: -11.830543518066406
Iteration 45801:
Training Loss: -8.126935958862305
Reconstruction Loss: -11.831679344177246
Iteration 45901:
Training Loss: -8.128630638122559
Reconstruction Loss: -11.832805633544922
Iteration 46001:
Training Loss: -8.130306243896484
Reconstruction Loss: -11.833940505981445
Iteration 46101:
Training Loss: -8.131989479064941
Reconstruction Loss: -11.835068702697754
Iteration 46201:
Training Loss: -8.133686065673828
Reconstruction Loss: -11.836194038391113
Iteration 46301:
Training Loss: -8.135391235351562
Reconstruction Loss: -11.837323188781738
Iteration 46401:
Training Loss: -8.137035369873047
Reconstruction Loss: -11.838443756103516
Iteration 46501:
Training Loss: -8.138701438903809
Reconstruction Loss: -11.839557647705078
Iteration 46601:
Training Loss: -8.14036750793457
Reconstruction Loss: -11.84067153930664
Iteration 46701:
Training Loss: -8.142062187194824
Reconstruction Loss: -11.841801643371582
Iteration 46801:
Training Loss: -8.143723487854004
Reconstruction Loss: -11.842920303344727
Iteration 46901:
Training Loss: -8.145415306091309
Reconstruction Loss: -11.844017028808594
Iteration 47001:
Training Loss: -8.147066116333008
Reconstruction Loss: -11.845124244689941
Iteration 47101:
Training Loss: -8.148712158203125
Reconstruction Loss: -11.846233367919922
Iteration 47201:
Training Loss: -8.150404930114746
Reconstruction Loss: -11.847333908081055
Iteration 47301:
Training Loss: -8.152057647705078
Reconstruction Loss: -11.848438262939453
Iteration 47401:
Training Loss: -8.15372085571289
Reconstruction Loss: -11.849540710449219
Iteration 47501:
Training Loss: -8.155364990234375
Reconstruction Loss: -11.850643157958984
Iteration 47601:
Training Loss: -8.157024383544922
Reconstruction Loss: -11.85174560546875
Iteration 47701:
Training Loss: -8.158670425415039
Reconstruction Loss: -11.852843284606934
Iteration 47801:
Training Loss: -8.160325050354004
Reconstruction Loss: -11.853947639465332
Iteration 47901:
Training Loss: -8.161971092224121
Reconstruction Loss: -11.855045318603516
Iteration 48001:
Training Loss: -8.163602828979492
Reconstruction Loss: -11.856141090393066
Iteration 48101:
Training Loss: -8.165233612060547
Reconstruction Loss: -11.857226371765137
Iteration 48201:
Training Loss: -8.1668701171875
Reconstruction Loss: -11.858315467834473
Iteration 48301:
Training Loss: -8.168496131896973
Reconstruction Loss: -11.859407424926758
Iteration 48401:
Training Loss: -8.170166969299316
Reconstruction Loss: -11.860499382019043
Iteration 48501:
Training Loss: -8.171781539916992
Reconstruction Loss: -11.861581802368164
Iteration 48601:
Training Loss: -8.173413276672363
Reconstruction Loss: -11.862671852111816
Iteration 48701:
Training Loss: -8.175032615661621
Reconstruction Loss: -11.863757133483887
Iteration 48801:
Training Loss: -8.176660537719727
Reconstruction Loss: -11.864835739135742
Iteration 48901:
Training Loss: -8.178303718566895
Reconstruction Loss: -11.8659086227417
Iteration 49001:
Training Loss: -8.17991828918457
Reconstruction Loss: -11.866999626159668
Iteration 49101:
Training Loss: -8.181549072265625
Reconstruction Loss: -11.868074417114258
Iteration 49201:
Training Loss: -8.18316650390625
Reconstruction Loss: -11.869144439697266
Iteration 49301:
Training Loss: -8.184779167175293
Reconstruction Loss: -11.870211601257324
Iteration 49401:
Training Loss: -8.186368942260742
Reconstruction Loss: -11.871288299560547
Iteration 49501:
Training Loss: -8.18801212310791
Reconstruction Loss: -11.872352600097656
Iteration 49601:
Training Loss: -8.189606666564941
Reconstruction Loss: -11.873427391052246
Iteration 49701:
Training Loss: -8.191215515136719
Reconstruction Loss: -11.87448787689209
Iteration 49801:
Training Loss: -8.192809104919434
Reconstruction Loss: -11.875555992126465
Iteration 49901:
Training Loss: -8.194414138793945
Reconstruction Loss: -11.876618385314941
