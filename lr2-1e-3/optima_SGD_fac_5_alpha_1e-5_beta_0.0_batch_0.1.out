5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.477022171020508
Reconstruction Loss: -0.5264808535575867
Iteration 11:
Training Loss: 5.575887680053711
Reconstruction Loss: -0.5332509875297546
Iteration 21:
Training Loss: 5.513665199279785
Reconstruction Loss: -0.5553420782089233
Iteration 31:
Training Loss: 4.498547077178955
Reconstruction Loss: -0.9074450135231018
Iteration 41:
Training Loss: 3.621473550796509
Reconstruction Loss: -1.3916783332824707
Iteration 51:
Training Loss: 3.017483949661255
Reconstruction Loss: -1.7032411098480225
Iteration 61:
Training Loss: 2.8196165561676025
Reconstruction Loss: -1.8738605976104736
Iteration 71:
Training Loss: 1.949375867843628
Reconstruction Loss: -2.576009750366211
Iteration 81:
Training Loss: 1.108515977859497
Reconstruction Loss: -3.538522720336914
Iteration 91:
Training Loss: -0.39544394612312317
Reconstruction Loss: -4.483564376831055
Iteration 101:
Training Loss: -1.1421815156936646
Reconstruction Loss: -5.39015531539917
Iteration 111:
Training Loss: -2.165536403656006
Reconstruction Loss: -6.274230480194092
Iteration 121:
Training Loss: -3.2904398441314697
Reconstruction Loss: -7.1347575187683105
Iteration 131:
Training Loss: -3.9575257301330566
Reconstruction Loss: -7.950130939483643
Iteration 141:
Training Loss: -4.747002601623535
Reconstruction Loss: -8.704176902770996
Iteration 151:
Training Loss: -5.2268571853637695
Reconstruction Loss: -9.3620023727417
Iteration 161:
Training Loss: -5.673984527587891
Reconstruction Loss: -9.821261405944824
Iteration 171:
Training Loss: -5.52384090423584
Reconstruction Loss: -10.159031867980957
Iteration 181:
Training Loss: -5.824866771697998
Reconstruction Loss: -10.352492332458496
Iteration 191:
Training Loss: -5.923583030700684
Reconstruction Loss: -10.49072551727295
Iteration 201:
Training Loss: -5.798558235168457
Reconstruction Loss: -10.536518096923828
Iteration 211:
Training Loss: -6.466411113739014
Reconstruction Loss: -10.568714141845703
Iteration 221:
Training Loss: -6.4799885749816895
Reconstruction Loss: -10.618033409118652
Iteration 231:
Training Loss: -6.459507465362549
Reconstruction Loss: -10.616107940673828
Iteration 241:
Training Loss: -6.313467025756836
Reconstruction Loss: -10.662554740905762
Iteration 251:
Training Loss: -6.036490440368652
Reconstruction Loss: -10.676831245422363
Iteration 261:
Training Loss: -6.1494879722595215
Reconstruction Loss: -10.66472053527832
Iteration 271:
Training Loss: -5.957192897796631
Reconstruction Loss: -10.682779312133789
Iteration 281:
Training Loss: -6.261538028717041
Reconstruction Loss: -10.686246871948242
Iteration 291:
Training Loss: -6.012115955352783
Reconstruction Loss: -10.711031913757324
Iteration 301:
Training Loss: -6.584732532501221
Reconstruction Loss: -10.715415954589844
Iteration 311:
Training Loss: -6.340385437011719
Reconstruction Loss: -10.726594924926758
Iteration 321:
Training Loss: -6.132543087005615
Reconstruction Loss: -10.706195831298828
Iteration 331:
Training Loss: -6.50504207611084
Reconstruction Loss: -10.743489265441895
Iteration 341:
Training Loss: -6.036651611328125
Reconstruction Loss: -10.73770523071289
Iteration 351:
Training Loss: -6.164556980133057
Reconstruction Loss: -10.72656536102295
Iteration 361:
Training Loss: -6.547528266906738
Reconstruction Loss: -10.75411319732666
Iteration 371:
Training Loss: -6.306358337402344
Reconstruction Loss: -10.759317398071289
Iteration 381:
Training Loss: -6.046909809112549
Reconstruction Loss: -10.74509048461914
Iteration 391:
Training Loss: -6.327421188354492
Reconstruction Loss: -10.773428916931152
Iteration 401:
Training Loss: -6.322500228881836
Reconstruction Loss: -10.759604454040527
Iteration 411:
Training Loss: -6.677429676055908
Reconstruction Loss: -10.759053230285645
Iteration 421:
Training Loss: -6.275493621826172
Reconstruction Loss: -10.764029502868652
Iteration 431:
Training Loss: -6.240871906280518
Reconstruction Loss: -10.779983520507812
Iteration 441:
Training Loss: -6.5381760597229
Reconstruction Loss: -10.78601360321045
Iteration 451:
Training Loss: -6.598540782928467
Reconstruction Loss: -10.808233261108398
Iteration 461:
Training Loss: -6.569578170776367
Reconstruction Loss: -10.817131996154785
Iteration 471:
Training Loss: -6.444650650024414
Reconstruction Loss: -10.799728393554688
Iteration 481:
Training Loss: -6.4773759841918945
Reconstruction Loss: -10.823137283325195
Iteration 491:
Training Loss: -6.6569504737854
Reconstruction Loss: -10.824631690979004
Iteration 501:
Training Loss: -6.007829666137695
Reconstruction Loss: -10.827987670898438
Iteration 511:
Training Loss: -6.039737701416016
Reconstruction Loss: -10.836648941040039
Iteration 521:
Training Loss: -6.757254123687744
Reconstruction Loss: -10.839804649353027
Iteration 531:
Training Loss: -6.085198879241943
Reconstruction Loss: -10.850845336914062
Iteration 541:
Training Loss: -6.3925933837890625
Reconstruction Loss: -10.851476669311523
Iteration 551:
Training Loss: -5.842089653015137
Reconstruction Loss: -10.854636192321777
Iteration 561:
Training Loss: -6.911826133728027
Reconstruction Loss: -10.857007026672363
Iteration 571:
Training Loss: -6.88861608505249
Reconstruction Loss: -10.869340896606445
Iteration 581:
Training Loss: -6.608975887298584
Reconstruction Loss: -10.84732723236084
Iteration 591:
Training Loss: -6.250309467315674
Reconstruction Loss: -10.885544776916504
Iteration 601:
Training Loss: -6.466198921203613
Reconstruction Loss: -10.873477935791016
Iteration 611:
Training Loss: -6.4231462478637695
Reconstruction Loss: -10.879663467407227
Iteration 621:
Training Loss: -6.722461700439453
Reconstruction Loss: -10.880813598632812
Iteration 631:
Training Loss: -6.191218852996826
Reconstruction Loss: -10.890605926513672
Iteration 641:
Training Loss: -6.148211479187012
Reconstruction Loss: -10.893255233764648
Iteration 651:
Training Loss: -6.424802303314209
Reconstruction Loss: -10.9082670211792
Iteration 661:
Training Loss: -6.46399450302124
Reconstruction Loss: -10.919612884521484
Iteration 671:
Training Loss: -6.2229790687561035
Reconstruction Loss: -10.908021926879883
Iteration 681:
Training Loss: -6.683777332305908
Reconstruction Loss: -10.908085823059082
Iteration 691:
Training Loss: -6.874287128448486
Reconstruction Loss: -10.913673400878906
Iteration 701:
Training Loss: -6.369452953338623
Reconstruction Loss: -10.938753128051758
Iteration 711:
Training Loss: -6.390562534332275
Reconstruction Loss: -10.94124698638916
Iteration 721:
Training Loss: -6.674802780151367
Reconstruction Loss: -10.937384605407715
Iteration 731:
Training Loss: -6.768104553222656
Reconstruction Loss: -10.946353912353516
Iteration 741:
Training Loss: -6.326253414154053
Reconstruction Loss: -10.927102088928223
Iteration 751:
Training Loss: -6.3893585205078125
Reconstruction Loss: -10.948797225952148
Iteration 761:
Training Loss: -6.600269794464111
Reconstruction Loss: -10.975435256958008
Iteration 771:
Training Loss: -6.169911861419678
Reconstruction Loss: -10.964874267578125
Iteration 781:
Training Loss: -6.576732158660889
Reconstruction Loss: -10.957979202270508
Iteration 791:
Training Loss: -6.54256534576416
Reconstruction Loss: -10.981467247009277
Iteration 801:
Training Loss: -6.240555763244629
Reconstruction Loss: -10.975617408752441
Iteration 811:
Training Loss: -6.189790725708008
Reconstruction Loss: -10.995112419128418
Iteration 821:
Training Loss: -6.1595354080200195
Reconstruction Loss: -10.993285179138184
Iteration 831:
Training Loss: -6.448631286621094
Reconstruction Loss: -11.000273704528809
Iteration 841:
Training Loss: -6.305601596832275
Reconstruction Loss: -10.99301528930664
Iteration 851:
Training Loss: -6.6559295654296875
Reconstruction Loss: -11.013679504394531
Iteration 861:
Training Loss: -6.204619407653809
Reconstruction Loss: -11.0115966796875
Iteration 871:
Training Loss: -6.2251787185668945
Reconstruction Loss: -11.031030654907227
Iteration 881:
Training Loss: -6.1381378173828125
Reconstruction Loss: -11.015043258666992
Iteration 891:
Training Loss: -6.316483974456787
Reconstruction Loss: -11.004758834838867
Iteration 901:
Training Loss: -6.53355073928833
Reconstruction Loss: -11.034433364868164
Iteration 911:
Training Loss: -6.705406188964844
Reconstruction Loss: -11.04194450378418
Iteration 921:
Training Loss: -6.639886856079102
Reconstruction Loss: -11.04189395904541
Iteration 931:
Training Loss: -6.171115875244141
Reconstruction Loss: -11.004629135131836
Iteration 941:
Training Loss: -7.093001365661621
Reconstruction Loss: -11.049674034118652
Iteration 951:
Training Loss: -6.904764175415039
Reconstruction Loss: -11.021958351135254
Iteration 961:
Training Loss: -7.040078163146973
Reconstruction Loss: -11.048646926879883
Iteration 971:
Training Loss: -6.70650577545166
Reconstruction Loss: -11.061692237854004
Iteration 981:
Training Loss: -6.850157737731934
Reconstruction Loss: -11.072810173034668
Iteration 991:
Training Loss: -6.574878692626953
Reconstruction Loss: -11.080710411071777
Iteration 1001:
Training Loss: -6.276331424713135
Reconstruction Loss: -11.068253517150879
Iteration 1011:
Training Loss: -6.088006496429443
Reconstruction Loss: -11.07459545135498
Iteration 1021:
Training Loss: -6.972312927246094
Reconstruction Loss: -11.087101936340332
Iteration 1031:
Training Loss: -6.58039665222168
Reconstruction Loss: -11.10329532623291
Iteration 1041:
Training Loss: -6.558374404907227
Reconstruction Loss: -11.078298568725586
Iteration 1051:
Training Loss: -6.467129230499268
Reconstruction Loss: -11.111684799194336
Iteration 1061:
Training Loss: -6.629606246948242
Reconstruction Loss: -11.117417335510254
Iteration 1071:
Training Loss: -6.650718688964844
Reconstruction Loss: -11.095272064208984
Iteration 1081:
Training Loss: -6.364802360534668
Reconstruction Loss: -11.102401733398438
Iteration 1091:
Training Loss: -6.8933258056640625
Reconstruction Loss: -11.107718467712402
Iteration 1101:
Training Loss: -6.72133731842041
Reconstruction Loss: -11.1187162399292
Iteration 1111:
Training Loss: -6.692113399505615
Reconstruction Loss: -11.113699913024902
Iteration 1121:
Training Loss: -6.71907377243042
Reconstruction Loss: -11.134655952453613
Iteration 1131:
Training Loss: -6.747792720794678
Reconstruction Loss: -11.135473251342773
Iteration 1141:
Training Loss: -6.747003555297852
Reconstruction Loss: -11.133379936218262
Iteration 1151:
Training Loss: -6.308401107788086
Reconstruction Loss: -11.118797302246094
Iteration 1161:
Training Loss: -6.432733058929443
Reconstruction Loss: -11.150226593017578
Iteration 1171:
Training Loss: -6.524543285369873
Reconstruction Loss: -11.137779235839844
Iteration 1181:
Training Loss: -6.425364017486572
Reconstruction Loss: -11.159361839294434
Iteration 1191:
Training Loss: -7.050363063812256
Reconstruction Loss: -11.167131423950195
Iteration 1201:
Training Loss: -6.771191596984863
Reconstruction Loss: -11.149595260620117
Iteration 1211:
Training Loss: -7.016613960266113
Reconstruction Loss: -11.162007331848145
Iteration 1221:
Training Loss: -6.642414093017578
Reconstruction Loss: -11.147098541259766
Iteration 1231:
Training Loss: -6.405810832977295
Reconstruction Loss: -11.168798446655273
Iteration 1241:
Training Loss: -6.236257553100586
Reconstruction Loss: -11.159520149230957
Iteration 1251:
Training Loss: -7.141430377960205
Reconstruction Loss: -11.169514656066895
Iteration 1261:
Training Loss: -6.576825141906738
Reconstruction Loss: -11.192444801330566
Iteration 1271:
Training Loss: -6.498324871063232
Reconstruction Loss: -11.20566177368164
Iteration 1281:
Training Loss: -6.969801425933838
Reconstruction Loss: -11.18322467803955
Iteration 1291:
Training Loss: -6.762712001800537
Reconstruction Loss: -11.175581932067871
Iteration 1301:
Training Loss: -6.767243385314941
Reconstruction Loss: -11.190467834472656
Iteration 1311:
Training Loss: -6.679862022399902
Reconstruction Loss: -11.206831932067871
Iteration 1321:
Training Loss: -6.5532026290893555
Reconstruction Loss: -11.215691566467285
Iteration 1331:
Training Loss: -7.232213973999023
Reconstruction Loss: -11.19450569152832
Iteration 1341:
Training Loss: -6.514187812805176
Reconstruction Loss: -11.215410232543945
Iteration 1351:
Training Loss: -6.547595500946045
Reconstruction Loss: -11.209759712219238
Iteration 1361:
Training Loss: -6.884343147277832
Reconstruction Loss: -11.221186637878418
Iteration 1371:
Training Loss: -6.519731521606445
Reconstruction Loss: -11.218680381774902
Iteration 1381:
Training Loss: -7.121189117431641
Reconstruction Loss: -11.23910140991211
Iteration 1391:
Training Loss: -7.239400863647461
Reconstruction Loss: -11.239139556884766
Iteration 1401:
Training Loss: -6.971618175506592
Reconstruction Loss: -11.239246368408203
Iteration 1411:
Training Loss: -6.79565954208374
Reconstruction Loss: -11.229822158813477
Iteration 1421:
Training Loss: -6.528360366821289
Reconstruction Loss: -11.242945671081543
Iteration 1431:
Training Loss: -6.786421775817871
Reconstruction Loss: -11.24248218536377
Iteration 1441:
Training Loss: -7.24652624130249
Reconstruction Loss: -11.24431037902832
Iteration 1451:
Training Loss: -6.864346981048584
Reconstruction Loss: -11.261815071105957
Iteration 1461:
Training Loss: -6.418583869934082
Reconstruction Loss: -11.260163307189941
Iteration 1471:
Training Loss: -6.816563606262207
Reconstruction Loss: -11.243369102478027
Iteration 1481:
Training Loss: -6.968120098114014
Reconstruction Loss: -11.266605377197266
Iteration 1491:
Training Loss: -6.573034286499023
Reconstruction Loss: -11.243911743164062
Iteration 1501:
Training Loss: -6.885758399963379
Reconstruction Loss: -11.259652137756348
Iteration 1511:
Training Loss: -6.926931858062744
Reconstruction Loss: -11.288809776306152
Iteration 1521:
Training Loss: -6.9395012855529785
Reconstruction Loss: -11.286478996276855
Iteration 1531:
Training Loss: -6.819441318511963
Reconstruction Loss: -11.279356002807617
Iteration 1541:
Training Loss: -6.878437042236328
Reconstruction Loss: -11.294634819030762
Iteration 1551:
Training Loss: -6.9345831871032715
Reconstruction Loss: -11.271394729614258
Iteration 1561:
Training Loss: -7.041928291320801
Reconstruction Loss: -11.284213066101074
Iteration 1571:
Training Loss: -7.248370170593262
Reconstruction Loss: -11.302284240722656
Iteration 1581:
Training Loss: -7.081536293029785
Reconstruction Loss: -11.306939125061035
Iteration 1591:
Training Loss: -6.789913654327393
Reconstruction Loss: -11.308048248291016
Iteration 1601:
Training Loss: -6.890979766845703
Reconstruction Loss: -11.309072494506836
Iteration 1611:
Training Loss: -6.628994941711426
Reconstruction Loss: -11.31944751739502
Iteration 1621:
Training Loss: -6.689879894256592
Reconstruction Loss: -11.306854248046875
Iteration 1631:
Training Loss: -7.150027751922607
Reconstruction Loss: -11.32180118560791
Iteration 1641:
Training Loss: -6.630062580108643
Reconstruction Loss: -11.312289237976074
Iteration 1651:
Training Loss: -7.358175754547119
Reconstruction Loss: -11.320393562316895
Iteration 1661:
Training Loss: -7.45224142074585
Reconstruction Loss: -11.330780982971191
Iteration 1671:
Training Loss: -7.083973407745361
Reconstruction Loss: -11.3168363571167
Iteration 1681:
Training Loss: -6.993606090545654
Reconstruction Loss: -11.350156784057617
Iteration 1691:
Training Loss: -6.755776882171631
Reconstruction Loss: -11.333063125610352
Iteration 1701:
Training Loss: -6.757292747497559
Reconstruction Loss: -11.339911460876465
Iteration 1711:
Training Loss: -7.030333518981934
Reconstruction Loss: -11.33313274383545
Iteration 1721:
Training Loss: -7.018124580383301
Reconstruction Loss: -11.329718589782715
Iteration 1731:
Training Loss: -6.6785888671875
Reconstruction Loss: -11.317727088928223
Iteration 1741:
Training Loss: -6.727921962738037
Reconstruction Loss: -11.362825393676758
Iteration 1751:
Training Loss: -7.0743818283081055
Reconstruction Loss: -11.369479179382324
Iteration 1761:
Training Loss: -6.710945129394531
Reconstruction Loss: -11.36252498626709
Iteration 1771:
Training Loss: -7.140092849731445
Reconstruction Loss: -11.367952346801758
Iteration 1781:
Training Loss: -6.929841995239258
Reconstruction Loss: -11.386348724365234
Iteration 1791:
Training Loss: -6.466094493865967
Reconstruction Loss: -11.370871543884277
Iteration 1801:
Training Loss: -6.886753559112549
Reconstruction Loss: -11.377409934997559
Iteration 1811:
Training Loss: -6.656745910644531
Reconstruction Loss: -11.393168449401855
Iteration 1821:
Training Loss: -6.982793807983398
Reconstruction Loss: -11.37264347076416
Iteration 1831:
Training Loss: -6.809762477874756
Reconstruction Loss: -11.381912231445312
Iteration 1841:
Training Loss: -6.941307544708252
Reconstruction Loss: -11.394129753112793
Iteration 1851:
Training Loss: -7.14686918258667
Reconstruction Loss: -11.395308494567871
Iteration 1861:
Training Loss: -7.000946998596191
Reconstruction Loss: -11.394259452819824
Iteration 1871:
Training Loss: -7.20768404006958
Reconstruction Loss: -11.42674446105957
Iteration 1881:
Training Loss: -6.764451026916504
Reconstruction Loss: -11.42679500579834
Iteration 1891:
Training Loss: -6.698612689971924
Reconstruction Loss: -11.40060806274414
Iteration 1901:
Training Loss: -6.812621116638184
Reconstruction Loss: -11.405778884887695
Iteration 1911:
Training Loss: -7.009955883026123
Reconstruction Loss: -11.41065788269043
Iteration 1921:
Training Loss: -7.120614051818848
Reconstruction Loss: -11.42559814453125
Iteration 1931:
Training Loss: -7.415308475494385
Reconstruction Loss: -11.43521499633789
Iteration 1941:
Training Loss: -6.958893299102783
Reconstruction Loss: -11.407751083374023
Iteration 1951:
Training Loss: -6.827783584594727
Reconstruction Loss: -11.439291000366211
Iteration 1961:
Training Loss: -7.621109962463379
Reconstruction Loss: -11.424092292785645
Iteration 1971:
Training Loss: -6.758289337158203
Reconstruction Loss: -11.444107055664062
Iteration 1981:
Training Loss: -7.138641357421875
Reconstruction Loss: -11.438192367553711
Iteration 1991:
Training Loss: -6.978334903717041
Reconstruction Loss: -11.4459810256958
Iteration 2001:
Training Loss: -6.958402633666992
Reconstruction Loss: -11.452064514160156
Iteration 2011:
Training Loss: -6.991988658905029
Reconstruction Loss: -11.437679290771484
Iteration 2021:
Training Loss: -6.828574180603027
Reconstruction Loss: -11.436896324157715
Iteration 2031:
Training Loss: -6.9627509117126465
Reconstruction Loss: -11.454263687133789
Iteration 2041:
Training Loss: -7.285158634185791
Reconstruction Loss: -11.44782829284668
Iteration 2051:
Training Loss: -6.426045894622803
Reconstruction Loss: -11.42556381225586
Iteration 2061:
Training Loss: -7.0630717277526855
Reconstruction Loss: -11.46058177947998
Iteration 2071:
Training Loss: -6.849534034729004
Reconstruction Loss: -11.46973705291748
Iteration 2081:
Training Loss: -7.169729709625244
Reconstruction Loss: -11.470110893249512
Iteration 2091:
Training Loss: -7.311708450317383
Reconstruction Loss: -11.477237701416016
Iteration 2101:
Training Loss: -6.789273738861084
Reconstruction Loss: -11.47330093383789
Iteration 2111:
Training Loss: -7.304904937744141
Reconstruction Loss: -11.452746391296387
Iteration 2121:
Training Loss: -7.036624908447266
Reconstruction Loss: -11.475143432617188
Iteration 2131:
Training Loss: -6.971037864685059
Reconstruction Loss: -11.481141090393066
Iteration 2141:
Training Loss: -6.9494242668151855
Reconstruction Loss: -11.455404281616211
Iteration 2151:
Training Loss: -6.920882701873779
Reconstruction Loss: -11.497309684753418
Iteration 2161:
Training Loss: -6.872770309448242
Reconstruction Loss: -11.486451148986816
Iteration 2171:
Training Loss: -7.22317361831665
Reconstruction Loss: -11.493610382080078
Iteration 2181:
Training Loss: -6.792457103729248
Reconstruction Loss: -11.49856185913086
Iteration 2191:
Training Loss: -7.41495943069458
Reconstruction Loss: -11.514437675476074
Iteration 2201:
Training Loss: -7.20503568649292
Reconstruction Loss: -11.49626350402832
Iteration 2211:
Training Loss: -7.415341377258301
Reconstruction Loss: -11.491737365722656
Iteration 2221:
Training Loss: -7.116890907287598
Reconstruction Loss: -11.514799118041992
Iteration 2231:
Training Loss: -7.125142574310303
Reconstruction Loss: -11.508878707885742
Iteration 2241:
Training Loss: -7.298004150390625
Reconstruction Loss: -11.506444931030273
Iteration 2251:
Training Loss: -7.003050327301025
Reconstruction Loss: -11.52092170715332
Iteration 2261:
Training Loss: -6.949919700622559
Reconstruction Loss: -11.523921966552734
Iteration 2271:
Training Loss: -7.06692361831665
Reconstruction Loss: -11.512312889099121
Iteration 2281:
Training Loss: -7.959784507751465
Reconstruction Loss: -11.527779579162598
Iteration 2291:
Training Loss: -7.321449279785156
Reconstruction Loss: -11.540132522583008
Iteration 2301:
Training Loss: -6.81823205947876
Reconstruction Loss: -11.527278900146484
Iteration 2311:
Training Loss: -7.480358123779297
Reconstruction Loss: -11.527694702148438
Iteration 2321:
Training Loss: -7.097259044647217
Reconstruction Loss: -11.532443046569824
Iteration 2331:
Training Loss: -7.124228000640869
Reconstruction Loss: -11.522764205932617
Iteration 2341:
Training Loss: -7.075903415679932
Reconstruction Loss: -11.544451713562012
Iteration 2351:
Training Loss: -6.96535587310791
Reconstruction Loss: -11.547094345092773
Iteration 2361:
Training Loss: -7.109927177429199
Reconstruction Loss: -11.554551124572754
Iteration 2371:
Training Loss: -7.586450099945068
Reconstruction Loss: -11.544621467590332
Iteration 2381:
Training Loss: -7.100437164306641
Reconstruction Loss: -11.548999786376953
Iteration 2391:
Training Loss: -7.148372173309326
Reconstruction Loss: -11.577886581420898
Iteration 2401:
Training Loss: -7.149056434631348
Reconstruction Loss: -11.571235656738281
Iteration 2411:
Training Loss: -7.1568603515625
Reconstruction Loss: -11.565997123718262
Iteration 2421:
Training Loss: -7.264032363891602
Reconstruction Loss: -11.574763298034668
Iteration 2431:
Training Loss: -7.264555931091309
Reconstruction Loss: -11.56537914276123
Iteration 2441:
Training Loss: -7.032961845397949
Reconstruction Loss: -11.575542449951172
Iteration 2451:
Training Loss: -6.945579528808594
Reconstruction Loss: -11.580351829528809
Iteration 2461:
Training Loss: -7.22007417678833
Reconstruction Loss: -11.590537071228027
Iteration 2471:
Training Loss: -7.389247417449951
Reconstruction Loss: -11.57668399810791
Iteration 2481:
Training Loss: -6.933229923248291
Reconstruction Loss: -11.583106994628906
Iteration 2491:
Training Loss: -7.200350284576416
Reconstruction Loss: -11.574111938476562
Iteration 2501:
Training Loss: -7.475167751312256
Reconstruction Loss: -11.592201232910156
Iteration 2511:
Training Loss: -7.493517875671387
Reconstruction Loss: -11.603525161743164
Iteration 2521:
Training Loss: -7.531187057495117
Reconstruction Loss: -11.58468246459961
Iteration 2531:
Training Loss: -7.335510730743408
Reconstruction Loss: -11.589204788208008
Iteration 2541:
Training Loss: -7.56666898727417
Reconstruction Loss: -11.614855766296387
Iteration 2551:
Training Loss: -7.356466770172119
Reconstruction Loss: -11.604207038879395
Iteration 2561:
Training Loss: -7.181546688079834
Reconstruction Loss: -11.621186256408691
Iteration 2571:
Training Loss: -7.3155951499938965
Reconstruction Loss: -11.611478805541992
Iteration 2581:
Training Loss: -7.271305084228516
Reconstruction Loss: -11.599342346191406
Iteration 2591:
Training Loss: -6.729630947113037
Reconstruction Loss: -11.637165069580078
Iteration 2601:
Training Loss: -7.462007999420166
Reconstruction Loss: -11.627889633178711
Iteration 2611:
Training Loss: -7.641166687011719
Reconstruction Loss: -11.609848022460938
Iteration 2621:
Training Loss: -7.6504807472229
Reconstruction Loss: -11.624502182006836
Iteration 2631:
Training Loss: -7.216644287109375
Reconstruction Loss: -11.634577751159668
Iteration 2641:
Training Loss: -7.242223262786865
Reconstruction Loss: -11.61358642578125
Iteration 2651:
Training Loss: -7.430283069610596
Reconstruction Loss: -11.640741348266602
Iteration 2661:
Training Loss: -7.254641532897949
Reconstruction Loss: -11.629182815551758
Iteration 2671:
Training Loss: -7.2017292976379395
Reconstruction Loss: -11.65073299407959
Iteration 2681:
Training Loss: -7.300515174865723
Reconstruction Loss: -11.645464897155762
Iteration 2691:
Training Loss: -7.452619552612305
Reconstruction Loss: -11.646727561950684
Iteration 2701:
Training Loss: -7.288175582885742
Reconstruction Loss: -11.6431303024292
Iteration 2711:
Training Loss: -7.446763515472412
Reconstruction Loss: -11.642704010009766
Iteration 2721:
Training Loss: -6.792737007141113
Reconstruction Loss: -11.644889831542969
Iteration 2731:
Training Loss: -7.255784511566162
Reconstruction Loss: -11.635429382324219
Iteration 2741:
Training Loss: -7.307798385620117
Reconstruction Loss: -11.64119815826416
Iteration 2751:
Training Loss: -7.701804161071777
Reconstruction Loss: -11.660188674926758
Iteration 2761:
Training Loss: -7.522324085235596
Reconstruction Loss: -11.669346809387207
Iteration 2771:
Training Loss: -7.476806163787842
Reconstruction Loss: -11.636713027954102
Iteration 2781:
Training Loss: -7.607114791870117
Reconstruction Loss: -11.64592170715332
Iteration 2791:
Training Loss: -7.332640171051025
Reconstruction Loss: -11.661117553710938
Iteration 2801:
Training Loss: -7.170736312866211
Reconstruction Loss: -11.645273208618164
Iteration 2811:
Training Loss: -7.087761402130127
Reconstruction Loss: -11.658116340637207
Iteration 2821:
Training Loss: -7.08911657333374
Reconstruction Loss: -11.680404663085938
Iteration 2831:
Training Loss: -7.922126770019531
Reconstruction Loss: -11.688782691955566
Iteration 2841:
Training Loss: -7.373947620391846
Reconstruction Loss: -11.672131538391113
Iteration 2851:
Training Loss: -7.088922500610352
Reconstruction Loss: -11.669233322143555
Iteration 2861:
Training Loss: -7.138644218444824
Reconstruction Loss: -11.670491218566895
Iteration 2871:
Training Loss: -7.065441131591797
Reconstruction Loss: -11.681294441223145
Iteration 2881:
Training Loss: -7.446028709411621
Reconstruction Loss: -11.689656257629395
Iteration 2891:
Training Loss: -7.525553226470947
Reconstruction Loss: -11.691183090209961
Iteration 2901:
Training Loss: -7.072531223297119
Reconstruction Loss: -11.686826705932617
Iteration 2911:
Training Loss: -7.49282693862915
Reconstruction Loss: -11.708748817443848
Iteration 2921:
Training Loss: -7.30943489074707
Reconstruction Loss: -11.691311836242676
Iteration 2931:
Training Loss: -7.32130765914917
Reconstruction Loss: -11.682585716247559
Iteration 2941:
Training Loss: -7.183694839477539
Reconstruction Loss: -11.707301139831543
Iteration 2951:
Training Loss: -7.759483337402344
Reconstruction Loss: -11.693218231201172
Iteration 2961:
Training Loss: -7.259808540344238
Reconstruction Loss: -11.694820404052734
Iteration 2971:
Training Loss: -7.302972793579102
Reconstruction Loss: -11.69684886932373
Iteration 2981:
Training Loss: -7.097654819488525
Reconstruction Loss: -11.733675003051758
Iteration 2991:
Training Loss: -7.271398544311523
Reconstruction Loss: -11.718713760375977
Iteration 3001:
Training Loss: -7.180482387542725
Reconstruction Loss: -11.70948314666748
Iteration 3011:
Training Loss: -7.418604850769043
Reconstruction Loss: -11.705950736999512
Iteration 3021:
Training Loss: -7.416867256164551
Reconstruction Loss: -11.708623886108398
Iteration 3031:
Training Loss: -7.112299919128418
Reconstruction Loss: -11.715987205505371
Iteration 3041:
Training Loss: -7.445638656616211
Reconstruction Loss: -11.72618293762207
Iteration 3051:
Training Loss: -7.866508483886719
Reconstruction Loss: -11.737695693969727
Iteration 3061:
Training Loss: -7.398016452789307
Reconstruction Loss: -11.727640151977539
Iteration 3071:
Training Loss: -7.437539577484131
Reconstruction Loss: -11.720877647399902
Iteration 3081:
Training Loss: -6.912071228027344
Reconstruction Loss: -11.738184928894043
Iteration 3091:
Training Loss: -7.404842376708984
Reconstruction Loss: -11.716193199157715
Iteration 3101:
Training Loss: -7.262518882751465
Reconstruction Loss: -11.720495223999023
Iteration 3111:
Training Loss: -7.4079976081848145
Reconstruction Loss: -11.732362747192383
Iteration 3121:
Training Loss: -7.308145046234131
Reconstruction Loss: -11.748734474182129
Iteration 3131:
Training Loss: -7.737648963928223
Reconstruction Loss: -11.730268478393555
Iteration 3141:
Training Loss: -7.051661968231201
Reconstruction Loss: -11.734820365905762
Iteration 3151:
Training Loss: -7.366698741912842
Reconstruction Loss: -11.74257755279541
Iteration 3161:
Training Loss: -7.4794745445251465
Reconstruction Loss: -11.744454383850098
Iteration 3171:
Training Loss: -7.411661624908447
Reconstruction Loss: -11.747424125671387
Iteration 3181:
Training Loss: -7.387814044952393
Reconstruction Loss: -11.747177124023438
Iteration 3191:
Training Loss: -7.02640438079834
Reconstruction Loss: -11.755949020385742
Iteration 3201:
Training Loss: -7.433116912841797
Reconstruction Loss: -11.762829780578613
Iteration 3211:
Training Loss: -7.52932071685791
Reconstruction Loss: -11.741022109985352
Iteration 3221:
Training Loss: -7.667559623718262
Reconstruction Loss: -11.767439842224121
Iteration 3231:
Training Loss: -7.333211421966553
Reconstruction Loss: -11.741045951843262
Iteration 3241:
Training Loss: -7.482975959777832
Reconstruction Loss: -11.757085800170898
Iteration 3251:
Training Loss: -7.43310022354126
Reconstruction Loss: -11.778364181518555
Iteration 3261:
Training Loss: -7.392207622528076
Reconstruction Loss: -11.780389785766602
Iteration 3271:
Training Loss: -7.431524276733398
Reconstruction Loss: -11.7952241897583
Iteration 3281:
Training Loss: -7.113180637359619
Reconstruction Loss: -11.795297622680664
Iteration 3291:
Training Loss: -7.433867454528809
Reconstruction Loss: -11.764732360839844
Iteration 3301:
Training Loss: -7.341817378997803
Reconstruction Loss: -11.789280891418457
Iteration 3311:
Training Loss: -7.26708459854126
Reconstruction Loss: -11.79885196685791
Iteration 3321:
Training Loss: -7.211702823638916
Reconstruction Loss: -11.776040077209473
Iteration 3331:
Training Loss: -7.339369297027588
Reconstruction Loss: -11.784944534301758
Iteration 3341:
Training Loss: -7.276937961578369
Reconstruction Loss: -11.802558898925781
Iteration 3351:
Training Loss: -7.463804721832275
Reconstruction Loss: -11.806241035461426
Iteration 3361:
Training Loss: -7.3549394607543945
Reconstruction Loss: -11.822296142578125
Iteration 3371:
Training Loss: -7.385059356689453
Reconstruction Loss: -11.806497573852539
Iteration 3381:
Training Loss: -7.318115711212158
Reconstruction Loss: -11.809800148010254
Iteration 3391:
Training Loss: -7.054447650909424
Reconstruction Loss: -11.795248985290527
Iteration 3401:
Training Loss: -7.638133525848389
Reconstruction Loss: -11.814664840698242
Iteration 3411:
Training Loss: -7.351452827453613
Reconstruction Loss: -11.795293807983398
Iteration 3421:
Training Loss: -7.542701721191406
Reconstruction Loss: -11.809089660644531
Iteration 3431:
Training Loss: -7.4947943687438965
Reconstruction Loss: -11.804976463317871
Iteration 3441:
Training Loss: -7.355381011962891
Reconstruction Loss: -11.808611869812012
Iteration 3451:
Training Loss: -6.979998588562012
Reconstruction Loss: -11.797260284423828
Iteration 3461:
Training Loss: -7.428412437438965
Reconstruction Loss: -11.803186416625977
Iteration 3471:
Training Loss: -7.776257038116455
Reconstruction Loss: -11.821763038635254
Iteration 3481:
Training Loss: -7.726583957672119
Reconstruction Loss: -11.827778816223145
Iteration 3491:
Training Loss: -7.7803778648376465
Reconstruction Loss: -11.82551383972168
Iteration 3501:
Training Loss: -7.158836364746094
Reconstruction Loss: -11.813822746276855
Iteration 3511:
Training Loss: -7.411631107330322
Reconstruction Loss: -11.836833953857422
Iteration 3521:
Training Loss: -7.449483394622803
Reconstruction Loss: -11.831260681152344
Iteration 3531:
Training Loss: -7.559655666351318
Reconstruction Loss: -11.827211380004883
Iteration 3541:
Training Loss: -7.580517768859863
Reconstruction Loss: -11.839738845825195
Iteration 3551:
Training Loss: -7.687076091766357
Reconstruction Loss: -11.832845687866211
Iteration 3561:
Training Loss: -7.581870079040527
Reconstruction Loss: -11.837552070617676
Iteration 3571:
Training Loss: -7.899363040924072
Reconstruction Loss: -11.833186149597168
Iteration 3581:
Training Loss: -7.653903484344482
Reconstruction Loss: -11.84229850769043
Iteration 3591:
Training Loss: -8.056499481201172
Reconstruction Loss: -11.844937324523926
Iteration 3601:
Training Loss: -7.812277317047119
Reconstruction Loss: -11.837550163269043
Iteration 3611:
Training Loss: -7.664158344268799
Reconstruction Loss: -11.862868309020996
Iteration 3621:
Training Loss: -7.990431785583496
Reconstruction Loss: -11.873882293701172
Iteration 3631:
Training Loss: -7.101817607879639
Reconstruction Loss: -11.859171867370605
Iteration 3641:
Training Loss: -7.607944488525391
Reconstruction Loss: -11.853723526000977
Iteration 3651:
Training Loss: -7.385030269622803
Reconstruction Loss: -11.856472969055176
Iteration 3661:
Training Loss: -7.868887901306152
Reconstruction Loss: -11.851452827453613
Iteration 3671:
Training Loss: -7.642233371734619
Reconstruction Loss: -11.867870330810547
Iteration 3681:
Training Loss: -7.410871982574463
Reconstruction Loss: -11.850088119506836
Iteration 3691:
Training Loss: -7.3324432373046875
Reconstruction Loss: -11.869852066040039
Iteration 3701:
Training Loss: -7.519556045532227
Reconstruction Loss: -11.859994888305664
Iteration 3711:
Training Loss: -7.694206714630127
Reconstruction Loss: -11.874024391174316
Iteration 3721:
Training Loss: -7.402381896972656
Reconstruction Loss: -11.866096496582031
Iteration 3731:
Training Loss: -7.598504543304443
Reconstruction Loss: -11.867247581481934
Iteration 3741:
Training Loss: -7.2640910148620605
Reconstruction Loss: -11.870770454406738
Iteration 3751:
Training Loss: -7.229295253753662
Reconstruction Loss: -11.8619966506958
Iteration 3761:
Training Loss: -7.8163557052612305
Reconstruction Loss: -11.888713836669922
Iteration 3771:
Training Loss: -7.741203308105469
Reconstruction Loss: -11.90165901184082
Iteration 3781:
Training Loss: -7.59252405166626
Reconstruction Loss: -11.869749069213867
Iteration 3791:
Training Loss: -7.428015232086182
Reconstruction Loss: -11.902713775634766
Iteration 3801:
Training Loss: -8.007798194885254
Reconstruction Loss: -11.903602600097656
Iteration 3811:
Training Loss: -7.315035820007324
Reconstruction Loss: -11.888644218444824
Iteration 3821:
Training Loss: -7.801962852478027
Reconstruction Loss: -11.898825645446777
Iteration 3831:
Training Loss: -7.371406555175781
Reconstruction Loss: -11.894293785095215
Iteration 3841:
Training Loss: -7.397304058074951
Reconstruction Loss: -11.906646728515625
Iteration 3851:
Training Loss: -7.405642509460449
Reconstruction Loss: -11.89462947845459
Iteration 3861:
Training Loss: -7.750243186950684
Reconstruction Loss: -11.916279792785645
Iteration 3871:
Training Loss: -7.841782569885254
Reconstruction Loss: -11.919483184814453
Iteration 3881:
Training Loss: -7.896103858947754
Reconstruction Loss: -11.918900489807129
Iteration 3891:
Training Loss: -7.482143878936768
Reconstruction Loss: -11.913134574890137
Iteration 3901:
Training Loss: -7.687368869781494
Reconstruction Loss: -11.904438972473145
Iteration 3911:
Training Loss: -7.8416290283203125
Reconstruction Loss: -11.910554885864258
Iteration 3921:
Training Loss: -7.557333469390869
Reconstruction Loss: -11.914539337158203
Iteration 3931:
Training Loss: -7.74784517288208
Reconstruction Loss: -11.890813827514648
Iteration 3941:
Training Loss: -8.246173858642578
Reconstruction Loss: -11.913434982299805
Iteration 3951:
Training Loss: -7.508449554443359
Reconstruction Loss: -11.930712699890137
Iteration 3961:
Training Loss: -7.3209004402160645
Reconstruction Loss: -11.930729866027832
Iteration 3971:
Training Loss: -8.097158432006836
Reconstruction Loss: -11.933808326721191
Iteration 3981:
Training Loss: -7.999083995819092
Reconstruction Loss: -11.924643516540527
Iteration 3991:
Training Loss: -7.473971843719482
Reconstruction Loss: -11.94552993774414
Iteration 4001:
Training Loss: -7.876480579376221
Reconstruction Loss: -11.9392728805542
Iteration 4011:
Training Loss: -7.530101299285889
Reconstruction Loss: -11.937438011169434
Iteration 4021:
Training Loss: -7.6328020095825195
Reconstruction Loss: -11.936721801757812
Iteration 4031:
Training Loss: -7.603114604949951
Reconstruction Loss: -11.952301025390625
Iteration 4041:
Training Loss: -7.654796123504639
Reconstruction Loss: -11.93729019165039
Iteration 4051:
Training Loss: -7.622460842132568
Reconstruction Loss: -11.952716827392578
Iteration 4061:
Training Loss: -7.437672138214111
Reconstruction Loss: -11.94796371459961
Iteration 4071:
Training Loss: -7.890538692474365
Reconstruction Loss: -11.956633567810059
Iteration 4081:
Training Loss: -7.39644193649292
Reconstruction Loss: -11.957045555114746
Iteration 4091:
Training Loss: -7.338367938995361
Reconstruction Loss: -11.938899993896484
Iteration 4101:
Training Loss: -7.5378217697143555
Reconstruction Loss: -11.955717086791992
Iteration 4111:
Training Loss: -7.697630882263184
Reconstruction Loss: -11.946138381958008
Iteration 4121:
Training Loss: -7.896876811981201
Reconstruction Loss: -11.945073127746582
Iteration 4131:
Training Loss: -7.368516445159912
Reconstruction Loss: -11.951390266418457
Iteration 4141:
Training Loss: -7.625380039215088
Reconstruction Loss: -11.94853401184082
Iteration 4151:
Training Loss: -8.572844505310059
Reconstruction Loss: -11.967763900756836
Iteration 4161:
Training Loss: -7.748999118804932
Reconstruction Loss: -11.96695613861084
Iteration 4171:
Training Loss: -7.928776741027832
Reconstruction Loss: -11.963406562805176
Iteration 4181:
Training Loss: -7.787975788116455
Reconstruction Loss: -11.969443321228027
Iteration 4191:
Training Loss: -7.527650833129883
Reconstruction Loss: -11.966876983642578
Iteration 4201:
Training Loss: -7.600649833679199
Reconstruction Loss: -11.978949546813965
Iteration 4211:
Training Loss: -7.360946178436279
Reconstruction Loss: -11.96879768371582
Iteration 4221:
Training Loss: -7.707810401916504
Reconstruction Loss: -11.9723539352417
Iteration 4231:
Training Loss: -7.577939033508301
Reconstruction Loss: -11.954707145690918
Iteration 4241:
Training Loss: -7.567682266235352
Reconstruction Loss: -11.987889289855957
Iteration 4251:
Training Loss: -7.796931743621826
Reconstruction Loss: -11.983686447143555
Iteration 4261:
Training Loss: -7.539486408233643
Reconstruction Loss: -11.979902267456055
Iteration 4271:
Training Loss: -7.699294090270996
Reconstruction Loss: -11.963934898376465
Iteration 4281:
Training Loss: -8.028926849365234
Reconstruction Loss: -11.991410255432129
Iteration 4291:
Training Loss: -7.975291728973389
Reconstruction Loss: -11.994095802307129
Iteration 4301:
Training Loss: -7.871641635894775
Reconstruction Loss: -11.997125625610352
Iteration 4311:
Training Loss: -7.888582706451416
Reconstruction Loss: -12.013124465942383
Iteration 4321:
Training Loss: -7.79761266708374
Reconstruction Loss: -11.985990524291992
Iteration 4331:
Training Loss: -8.241546630859375
Reconstruction Loss: -11.991676330566406
Iteration 4341:
Training Loss: -7.969637393951416
Reconstruction Loss: -11.997817993164062
Iteration 4351:
Training Loss: -7.293308734893799
Reconstruction Loss: -12.00906753540039
Iteration 4361:
Training Loss: -7.953983306884766
Reconstruction Loss: -11.999656677246094
Iteration 4371:
Training Loss: -8.173192024230957
Reconstruction Loss: -11.985832214355469
Iteration 4381:
Training Loss: -7.463472366333008
Reconstruction Loss: -11.985527992248535
Iteration 4391:
Training Loss: -7.573968410491943
Reconstruction Loss: -12.006141662597656
Iteration 4401:
Training Loss: -7.635729789733887
Reconstruction Loss: -12.000770568847656
Iteration 4411:
Training Loss: -7.738950729370117
Reconstruction Loss: -12.023731231689453
Iteration 4421:
Training Loss: -7.790849208831787
Reconstruction Loss: -12.007623672485352
Iteration 4431:
Training Loss: -7.749837875366211
Reconstruction Loss: -12.024762153625488
Iteration 4441:
Training Loss: -8.07807731628418
Reconstruction Loss: -12.0186128616333
Iteration 4451:
Training Loss: -7.667701244354248
Reconstruction Loss: -12.008516311645508
Iteration 4461:
Training Loss: -7.889917373657227
Reconstruction Loss: -12.010923385620117
Iteration 4471:
Training Loss: -7.621090888977051
Reconstruction Loss: -12.030247688293457
Iteration 4481:
Training Loss: -8.11810302734375
Reconstruction Loss: -12.014396667480469
Iteration 4491:
Training Loss: -7.883001327514648
Reconstruction Loss: -12.040409088134766
Iteration 4501:
Training Loss: -7.8057332038879395
Reconstruction Loss: -12.036210060119629
Iteration 4511:
Training Loss: -7.7409749031066895
Reconstruction Loss: -12.03791332244873
Iteration 4521:
Training Loss: -7.504232406616211
Reconstruction Loss: -12.002481460571289
Iteration 4531:
Training Loss: -7.9320855140686035
Reconstruction Loss: -12.030379295349121
Iteration 4541:
Training Loss: -8.212740898132324
Reconstruction Loss: -12.038910865783691
Iteration 4551:
Training Loss: -7.5897111892700195
Reconstruction Loss: -12.040724754333496
Iteration 4561:
Training Loss: -7.410664081573486
Reconstruction Loss: -12.027907371520996
Iteration 4571:
Training Loss: -7.625240325927734
Reconstruction Loss: -12.02579116821289
Iteration 4581:
Training Loss: -7.90230655670166
Reconstruction Loss: -12.037946701049805
Iteration 4591:
Training Loss: -8.288972854614258
Reconstruction Loss: -12.045466423034668
Iteration 4601:
Training Loss: -7.967401027679443
Reconstruction Loss: -12.03718376159668
Iteration 4611:
Training Loss: -7.614661693572998
Reconstruction Loss: -12.065058708190918
Iteration 4621:
Training Loss: -8.159346580505371
Reconstruction Loss: -12.073003768920898
Iteration 4631:
Training Loss: -7.76078462600708
Reconstruction Loss: -12.051871299743652
Iteration 4641:
Training Loss: -8.229205131530762
Reconstruction Loss: -12.050039291381836
Iteration 4651:
Training Loss: -7.844964504241943
Reconstruction Loss: -12.051830291748047
Iteration 4661:
Training Loss: -7.771143436431885
Reconstruction Loss: -12.047882080078125
Iteration 4671:
Training Loss: -7.826149940490723
Reconstruction Loss: -12.056751251220703
Iteration 4681:
Training Loss: -7.7858076095581055
Reconstruction Loss: -12.044279098510742
Iteration 4691:
Training Loss: -7.737952709197998
Reconstruction Loss: -12.05630874633789
Iteration 4701:
Training Loss: -7.807868480682373
Reconstruction Loss: -12.063701629638672
Iteration 4711:
Training Loss: -7.82118558883667
Reconstruction Loss: -12.080635070800781
Iteration 4721:
Training Loss: -7.656844139099121
Reconstruction Loss: -12.063328742980957
Iteration 4731:
Training Loss: -7.859800815582275
Reconstruction Loss: -12.061929702758789
Iteration 4741:
Training Loss: -7.933567047119141
Reconstruction Loss: -12.075675010681152
Iteration 4751:
Training Loss: -7.971565246582031
Reconstruction Loss: -12.072843551635742
Iteration 4761:
Training Loss: -7.5998969078063965
Reconstruction Loss: -12.078478813171387
Iteration 4771:
Training Loss: -7.618045330047607
Reconstruction Loss: -12.086944580078125
Iteration 4781:
Training Loss: -7.584284782409668
Reconstruction Loss: -12.066892623901367
Iteration 4791:
Training Loss: -7.979310035705566
Reconstruction Loss: -12.063410758972168
Iteration 4801:
Training Loss: -8.509991645812988
Reconstruction Loss: -12.082924842834473
Iteration 4811:
Training Loss: -7.825510501861572
Reconstruction Loss: -12.07529067993164
Iteration 4821:
Training Loss: -8.316505432128906
Reconstruction Loss: -12.10301399230957
Iteration 4831:
Training Loss: -8.112964630126953
Reconstruction Loss: -12.060303688049316
Iteration 4841:
Training Loss: -7.982897758483887
Reconstruction Loss: -12.081310272216797
Iteration 4851:
Training Loss: -8.270604133605957
Reconstruction Loss: -12.089422225952148
Iteration 4861:
Training Loss: -8.005030632019043
Reconstruction Loss: -12.080082893371582
Iteration 4871:
Training Loss: -7.707444667816162
Reconstruction Loss: -12.090383529663086
Iteration 4881:
Training Loss: -7.914230823516846
Reconstruction Loss: -12.10101318359375
Iteration 4891:
Training Loss: -7.808052062988281
Reconstruction Loss: -12.086468696594238
Iteration 4901:
Training Loss: -7.656389236450195
Reconstruction Loss: -12.067134857177734
Iteration 4911:
Training Loss: -7.936683177947998
Reconstruction Loss: -12.08791732788086
Iteration 4921:
Training Loss: -7.4988908767700195
Reconstruction Loss: -12.080397605895996
Iteration 4931:
Training Loss: -8.13095474243164
Reconstruction Loss: -12.084924697875977
Iteration 4941:
Training Loss: -7.4562249183654785
Reconstruction Loss: -12.081087112426758
Iteration 4951:
Training Loss: -8.428741455078125
Reconstruction Loss: -12.111769676208496
Iteration 4961:
Training Loss: -7.980280876159668
Reconstruction Loss: -12.097673416137695
Iteration 4971:
Training Loss: -8.055848121643066
Reconstruction Loss: -12.1122465133667
Iteration 4981:
Training Loss: -8.446533203125
Reconstruction Loss: -12.126587867736816
Iteration 4991:
Training Loss: -7.917927265167236
Reconstruction Loss: -12.12276554107666
