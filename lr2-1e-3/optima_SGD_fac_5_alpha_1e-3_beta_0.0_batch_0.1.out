5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.803114414215088
Reconstruction Loss: -0.3783377408981323
Iteration 11:
Training Loss: 3.249610662460327
Reconstruction Loss: -1.2336952686309814
Iteration 21:
Training Loss: 2.0882725715637207
Reconstruction Loss: -1.685781717300415
Iteration 31:
Training Loss: 1.3236738443374634
Reconstruction Loss: -2.1229801177978516
Iteration 41:
Training Loss: 0.6417183876037598
Reconstruction Loss: -2.472219705581665
Iteration 51:
Training Loss: -0.4439750015735626
Reconstruction Loss: -2.730480194091797
Iteration 61:
Training Loss: -0.2472851276397705
Reconstruction Loss: -2.907639265060425
Iteration 71:
Training Loss: -0.15854941308498383
Reconstruction Loss: -3.0712642669677734
Iteration 81:
Training Loss: -0.5665911436080933
Reconstruction Loss: -3.195389986038208
Iteration 91:
Training Loss: -1.193358302116394
Reconstruction Loss: -3.29510498046875
Iteration 101:
Training Loss: -1.270035743713379
Reconstruction Loss: -3.375675916671753
Iteration 111:
Training Loss: -1.4418067932128906
Reconstruction Loss: -3.446403741836548
Iteration 121:
Training Loss: -1.5365734100341797
Reconstruction Loss: -3.5148677825927734
Iteration 131:
Training Loss: -1.4774141311645508
Reconstruction Loss: -3.5748419761657715
Iteration 141:
Training Loss: -1.7013483047485352
Reconstruction Loss: -3.627323627471924
Iteration 151:
Training Loss: -1.8459362983703613
Reconstruction Loss: -3.6741623878479004
Iteration 161:
Training Loss: -1.7455627918243408
Reconstruction Loss: -3.735201597213745
Iteration 171:
Training Loss: -2.092927932739258
Reconstruction Loss: -3.7736127376556396
Iteration 181:
Training Loss: -2.0885977745056152
Reconstruction Loss: -3.8027424812316895
Iteration 191:
Training Loss: -1.6898553371429443
Reconstruction Loss: -3.8455350399017334
Iteration 201:
Training Loss: -2.555509328842163
Reconstruction Loss: -3.8766565322875977
Iteration 211:
Training Loss: -2.537975311279297
Reconstruction Loss: -3.922104835510254
Iteration 221:
Training Loss: -2.769275426864624
Reconstruction Loss: -3.943709135055542
Iteration 231:
Training Loss: -2.4413859844207764
Reconstruction Loss: -3.9703681468963623
Iteration 241:
Training Loss: -2.5562479496002197
Reconstruction Loss: -4.0023369789123535
Iteration 251:
Training Loss: -2.8765125274658203
Reconstruction Loss: -4.02678918838501
Iteration 261:
Training Loss: -2.3634629249572754
Reconstruction Loss: -4.048135757446289
Iteration 271:
Training Loss: -3.108267307281494
Reconstruction Loss: -4.068373203277588
Iteration 281:
Training Loss: -3.0242204666137695
Reconstruction Loss: -4.097139358520508
Iteration 291:
Training Loss: -2.7178151607513428
Reconstruction Loss: -4.119714736938477
Iteration 301:
Training Loss: -2.701903820037842
Reconstruction Loss: -4.137332439422607
Iteration 311:
Training Loss: -2.8701539039611816
Reconstruction Loss: -4.152477264404297
Iteration 321:
Training Loss: -3.0883350372314453
Reconstruction Loss: -4.167850017547607
Iteration 331:
Training Loss: -3.524233818054199
Reconstruction Loss: -4.18996524810791
Iteration 341:
Training Loss: -3.2980356216430664
Reconstruction Loss: -4.212224960327148
Iteration 351:
Training Loss: -3.1556529998779297
Reconstruction Loss: -4.224086761474609
Iteration 361:
Training Loss: -3.5105676651000977
Reconstruction Loss: -4.238882541656494
Iteration 371:
Training Loss: -2.8763198852539062
Reconstruction Loss: -4.251910209655762
Iteration 381:
Training Loss: -3.246866226196289
Reconstruction Loss: -4.264880180358887
Iteration 391:
Training Loss: -3.185476779937744
Reconstruction Loss: -4.284176349639893
Iteration 401:
Training Loss: -3.368370294570923
Reconstruction Loss: -4.300459384918213
Iteration 411:
Training Loss: -3.380875587463379
Reconstruction Loss: -4.306687831878662
Iteration 421:
Training Loss: -3.3022782802581787
Reconstruction Loss: -4.323241233825684
Iteration 431:
Training Loss: -3.1775028705596924
Reconstruction Loss: -4.330751895904541
Iteration 441:
Training Loss: -3.064105272293091
Reconstruction Loss: -4.341673851013184
Iteration 451:
Training Loss: -3.2882893085479736
Reconstruction Loss: -4.352320194244385
Iteration 461:
Training Loss: -3.335578680038452
Reconstruction Loss: -4.368093967437744
Iteration 471:
Training Loss: -3.7545151710510254
Reconstruction Loss: -4.373488426208496
Iteration 481:
Training Loss: -3.5189850330352783
Reconstruction Loss: -4.385807514190674
Iteration 491:
Training Loss: -3.5798115730285645
Reconstruction Loss: -4.390671253204346
Iteration 501:
Training Loss: -3.6923742294311523
Reconstruction Loss: -4.408731460571289
Iteration 511:
Training Loss: -3.765467643737793
Reconstruction Loss: -4.4185991287231445
Iteration 521:
Training Loss: -3.906500816345215
Reconstruction Loss: -4.430234432220459
Iteration 531:
Training Loss: -3.6342294216156006
Reconstruction Loss: -4.44196081161499
Iteration 541:
Training Loss: -3.4388604164123535
Reconstruction Loss: -4.446870803833008
Iteration 551:
Training Loss: -3.9207634925842285
Reconstruction Loss: -4.453361511230469
Iteration 561:
Training Loss: -4.070409297943115
Reconstruction Loss: -4.467744827270508
Iteration 571:
Training Loss: -3.6508374214172363
Reconstruction Loss: -4.467618942260742
Iteration 581:
Training Loss: -3.7544198036193848
Reconstruction Loss: -4.477067470550537
Iteration 591:
Training Loss: -4.28482723236084
Reconstruction Loss: -4.4896626472473145
Iteration 601:
Training Loss: -3.6414051055908203
Reconstruction Loss: -4.49682092666626
Iteration 611:
Training Loss: -3.862443447113037
Reconstruction Loss: -4.500527858734131
Iteration 621:
Training Loss: -4.116511821746826
Reconstruction Loss: -4.5090227127075195
Iteration 631:
Training Loss: -3.641331195831299
Reconstruction Loss: -4.513803005218506
Iteration 641:
Training Loss: -4.48228120803833
Reconstruction Loss: -4.523263931274414
Iteration 651:
Training Loss: -4.005971431732178
Reconstruction Loss: -4.531410217285156
Iteration 661:
Training Loss: -3.8728644847869873
Reconstruction Loss: -4.536781311035156
Iteration 671:
Training Loss: -3.608612537384033
Reconstruction Loss: -4.541722297668457
Iteration 681:
Training Loss: -4.005272388458252
Reconstruction Loss: -4.551056385040283
Iteration 691:
Training Loss: -4.289313316345215
Reconstruction Loss: -4.555507183074951
Iteration 701:
Training Loss: -4.467810153961182
Reconstruction Loss: -4.562335968017578
Iteration 711:
Training Loss: -4.347623825073242
Reconstruction Loss: -4.568759441375732
Iteration 721:
Training Loss: -4.508872032165527
Reconstruction Loss: -4.569767951965332
Iteration 731:
Training Loss: -4.332716464996338
Reconstruction Loss: -4.578200340270996
Iteration 741:
Training Loss: -3.894275188446045
Reconstruction Loss: -4.583466053009033
Iteration 751:
Training Loss: -3.996225595474243
Reconstruction Loss: -4.589010238647461
Iteration 761:
Training Loss: -4.1541595458984375
Reconstruction Loss: -4.594025611877441
Iteration 771:
Training Loss: -4.378078460693359
Reconstruction Loss: -4.603643894195557
Iteration 781:
Training Loss: -4.217709541320801
Reconstruction Loss: -4.608450889587402
Iteration 791:
Training Loss: -4.3309173583984375
Reconstruction Loss: -4.608876705169678
Iteration 801:
Training Loss: -4.5023603439331055
Reconstruction Loss: -4.617467403411865
Iteration 811:
Training Loss: -4.14082670211792
Reconstruction Loss: -4.626870632171631
Iteration 821:
Training Loss: -4.509920597076416
Reconstruction Loss: -4.627678394317627
Iteration 831:
Training Loss: -4.341824531555176
Reconstruction Loss: -4.626306533813477
Iteration 841:
Training Loss: -4.537448406219482
Reconstruction Loss: -4.635192394256592
Iteration 851:
Training Loss: -4.504306316375732
Reconstruction Loss: -4.642802715301514
Iteration 861:
Training Loss: -4.326604843139648
Reconstruction Loss: -4.646566390991211
Iteration 871:
Training Loss: -4.458068370819092
Reconstruction Loss: -4.649211406707764
Iteration 881:
Training Loss: -4.643777847290039
Reconstruction Loss: -4.652798652648926
Iteration 891:
Training Loss: -4.415052890777588
Reconstruction Loss: -4.658254146575928
Iteration 901:
Training Loss: -4.304337024688721
Reconstruction Loss: -4.6611647605896
Iteration 911:
Training Loss: -5.2487616539001465
Reconstruction Loss: -4.670353889465332
Iteration 921:
Training Loss: -4.3666815757751465
Reconstruction Loss: -4.6703925132751465
Iteration 931:
Training Loss: -4.564866542816162
Reconstruction Loss: -4.6738786697387695
Iteration 941:
Training Loss: -4.476901054382324
Reconstruction Loss: -4.678525924682617
Iteration 951:
Training Loss: -4.48451566696167
Reconstruction Loss: -4.680385589599609
Iteration 961:
Training Loss: -4.3697123527526855
Reconstruction Loss: -4.685263633728027
Iteration 971:
Training Loss: -4.712042808532715
Reconstruction Loss: -4.688220500946045
Iteration 981:
Training Loss: -4.951897144317627
Reconstruction Loss: -4.693787574768066
Iteration 991:
Training Loss: -5.1834821701049805
Reconstruction Loss: -4.700721740722656
Iteration 1001:
Training Loss: -4.89997673034668
Reconstruction Loss: -4.699000835418701
Iteration 1011:
Training Loss: -5.143882751464844
Reconstruction Loss: -4.703613758087158
Iteration 1021:
Training Loss: -4.845760822296143
Reconstruction Loss: -4.707327365875244
Iteration 1031:
Training Loss: -4.653661251068115
Reconstruction Loss: -4.712214469909668
Iteration 1041:
Training Loss: -4.350410461425781
Reconstruction Loss: -4.715916633605957
Iteration 1051:
Training Loss: -4.362637519836426
Reconstruction Loss: -4.719117164611816
Iteration 1061:
Training Loss: -4.5245771408081055
Reconstruction Loss: -4.718155860900879
Iteration 1071:
Training Loss: -5.130225658416748
Reconstruction Loss: -4.723289489746094
Iteration 1081:
Training Loss: -4.952872276306152
Reconstruction Loss: -4.7280592918396
Iteration 1091:
Training Loss: -4.548892498016357
Reconstruction Loss: -4.732029438018799
Iteration 1101:
Training Loss: -4.793959140777588
Reconstruction Loss: -4.736159801483154
Iteration 1111:
Training Loss: -4.80214786529541
Reconstruction Loss: -4.738988399505615
Iteration 1121:
Training Loss: -4.775319576263428
Reconstruction Loss: -4.738584518432617
Iteration 1131:
Training Loss: -4.860270023345947
Reconstruction Loss: -4.743179798126221
Iteration 1141:
Training Loss: -4.917756080627441
Reconstruction Loss: -4.744670867919922
Iteration 1151:
Training Loss: -4.893883228302002
Reconstruction Loss: -4.750487804412842
Iteration 1161:
Training Loss: -4.700462341308594
Reconstruction Loss: -4.752145290374756
Iteration 1171:
Training Loss: -4.98193359375
Reconstruction Loss: -4.755685329437256
Iteration 1181:
Training Loss: -5.193360805511475
Reconstruction Loss: -4.759243965148926
Iteration 1191:
Training Loss: -5.02716064453125
Reconstruction Loss: -4.759296417236328
Iteration 1201:
Training Loss: -5.1313395500183105
Reconstruction Loss: -4.762939929962158
Iteration 1211:
Training Loss: -4.999507427215576
Reconstruction Loss: -4.7669525146484375
Iteration 1221:
Training Loss: -5.165016174316406
Reconstruction Loss: -4.7732648849487305
Iteration 1231:
Training Loss: -5.425081729888916
Reconstruction Loss: -4.772400379180908
Iteration 1241:
Training Loss: -5.380265712738037
Reconstruction Loss: -4.774580478668213
Iteration 1251:
Training Loss: -5.030052661895752
Reconstruction Loss: -4.779603958129883
Iteration 1261:
Training Loss: -4.816190719604492
Reconstruction Loss: -4.780780792236328
Iteration 1271:
Training Loss: -5.2222981452941895
Reconstruction Loss: -4.781326770782471
Iteration 1281:
Training Loss: -5.218693256378174
Reconstruction Loss: -4.785458564758301
Iteration 1291:
Training Loss: -5.045797824859619
Reconstruction Loss: -4.789659023284912
Iteration 1301:
Training Loss: -5.4882636070251465
Reconstruction Loss: -4.788876533508301
Iteration 1311:
Training Loss: -5.595808029174805
Reconstruction Loss: -4.794129848480225
Iteration 1321:
Training Loss: -4.967520713806152
Reconstruction Loss: -4.798048496246338
Iteration 1331:
Training Loss: -4.976737976074219
Reconstruction Loss: -4.795401096343994
Iteration 1341:
Training Loss: -5.139944553375244
Reconstruction Loss: -4.798801898956299
Iteration 1351:
Training Loss: -5.044985771179199
Reconstruction Loss: -4.801321983337402
Iteration 1361:
Training Loss: -5.2983078956604
Reconstruction Loss: -4.8051862716674805
Iteration 1371:
Training Loss: -5.297175884246826
Reconstruction Loss: -4.807041645050049
Iteration 1381:
Training Loss: -5.079423904418945
Reconstruction Loss: -4.81026029586792
Iteration 1391:
Training Loss: -5.274706840515137
Reconstruction Loss: -4.810803413391113
Iteration 1401:
Training Loss: -5.227034091949463
Reconstruction Loss: -4.814484596252441
Iteration 1411:
Training Loss: -5.100677490234375
Reconstruction Loss: -4.815724849700928
Iteration 1421:
Training Loss: -4.672276020050049
Reconstruction Loss: -4.817194938659668
Iteration 1431:
Training Loss: -5.297266006469727
Reconstruction Loss: -4.820315361022949
Iteration 1441:
Training Loss: -5.689743518829346
Reconstruction Loss: -4.824005126953125
Iteration 1451:
Training Loss: -5.696788787841797
Reconstruction Loss: -4.824000835418701
Iteration 1461:
Training Loss: -5.583427429199219
Reconstruction Loss: -4.826069355010986
Iteration 1471:
Training Loss: -4.9970808029174805
Reconstruction Loss: -4.828501224517822
Iteration 1481:
Training Loss: -5.102046012878418
Reconstruction Loss: -4.830228805541992
Iteration 1491:
Training Loss: -5.4833760261535645
Reconstruction Loss: -4.831973075866699
Iteration 1501:
Training Loss: -5.342283248901367
Reconstruction Loss: -4.833856582641602
Iteration 1511:
Training Loss: -5.451186656951904
Reconstruction Loss: -4.837048053741455
Iteration 1521:
Training Loss: -5.471484184265137
Reconstruction Loss: -4.837154865264893
Iteration 1531:
Training Loss: -5.716343402862549
Reconstruction Loss: -4.840487003326416
Iteration 1541:
Training Loss: -5.5085368156433105
Reconstruction Loss: -4.8421220779418945
Iteration 1551:
Training Loss: -5.646574020385742
Reconstruction Loss: -4.843622207641602
Iteration 1561:
Training Loss: -5.735540866851807
Reconstruction Loss: -4.847073078155518
Iteration 1571:
Training Loss: -5.499814987182617
Reconstruction Loss: -4.846380233764648
Iteration 1581:
Training Loss: -5.175724029541016
Reconstruction Loss: -4.8482770919799805
Iteration 1591:
Training Loss: -5.896410942077637
Reconstruction Loss: -4.848863124847412
Iteration 1601:
Training Loss: -5.1593098640441895
Reconstruction Loss: -4.852235794067383
Iteration 1611:
Training Loss: -5.68448543548584
Reconstruction Loss: -4.853506565093994
Iteration 1621:
Training Loss: -5.749353408813477
Reconstruction Loss: -4.855968475341797
Iteration 1631:
Training Loss: -5.411404609680176
Reconstruction Loss: -4.857469081878662
Iteration 1641:
Training Loss: -5.566877365112305
Reconstruction Loss: -4.85819673538208
Iteration 1651:
Training Loss: -5.970067977905273
Reconstruction Loss: -4.860733985900879
Iteration 1661:
Training Loss: -5.424651145935059
Reconstruction Loss: -4.863032341003418
Iteration 1671:
Training Loss: -5.601036071777344
Reconstruction Loss: -4.8644185066223145
Iteration 1681:
Training Loss: -5.604772567749023
Reconstruction Loss: -4.864888668060303
Iteration 1691:
Training Loss: -5.68610954284668
Reconstruction Loss: -4.866829872131348
Iteration 1701:
Training Loss: -5.232105255126953
Reconstruction Loss: -4.87042236328125
Iteration 1711:
Training Loss: -5.714199066162109
Reconstruction Loss: -4.871580600738525
Iteration 1721:
Training Loss: -5.577491283416748
Reconstruction Loss: -4.876539707183838
Iteration 1731:
Training Loss: -5.6050848960876465
Reconstruction Loss: -4.871142864227295
Iteration 1741:
Training Loss: -6.081905841827393
Reconstruction Loss: -4.878842353820801
Iteration 1751:
Training Loss: -6.055667877197266
Reconstruction Loss: -4.877262592315674
Iteration 1761:
Training Loss: -5.724567890167236
Reconstruction Loss: -4.879209518432617
Iteration 1771:
Training Loss: -5.966611862182617
Reconstruction Loss: -4.882601261138916
Iteration 1781:
Training Loss: -5.826298713684082
Reconstruction Loss: -4.883373260498047
Iteration 1791:
Training Loss: -5.830783843994141
Reconstruction Loss: -4.884466171264648
Iteration 1801:
Training Loss: -6.11285400390625
Reconstruction Loss: -4.88555908203125
Iteration 1811:
Training Loss: -5.703529357910156
Reconstruction Loss: -4.885743618011475
Iteration 1821:
Training Loss: -5.47432279586792
Reconstruction Loss: -4.888753414154053
Iteration 1831:
Training Loss: -5.348093509674072
Reconstruction Loss: -4.889847278594971
Iteration 1841:
Training Loss: -5.451356410980225
Reconstruction Loss: -4.889785289764404
Iteration 1851:
Training Loss: -5.717615127563477
Reconstruction Loss: -4.892273426055908
Iteration 1861:
Training Loss: -5.473222732543945
Reconstruction Loss: -4.89342737197876
Iteration 1871:
Training Loss: -5.8393025398254395
Reconstruction Loss: -4.894590377807617
Iteration 1881:
Training Loss: -5.914947986602783
Reconstruction Loss: -4.895464897155762
Iteration 1891:
Training Loss: -5.981769561767578
Reconstruction Loss: -4.89609432220459
Iteration 1901:
Training Loss: -5.9952921867370605
Reconstruction Loss: -4.898758411407471
Iteration 1911:
Training Loss: -5.564600467681885
Reconstruction Loss: -4.8996663093566895
Iteration 1921:
Training Loss: -6.414608955383301
Reconstruction Loss: -4.902141094207764
Iteration 1931:
Training Loss: -5.729654312133789
Reconstruction Loss: -4.901935577392578
Iteration 1941:
Training Loss: -5.784759521484375
Reconstruction Loss: -4.901523113250732
Iteration 1951:
Training Loss: -5.9397711753845215
Reconstruction Loss: -4.905889987945557
Iteration 1961:
Training Loss: -6.208249568939209
Reconstruction Loss: -4.90683650970459
Iteration 1971:
Training Loss: -5.6752424240112305
Reconstruction Loss: -4.908761501312256
Iteration 1981:
Training Loss: -6.012292385101318
Reconstruction Loss: -4.9083571434021
Iteration 1991:
Training Loss: -5.808044910430908
Reconstruction Loss: -4.909822940826416
Iteration 2001:
Training Loss: -5.7918195724487305
Reconstruction Loss: -4.912266731262207
Iteration 2011:
Training Loss: -6.214150428771973
Reconstruction Loss: -4.911623001098633
Iteration 2021:
Training Loss: -6.2119140625
Reconstruction Loss: -4.915913105010986
Iteration 2031:
Training Loss: -6.040960788726807
Reconstruction Loss: -4.916433334350586
Iteration 2041:
Training Loss: -5.941671371459961
Reconstruction Loss: -4.916220664978027
Iteration 2051:
Training Loss: -6.029061317443848
Reconstruction Loss: -4.917855262756348
Iteration 2061:
Training Loss: -5.832178115844727
Reconstruction Loss: -4.918528079986572
Iteration 2071:
Training Loss: -6.1988205909729
Reconstruction Loss: -4.920493125915527
Iteration 2081:
Training Loss: -6.42594575881958
Reconstruction Loss: -4.921875
Iteration 2091:
Training Loss: -6.640664100646973
Reconstruction Loss: -4.923117637634277
Iteration 2101:
Training Loss: -6.210352897644043
Reconstruction Loss: -4.925732612609863
Iteration 2111:
Training Loss: -6.051208972930908
Reconstruction Loss: -4.927090167999268
Iteration 2121:
Training Loss: -5.950102806091309
Reconstruction Loss: -4.925117015838623
Iteration 2131:
Training Loss: -6.073662757873535
Reconstruction Loss: -4.927476406097412
Iteration 2141:
Training Loss: -6.2140398025512695
Reconstruction Loss: -4.925949573516846
Iteration 2151:
Training Loss: -5.931815147399902
Reconstruction Loss: -4.929450035095215
Iteration 2161:
Training Loss: -6.4838128089904785
Reconstruction Loss: -4.9319233894348145
Iteration 2171:
Training Loss: -6.355792045593262
Reconstruction Loss: -4.930471897125244
Iteration 2181:
Training Loss: -6.327783584594727
Reconstruction Loss: -4.933269023895264
Iteration 2191:
Training Loss: -5.880490779876709
Reconstruction Loss: -4.932638168334961
Iteration 2201:
Training Loss: -5.958425998687744
Reconstruction Loss: -4.9350996017456055
Iteration 2211:
Training Loss: -6.485392093658447
Reconstruction Loss: -4.936176300048828
Iteration 2221:
Training Loss: -6.17973518371582
Reconstruction Loss: -4.938593864440918
Iteration 2231:
Training Loss: -6.513841152191162
Reconstruction Loss: -4.9361891746521
Iteration 2241:
Training Loss: -6.865570068359375
Reconstruction Loss: -4.938868999481201
Iteration 2251:
Training Loss: -6.350043296813965
Reconstruction Loss: -4.939833164215088
Iteration 2261:
Training Loss: -6.063038349151611
Reconstruction Loss: -4.940578460693359
Iteration 2271:
Training Loss: -6.657375812530518
Reconstruction Loss: -4.943148612976074
Iteration 2281:
Training Loss: -6.032268524169922
Reconstruction Loss: -4.941867351531982
Iteration 2291:
Training Loss: -5.931200981140137
Reconstruction Loss: -4.943365097045898
Iteration 2301:
Training Loss: -6.0723395347595215
Reconstruction Loss: -4.944197654724121
Iteration 2311:
Training Loss: -6.5271453857421875
Reconstruction Loss: -4.946436882019043
Iteration 2321:
Training Loss: -5.975965976715088
Reconstruction Loss: -4.946069717407227
Iteration 2331:
Training Loss: -6.615568161010742
Reconstruction Loss: -4.946834564208984
Iteration 2341:
Training Loss: -6.360013484954834
Reconstruction Loss: -4.948086738586426
Iteration 2351:
Training Loss: -6.1442036628723145
Reconstruction Loss: -4.950788497924805
Iteration 2361:
Training Loss: -6.512350082397461
Reconstruction Loss: -4.950645923614502
Iteration 2371:
Training Loss: -6.111157417297363
Reconstruction Loss: -4.952301025390625
Iteration 2381:
Training Loss: -6.427604675292969
Reconstruction Loss: -4.95074462890625
Iteration 2391:
Training Loss: -6.520450115203857
Reconstruction Loss: -4.952505588531494
Iteration 2401:
Training Loss: -6.69574499130249
Reconstruction Loss: -4.953695774078369
Iteration 2411:
Training Loss: -6.3617987632751465
Reconstruction Loss: -4.952408790588379
Iteration 2421:
Training Loss: -6.375929832458496
Reconstruction Loss: -4.954469680786133
Iteration 2431:
Training Loss: -6.737314701080322
Reconstruction Loss: -4.956604957580566
Iteration 2441:
Training Loss: -6.153477191925049
Reconstruction Loss: -4.957643508911133
Iteration 2451:
Training Loss: -6.4173407554626465
Reconstruction Loss: -4.958373546600342
Iteration 2461:
Training Loss: -6.435760021209717
Reconstruction Loss: -4.956989288330078
Iteration 2471:
Training Loss: -6.6121296882629395
Reconstruction Loss: -4.959686756134033
Iteration 2481:
Training Loss: -6.710241317749023
Reconstruction Loss: -4.9610137939453125
Iteration 2491:
Training Loss: -6.554360866546631
Reconstruction Loss: -4.961876392364502
Iteration 2501:
Training Loss: -6.2979865074157715
Reconstruction Loss: -4.962128639221191
Iteration 2511:
Training Loss: -6.655412197113037
Reconstruction Loss: -4.9615912437438965
Iteration 2521:
Training Loss: -6.225803852081299
Reconstruction Loss: -4.963719844818115
Iteration 2531:
Training Loss: -6.383522987365723
Reconstruction Loss: -4.96439266204834
Iteration 2541:
Training Loss: -6.894065856933594
Reconstruction Loss: -4.965991973876953
Iteration 2551:
Training Loss: -6.344529628753662
Reconstruction Loss: -4.967106819152832
Iteration 2561:
Training Loss: -6.581110000610352
Reconstruction Loss: -4.96772575378418
Iteration 2571:
Training Loss: -6.641505241394043
Reconstruction Loss: -4.967236518859863
Iteration 2581:
Training Loss: -6.593181133270264
Reconstruction Loss: -4.967848777770996
Iteration 2591:
Training Loss: -6.673254489898682
Reconstruction Loss: -4.968040466308594
Iteration 2601:
Training Loss: -7.134758949279785
Reconstruction Loss: -4.970066070556641
Iteration 2611:
Training Loss: -6.846611976623535
Reconstruction Loss: -4.970325946807861
Iteration 2621:
Training Loss: -6.15212345123291
Reconstruction Loss: -4.971217155456543
Iteration 2631:
Training Loss: -6.888434886932373
Reconstruction Loss: -4.971553802490234
Iteration 2641:
Training Loss: -6.193037033081055
Reconstruction Loss: -4.973056793212891
Iteration 2651:
Training Loss: -6.65686559677124
Reconstruction Loss: -4.974589824676514
Iteration 2661:
Training Loss: -6.879070281982422
Reconstruction Loss: -4.97420072555542
Iteration 2671:
Training Loss: -6.950532913208008
Reconstruction Loss: -4.977417469024658
Iteration 2681:
Training Loss: -6.547535419464111
Reconstruction Loss: -4.976383209228516
Iteration 2691:
Training Loss: -6.821632385253906
Reconstruction Loss: -4.9769673347473145
Iteration 2701:
Training Loss: -6.30217981338501
Reconstruction Loss: -4.977494239807129
Iteration 2711:
Training Loss: -6.504484176635742
Reconstruction Loss: -4.979709148406982
Iteration 2721:
Training Loss: -6.928986072540283
Reconstruction Loss: -4.978781700134277
Iteration 2731:
Training Loss: -6.692535400390625
Reconstruction Loss: -4.97902774810791
Iteration 2741:
Training Loss: -7.299002647399902
Reconstruction Loss: -4.980097770690918
Iteration 2751:
Training Loss: -6.654209136962891
Reconstruction Loss: -4.9809064865112305
Iteration 2761:
Training Loss: -6.8769917488098145
Reconstruction Loss: -4.982647895812988
Iteration 2771:
Training Loss: -6.907617092132568
Reconstruction Loss: -4.98416805267334
Iteration 2781:
Training Loss: -7.364713191986084
Reconstruction Loss: -4.98276424407959
Iteration 2791:
Training Loss: -6.548783302307129
Reconstruction Loss: -4.983144283294678
Iteration 2801:
Training Loss: -6.939754486083984
Reconstruction Loss: -4.984624862670898
Iteration 2811:
Training Loss: -6.726404666900635
Reconstruction Loss: -4.985991477966309
Iteration 2821:
Training Loss: -6.892449855804443
Reconstruction Loss: -4.9858880043029785
Iteration 2831:
Training Loss: -6.345922470092773
Reconstruction Loss: -4.986691474914551
Iteration 2841:
Training Loss: -6.4240851402282715
Reconstruction Loss: -4.987907409667969
Iteration 2851:
Training Loss: -6.7086944580078125
Reconstruction Loss: -4.986635684967041
Iteration 2861:
Training Loss: -7.072251319885254
Reconstruction Loss: -4.988770008087158
Iteration 2871:
Training Loss: -6.866892337799072
Reconstruction Loss: -4.988982200622559
Iteration 2881:
Training Loss: -6.718234539031982
Reconstruction Loss: -4.989027500152588
Iteration 2891:
Training Loss: -6.8425822257995605
Reconstruction Loss: -4.990463733673096
Iteration 2901:
Training Loss: -7.009925365447998
Reconstruction Loss: -4.99129581451416
Iteration 2911:
Training Loss: -6.7247419357299805
Reconstruction Loss: -4.9922895431518555
Iteration 2921:
Training Loss: -7.162513256072998
Reconstruction Loss: -4.992230415344238
Iteration 2931:
Training Loss: -7.175125598907471
Reconstruction Loss: -4.992857933044434
Iteration 2941:
Training Loss: -7.085090160369873
Reconstruction Loss: -4.99276876449585
Iteration 2951:
Training Loss: -6.928632736206055
Reconstruction Loss: -4.9943742752075195
Iteration 2961:
Training Loss: -6.569754600524902
Reconstruction Loss: -4.996469497680664
Iteration 2971:
Training Loss: -7.100586891174316
Reconstruction Loss: -4.9960408210754395
Iteration 2981:
Training Loss: -6.7744646072387695
Reconstruction Loss: -4.99624490737915
Iteration 2991:
Training Loss: -7.272617340087891
Reconstruction Loss: -4.996798038482666
Iteration 3001:
Training Loss: -6.802138805389404
Reconstruction Loss: -4.997391700744629
Iteration 3011:
Training Loss: -7.191622734069824
Reconstruction Loss: -4.99833869934082
Iteration 3021:
Training Loss: -7.009705543518066
Reconstruction Loss: -4.999273777008057
Iteration 3031:
Training Loss: -6.918174743652344
Reconstruction Loss: -4.998757839202881
Iteration 3041:
Training Loss: -6.92436408996582
Reconstruction Loss: -4.999789714813232
Iteration 3051:
Training Loss: -6.463921546936035
Reconstruction Loss: -5.000730037689209
Iteration 3061:
Training Loss: -7.507928848266602
Reconstruction Loss: -5.00139045715332
Iteration 3071:
Training Loss: -7.713545799255371
Reconstruction Loss: -5.00248908996582
Iteration 3081:
Training Loss: -7.017972946166992
Reconstruction Loss: -5.003016471862793
Iteration 3091:
Training Loss: -7.116384983062744
Reconstruction Loss: -5.00217866897583
Iteration 3101:
Training Loss: -7.075543403625488
Reconstruction Loss: -5.0029096603393555
Iteration 3111:
Training Loss: -7.208624839782715
Reconstruction Loss: -5.003061294555664
Iteration 3121:
Training Loss: -7.0675272941589355
Reconstruction Loss: -5.005706787109375
Iteration 3131:
Training Loss: -6.939380645751953
Reconstruction Loss: -5.0057902336120605
Iteration 3141:
Training Loss: -6.800622940063477
Reconstruction Loss: -5.0064473152160645
Iteration 3151:
Training Loss: -7.44130277633667
Reconstruction Loss: -5.005617618560791
Iteration 3161:
Training Loss: -7.082233905792236
Reconstruction Loss: -5.005555152893066
Iteration 3171:
Training Loss: -7.423904895782471
Reconstruction Loss: -5.007152080535889
Iteration 3181:
Training Loss: -7.087756156921387
Reconstruction Loss: -5.007329940795898
Iteration 3191:
Training Loss: -6.910608768463135
Reconstruction Loss: -5.008127689361572
Iteration 3201:
Training Loss: -6.837133407592773
Reconstruction Loss: -5.0093302726745605
Iteration 3211:
Training Loss: -7.526442527770996
Reconstruction Loss: -5.00980806350708
Iteration 3221:
Training Loss: -6.947956085205078
Reconstruction Loss: -5.009262561798096
Iteration 3231:
Training Loss: -7.294290065765381
Reconstruction Loss: -5.009856700897217
Iteration 3241:
Training Loss: -6.98706579208374
Reconstruction Loss: -5.010242938995361
Iteration 3251:
Training Loss: -6.93460750579834
Reconstruction Loss: -5.011525630950928
Iteration 3261:
Training Loss: -6.967353820800781
Reconstruction Loss: -5.010856628417969
Iteration 3271:
Training Loss: -7.5438947677612305
Reconstruction Loss: -5.01315450668335
Iteration 3281:
Training Loss: -7.360921382904053
Reconstruction Loss: -5.012142658233643
Iteration 3291:
Training Loss: -7.194110870361328
Reconstruction Loss: -5.012531757354736
Iteration 3301:
Training Loss: -7.043255805969238
Reconstruction Loss: -5.0132222175598145
Iteration 3311:
Training Loss: -7.7161545753479
Reconstruction Loss: -5.0145649909973145
Iteration 3321:
Training Loss: -7.480729579925537
Reconstruction Loss: -5.0149922370910645
Iteration 3331:
Training Loss: -7.257674694061279
Reconstruction Loss: -5.0149688720703125
Iteration 3341:
Training Loss: -7.64295768737793
Reconstruction Loss: -5.014958381652832
Iteration 3351:
Training Loss: -7.30887508392334
Reconstruction Loss: -5.016329288482666
Iteration 3361:
Training Loss: -7.546538352966309
Reconstruction Loss: -5.016579627990723
Iteration 3371:
Training Loss: -7.257941246032715
Reconstruction Loss: -5.016303539276123
Iteration 3381:
Training Loss: -7.393906593322754
Reconstruction Loss: -5.016345977783203
Iteration 3391:
Training Loss: -7.1055145263671875
Reconstruction Loss: -5.017426490783691
Iteration 3401:
Training Loss: -7.4256911277771
Reconstruction Loss: -5.018889904022217
Iteration 3411:
Training Loss: -7.104896545410156
Reconstruction Loss: -5.019026279449463
Iteration 3421:
Training Loss: -7.314308166503906
Reconstruction Loss: -5.018366813659668
Iteration 3431:
Training Loss: -7.464729309082031
Reconstruction Loss: -5.018750190734863
Iteration 3441:
Training Loss: -7.43829345703125
Reconstruction Loss: -5.018996238708496
Iteration 3451:
Training Loss: -7.490535259246826
Reconstruction Loss: -5.020767688751221
Iteration 3461:
Training Loss: -7.683078765869141
Reconstruction Loss: -5.021557331085205
Iteration 3471:
Training Loss: -7.014684677124023
Reconstruction Loss: -5.021512508392334
Iteration 3481:
Training Loss: -7.599577903747559
Reconstruction Loss: -5.022425651550293
Iteration 3491:
Training Loss: -7.379339218139648
Reconstruction Loss: -5.021942138671875
Iteration 3501:
Training Loss: -7.201993942260742
Reconstruction Loss: -5.022629737854004
Iteration 3511:
Training Loss: -7.6958746910095215
Reconstruction Loss: -5.0233378410339355
Iteration 3521:
Training Loss: -7.604145526885986
Reconstruction Loss: -5.02363920211792
Iteration 3531:
Training Loss: -7.671197891235352
Reconstruction Loss: -5.024509906768799
Iteration 3541:
Training Loss: -7.389577388763428
Reconstruction Loss: -5.023715496063232
Iteration 3551:
Training Loss: -7.184810638427734
Reconstruction Loss: -5.024765491485596
Iteration 3561:
Training Loss: -7.720649719238281
Reconstruction Loss: -5.025409698486328
Iteration 3571:
Training Loss: -7.940985202789307
Reconstruction Loss: -5.025437831878662
Iteration 3581:
Training Loss: -7.422098159790039
Reconstruction Loss: -5.025140285491943
Iteration 3591:
Training Loss: -7.625915050506592
Reconstruction Loss: -5.025157451629639
Iteration 3601:
Training Loss: -8.00634479522705
Reconstruction Loss: -5.027194023132324
Iteration 3611:
Training Loss: -7.834512233734131
Reconstruction Loss: -5.026909828186035
Iteration 3621:
Training Loss: -7.797997951507568
Reconstruction Loss: -5.028176784515381
Iteration 3631:
Training Loss: -7.500147342681885
Reconstruction Loss: -5.027667045593262
Iteration 3641:
Training Loss: -7.350125312805176
Reconstruction Loss: -5.028359889984131
Iteration 3651:
Training Loss: -7.881272315979004
Reconstruction Loss: -5.0298004150390625
Iteration 3661:
Training Loss: -7.588364124298096
Reconstruction Loss: -5.028964042663574
Iteration 3671:
Training Loss: -7.791958808898926
Reconstruction Loss: -5.029253005981445
Iteration 3681:
Training Loss: -7.8367133140563965
Reconstruction Loss: -5.0298309326171875
Iteration 3691:
Training Loss: -7.950981140136719
Reconstruction Loss: -5.030846118927002
Iteration 3701:
Training Loss: -7.5200910568237305
Reconstruction Loss: -5.030817031860352
Iteration 3711:
Training Loss: -7.473638534545898
Reconstruction Loss: -5.0305962562561035
Iteration 3721:
Training Loss: -7.56942892074585
Reconstruction Loss: -5.030336380004883
Iteration 3731:
Training Loss: -7.482616901397705
Reconstruction Loss: -5.030930995941162
Iteration 3741:
Training Loss: -7.579329967498779
Reconstruction Loss: -5.031130790710449
Iteration 3751:
Training Loss: -7.795678615570068
Reconstruction Loss: -5.0323710441589355
Iteration 3761:
Training Loss: -8.096360206604004
Reconstruction Loss: -5.032472610473633
Iteration 3771:
Training Loss: -7.460716724395752
Reconstruction Loss: -5.0326828956604
Iteration 3781:
Training Loss: -7.858386516571045
Reconstruction Loss: -5.033473014831543
Iteration 3791:
Training Loss: -7.638609886169434
Reconstruction Loss: -5.03391695022583
Iteration 3801:
Training Loss: -7.858033657073975
Reconstruction Loss: -5.033102035522461
Iteration 3811:
Training Loss: -7.627644062042236
Reconstruction Loss: -5.034061908721924
Iteration 3821:
Training Loss: -8.325428009033203
Reconstruction Loss: -5.035655498504639
Iteration 3831:
Training Loss: -7.691187858581543
Reconstruction Loss: -5.036203384399414
Iteration 3841:
Training Loss: -7.930190563201904
Reconstruction Loss: -5.035854339599609
Iteration 3851:
Training Loss: -7.443753719329834
Reconstruction Loss: -5.0359296798706055
Iteration 3861:
Training Loss: -7.62596321105957
Reconstruction Loss: -5.036471843719482
Iteration 3871:
Training Loss: -7.531123161315918
Reconstruction Loss: -5.036926746368408
Iteration 3881:
Training Loss: -7.602620601654053
Reconstruction Loss: -5.037168979644775
Iteration 3891:
Training Loss: -8.016030311584473
Reconstruction Loss: -5.03767728805542
Iteration 3901:
Training Loss: -7.743396759033203
Reconstruction Loss: -5.0376176834106445
Iteration 3911:
Training Loss: -7.4103684425354
Reconstruction Loss: -5.038356781005859
Iteration 3921:
Training Loss: -7.888722896575928
Reconstruction Loss: -5.038773536682129
Iteration 3931:
Training Loss: -8.099201202392578
Reconstruction Loss: -5.038941860198975
Iteration 3941:
Training Loss: -7.6483588218688965
Reconstruction Loss: -5.039041519165039
Iteration 3951:
Training Loss: -7.689777851104736
Reconstruction Loss: -5.039149284362793
Iteration 3961:
Training Loss: -7.918066024780273
Reconstruction Loss: -5.038735866546631
Iteration 3971:
Training Loss: -7.861240386962891
Reconstruction Loss: -5.040802478790283
Iteration 3981:
Training Loss: -8.150527000427246
Reconstruction Loss: -5.039945602416992
Iteration 3991:
Training Loss: -7.451634407043457
Reconstruction Loss: -5.040777206420898
Iteration 4001:
Training Loss: -7.762527942657471
Reconstruction Loss: -5.040818214416504
Iteration 4011:
Training Loss: -8.216410636901855
Reconstruction Loss: -5.041007995605469
Iteration 4021:
Training Loss: -7.9343461990356445
Reconstruction Loss: -5.041864395141602
Iteration 4031:
Training Loss: -7.934319496154785
Reconstruction Loss: -5.041675090789795
Iteration 4041:
Training Loss: -7.867509841918945
Reconstruction Loss: -5.042019367218018
Iteration 4051:
Training Loss: -7.938173294067383
Reconstruction Loss: -5.041431903839111
Iteration 4061:
Training Loss: -8.355500221252441
Reconstruction Loss: -5.0433220863342285
Iteration 4071:
Training Loss: -8.207108497619629
Reconstruction Loss: -5.04271936416626
Iteration 4081:
Training Loss: -7.945533752441406
Reconstruction Loss: -5.043167591094971
Iteration 4091:
Training Loss: -7.661168575286865
Reconstruction Loss: -5.044372081756592
Iteration 4101:
Training Loss: -7.846185207366943
Reconstruction Loss: -5.0427470207214355
Iteration 4111:
Training Loss: -7.890041351318359
Reconstruction Loss: -5.0447845458984375
Iteration 4121:
Training Loss: -7.920553684234619
Reconstruction Loss: -5.044132232666016
Iteration 4131:
Training Loss: -7.804676532745361
Reconstruction Loss: -5.044986724853516
Iteration 4141:
Training Loss: -8.202523231506348
Reconstruction Loss: -5.045069694519043
Iteration 4151:
Training Loss: -7.881655216217041
Reconstruction Loss: -5.046041965484619
Iteration 4161:
Training Loss: -7.803908824920654
Reconstruction Loss: -5.045788764953613
Iteration 4171:
Training Loss: -7.738167762756348
Reconstruction Loss: -5.045994281768799
Iteration 4181:
Training Loss: -8.272109031677246
Reconstruction Loss: -5.046240329742432
Iteration 4191:
Training Loss: -8.177454948425293
Reconstruction Loss: -5.0466485023498535
Iteration 4201:
Training Loss: -8.03708267211914
Reconstruction Loss: -5.0471930503845215
Iteration 4211:
Training Loss: -8.068037033081055
Reconstruction Loss: -5.047266006469727
Iteration 4221:
Training Loss: -8.273168563842773
Reconstruction Loss: -5.047436714172363
Iteration 4231:
Training Loss: -7.806218147277832
Reconstruction Loss: -5.0477752685546875
Iteration 4241:
Training Loss: -8.187762260437012
Reconstruction Loss: -5.048052787780762
Iteration 4251:
Training Loss: -8.14559555053711
Reconstruction Loss: -5.04789924621582
Iteration 4261:
Training Loss: -8.23452377319336
Reconstruction Loss: -5.048271179199219
Iteration 4271:
Training Loss: -8.196660041809082
Reconstruction Loss: -5.048585891723633
Iteration 4281:
Training Loss: -8.41071891784668
Reconstruction Loss: -5.048714637756348
Iteration 4291:
Training Loss: -8.03402328491211
Reconstruction Loss: -5.04920768737793
Iteration 4301:
Training Loss: -8.040040969848633
Reconstruction Loss: -5.04876708984375
Iteration 4311:
Training Loss: -8.246397018432617
Reconstruction Loss: -5.049890518188477
Iteration 4321:
Training Loss: -8.21753978729248
Reconstruction Loss: -5.049802780151367
Iteration 4331:
Training Loss: -7.723610877990723
Reconstruction Loss: -5.049706935882568
Iteration 4341:
Training Loss: -8.355650901794434
Reconstruction Loss: -5.0497283935546875
Iteration 4351:
Training Loss: -8.233282089233398
Reconstruction Loss: -5.050003528594971
Iteration 4361:
Training Loss: -8.35612964630127
Reconstruction Loss: -5.051335334777832
Iteration 4371:
Training Loss: -8.13318920135498
Reconstruction Loss: -5.051516056060791
Iteration 4381:
Training Loss: -7.9191508293151855
Reconstruction Loss: -5.05168342590332
Iteration 4391:
Training Loss: -8.499587059020996
Reconstruction Loss: -5.052153587341309
Iteration 4401:
Training Loss: -8.7542724609375
Reconstruction Loss: -5.052119255065918
Iteration 4411:
Training Loss: -8.026956558227539
Reconstruction Loss: -5.05268669128418
Iteration 4421:
Training Loss: -8.143653869628906
Reconstruction Loss: -5.052982807159424
Iteration 4431:
Training Loss: -8.752437591552734
Reconstruction Loss: -5.053050994873047
Iteration 4441:
Training Loss: -8.131762504577637
Reconstruction Loss: -5.053022384643555
Iteration 4451:
Training Loss: -8.796590805053711
Reconstruction Loss: -5.053432464599609
Iteration 4461:
Training Loss: -8.577301025390625
Reconstruction Loss: -5.054762363433838
Iteration 4471:
Training Loss: -8.14274787902832
Reconstruction Loss: -5.054798603057861
Iteration 4481:
Training Loss: -7.924857139587402
Reconstruction Loss: -5.0545654296875
Iteration 4491:
Training Loss: -8.055152893066406
Reconstruction Loss: -5.054591655731201
Iteration 4501:
Training Loss: -8.193140983581543
Reconstruction Loss: -5.054553031921387
Iteration 4511:
Training Loss: -8.081319808959961
Reconstruction Loss: -5.055485248565674
Iteration 4521:
Training Loss: -8.575407028198242
Reconstruction Loss: -5.055477142333984
Iteration 4531:
Training Loss: -8.80556583404541
Reconstruction Loss: -5.054853916168213
Iteration 4541:
Training Loss: -8.670867919921875
Reconstruction Loss: -5.055997371673584
Iteration 4551:
Training Loss: -8.08779525756836
Reconstruction Loss: -5.055551052093506
Iteration 4561:
Training Loss: -8.586003303527832
Reconstruction Loss: -5.055682182312012
Iteration 4571:
Training Loss: -7.8939032554626465
Reconstruction Loss: -5.056034088134766
Iteration 4581:
Training Loss: -8.31174087524414
Reconstruction Loss: -5.055942535400391
Iteration 4591:
Training Loss: -8.11832332611084
Reconstruction Loss: -5.057168006896973
Iteration 4601:
Training Loss: -8.516708374023438
Reconstruction Loss: -5.05738639831543
Iteration 4611:
Training Loss: -8.509004592895508
Reconstruction Loss: -5.056971073150635
Iteration 4621:
Training Loss: -8.615129470825195
Reconstruction Loss: -5.056848526000977
Iteration 4631:
Training Loss: -8.182476043701172
Reconstruction Loss: -5.057834625244141
Iteration 4641:
Training Loss: -8.200543403625488
Reconstruction Loss: -5.058077812194824
Iteration 4651:
Training Loss: -8.563664436340332
Reconstruction Loss: -5.058358669281006
Iteration 4661:
Training Loss: -8.711151123046875
Reconstruction Loss: -5.058712482452393
Iteration 4671:
Training Loss: -8.318185806274414
Reconstruction Loss: -5.058917999267578
Iteration 4681:
Training Loss: -8.816828727722168
Reconstruction Loss: -5.059432506561279
Iteration 4691:
Training Loss: -8.561184883117676
Reconstruction Loss: -5.059215068817139
Iteration 4701:
Training Loss: -8.280458450317383
Reconstruction Loss: -5.059412956237793
Iteration 4711:
Training Loss: -8.26226806640625
Reconstruction Loss: -5.059872627258301
Iteration 4721:
Training Loss: -8.41822624206543
Reconstruction Loss: -5.0597381591796875
Iteration 4731:
Training Loss: -8.485033988952637
Reconstruction Loss: -5.059825897216797
Iteration 4741:
Training Loss: -8.404545783996582
Reconstruction Loss: -5.060559272766113
Iteration 4751:
Training Loss: -8.38748836517334
Reconstruction Loss: -5.0603461265563965
Iteration 4761:
Training Loss: -8.370027542114258
Reconstruction Loss: -5.0605268478393555
Iteration 4771:
Training Loss: -8.763946533203125
Reconstruction Loss: -5.059993743896484
Iteration 4781:
Training Loss: -8.924951553344727
Reconstruction Loss: -5.061511039733887
Iteration 4791:
Training Loss: -8.118974685668945
Reconstruction Loss: -5.0613694190979
Iteration 4801:
Training Loss: -8.594507217407227
Reconstruction Loss: -5.061159610748291
Iteration 4811:
Training Loss: -8.947437286376953
Reconstruction Loss: -5.061251640319824
Iteration 4821:
Training Loss: -8.387001037597656
Reconstruction Loss: -5.062448501586914
Iteration 4831:
Training Loss: -8.668146133422852
Reconstruction Loss: -5.062049865722656
Iteration 4841:
Training Loss: -8.638030052185059
Reconstruction Loss: -5.062989234924316
Iteration 4851:
Training Loss: -8.291216850280762
Reconstruction Loss: -5.062811374664307
Iteration 4861:
Training Loss: -8.541720390319824
Reconstruction Loss: -5.062173843383789
Iteration 4871:
Training Loss: -8.478796005249023
Reconstruction Loss: -5.0627875328063965
Iteration 4881:
Training Loss: -8.61037540435791
Reconstruction Loss: -5.06301212310791
Iteration 4891:
Training Loss: -8.318784713745117
Reconstruction Loss: -5.063067436218262
Iteration 4901:
Training Loss: -8.425085067749023
Reconstruction Loss: -5.063220500946045
Iteration 4911:
Training Loss: -8.456448554992676
Reconstruction Loss: -5.063838005065918
Iteration 4921:
Training Loss: -8.59068489074707
Reconstruction Loss: -5.064267635345459
Iteration 4931:
Training Loss: -8.76088809967041
Reconstruction Loss: -5.064229965209961
Iteration 4941:
Training Loss: -8.78176212310791
Reconstruction Loss: -5.064431190490723
Iteration 4951:
Training Loss: -8.321700096130371
Reconstruction Loss: -5.063924789428711
Iteration 4961:
Training Loss: -8.842504501342773
Reconstruction Loss: -5.064934253692627
Iteration 4971:
Training Loss: -8.403620719909668
Reconstruction Loss: -5.065256595611572
Iteration 4981:
Training Loss: -8.79684829711914
Reconstruction Loss: -5.064810276031494
Iteration 4991:
Training Loss: -8.577881813049316
Reconstruction Loss: -5.065296649932861
