5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.79518461227417
Reconstruction Loss: -0.4619973301887512
Iteration 11:
Training Loss: 4.922479629516602
Reconstruction Loss: -0.4619973301887512
Iteration 21:
Training Loss: 5.7615509033203125
Reconstruction Loss: -0.46199753880500793
Iteration 31:
Training Loss: 5.546694278717041
Reconstruction Loss: -0.46199753880500793
Iteration 41:
Training Loss: 6.0468525886535645
Reconstruction Loss: -0.46199753880500793
Iteration 51:
Training Loss: 5.718569278717041
Reconstruction Loss: -0.46199753880500793
Iteration 61:
Training Loss: 5.255272388458252
Reconstruction Loss: -0.4619976282119751
Iteration 71:
Training Loss: 5.4856767654418945
Reconstruction Loss: -0.46199771761894226
Iteration 81:
Training Loss: 6.10478401184082
Reconstruction Loss: -0.46199771761894226
Iteration 91:
Training Loss: 5.52111291885376
Reconstruction Loss: -0.4619978070259094
Iteration 101:
Training Loss: 5.271544933319092
Reconstruction Loss: -0.4619978070259094
Iteration 111:
Training Loss: 5.702323913574219
Reconstruction Loss: -0.4619978070259094
Iteration 121:
Training Loss: 5.470300674438477
Reconstruction Loss: -0.461997926235199
Iteration 131:
Training Loss: 5.440458297729492
Reconstruction Loss: -0.46199798583984375
Iteration 141:
Training Loss: 5.4392218589782715
Reconstruction Loss: -0.46199798583984375
Iteration 151:
Training Loss: 6.158884525299072
Reconstruction Loss: -0.4619981050491333
Iteration 161:
Training Loss: 5.559594631195068
Reconstruction Loss: -0.46199819445610046
Iteration 171:
Training Loss: 5.6215972900390625
Reconstruction Loss: -0.4619982838630676
Iteration 181:
Training Loss: 5.215826988220215
Reconstruction Loss: -0.4619982838630676
Iteration 191:
Training Loss: 5.630768775939941
Reconstruction Loss: -0.4619984030723572
Iteration 201:
Training Loss: 5.686215877532959
Reconstruction Loss: -0.4619984030723572
Iteration 211:
Training Loss: 6.026341915130615
Reconstruction Loss: -0.4619985818862915
Iteration 221:
Training Loss: 4.978690147399902
Reconstruction Loss: -0.4619985818862915
Iteration 231:
Training Loss: 5.510128498077393
Reconstruction Loss: -0.4619985818862915
Iteration 241:
Training Loss: 5.369894981384277
Reconstruction Loss: -0.46199867129325867
Iteration 251:
Training Loss: 5.459540367126465
Reconstruction Loss: -0.46199867129325867
Iteration 261:
Training Loss: 5.892187595367432
Reconstruction Loss: -0.46199867129325867
Iteration 271:
Training Loss: 5.273455619812012
Reconstruction Loss: -0.461998850107193
Iteration 281:
Training Loss: 5.342682361602783
Reconstruction Loss: -0.461998850107193
Iteration 291:
Training Loss: 5.469913005828857
Reconstruction Loss: -0.461998850107193
Iteration 301:
Training Loss: 5.419507026672363
Reconstruction Loss: -0.46199893951416016
Iteration 311:
Training Loss: 5.867458343505859
Reconstruction Loss: -0.4619990587234497
Iteration 321:
Training Loss: 5.348722457885742
Reconstruction Loss: -0.4619990587234497
Iteration 331:
Training Loss: 5.061664581298828
Reconstruction Loss: -0.4619991183280945
Iteration 341:
Training Loss: 5.450103759765625
Reconstruction Loss: -0.4619993269443512
Iteration 351:
Training Loss: 6.023448467254639
Reconstruction Loss: -0.46199941635131836
Iteration 361:
Training Loss: 5.791091442108154
Reconstruction Loss: -0.46199941635131836
Iteration 371:
Training Loss: 5.332369327545166
Reconstruction Loss: -0.4619995951652527
Iteration 381:
Training Loss: 6.086282730102539
Reconstruction Loss: -0.46199971437454224
Iteration 391:
Training Loss: 5.256441593170166
Reconstruction Loss: -0.4619998037815094
Iteration 401:
Training Loss: 5.306523323059082
Reconstruction Loss: -0.46199989318847656
Iteration 411:
Training Loss: 5.338139533996582
Reconstruction Loss: -0.46199989318847656
Iteration 421:
Training Loss: 5.028951644897461
Reconstruction Loss: -0.4620000720024109
Iteration 431:
Training Loss: 4.915757179260254
Reconstruction Loss: -0.46200019121170044
Iteration 441:
Training Loss: 6.2950215339660645
Reconstruction Loss: -0.4620002806186676
Iteration 451:
Training Loss: 5.439609527587891
Reconstruction Loss: -0.46200037002563477
Iteration 461:
Training Loss: 5.328133583068848
Reconstruction Loss: -0.4620005488395691
Iteration 471:
Training Loss: 5.7652974128723145
Reconstruction Loss: -0.46200066804885864
Iteration 481:
Training Loss: 5.787238121032715
Reconstruction Loss: -0.46200084686279297
Iteration 491:
Training Loss: 5.636538982391357
Reconstruction Loss: -0.46200093626976013
Iteration 501:
Training Loss: 5.667937278747559
Reconstruction Loss: -0.4620010256767273
Iteration 511:
Training Loss: 5.558323383331299
Reconstruction Loss: -0.46200132369995117
Iteration 521:
Training Loss: 6.159147262573242
Reconstruction Loss: -0.46200141310691833
Iteration 531:
Training Loss: 5.403818130493164
Reconstruction Loss: -0.4620016813278198
Iteration 541:
Training Loss: 5.942907333374023
Reconstruction Loss: -0.4620018005371094
Iteration 551:
Training Loss: 5.729882717132568
Reconstruction Loss: -0.46200206875801086
Iteration 561:
Training Loss: 5.762444019317627
Reconstruction Loss: -0.4620022773742676
Iteration 571:
Training Loss: 5.739854335784912
Reconstruction Loss: -0.46200233697891235
Iteration 581:
Training Loss: 5.26553201675415
Reconstruction Loss: -0.4620029330253601
Iteration 591:
Training Loss: 5.429058074951172
Reconstruction Loss: -0.46200302243232727
Iteration 601:
Training Loss: 5.501596927642822
Reconstruction Loss: -0.46200329065322876
Iteration 611:
Training Loss: 5.956698894500732
Reconstruction Loss: -0.46200358867645264
Iteration 621:
Training Loss: 5.424783706665039
Reconstruction Loss: -0.46200406551361084
Iteration 631:
Training Loss: 5.435293197631836
Reconstruction Loss: -0.4620043635368347
Iteration 641:
Training Loss: 4.987061977386475
Reconstruction Loss: -0.46200481057167053
Iteration 651:
Training Loss: 5.510595798492432
Reconstruction Loss: -0.46200528740882874
Iteration 661:
Training Loss: 5.000030517578125
Reconstruction Loss: -0.46200576424598694
Iteration 671:
Training Loss: 5.3462371826171875
Reconstruction Loss: -0.46200624108314514
Iteration 681:
Training Loss: 5.757503032684326
Reconstruction Loss: -0.46200689673423767
Iteration 691:
Training Loss: 5.695586204528809
Reconstruction Loss: -0.46200764179229736
Iteration 701:
Training Loss: 5.313847064971924
Reconstruction Loss: -0.4620082974433899
Iteration 711:
Training Loss: 4.9889020919799805
Reconstruction Loss: -0.4620092511177063
Iteration 721:
Training Loss: 5.184017658233643
Reconstruction Loss: -0.46201032400131226
Iteration 731:
Training Loss: 5.510430812835693
Reconstruction Loss: -0.46201133728027344
Iteration 741:
Training Loss: 5.028419494628906
Reconstruction Loss: -0.4620126485824585
Iteration 751:
Training Loss: 5.286873817443848
Reconstruction Loss: -0.46201419830322266
Iteration 761:
Training Loss: 4.891824722290039
Reconstruction Loss: -0.4620159864425659
Iteration 771:
Training Loss: 5.182765960693359
Reconstruction Loss: -0.4620181620121002
Iteration 781:
Training Loss: 5.493314743041992
Reconstruction Loss: -0.4620208144187927
Iteration 791:
Training Loss: 5.483961582183838
Reconstruction Loss: -0.46202412247657776
Iteration 801:
Training Loss: 5.869047164916992
Reconstruction Loss: -0.4620280861854553
Iteration 811:
Training Loss: 5.9227752685546875
Reconstruction Loss: -0.46203330159187317
Iteration 821:
Training Loss: 5.161417007446289
Reconstruction Loss: -0.46204012632369995
Iteration 831:
Training Loss: 5.652701377868652
Reconstruction Loss: -0.46204936504364014
Iteration 841:
Training Loss: 5.631304740905762
Reconstruction Loss: -0.46206262707710266
Iteration 851:
Training Loss: 5.5903801918029785
Reconstruction Loss: -0.4620821177959442
Iteration 861:
Training Loss: 6.054961681365967
Reconstruction Loss: -0.46211373805999756
Iteration 871:
Training Loss: 5.7409586906433105
Reconstruction Loss: -0.46217018365859985
Iteration 881:
Training Loss: 5.437796115875244
Reconstruction Loss: -0.46228885650634766
Iteration 891:
Training Loss: 5.637777328491211
Reconstruction Loss: -0.4626222252845764
Iteration 901:
Training Loss: 6.185205459594727
Reconstruction Loss: -0.4645436108112335
Iteration 911:
Training Loss: 5.053097248077393
Reconstruction Loss: -0.5978615880012512
Iteration 921:
Training Loss: 4.948524475097656
Reconstruction Loss: -0.5382527112960815
Iteration 931:
Training Loss: 4.8883957862854
Reconstruction Loss: -0.48885613679885864
Iteration 941:
Training Loss: 4.93972635269165
Reconstruction Loss: -0.5767223834991455
Iteration 951:
Training Loss: 5.3417558670043945
Reconstruction Loss: -0.463356077671051
Iteration 961:
Training Loss: 4.887938022613525
Reconstruction Loss: -0.42764368653297424
Iteration 971:
Training Loss: 5.34390926361084
Reconstruction Loss: -0.5066823959350586
Iteration 981:
Training Loss: 4.821779727935791
Reconstruction Loss: -0.4679333567619324
Iteration 991:
Training Loss: 4.779477119445801
Reconstruction Loss: -0.5914695262908936
Iteration 1001:
Training Loss: 5.166990756988525
Reconstruction Loss: -0.4749355912208557
Iteration 1011:
Training Loss: 4.948329448699951
Reconstruction Loss: -0.5389213562011719
Iteration 1021:
Training Loss: 5.144107818603516
Reconstruction Loss: -0.5663139820098877
Iteration 1031:
Training Loss: 5.479986667633057
Reconstruction Loss: -0.5781897306442261
Iteration 1041:
Training Loss: 4.814699172973633
Reconstruction Loss: -0.2905064821243286
Iteration 1051:
Training Loss: 5.006859302520752
Reconstruction Loss: -0.5971360802650452
Iteration 1061:
Training Loss: 4.98787784576416
Reconstruction Loss: -0.4786560535430908
Iteration 1071:
Training Loss: 5.221658706665039
Reconstruction Loss: -0.5712220072746277
Iteration 1081:
Training Loss: 4.994583606719971
Reconstruction Loss: -0.45810627937316895
Iteration 1091:
Training Loss: 4.922445774078369
Reconstruction Loss: -0.5095298290252686
Iteration 1101:
Training Loss: 5.254310607910156
Reconstruction Loss: -0.5752043724060059
Iteration 1111:
Training Loss: 4.740686416625977
Reconstruction Loss: -0.4466758370399475
Iteration 1121:
Training Loss: 4.924142837524414
Reconstruction Loss: -0.6107871532440186
Iteration 1131:
Training Loss: 4.581050872802734
Reconstruction Loss: -0.5553278923034668
Iteration 1141:
Training Loss: 4.974972724914551
Reconstruction Loss: -0.5125203132629395
Iteration 1151:
Training Loss: 4.943583011627197
Reconstruction Loss: -0.4770972728729248
Iteration 1161:
Training Loss: 4.714127540588379
Reconstruction Loss: -0.4716961979866028
Iteration 1171:
Training Loss: 4.999866962432861
Reconstruction Loss: -0.5577229857444763
Iteration 1181:
Training Loss: 5.126657962799072
Reconstruction Loss: -0.5799767971038818
Iteration 1191:
Training Loss: 4.736397743225098
Reconstruction Loss: -0.502911388874054
Iteration 1201:
Training Loss: 5.069644927978516
Reconstruction Loss: -0.48330166935920715
Iteration 1211:
Training Loss: 5.098005771636963
Reconstruction Loss: -0.42549169063568115
Iteration 1221:
Training Loss: 5.028226852416992
Reconstruction Loss: -0.605506420135498
Iteration 1231:
Training Loss: 5.198092937469482
Reconstruction Loss: -0.5678823590278625
Iteration 1241:
Training Loss: 4.489105224609375
Reconstruction Loss: -0.5718657374382019
Iteration 1251:
Training Loss: 4.601503372192383
Reconstruction Loss: -0.5951826572418213
Iteration 1261:
Training Loss: 4.752504348754883
Reconstruction Loss: -0.5794143080711365
Iteration 1271:
Training Loss: 5.346191883087158
Reconstruction Loss: -0.5706273317337036
Iteration 1281:
Training Loss: 5.260415077209473
Reconstruction Loss: -0.5549699068069458
Iteration 1291:
Training Loss: 5.260972023010254
Reconstruction Loss: -0.590314507484436
Iteration 1301:
Training Loss: 4.675179958343506
Reconstruction Loss: -0.5711323022842407
Iteration 1311:
Training Loss: 5.083645343780518
Reconstruction Loss: -0.5793907046318054
Iteration 1321:
Training Loss: 5.203365325927734
Reconstruction Loss: -0.4860432744026184
Iteration 1331:
Training Loss: 5.1208720207214355
Reconstruction Loss: -0.4838462173938751
Iteration 1341:
Training Loss: 5.340437412261963
Reconstruction Loss: -0.4953998923301697
Iteration 1351:
Training Loss: 5.21533203125
Reconstruction Loss: -0.3648589551448822
Iteration 1361:
Training Loss: 5.394534111022949
Reconstruction Loss: -0.1095413938164711
Iteration 1371:
Training Loss: 5.021085739135742
Reconstruction Loss: -0.4675063192844391
Iteration 1381:
Training Loss: 4.975675582885742
Reconstruction Loss: -0.4315967559814453
Iteration 1391:
Training Loss: 5.293780326843262
Reconstruction Loss: -0.46977078914642334
Iteration 1401:
Training Loss: 5.123066425323486
Reconstruction Loss: -0.5863156318664551
Iteration 1411:
Training Loss: 4.9398722648620605
Reconstruction Loss: -0.25834107398986816
Iteration 1421:
Training Loss: 4.837707042694092
Reconstruction Loss: -0.49973851442337036
Iteration 1431:
Training Loss: 5.023545265197754
Reconstruction Loss: -0.6110386848449707
Iteration 1441:
Training Loss: 5.19605016708374
Reconstruction Loss: -0.41475310921669006
Iteration 1451:
Training Loss: 5.066154479980469
Reconstruction Loss: -0.4246653616428375
Iteration 1461:
Training Loss: 5.032378196716309
Reconstruction Loss: -0.5755132436752319
Iteration 1471:
Training Loss: 4.464806079864502
Reconstruction Loss: -0.5486071705818176
Iteration 1481:
Training Loss: 5.068982124328613
Reconstruction Loss: -0.5842887163162231
Iteration 1491:
Training Loss: 4.552933216094971
Reconstruction Loss: -0.5389038324356079
Iteration 1501:
Training Loss: 5.034513473510742
Reconstruction Loss: -0.4944974184036255
Iteration 1511:
Training Loss: 5.0941267013549805
Reconstruction Loss: -0.559978187084198
Iteration 1521:
Training Loss: 5.019894123077393
Reconstruction Loss: -0.5622029900550842
Iteration 1531:
Training Loss: 5.056750774383545
Reconstruction Loss: -0.48002395033836365
Iteration 1541:
Training Loss: 5.27583122253418
Reconstruction Loss: -0.559872567653656
Iteration 1551:
Training Loss: 4.32942008972168
Reconstruction Loss: -0.558158278465271
Iteration 1561:
Training Loss: 5.331971645355225
Reconstruction Loss: -0.5778157711029053
Iteration 1571:
Training Loss: 4.917747974395752
Reconstruction Loss: -0.5864989161491394
Iteration 1581:
Training Loss: 4.978179454803467
Reconstruction Loss: -0.5012428164482117
Iteration 1591:
Training Loss: 4.65202522277832
Reconstruction Loss: -0.6612151265144348
Iteration 1601:
Training Loss: 4.326752185821533
Reconstruction Loss: -0.7974001169204712
Iteration 1611:
Training Loss: 4.663949012756348
Reconstruction Loss: -0.7268392443656921
Iteration 1621:
Training Loss: 4.5843706130981445
Reconstruction Loss: -0.7250791788101196
Iteration 1631:
Training Loss: 4.398629665374756
Reconstruction Loss: -0.7499386072158813
Iteration 1641:
Training Loss: 4.75576114654541
Reconstruction Loss: -0.7757236957550049
Iteration 1651:
Training Loss: 4.569489479064941
Reconstruction Loss: -0.7020276188850403
Iteration 1661:
Training Loss: 4.314788341522217
Reconstruction Loss: -0.7994574904441833
Iteration 1671:
Training Loss: 4.43693733215332
Reconstruction Loss: -0.8064007759094238
Iteration 1681:
Training Loss: 4.4496378898620605
Reconstruction Loss: -0.7411572933197021
Iteration 1691:
Training Loss: 4.235330104827881
Reconstruction Loss: -0.7709352374076843
Iteration 1701:
Training Loss: 4.6528639793396
Reconstruction Loss: -0.6937007904052734
Iteration 1711:
Training Loss: 4.48098611831665
Reconstruction Loss: -0.7336534857749939
Iteration 1721:
Training Loss: 4.3684611320495605
Reconstruction Loss: -0.765472948551178
Iteration 1731:
Training Loss: 4.712718486785889
Reconstruction Loss: -0.5711592435836792
Iteration 1741:
Training Loss: 4.50005578994751
Reconstruction Loss: -0.6475633382797241
Iteration 1751:
Training Loss: 4.348587989807129
Reconstruction Loss: -0.7661522626876831
Iteration 1761:
Training Loss: 4.515772342681885
Reconstruction Loss: -0.7157039642333984
Iteration 1771:
Training Loss: 4.7162957191467285
Reconstruction Loss: -0.715556263923645
Iteration 1781:
Training Loss: 4.838217735290527
Reconstruction Loss: -0.6790905594825745
Iteration 1791:
Training Loss: 4.133367538452148
Reconstruction Loss: -0.7851404547691345
Iteration 1801:
Training Loss: 4.309856414794922
Reconstruction Loss: -0.7140213251113892
Iteration 1811:
Training Loss: 4.515770435333252
Reconstruction Loss: -0.6952439546585083
Iteration 1821:
Training Loss: 4.671962738037109
Reconstruction Loss: -0.676115095615387
Iteration 1831:
Training Loss: 4.911340713500977
Reconstruction Loss: -0.7962837815284729
Iteration 1841:
Training Loss: 4.760250568389893
Reconstruction Loss: -0.662261426448822
Iteration 1851:
Training Loss: 4.2482476234436035
Reconstruction Loss: -0.7763379216194153
Iteration 1861:
Training Loss: 4.427762508392334
Reconstruction Loss: -0.718533456325531
Iteration 1871:
Training Loss: 4.613391399383545
Reconstruction Loss: -0.7785702347755432
Iteration 1881:
Training Loss: 4.429774761199951
Reconstruction Loss: -0.697829008102417
Iteration 1891:
Training Loss: 3.987717628479004
Reconstruction Loss: -0.7796252369880676
Iteration 1901:
Training Loss: 4.467578887939453
Reconstruction Loss: -0.7678908705711365
Iteration 1911:
Training Loss: 4.617351531982422
Reconstruction Loss: -0.7043451070785522
Iteration 1921:
Training Loss: 4.786438465118408
Reconstruction Loss: -0.7852432727813721
Iteration 1931:
Training Loss: 4.642602920532227
Reconstruction Loss: -0.7196481227874756
Iteration 1941:
Training Loss: 4.40503454208374
Reconstruction Loss: -0.5668131709098816
Iteration 1951:
Training Loss: 4.161120414733887
Reconstruction Loss: -0.7425693869590759
Iteration 1961:
Training Loss: 4.376487731933594
Reconstruction Loss: -0.7454703450202942
Iteration 1971:
Training Loss: 4.863991737365723
Reconstruction Loss: -0.6723625063896179
Iteration 1981:
Training Loss: 4.502418518066406
Reconstruction Loss: -0.8056856989860535
Iteration 1991:
Training Loss: 4.732913494110107
Reconstruction Loss: -0.7887488603591919
Iteration 2001:
Training Loss: 4.53682279586792
Reconstruction Loss: -0.7176246643066406
Iteration 2011:
Training Loss: 4.6013288497924805
Reconstruction Loss: -0.7691394686698914
Iteration 2021:
Training Loss: 4.408535957336426
Reconstruction Loss: -0.7408609986305237
Iteration 2031:
Training Loss: 4.422054767608643
Reconstruction Loss: -0.6896928548812866
Iteration 2041:
Training Loss: 4.667928695678711
Reconstruction Loss: -0.7093663215637207
Iteration 2051:
Training Loss: 4.432668209075928
Reconstruction Loss: -0.7301389575004578
Iteration 2061:
Training Loss: 4.637877941131592
Reconstruction Loss: -0.6291552782058716
Iteration 2071:
Training Loss: 4.766687393188477
Reconstruction Loss: -0.7345211505889893
Iteration 2081:
Training Loss: 4.614088535308838
Reconstruction Loss: -0.7441654801368713
Iteration 2091:
Training Loss: 4.520346641540527
Reconstruction Loss: -0.7972391843795776
Iteration 2101:
Training Loss: 4.0809197425842285
Reconstruction Loss: -0.7563153505325317
Iteration 2111:
Training Loss: 4.405071258544922
Reconstruction Loss: -0.7326015830039978
Iteration 2121:
Training Loss: 4.667746067047119
Reconstruction Loss: -0.683646559715271
Iteration 2131:
Training Loss: 4.471179962158203
Reconstruction Loss: -0.827093780040741
Iteration 2141:
Training Loss: 4.659520626068115
Reconstruction Loss: -0.7130957245826721
Iteration 2151:
Training Loss: 4.315824031829834
Reconstruction Loss: -0.7868841886520386
Iteration 2161:
Training Loss: 4.524884223937988
Reconstruction Loss: -0.7061752676963806
Iteration 2171:
Training Loss: 4.191360950469971
Reconstruction Loss: -0.6825910806655884
Iteration 2181:
Training Loss: 4.448666572570801
Reconstruction Loss: -0.6657936573028564
Iteration 2191:
Training Loss: 4.4879069328308105
Reconstruction Loss: -0.778723418712616
Iteration 2201:
Training Loss: 4.495619297027588
Reconstruction Loss: -0.7007544636726379
Iteration 2211:
Training Loss: 4.391849517822266
Reconstruction Loss: -0.7875756621360779
Iteration 2221:
Training Loss: 4.275424003601074
Reconstruction Loss: -0.7073445320129395
Iteration 2231:
Training Loss: 4.732388019561768
Reconstruction Loss: -0.7282649874687195
Iteration 2241:
Training Loss: 4.684103965759277
Reconstruction Loss: -0.7259805798530579
Iteration 2251:
Training Loss: 4.581047534942627
Reconstruction Loss: -0.7595083117485046
Iteration 2261:
Training Loss: 4.560150623321533
Reconstruction Loss: -0.7619752883911133
Iteration 2271:
Training Loss: 4.352054595947266
Reconstruction Loss: -0.7089003324508667
Iteration 2281:
Training Loss: 4.573491096496582
Reconstruction Loss: -0.7475134134292603
Iteration 2291:
Training Loss: 4.569849967956543
Reconstruction Loss: -0.7497870326042175
Iteration 2301:
Training Loss: 4.401680946350098
Reconstruction Loss: -0.7611730694770813
Iteration 2311:
Training Loss: 4.930015563964844
Reconstruction Loss: -0.5966057777404785
Iteration 2321:
Training Loss: 4.671510696411133
Reconstruction Loss: -0.7195103168487549
Iteration 2331:
Training Loss: 4.579649925231934
Reconstruction Loss: -0.7502708435058594
Iteration 2341:
Training Loss: 4.39671516418457
Reconstruction Loss: -0.6396916508674622
Iteration 2351:
Training Loss: 4.760148048400879
Reconstruction Loss: -0.8152363896369934
Iteration 2361:
Training Loss: 4.441781997680664
Reconstruction Loss: -0.7755222320556641
Iteration 2371:
Training Loss: 4.093649864196777
Reconstruction Loss: -0.793483316898346
Iteration 2381:
Training Loss: 4.305690765380859
Reconstruction Loss: -0.670404314994812
Iteration 2391:
Training Loss: 4.489755630493164
Reconstruction Loss: -0.7688076496124268
Iteration 2401:
Training Loss: 4.793421745300293
Reconstruction Loss: -0.5548197031021118
Iteration 2411:
Training Loss: 4.398495197296143
Reconstruction Loss: -0.7760691046714783
Iteration 2421:
Training Loss: 4.248952388763428
Reconstruction Loss: -0.760985255241394
Iteration 2431:
Training Loss: 4.446720600128174
Reconstruction Loss: -0.7412832975387573
Iteration 2441:
Training Loss: 4.102034568786621
Reconstruction Loss: -0.7798786163330078
Iteration 2451:
Training Loss: 4.627111911773682
Reconstruction Loss: -0.7627273797988892
Iteration 2461:
Training Loss: 4.81292724609375
Reconstruction Loss: -0.7187075614929199
Iteration 2471:
Training Loss: 4.6814351081848145
Reconstruction Loss: -0.6820873022079468
Iteration 2481:
Training Loss: 4.888275146484375
Reconstruction Loss: -0.7861020565032959
Iteration 2491:
Training Loss: 4.6786699295043945
Reconstruction Loss: -0.7466022968292236
Iteration 2501:
Training Loss: 4.312299728393555
Reconstruction Loss: -0.7577074766159058
Iteration 2511:
Training Loss: 4.7203898429870605
Reconstruction Loss: -0.7807111144065857
Iteration 2521:
Training Loss: 4.3806257247924805
Reconstruction Loss: -0.8034995794296265
Iteration 2531:
Training Loss: 4.594333648681641
Reconstruction Loss: -0.7770593762397766
Iteration 2541:
Training Loss: 4.498710632324219
Reconstruction Loss: -0.7751187086105347
Iteration 2551:
Training Loss: 4.692286491394043
Reconstruction Loss: -0.7768480181694031
Iteration 2561:
Training Loss: 4.741247653961182
Reconstruction Loss: -0.7324446439743042
Iteration 2571:
Training Loss: 4.641666412353516
Reconstruction Loss: -0.7936119437217712
Iteration 2581:
Training Loss: 4.61149263381958
Reconstruction Loss: -0.7237386703491211
Iteration 2591:
Training Loss: 4.429927825927734
Reconstruction Loss: -0.730654239654541
Iteration 2601:
Training Loss: 4.600708961486816
Reconstruction Loss: -0.6634243130683899
Iteration 2611:
Training Loss: 4.665500164031982
Reconstruction Loss: -0.7316447496414185
Iteration 2621:
Training Loss: 4.488747596740723
Reconstruction Loss: -0.6666342616081238
Iteration 2631:
Training Loss: 4.5677971839904785
Reconstruction Loss: -0.6720563173294067
Iteration 2641:
Training Loss: 4.480083465576172
Reconstruction Loss: -0.7308162450790405
Iteration 2651:
Training Loss: 4.094080924987793
Reconstruction Loss: -0.7065885663032532
Iteration 2661:
Training Loss: 4.204408645629883
Reconstruction Loss: -0.7667402625083923
Iteration 2671:
Training Loss: 4.459174633026123
Reconstruction Loss: -0.7545070052146912
Iteration 2681:
Training Loss: 4.454076290130615
Reconstruction Loss: -0.7377418875694275
Iteration 2691:
Training Loss: 4.69925594329834
Reconstruction Loss: -0.7566346526145935
Iteration 2701:
Training Loss: 4.625303268432617
Reconstruction Loss: -0.7712774276733398
Iteration 2711:
Training Loss: 4.554284572601318
Reconstruction Loss: -0.7380335330963135
Iteration 2721:
Training Loss: 4.311363220214844
Reconstruction Loss: -0.786159873008728
Iteration 2731:
Training Loss: 4.373956203460693
Reconstruction Loss: -0.7245319485664368
Iteration 2741:
Training Loss: 4.5673089027404785
Reconstruction Loss: -0.7829007506370544
Iteration 2751:
Training Loss: 4.620955944061279
Reconstruction Loss: -0.7493860721588135
Iteration 2761:
Training Loss: 4.649111747741699
Reconstruction Loss: -0.7653139233589172
Iteration 2771:
Training Loss: 4.496076583862305
Reconstruction Loss: -0.6366837024688721
Iteration 2781:
Training Loss: 4.674034595489502
Reconstruction Loss: -0.7459027767181396
Iteration 2791:
Training Loss: 4.061102867126465
Reconstruction Loss: -0.7174062728881836
Iteration 2801:
Training Loss: 4.499747276306152
Reconstruction Loss: -0.769868016242981
Iteration 2811:
Training Loss: 4.84528923034668
Reconstruction Loss: -0.6417895555496216
Iteration 2821:
Training Loss: 4.573493003845215
Reconstruction Loss: -0.7706359624862671
Iteration 2831:
Training Loss: 4.589508056640625
Reconstruction Loss: -0.7338670492172241
Iteration 2841:
Training Loss: 3.94755220413208
Reconstruction Loss: -0.7333382368087769
Iteration 2851:
Training Loss: 4.69914436340332
Reconstruction Loss: -0.7638372182846069
Iteration 2861:
Training Loss: 4.798439025878906
Reconstruction Loss: -0.7703386545181274
Iteration 2871:
Training Loss: 4.485443592071533
Reconstruction Loss: -0.7527666687965393
Iteration 2881:
Training Loss: 4.265345096588135
Reconstruction Loss: -0.7260823249816895
Iteration 2891:
Training Loss: 4.391910552978516
Reconstruction Loss: -0.7901080846786499
Iteration 2901:
Training Loss: 4.680585861206055
Reconstruction Loss: -0.7142731547355652
Iteration 2911:
Training Loss: 4.88654088973999
Reconstruction Loss: -0.7537054419517517
Iteration 2921:
Training Loss: 4.427487373352051
Reconstruction Loss: -0.740428626537323
Iteration 2931:
Training Loss: 4.090982913970947
Reconstruction Loss: -0.7806439399719238
Iteration 2941:
Training Loss: 4.460895538330078
Reconstruction Loss: -0.7517070770263672
Iteration 2951:
Training Loss: 4.254915237426758
Reconstruction Loss: -0.7277170419692993
Iteration 2961:
Training Loss: 4.407043933868408
Reconstruction Loss: -0.7389894723892212
Iteration 2971:
Training Loss: 4.873867988586426
Reconstruction Loss: -0.6165046095848083
Iteration 2981:
Training Loss: 4.243955612182617
Reconstruction Loss: -0.7405596971511841
Iteration 2991:
Training Loss: 4.957278251647949
Reconstruction Loss: -0.6141436100006104
Iteration 3001:
Training Loss: 4.132324695587158
Reconstruction Loss: -0.6495072841644287
Iteration 3011:
Training Loss: 4.747450828552246
Reconstruction Loss: -0.7018089294433594
Iteration 3021:
Training Loss: 4.26917839050293
Reconstruction Loss: -0.7154361009597778
Iteration 3031:
Training Loss: 4.7458391189575195
Reconstruction Loss: -0.782897412776947
Iteration 3041:
Training Loss: 4.640531539916992
Reconstruction Loss: -0.7601965069770813
Iteration 3051:
Training Loss: 4.455417633056641
Reconstruction Loss: -0.70751953125
Iteration 3061:
Training Loss: 4.416907787322998
Reconstruction Loss: -0.7829747200012207
Iteration 3071:
Training Loss: 4.5920939445495605
Reconstruction Loss: -0.7846186757087708
Iteration 3081:
Training Loss: 4.572494029998779
Reconstruction Loss: -0.8066005706787109
Iteration 3091:
Training Loss: 4.415921211242676
Reconstruction Loss: -0.8128622770309448
Iteration 3101:
Training Loss: 4.719917297363281
Reconstruction Loss: -0.7250342965126038
Iteration 3111:
Training Loss: 4.328927516937256
Reconstruction Loss: -0.767862856388092
Iteration 3121:
Training Loss: 4.9540019035339355
Reconstruction Loss: -0.6415929794311523
Iteration 3131:
Training Loss: 4.62845516204834
Reconstruction Loss: -0.7415006756782532
Iteration 3141:
Training Loss: 4.852060317993164
Reconstruction Loss: -0.8054535388946533
Iteration 3151:
Training Loss: 4.646241188049316
Reconstruction Loss: -0.7451366782188416
Iteration 3161:
Training Loss: 4.495047569274902
Reconstruction Loss: -0.7353538870811462
Iteration 3171:
Training Loss: 4.361746788024902
Reconstruction Loss: -0.7695845365524292
Iteration 3181:
Training Loss: 4.656373500823975
Reconstruction Loss: -0.757919430732727
Iteration 3191:
Training Loss: 4.2377543449401855
Reconstruction Loss: -0.7733327150344849
Iteration 3201:
Training Loss: 4.453451633453369
Reconstruction Loss: -0.7106553912162781
Iteration 3211:
Training Loss: 4.511439323425293
Reconstruction Loss: -0.640590250492096
Iteration 3221:
Training Loss: 4.092410564422607
Reconstruction Loss: -0.7472071051597595
Iteration 3231:
Training Loss: 3.983011484146118
Reconstruction Loss: -0.8120755553245544
Iteration 3241:
Training Loss: 4.593236446380615
Reconstruction Loss: -0.7527732253074646
Iteration 3251:
Training Loss: 4.7039666175842285
Reconstruction Loss: -0.7919805645942688
Iteration 3261:
Training Loss: 4.030950546264648
Reconstruction Loss: -0.6759600639343262
Iteration 3271:
Training Loss: 4.704796314239502
Reconstruction Loss: -0.7883668541908264
Iteration 3281:
Training Loss: 4.594708442687988
Reconstruction Loss: -0.7979881763458252
Iteration 3291:
Training Loss: 4.4824652671813965
Reconstruction Loss: -0.734598696231842
Iteration 3301:
Training Loss: 4.599684238433838
Reconstruction Loss: -0.6673352122306824
Iteration 3311:
Training Loss: 4.654068946838379
Reconstruction Loss: -0.7442395687103271
Iteration 3321:
Training Loss: 4.211180210113525
Reconstruction Loss: -0.7563049793243408
Iteration 3331:
Training Loss: 4.661951541900635
Reconstruction Loss: -0.7929796576499939
Iteration 3341:
Training Loss: 4.735960006713867
Reconstruction Loss: -0.6197786331176758
Iteration 3351:
Training Loss: 4.4153008460998535
Reconstruction Loss: -0.7589725852012634
Iteration 3361:
Training Loss: 3.983896493911743
Reconstruction Loss: -0.7161951065063477
Iteration 3371:
Training Loss: 4.209467887878418
Reconstruction Loss: -0.741356372833252
Iteration 3381:
Training Loss: 4.137154579162598
Reconstruction Loss: -0.7563597559928894
Iteration 3391:
Training Loss: 4.542166709899902
Reconstruction Loss: -0.7676985859870911
Iteration 3401:
Training Loss: 4.718476295471191
Reconstruction Loss: -0.7493391036987305
Iteration 3411:
Training Loss: 4.204108715057373
Reconstruction Loss: -0.6821399927139282
Iteration 3421:
Training Loss: 4.750908851623535
Reconstruction Loss: -0.6342063546180725
Iteration 3431:
Training Loss: 3.8972346782684326
Reconstruction Loss: -0.7942728400230408
Iteration 3441:
Training Loss: 4.595893383026123
Reconstruction Loss: -0.7966182231903076
Iteration 3451:
Training Loss: 4.302154064178467
Reconstruction Loss: -0.6832320094108582
Iteration 3461:
Training Loss: 4.443410396575928
Reconstruction Loss: -0.7677848935127258
Iteration 3471:
Training Loss: 4.611629486083984
Reconstruction Loss: -0.7416068315505981
Iteration 3481:
Training Loss: 3.942805528640747
Reconstruction Loss: -0.6825620532035828
Iteration 3491:
Training Loss: 4.29079532623291
Reconstruction Loss: -0.7711751461029053
Iteration 3501:
Training Loss: 4.892012119293213
Reconstruction Loss: -0.7056029438972473
Iteration 3511:
Training Loss: 4.379271030426025
Reconstruction Loss: -0.7128888964653015
Iteration 3521:
Training Loss: 4.7276740074157715
Reconstruction Loss: -0.7828478217124939
Iteration 3531:
Training Loss: 4.814044952392578
Reconstruction Loss: -0.770846962928772
Iteration 3541:
Training Loss: 4.79046630859375
Reconstruction Loss: -0.7766685485839844
Iteration 3551:
Training Loss: 4.492238998413086
Reconstruction Loss: -0.6854978799819946
Iteration 3561:
Training Loss: 4.496021270751953
Reconstruction Loss: -0.7295258641242981
Iteration 3571:
Training Loss: 4.515213966369629
Reconstruction Loss: -0.7609388828277588
Iteration 3581:
Training Loss: 4.754963397979736
Reconstruction Loss: -0.6430503726005554
Iteration 3591:
Training Loss: 4.6293463706970215
Reconstruction Loss: -0.7998514175415039
Iteration 3601:
Training Loss: 4.484906196594238
Reconstruction Loss: -0.7883976697921753
Iteration 3611:
Training Loss: 4.353769302368164
Reconstruction Loss: -0.7735903859138489
Iteration 3621:
Training Loss: 4.579289436340332
Reconstruction Loss: -0.728728175163269
Iteration 3631:
Training Loss: 4.293605804443359
Reconstruction Loss: -0.6636555194854736
Iteration 3641:
Training Loss: 4.37496280670166
Reconstruction Loss: -0.680819034576416
Iteration 3651:
Training Loss: 4.666146278381348
Reconstruction Loss: -0.7589695453643799
Iteration 3661:
Training Loss: 4.952462196350098
Reconstruction Loss: -0.5454232692718506
Iteration 3671:
Training Loss: 4.731376647949219
Reconstruction Loss: -0.754206657409668
Iteration 3681:
Training Loss: 4.4244794845581055
Reconstruction Loss: -0.7509479522705078
Iteration 3691:
Training Loss: 4.562414169311523
Reconstruction Loss: -0.7659008502960205
Iteration 3701:
Training Loss: 4.719668865203857
Reconstruction Loss: -0.7843631505966187
Iteration 3711:
Training Loss: 4.4750142097473145
Reconstruction Loss: -0.7197077870368958
Iteration 3721:
Training Loss: 4.576459884643555
Reconstruction Loss: -0.7559071183204651
Iteration 3731:
Training Loss: 4.221678733825684
Reconstruction Loss: -0.7543955445289612
Iteration 3741:
Training Loss: 4.325723171234131
Reconstruction Loss: -0.7658977508544922
Iteration 3751:
Training Loss: 4.659029006958008
Reconstruction Loss: -0.7775036692619324
Iteration 3761:
Training Loss: 4.8390021324157715
Reconstruction Loss: -0.8023198246955872
Iteration 3771:
Training Loss: 4.789161205291748
Reconstruction Loss: -0.741113543510437
Iteration 3781:
Training Loss: 4.5404052734375
Reconstruction Loss: -0.7752991914749146
Iteration 3791:
Training Loss: 4.657044410705566
Reconstruction Loss: -0.7641294002532959
Iteration 3801:
Training Loss: 4.4610724449157715
Reconstruction Loss: -0.7159377932548523
Iteration 3811:
Training Loss: 4.0979719161987305
Reconstruction Loss: -0.7726172208786011
Iteration 3821:
Training Loss: 4.667343616485596
Reconstruction Loss: -0.7703421115875244
Iteration 3831:
Training Loss: 4.034904956817627
Reconstruction Loss: -0.782770037651062
Iteration 3841:
Training Loss: 4.5123701095581055
Reconstruction Loss: -0.7803915739059448
Iteration 3851:
Training Loss: 4.693095684051514
Reconstruction Loss: -0.7022649645805359
Iteration 3861:
Training Loss: 4.791715621948242
Reconstruction Loss: -0.7893819212913513
Iteration 3871:
Training Loss: 4.141971111297607
Reconstruction Loss: -0.7625698447227478
Iteration 3881:
Training Loss: 4.697795867919922
Reconstruction Loss: -0.7731145620346069
Iteration 3891:
Training Loss: 4.432554721832275
Reconstruction Loss: -0.7765762209892273
Iteration 3901:
Training Loss: 4.792277812957764
Reconstruction Loss: -0.6764497756958008
Iteration 3911:
Training Loss: 4.579386234283447
Reconstruction Loss: -0.7081599831581116
Iteration 3921:
Training Loss: 4.78695821762085
Reconstruction Loss: -0.8177496194839478
Iteration 3931:
Training Loss: 4.80388069152832
Reconstruction Loss: -0.7958589196205139
Iteration 3941:
Training Loss: 4.219973564147949
Reconstruction Loss: -0.807945191860199
Iteration 3951:
Training Loss: 4.052828788757324
Reconstruction Loss: -0.7477948069572449
Iteration 3961:
Training Loss: 4.871129035949707
Reconstruction Loss: -0.7108383774757385
Iteration 3971:
Training Loss: 4.188130855560303
Reconstruction Loss: -0.7102253437042236
Iteration 3981:
Training Loss: 4.609243869781494
Reconstruction Loss: -0.7901496291160583
Iteration 3991:
Training Loss: 4.6305766105651855
Reconstruction Loss: -0.6126511693000793
Iteration 4001:
Training Loss: 4.734127998352051
Reconstruction Loss: -0.7081644535064697
Iteration 4011:
Training Loss: 4.445736885070801
Reconstruction Loss: -0.6895649433135986
Iteration 4021:
Training Loss: 4.546724796295166
Reconstruction Loss: -0.7518086433410645
Iteration 4031:
Training Loss: 4.775155544281006
Reconstruction Loss: -0.8102577328681946
Iteration 4041:
Training Loss: 3.994727849960327
Reconstruction Loss: -0.7796780467033386
Iteration 4051:
Training Loss: 4.160358428955078
Reconstruction Loss: -0.762147068977356
Iteration 4061:
Training Loss: 3.997511148452759
Reconstruction Loss: -0.7761437296867371
Iteration 4071:
Training Loss: 4.797760009765625
Reconstruction Loss: -0.756537139415741
Iteration 4081:
Training Loss: 4.159994125366211
Reconstruction Loss: -0.7791916131973267
Iteration 4091:
Training Loss: 4.320645809173584
Reconstruction Loss: -0.5813103914260864
Iteration 4101:
Training Loss: 4.3746843338012695
Reconstruction Loss: -0.7513601183891296
Iteration 4111:
Training Loss: 4.663975715637207
Reconstruction Loss: -0.7882296442985535
Iteration 4121:
Training Loss: 4.656971454620361
Reconstruction Loss: -0.77479088306427
Iteration 4131:
Training Loss: 4.592595100402832
Reconstruction Loss: -0.6169445514678955
Iteration 4141:
Training Loss: 4.625796794891357
Reconstruction Loss: -0.7280282974243164
Iteration 4151:
Training Loss: 4.589480876922607
Reconstruction Loss: -0.6945502758026123
Iteration 4161:
Training Loss: 4.521019458770752
Reconstruction Loss: -0.6887689232826233
Iteration 4171:
Training Loss: 4.99247932434082
Reconstruction Loss: -0.7799856066703796
Iteration 4181:
Training Loss: 4.340363025665283
Reconstruction Loss: -0.7043850421905518
Iteration 4191:
Training Loss: 4.6311421394348145
Reconstruction Loss: -0.7826863527297974
Iteration 4201:
Training Loss: 4.6826677322387695
Reconstruction Loss: -0.7173371911048889
Iteration 4211:
Training Loss: 4.6777215003967285
Reconstruction Loss: -0.6719916462898254
Iteration 4221:
Training Loss: 4.387104511260986
Reconstruction Loss: -0.7707462906837463
Iteration 4231:
Training Loss: 4.471262454986572
Reconstruction Loss: -0.6303083300590515
Iteration 4241:
Training Loss: 4.474620342254639
Reconstruction Loss: -0.6520931720733643
Iteration 4251:
Training Loss: 4.233712196350098
Reconstruction Loss: -0.7689526081085205
Iteration 4261:
Training Loss: 4.620007038116455
Reconstruction Loss: -0.7716243267059326
Iteration 4271:
Training Loss: 4.376338481903076
Reconstruction Loss: -0.6912233233451843
Iteration 4281:
Training Loss: 4.326113224029541
Reconstruction Loss: -0.7327398657798767
Iteration 4291:
Training Loss: 4.365336894989014
Reconstruction Loss: -0.7366481423377991
Iteration 4301:
Training Loss: 4.794034004211426
Reconstruction Loss: -0.7275818586349487
Iteration 4311:
Training Loss: 4.307845592498779
Reconstruction Loss: -0.8043942451477051
Iteration 4321:
Training Loss: 4.588717937469482
Reconstruction Loss: -0.7040500640869141
Iteration 4331:
Training Loss: 4.340906143188477
Reconstruction Loss: -0.7194585204124451
Iteration 4341:
Training Loss: 4.590632915496826
Reconstruction Loss: -0.6911407709121704
Iteration 4351:
Training Loss: 3.8694586753845215
Reconstruction Loss: -0.7429269552230835
Iteration 4361:
Training Loss: 5.139241695404053
Reconstruction Loss: -0.45957210659980774
Iteration 4371:
Training Loss: 4.618721008300781
Reconstruction Loss: -0.7917348742485046
Iteration 4381:
Training Loss: 4.618503093719482
Reconstruction Loss: -0.7954972982406616
Iteration 4391:
Training Loss: 4.3964128494262695
Reconstruction Loss: -0.7849376201629639
Iteration 4401:
Training Loss: 4.142976760864258
Reconstruction Loss: -0.7843753099441528
Iteration 4411:
Training Loss: 4.747039794921875
Reconstruction Loss: -0.7987393736839294
Iteration 4421:
Training Loss: 4.703719615936279
Reconstruction Loss: -0.6693403720855713
Iteration 4431:
Training Loss: 4.7680864334106445
Reconstruction Loss: -0.696905791759491
Iteration 4441:
Training Loss: 4.429184913635254
Reconstruction Loss: -0.6885063052177429
Iteration 4451:
Training Loss: 4.543723106384277
Reconstruction Loss: -0.7511224150657654
Iteration 4461:
Training Loss: 4.396726608276367
Reconstruction Loss: -0.7850112318992615
Iteration 4471:
Training Loss: 4.535691738128662
Reconstruction Loss: -0.7370789051055908
Iteration 4481:
Training Loss: 4.282995223999023
Reconstruction Loss: -0.7867498993873596
Iteration 4491:
Training Loss: 4.3190598487854
Reconstruction Loss: -0.7672094106674194
Iteration 4501:
Training Loss: 4.675317764282227
Reconstruction Loss: -0.7888432145118713
Iteration 4511:
Training Loss: 4.5998969078063965
Reconstruction Loss: -0.6954831480979919
Iteration 4521:
Training Loss: 4.318365573883057
Reconstruction Loss: -0.7746217250823975
Iteration 4531:
Training Loss: 3.8121871948242188
Reconstruction Loss: -0.7614147067070007
Iteration 4541:
Training Loss: 4.214107990264893
Reconstruction Loss: -0.7878892421722412
Iteration 4551:
Training Loss: 4.40857458114624
Reconstruction Loss: -0.7362649440765381
Iteration 4561:
Training Loss: 4.960520267486572
Reconstruction Loss: -0.8052688837051392
Iteration 4571:
Training Loss: 4.448282241821289
Reconstruction Loss: -0.7354834675788879
Iteration 4581:
Training Loss: 4.657908916473389
Reconstruction Loss: -0.7901426553726196
Iteration 4591:
Training Loss: 4.252319812774658
Reconstruction Loss: -0.724057137966156
Iteration 4601:
Training Loss: 4.340688228607178
Reconstruction Loss: -0.7357088923454285
Iteration 4611:
Training Loss: 4.443635940551758
Reconstruction Loss: -0.7553650140762329
Iteration 4621:
Training Loss: 4.430898666381836
Reconstruction Loss: -0.610640823841095
Iteration 4631:
Training Loss: 4.28857946395874
Reconstruction Loss: -0.6101609468460083
Iteration 4641:
Training Loss: 4.794790267944336
Reconstruction Loss: -0.6950393319129944
Iteration 4651:
Training Loss: 4.565742015838623
Reconstruction Loss: -0.7622372508049011
Iteration 4661:
Training Loss: 4.255287170410156
Reconstruction Loss: -0.7289367914199829
Iteration 4671:
Training Loss: 4.973428249359131
Reconstruction Loss: -0.575262725353241
Iteration 4681:
Training Loss: 4.33248233795166
Reconstruction Loss: -0.73931884765625
Iteration 4691:
Training Loss: 4.352507591247559
Reconstruction Loss: -0.7318173050880432
Iteration 4701:
Training Loss: 3.985837936401367
Reconstruction Loss: -0.6967179179191589
Iteration 4711:
Training Loss: 4.896711826324463
Reconstruction Loss: -0.7044724225997925
Iteration 4721:
Training Loss: 4.468759536743164
Reconstruction Loss: -0.7051429748535156
Iteration 4731:
Training Loss: 4.491204738616943
Reconstruction Loss: -0.7298485040664673
Iteration 4741:
Training Loss: 4.366940975189209
Reconstruction Loss: -0.7776308059692383
Iteration 4751:
Training Loss: 4.722585201263428
Reconstruction Loss: -0.7614877820014954
Iteration 4761:
Training Loss: 4.664895534515381
Reconstruction Loss: -0.7155347466468811
Iteration 4771:
Training Loss: 4.825415134429932
Reconstruction Loss: -0.7634959816932678
Iteration 4781:
Training Loss: 4.117583751678467
Reconstruction Loss: -0.7107136249542236
Iteration 4791:
Training Loss: 4.117400646209717
Reconstruction Loss: -0.7409433722496033
Iteration 4801:
Training Loss: 4.284435272216797
Reconstruction Loss: -0.7600362300872803
Iteration 4811:
Training Loss: 4.559061527252197
Reconstruction Loss: -0.7506992816925049
Iteration 4821:
Training Loss: 4.3823161125183105
Reconstruction Loss: -0.6631720662117004
Iteration 4831:
Training Loss: 4.575207233428955
Reconstruction Loss: -0.7288398146629333
Iteration 4841:
Training Loss: 4.36046838760376
Reconstruction Loss: -0.6330808997154236
Iteration 4851:
Training Loss: 4.910953521728516
Reconstruction Loss: -0.7163201570510864
Iteration 4861:
Training Loss: 4.3795647621154785
Reconstruction Loss: -0.729534387588501
Iteration 4871:
Training Loss: 4.396961688995361
Reconstruction Loss: -0.69631028175354
Iteration 4881:
Training Loss: 4.573227405548096
Reconstruction Loss: -0.799626886844635
Iteration 4891:
Training Loss: 4.989907264709473
Reconstruction Loss: -0.6726444959640503
Iteration 4901:
Training Loss: 4.501537322998047
Reconstruction Loss: -0.7467722296714783
Iteration 4911:
Training Loss: 4.555144309997559
Reconstruction Loss: -0.6483616232872009
Iteration 4921:
Training Loss: 4.885301113128662
Reconstruction Loss: -0.7206988334655762
Iteration 4931:
Training Loss: 4.678548336029053
Reconstruction Loss: -0.738298773765564
Iteration 4941:
Training Loss: 4.538342475891113
Reconstruction Loss: -0.670048713684082
Iteration 4951:
Training Loss: 4.756382465362549
Reconstruction Loss: -0.7347865104675293
Iteration 4961:
Training Loss: 4.409564018249512
Reconstruction Loss: -0.6687642335891724
Iteration 4971:
Training Loss: 4.254082202911377
Reconstruction Loss: -0.7489439249038696
Iteration 4981:
Training Loss: 4.6425909996032715
Reconstruction Loss: -0.7794898748397827
Iteration 4991:
Training Loss: 4.273538112640381
Reconstruction Loss: -0.70172518491745
