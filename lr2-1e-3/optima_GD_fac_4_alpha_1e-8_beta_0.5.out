5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.454336643218994
Reconstruction Loss: -0.5067666172981262
Iteration 101:
Training Loss: 5.454336643218994
Reconstruction Loss: -0.5067666172981262
Iteration 201:
Training Loss: 5.454336643218994
Reconstruction Loss: -0.5067666172981262
Iteration 301:
Training Loss: 5.454336643218994
Reconstruction Loss: -0.5067666172981262
Iteration 401:
Training Loss: 5.454336643218994
Reconstruction Loss: -0.5067666172981262
Iteration 501:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.5067666172981262
Iteration 601:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.506766676902771
Iteration 701:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.506766676902771
Iteration 801:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.506766676902771
Iteration 901:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.5067667961120605
Iteration 1001:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.506766676902771
Iteration 1101:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.506766676902771
Iteration 1201:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.5067667961120605
Iteration 1301:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.506766676902771
Iteration 1401:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.506766676902771
Iteration 1501:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.5067667961120605
Iteration 1601:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.5067667961120605
Iteration 1701:
Training Loss: 5.454336166381836
Reconstruction Loss: -0.5067667961120605
Iteration 1801:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067667961120605
Iteration 1901:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067669153213501
Iteration 2001:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067669153213501
Iteration 2101:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067669153213501
Iteration 2201:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067669153213501
Iteration 2301:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067669153213501
Iteration 2401:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067670345306396
Iteration 2501:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067670345306396
Iteration 2601:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067670345306396
Iteration 2701:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067670345306396
Iteration 2801:
Training Loss: 5.454335689544678
Reconstruction Loss: -0.5067670345306396
Iteration 2901:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3001:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3101:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3201:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3301:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3401:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3501:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3601:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.5067670941352844
Iteration 3701:
Training Loss: 5.4543352127075195
Reconstruction Loss: -0.506767213344574
Iteration 3801:
Training Loss: 5.454334735870361
Reconstruction Loss: -0.506767213344574
Iteration 3901:
Training Loss: 5.454334735870361
Reconstruction Loss: -0.506767213344574
Iteration 4001:
Training Loss: 5.454334735870361
Reconstruction Loss: -0.5067672729492188
Iteration 4101:
Training Loss: 5.454334735870361
Reconstruction Loss: -0.5067672729492188
Iteration 4201:
Training Loss: 5.454334735870361
Reconstruction Loss: -0.5067672729492188
Iteration 4301:
Training Loss: 5.454334735870361
Reconstruction Loss: -0.5067672729492188
Iteration 4401:
Training Loss: 5.454334735870361
Reconstruction Loss: -0.5067673921585083
Iteration 4501:
Training Loss: 5.454334259033203
Reconstruction Loss: -0.5067673921585083
Iteration 4601:
Training Loss: 5.454334259033203
Reconstruction Loss: -0.5067673921585083
Iteration 4701:
Training Loss: 5.454334259033203
Reconstruction Loss: -0.5067673921585083
Iteration 4801:
Training Loss: 5.454334259033203
Reconstruction Loss: -0.5067675113677979
Iteration 4901:
Training Loss: 5.454334259033203
Reconstruction Loss: -0.5067675113677979
Iteration 5001:
Training Loss: 5.454334259033203
Reconstruction Loss: -0.5067676305770874
Iteration 5101:
Training Loss: 5.454333782196045
Reconstruction Loss: -0.5067676901817322
Iteration 5201:
Training Loss: 5.454333782196045
Reconstruction Loss: -0.5067676901817322
Iteration 5301:
Training Loss: 5.454333782196045
Reconstruction Loss: -0.5067678093910217
Iteration 5401:
Training Loss: 5.454333782196045
Reconstruction Loss: -0.5067678093910217
Iteration 5501:
Training Loss: 5.454333782196045
Reconstruction Loss: -0.5067678093910217
Iteration 5601:
Training Loss: 5.454333305358887
Reconstruction Loss: -0.5067678093910217
Iteration 5701:
Training Loss: 5.454333305358887
Reconstruction Loss: -0.5067678093910217
Iteration 5801:
Training Loss: 5.454333305358887
Reconstruction Loss: -0.506767988204956
Iteration 5901:
Training Loss: 5.4543328285217285
Reconstruction Loss: -0.506767988204956
Iteration 6001:
Training Loss: 5.4543328285217285
Reconstruction Loss: -0.5067681074142456
Iteration 6101:
Training Loss: 5.4543328285217285
Reconstruction Loss: -0.5067682266235352
Iteration 6201:
Training Loss: 5.45433235168457
Reconstruction Loss: -0.5067682266235352
Iteration 6301:
Training Loss: 5.45433235168457
Reconstruction Loss: -0.5067682862281799
Iteration 6401:
Training Loss: 5.45433235168457
Reconstruction Loss: -0.5067682862281799
Iteration 6501:
Training Loss: 5.454331874847412
Reconstruction Loss: -0.5067684650421143
Iteration 6601:
Training Loss: 5.454331874847412
Reconstruction Loss: -0.5067685842514038
Iteration 6701:
Training Loss: 5.454331874847412
Reconstruction Loss: -0.5067685842514038
Iteration 6801:
Training Loss: 5.454331398010254
Reconstruction Loss: -0.5067687034606934
Iteration 6901:
Training Loss: 5.454331398010254
Reconstruction Loss: -0.5067688226699829
Iteration 7001:
Training Loss: 5.454330921173096
Reconstruction Loss: -0.5067688822746277
Iteration 7101:
Training Loss: 5.454330921173096
Reconstruction Loss: -0.506769061088562
Iteration 7201:
Training Loss: 5.4543304443359375
Reconstruction Loss: -0.506769061088562
Iteration 7301:
Training Loss: 5.454329967498779
Reconstruction Loss: -0.5067691802978516
Iteration 7401:
Training Loss: 5.454329967498779
Reconstruction Loss: -0.5067694187164307
Iteration 7501:
Training Loss: 5.454329490661621
Reconstruction Loss: -0.5067694187164307
Iteration 7601:
Training Loss: 5.454329013824463
Reconstruction Loss: -0.5067696571350098
Iteration 7701:
Training Loss: 5.454328536987305
Reconstruction Loss: -0.5067699551582336
Iteration 7801:
Training Loss: 5.4543280601501465
Reconstruction Loss: -0.5067700743675232
Iteration 7901:
Training Loss: 5.4543280601501465
Reconstruction Loss: -0.5067702531814575
Iteration 8001:
Training Loss: 5.454327583312988
Reconstruction Loss: -0.5067705512046814
Iteration 8101:
Training Loss: 5.454326629638672
Reconstruction Loss: -0.506770670413971
Iteration 8201:
Training Loss: 5.454326152801514
Reconstruction Loss: -0.5067710876464844
Iteration 8301:
Training Loss: 5.4543256759643555
Reconstruction Loss: -0.5067713260650635
Iteration 8401:
Training Loss: 5.454324722290039
Reconstruction Loss: -0.5067716836929321
Iteration 8501:
Training Loss: 5.454323768615723
Reconstruction Loss: -0.5067719221115112
Iteration 8601:
Training Loss: 5.4543232917785645
Reconstruction Loss: -0.5067724585533142
Iteration 8701:
Training Loss: 5.454322338104248
Reconstruction Loss: -0.5067728757858276
Iteration 8801:
Training Loss: 5.454320907592773
Reconstruction Loss: -0.5067734718322754
Iteration 8901:
Training Loss: 5.454319953918457
Reconstruction Loss: -0.5067740678787231
Iteration 9001:
Training Loss: 5.454318523406982
Reconstruction Loss: -0.5067746639251709
Iteration 9101:
Training Loss: 5.45431661605835
Reconstruction Loss: -0.5067756175994873
Iteration 9201:
Training Loss: 5.454314708709717
Reconstruction Loss: -0.5067765116691589
Iteration 9301:
Training Loss: 5.454312801361084
Reconstruction Loss: -0.5067775845527649
Iteration 9401:
Training Loss: 5.454309940338135
Reconstruction Loss: -0.5067787766456604
Iteration 9501:
Training Loss: 5.4543070793151855
Reconstruction Loss: -0.5067803859710693
Iteration 9601:
Training Loss: 5.45430326461792
Reconstruction Loss: -0.5067822337150574
Iteration 9701:
Training Loss: 5.454298496246338
Reconstruction Loss: -0.5067845582962036
Iteration 9801:
Training Loss: 5.454293251037598
Reconstruction Loss: -0.5067874193191528
Iteration 9901:
Training Loss: 5.454285621643066
Reconstruction Loss: -0.5067909359931946
Iteration 10001:
Training Loss: 5.4542765617370605
Reconstruction Loss: -0.5067957639694214
Iteration 10101:
Training Loss: 5.4542646408081055
Reconstruction Loss: -0.5068020224571228
Iteration 10201:
Training Loss: 5.454247951507568
Reconstruction Loss: -0.50681072473526
Iteration 10301:
Training Loss: 5.4542236328125
Reconstruction Loss: -0.5068234205245972
Iteration 10401:
Training Loss: 5.454187870025635
Reconstruction Loss: -0.5068424940109253
Iteration 10501:
Training Loss: 5.454129695892334
Reconstruction Loss: -0.506873369216919
Iteration 10601:
Training Loss: 5.454026699066162
Reconstruction Loss: -0.506928563117981
Iteration 10701:
Training Loss: 5.453815937042236
Reconstruction Loss: -0.5070425868034363
Iteration 10801:
Training Loss: 5.453266143798828
Reconstruction Loss: -0.5073407292366028
Iteration 10901:
Training Loss: 5.450901985168457
Reconstruction Loss: -0.5086253881454468
Iteration 11001:
Training Loss: 5.377813816070557
Reconstruction Loss: -0.5463054180145264
Iteration 11101:
Training Loss: 4.859738349914551
Reconstruction Loss: -0.5659308433532715
Iteration 11201:
Training Loss: 4.856675624847412
Reconstruction Loss: -0.5575899481773376
Iteration 11301:
Training Loss: 4.8564066886901855
Reconstruction Loss: -0.5566070079803467
Iteration 11401:
Training Loss: 4.856348514556885
Reconstruction Loss: -0.5568835735321045
Iteration 11501:
Training Loss: 4.856325149536133
Reconstruction Loss: -0.5573666095733643
Iteration 11601:
Training Loss: 4.856313705444336
Reconstruction Loss: -0.5578113794326782
Iteration 11701:
Training Loss: 4.856307506561279
Reconstruction Loss: -0.558169960975647
Iteration 11801:
Training Loss: 4.856303691864014
Reconstruction Loss: -0.5584449768066406
Iteration 11901:
Training Loss: 4.856301307678223
Reconstruction Loss: -0.5586516857147217
Iteration 12001:
Training Loss: 4.85629940032959
Reconstruction Loss: -0.5588055849075317
Iteration 12101:
Training Loss: 4.856297492980957
Reconstruction Loss: -0.5589200258255005
Iteration 12201:
Training Loss: 4.856296062469482
Reconstruction Loss: -0.5590047836303711
Iteration 12301:
Training Loss: 4.856294631958008
Reconstruction Loss: -0.5590677261352539
Iteration 12401:
Training Loss: 4.856292724609375
Reconstruction Loss: -0.5591146945953369
Iteration 12501:
Training Loss: 4.856290817260742
Reconstruction Loss: -0.5591496825218201
Iteration 12601:
Training Loss: 4.856288909912109
Reconstruction Loss: -0.5591762065887451
Iteration 12701:
Training Loss: 4.856286525726318
Reconstruction Loss: -0.5591962337493896
Iteration 12801:
Training Loss: 4.856284141540527
Reconstruction Loss: -0.5592117309570312
Iteration 12901:
Training Loss: 4.856281280517578
Reconstruction Loss: -0.559224009513855
Iteration 13001:
Training Loss: 4.856277942657471
Reconstruction Loss: -0.5592340230941772
Iteration 13101:
Training Loss: 4.856274127960205
Reconstruction Loss: -0.5592423677444458
Iteration 13201:
Training Loss: 4.856269836425781
Reconstruction Loss: -0.5592496991157532
Iteration 13301:
Training Loss: 4.856265068054199
Reconstruction Loss: -0.5592566728591919
Iteration 13401:
Training Loss: 4.856259346008301
Reconstruction Loss: -0.5592634677886963
Iteration 13501:
Training Loss: 4.856252670288086
Reconstruction Loss: -0.559270441532135
Iteration 13601:
Training Loss: 4.8562445640563965
Reconstruction Loss: -0.5592779517173767
Iteration 13701:
Training Loss: 4.856234550476074
Reconstruction Loss: -0.5592864751815796
Iteration 13801:
Training Loss: 4.856222629547119
Reconstruction Loss: -0.5592966079711914
Iteration 13901:
Training Loss: 4.856207847595215
Reconstruction Loss: -0.5593087077140808
Iteration 14001:
Training Loss: 4.856188774108887
Reconstruction Loss: -0.5593239068984985
Iteration 14101:
Training Loss: 4.856164455413818
Reconstruction Loss: -0.5593430995941162
Iteration 14201:
Training Loss: 4.8561320304870605
Reconstruction Loss: -0.5593682527542114
Iteration 14301:
Training Loss: 4.856088161468506
Reconstruction Loss: -0.5594024658203125
Iteration 14401:
Training Loss: 4.856025218963623
Reconstruction Loss: -0.559450626373291
Iteration 14501:
Training Loss: 4.855931758880615
Reconstruction Loss: -0.5595213174819946
Iteration 14601:
Training Loss: 4.855782985687256
Reconstruction Loss: -0.5596321225166321
Iteration 14701:
Training Loss: 4.855525493621826
Reconstruction Loss: -0.559820294380188
Iteration 14801:
Training Loss: 4.8550190925598145
Reconstruction Loss: -0.5601815581321716
Iteration 14901:
Training Loss: 4.853794097900391
Reconstruction Loss: -0.5610242486000061
