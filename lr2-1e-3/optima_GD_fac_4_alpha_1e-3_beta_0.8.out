5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.452760696411133
Reconstruction Loss: -0.45541757345199585
Iteration 101:
Training Loss: 3.6357686519622803
Reconstruction Loss: -1.0663182735443115
Iteration 201:
Training Loss: 2.4114460945129395
Reconstruction Loss: -1.6339653730392456
Iteration 301:
Training Loss: 1.7800092697143555
Reconstruction Loss: -2.036914587020874
Iteration 401:
Training Loss: 1.0902094841003418
Reconstruction Loss: -2.494473457336426
Iteration 501:
Training Loss: 0.32164913415908813
Reconstruction Loss: -2.9993748664855957
Iteration 601:
Training Loss: -0.31112030148506165
Reconstruction Loss: -3.4274184703826904
Iteration 701:
Training Loss: -0.76046222448349
Reconstruction Loss: -3.7483954429626465
Iteration 801:
Training Loss: -1.0870914459228516
Reconstruction Loss: -3.988788366317749
Iteration 901:
Training Loss: -1.3421825170516968
Reconstruction Loss: -4.175909996032715
Iteration 1001:
Training Loss: -1.5531781911849976
Reconstruction Loss: -4.327685832977295
Iteration 1101:
Training Loss: -1.7343454360961914
Reconstruction Loss: -4.454999923706055
Iteration 1201:
Training Loss: -1.8937891721725464
Reconstruction Loss: -4.564546585083008
Iteration 1301:
Training Loss: -2.036576747894287
Reconstruction Loss: -4.660609245300293
Iteration 1401:
Training Loss: -2.1661226749420166
Reconstruction Loss: -4.746058464050293
Iteration 1501:
Training Loss: -2.284850835800171
Reconstruction Loss: -4.8229079246521
Iteration 1601:
Training Loss: -2.394538640975952
Reconstruction Loss: -4.892627239227295
Iteration 1701:
Training Loss: -2.4965455532073975
Reconstruction Loss: -4.9563307762146
Iteration 1801:
Training Loss: -2.5919277667999268
Reconstruction Loss: -5.014885425567627
Iteration 1901:
Training Loss: -2.6815247535705566
Reconstruction Loss: -5.06898307800293
Iteration 2001:
Training Loss: -2.7660105228424072
Reconstruction Loss: -5.119185447692871
Iteration 2101:
Training Loss: -2.8459482192993164
Reconstruction Loss: -5.165957450866699
Iteration 2201:
Training Loss: -2.9218027591705322
Reconstruction Loss: -5.209687232971191
Iteration 2301:
Training Loss: -2.9939632415771484
Reconstruction Loss: -5.250701904296875
Iteration 2401:
Training Loss: -3.062770128250122
Reconstruction Loss: -5.289281845092773
Iteration 2501:
Training Loss: -3.128511667251587
Reconstruction Loss: -5.325667381286621
Iteration 2601:
Training Loss: -3.1914401054382324
Reconstruction Loss: -5.360068321228027
Iteration 2701:
Training Loss: -3.2517733573913574
Reconstruction Loss: -5.392665386199951
Iteration 2801:
Training Loss: -3.3097071647644043
Reconstruction Loss: -5.423616886138916
Iteration 2901:
Training Loss: -3.3654141426086426
Reconstruction Loss: -5.453063488006592
Iteration 3001:
Training Loss: -3.419045925140381
Reconstruction Loss: -5.481128215789795
Iteration 3101:
Training Loss: -3.4707460403442383
Reconstruction Loss: -5.507920265197754
Iteration 3201:
Training Loss: -3.520634412765503
Reconstruction Loss: -5.533540725708008
Iteration 3301:
Training Loss: -3.5688223838806152
Reconstruction Loss: -5.558075428009033
Iteration 3401:
Training Loss: -3.6154258251190186
Reconstruction Loss: -5.581602573394775
Iteration 3501:
Training Loss: -3.6605212688446045
Reconstruction Loss: -5.60419225692749
Iteration 3601:
Training Loss: -3.70420241355896
Reconstruction Loss: -5.625911235809326
Iteration 3701:
Training Loss: -3.7465505599975586
Reconstruction Loss: -5.646815776824951
Iteration 3801:
Training Loss: -3.787632942199707
Reconstruction Loss: -5.666959285736084
Iteration 3901:
Training Loss: -3.827518939971924
Reconstruction Loss: -5.686388969421387
Iteration 4001:
Training Loss: -3.866266965866089
Reconstruction Loss: -5.705150127410889
Iteration 4101:
Training Loss: -3.9039394855499268
Reconstruction Loss: -5.723282337188721
Iteration 4201:
Training Loss: -3.940584182739258
Reconstruction Loss: -5.740821361541748
Iteration 4301:
Training Loss: -3.976256847381592
Reconstruction Loss: -5.757802486419678
Iteration 4401:
Training Loss: -4.010998249053955
Reconstruction Loss: -5.77425479888916
Iteration 4501:
Training Loss: -4.044855117797852
Reconstruction Loss: -5.790207862854004
Iteration 4601:
Training Loss: -4.077866077423096
Reconstruction Loss: -5.805689811706543
Iteration 4701:
Training Loss: -4.1100687980651855
Reconstruction Loss: -5.820722579956055
Iteration 4801:
Training Loss: -4.141503810882568
Reconstruction Loss: -5.835331916809082
Iteration 4901:
Training Loss: -4.172197341918945
Reconstruction Loss: -5.8495378494262695
Iteration 5001:
Training Loss: -4.202182292938232
Reconstruction Loss: -5.863359451293945
Iteration 5101:
Training Loss: -4.2314910888671875
Reconstruction Loss: -5.876817226409912
Iteration 5201:
Training Loss: -4.26015043258667
Reconstruction Loss: -5.88992166519165
Iteration 5301:
Training Loss: -4.288189888000488
Reconstruction Loss: -5.902694225311279
Iteration 5401:
Training Loss: -4.3156280517578125
Reconstruction Loss: -5.915152549743652
Iteration 5501:
Training Loss: -4.342491626739502
Reconstruction Loss: -5.92730712890625
Iteration 5601:
Training Loss: -4.368805885314941
Reconstruction Loss: -5.939168930053711
Iteration 5701:
Training Loss: -4.394590854644775
Reconstruction Loss: -5.950753688812256
Iteration 5801:
Training Loss: -4.419865608215332
Reconstruction Loss: -5.962069511413574
Iteration 5901:
Training Loss: -4.444643497467041
Reconstruction Loss: -5.9731316566467285
Iteration 6001:
Training Loss: -4.468950271606445
Reconstruction Loss: -5.983943939208984
Iteration 6101:
Training Loss: -4.492801666259766
Reconstruction Loss: -5.994523525238037
Iteration 6201:
Training Loss: -4.516214370727539
Reconstruction Loss: -6.004879951477051
Iteration 6301:
Training Loss: -4.539203643798828
Reconstruction Loss: -6.015013217926025
Iteration 6401:
Training Loss: -4.5617804527282715
Reconstruction Loss: -6.024935245513916
Iteration 6501:
Training Loss: -4.5839715003967285
Reconstruction Loss: -6.034658908843994
Iteration 6601:
Training Loss: -4.605771541595459
Reconstruction Loss: -6.044186115264893
Iteration 6701:
Training Loss: -4.627209663391113
Reconstruction Loss: -6.053524017333984
Iteration 6801:
Training Loss: -4.648285865783691
Reconstruction Loss: -6.062677383422852
Iteration 6901:
Training Loss: -4.669020652770996
Reconstruction Loss: -6.071658134460449
Iteration 7001:
Training Loss: -4.689422607421875
Reconstruction Loss: -6.080467224121094
Iteration 7101:
Training Loss: -4.709502696990967
Reconstruction Loss: -6.089117050170898
Iteration 7201:
Training Loss: -4.729274272918701
Reconstruction Loss: -6.0976057052612305
Iteration 7301:
Training Loss: -4.748735427856445
Reconstruction Loss: -6.105942726135254
Iteration 7401:
Training Loss: -4.767904758453369
Reconstruction Loss: -6.114128112792969
Iteration 7501:
Training Loss: -4.786794185638428
Reconstruction Loss: -6.122167110443115
Iteration 7601:
Training Loss: -4.805408954620361
Reconstruction Loss: -6.130073070526123
Iteration 7701:
Training Loss: -4.823753833770752
Reconstruction Loss: -6.137842655181885
Iteration 7801:
Training Loss: -4.841837406158447
Reconstruction Loss: -6.145477771759033
Iteration 7901:
Training Loss: -4.8596696853637695
Reconstruction Loss: -6.1529860496521
Iteration 8001:
Training Loss: -4.877264499664307
Reconstruction Loss: -6.160367965698242
Iteration 8101:
Training Loss: -4.894625186920166
Reconstruction Loss: -6.167632102966309
Iteration 8201:
Training Loss: -4.911746501922607
Reconstruction Loss: -6.174783229827881
Iteration 8301:
Training Loss: -4.928648948669434
Reconstruction Loss: -6.181816101074219
Iteration 8401:
Training Loss: -4.9453301429748535
Reconstruction Loss: -6.188740253448486
Iteration 8501:
Training Loss: -4.9618024826049805
Reconstruction Loss: -6.195556163787842
Iteration 8601:
Training Loss: -4.978072643280029
Reconstruction Loss: -6.202267646789551
Iteration 8701:
Training Loss: -4.994133472442627
Reconstruction Loss: -6.208874225616455
Iteration 8801:
Training Loss: -5.010006427764893
Reconstruction Loss: -6.215384483337402
Iteration 8901:
Training Loss: -5.025685787200928
Reconstruction Loss: -6.221799373626709
Iteration 9001:
Training Loss: -5.041186332702637
Reconstruction Loss: -6.228116512298584
Iteration 9101:
Training Loss: -5.056497573852539
Reconstruction Loss: -6.234344005584717
Iteration 9201:
Training Loss: -5.071637153625488
Reconstruction Loss: -6.240481853485107
Iteration 9301:
Training Loss: -5.08660364151001
Reconstruction Loss: -6.246526718139648
Iteration 9401:
Training Loss: -5.101407051086426
Reconstruction Loss: -6.252493858337402
Iteration 9501:
Training Loss: -5.116045951843262
Reconstruction Loss: -6.2583746910095215
Iteration 9601:
Training Loss: -5.130526065826416
Reconstruction Loss: -6.264171600341797
Iteration 9701:
Training Loss: -5.144845485687256
Reconstruction Loss: -6.269893646240234
Iteration 9801:
Training Loss: -5.159019470214844
Reconstruction Loss: -6.275538444519043
Iteration 9901:
Training Loss: -5.173040866851807
Reconstruction Loss: -6.281100749969482
Iteration 10001:
Training Loss: -5.186913967132568
Reconstruction Loss: -6.286592960357666
Iteration 10101:
Training Loss: -5.200659275054932
Reconstruction Loss: -6.29201602935791
Iteration 10201:
Training Loss: -5.214255332946777
Reconstruction Loss: -6.297367572784424
Iteration 10301:
Training Loss: -5.227725982666016
Reconstruction Loss: -6.302648544311523
Iteration 10401:
Training Loss: -5.241052627563477
Reconstruction Loss: -6.307858467102051
Iteration 10501:
Training Loss: -5.25425910949707
Reconstruction Loss: -6.313004016876221
Iteration 10601:
Training Loss: -5.267334461212158
Reconstruction Loss: -6.318090438842773
Iteration 10701:
Training Loss: -5.280285358428955
Reconstruction Loss: -6.323103427886963
Iteration 10801:
Training Loss: -5.293118953704834
Reconstruction Loss: -6.328055381774902
Iteration 10901:
Training Loss: -5.305826187133789
Reconstruction Loss: -6.332951068878174
Iteration 11001:
Training Loss: -5.318424701690674
Reconstruction Loss: -6.3377861976623535
Iteration 11101:
Training Loss: -5.330902576446533
Reconstruction Loss: -6.342556476593018
Iteration 11201:
Training Loss: -5.3432745933532715
Reconstruction Loss: -6.347273349761963
Iteration 11301:
Training Loss: -5.355525016784668
Reconstruction Loss: -6.351942539215088
Iteration 11401:
Training Loss: -5.367692470550537
Reconstruction Loss: -6.356554985046387
Iteration 11501:
Training Loss: -5.379738807678223
Reconstruction Loss: -6.361107349395752
Iteration 11601:
Training Loss: -5.391682147979736
Reconstruction Loss: -6.365605354309082
Iteration 11701:
Training Loss: -5.403528690338135
Reconstruction Loss: -6.370055198669434
Iteration 11801:
Training Loss: -5.415276050567627
Reconstruction Loss: -6.374458312988281
Iteration 11901:
Training Loss: -5.426933288574219
Reconstruction Loss: -6.3788065910339355
Iteration 12001:
Training Loss: -5.438491344451904
Reconstruction Loss: -6.383107662200928
Iteration 12101:
Training Loss: -5.449948310852051
Reconstruction Loss: -6.3873515129089355
Iteration 12201:
Training Loss: -5.4613165855407715
Reconstruction Loss: -6.391550540924072
Iteration 12301:
Training Loss: -5.47259521484375
Reconstruction Loss: -6.395709991455078
Iteration 12401:
Training Loss: -5.483787536621094
Reconstruction Loss: -6.399822235107422
Iteration 12501:
Training Loss: -5.4948906898498535
Reconstruction Loss: -6.403892517089844
Iteration 12601:
Training Loss: -5.5059099197387695
Reconstruction Loss: -6.407915115356445
Iteration 12701:
Training Loss: -5.516833782196045
Reconstruction Loss: -6.411892890930176
Iteration 12801:
Training Loss: -5.527685642242432
Reconstruction Loss: -6.415825843811035
Iteration 12901:
Training Loss: -5.538455486297607
Reconstruction Loss: -6.4197163581848145
Iteration 13001:
Training Loss: -5.5491437911987305
Reconstruction Loss: -6.423567771911621
Iteration 13101:
Training Loss: -5.559753894805908
Reconstruction Loss: -6.4273810386657715
Iteration 13201:
Training Loss: -5.570285797119141
Reconstruction Loss: -6.431157112121582
Iteration 13301:
Training Loss: -5.580747127532959
Reconstruction Loss: -6.434894561767578
Iteration 13401:
Training Loss: -5.591123580932617
Reconstruction Loss: -6.4385905265808105
Iteration 13501:
Training Loss: -5.601438522338867
Reconstruction Loss: -6.4422454833984375
Iteration 13601:
Training Loss: -5.611679553985596
Reconstruction Loss: -6.445868968963623
Iteration 13701:
Training Loss: -5.6218390464782715
Reconstruction Loss: -6.449456691741943
Iteration 13801:
Training Loss: -5.631932258605957
Reconstruction Loss: -6.453012943267822
Iteration 13901:
Training Loss: -5.641952037811279
Reconstruction Loss: -6.456531047821045
Iteration 14001:
Training Loss: -5.6519036293029785
Reconstruction Loss: -6.460015773773193
Iteration 14101:
Training Loss: -5.661808490753174
Reconstruction Loss: -6.463464736938477
Iteration 14201:
Training Loss: -5.671631813049316
Reconstruction Loss: -6.466878414154053
Iteration 14301:
Training Loss: -5.681396007537842
Reconstruction Loss: -6.470256805419922
Iteration 14401:
Training Loss: -5.691088676452637
Reconstruction Loss: -6.473599910736084
Iteration 14501:
Training Loss: -5.700724124908447
Reconstruction Loss: -6.4769158363342285
Iteration 14601:
Training Loss: -5.710292339324951
Reconstruction Loss: -6.480205535888672
Iteration 14701:
Training Loss: -5.719801902770996
Reconstruction Loss: -6.48345947265625
Iteration 14801:
Training Loss: -5.729245185852051
Reconstruction Loss: -6.486688137054443
Iteration 14901:
Training Loss: -5.7386345863342285
Reconstruction Loss: -6.4898858070373535
