5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.616037845611572
Reconstruction Loss: -0.43803250789642334
Iteration 21:
Training Loss: 5.888585090637207
Reconstruction Loss: -0.43803250789642334
Iteration 41:
Training Loss: 5.4803690910339355
Reconstruction Loss: -0.43803250789642334
Iteration 61:
Training Loss: 5.326820373535156
Reconstruction Loss: -0.43803250789642334
Iteration 81:
Training Loss: 5.22958517074585
Reconstruction Loss: -0.43803250789642334
Iteration 101:
Training Loss: 5.641565799713135
Reconstruction Loss: -0.43803250789642334
Iteration 121:
Training Loss: 5.501495361328125
Reconstruction Loss: -0.43803250789642334
Iteration 141:
Training Loss: 5.6741042137146
Reconstruction Loss: -0.43803250789642334
Iteration 161:
Training Loss: 5.682466983795166
Reconstruction Loss: -0.43803250789642334
Iteration 181:
Training Loss: 5.469958782196045
Reconstruction Loss: -0.43803250789642334
Iteration 201:
Training Loss: 5.553525924682617
Reconstruction Loss: -0.43803250789642334
Iteration 221:
Training Loss: 5.339216232299805
Reconstruction Loss: -0.4380326271057129
Iteration 241:
Training Loss: 5.581587314605713
Reconstruction Loss: -0.4380326271057129
Iteration 261:
Training Loss: 5.485385417938232
Reconstruction Loss: -0.4380326271057129
Iteration 281:
Training Loss: 5.52261209487915
Reconstruction Loss: -0.4380326271057129
Iteration 301:
Training Loss: 5.642488956451416
Reconstruction Loss: -0.4380326271057129
Iteration 321:
Training Loss: 5.4158935546875
Reconstruction Loss: -0.4380326271057129
Iteration 341:
Training Loss: 5.671939849853516
Reconstruction Loss: -0.4380326271057129
Iteration 361:
Training Loss: 5.616191864013672
Reconstruction Loss: -0.4380328059196472
Iteration 381:
Training Loss: 5.440439224243164
Reconstruction Loss: -0.4380328059196472
Iteration 401:
Training Loss: 5.714400768280029
Reconstruction Loss: -0.4380328059196472
Iteration 421:
Training Loss: 5.424821853637695
Reconstruction Loss: -0.4380328059196472
Iteration 441:
Training Loss: 5.409893035888672
Reconstruction Loss: -0.4380328059196472
Iteration 461:
Training Loss: 5.779545783996582
Reconstruction Loss: -0.4380328953266144
Iteration 481:
Training Loss: 5.678304195404053
Reconstruction Loss: -0.4380328953266144
Iteration 501:
Training Loss: 5.504058837890625
Reconstruction Loss: -0.4380328953266144
Iteration 521:
Training Loss: 5.80712366104126
Reconstruction Loss: -0.4380328953266144
Iteration 541:
Training Loss: 5.850915908813477
Reconstruction Loss: -0.43803298473358154
Iteration 561:
Training Loss: 5.938426971435547
Reconstruction Loss: -0.4380330741405487
Iteration 581:
Training Loss: 5.94789457321167
Reconstruction Loss: -0.43803325295448303
Iteration 601:
Training Loss: 5.7416887283325195
Reconstruction Loss: -0.43803325295448303
Iteration 621:
Training Loss: 5.225008964538574
Reconstruction Loss: -0.4380333423614502
Iteration 641:
Training Loss: 5.712950229644775
Reconstruction Loss: -0.43803343176841736
Iteration 661:
Training Loss: 5.474065780639648
Reconstruction Loss: -0.43803343176841736
Iteration 681:
Training Loss: 5.898377895355225
Reconstruction Loss: -0.4380338191986084
Iteration 701:
Training Loss: 5.291820049285889
Reconstruction Loss: -0.43803390860557556
Iteration 721:
Training Loss: 5.537097930908203
Reconstruction Loss: -0.43803417682647705
Iteration 741:
Training Loss: 5.429469585418701
Reconstruction Loss: -0.4380345344543457
Iteration 761:
Training Loss: 5.1332831382751465
Reconstruction Loss: -0.43803492188453674
Iteration 781:
Training Loss: 5.249780178070068
Reconstruction Loss: -0.43803536891937256
Iteration 801:
Training Loss: 5.197340488433838
Reconstruction Loss: -0.4380359351634979
Iteration 821:
Training Loss: 5.626562595367432
Reconstruction Loss: -0.43803703784942627
Iteration 841:
Training Loss: 5.263579368591309
Reconstruction Loss: -0.4380384385585785
Iteration 861:
Training Loss: 5.473118782043457
Reconstruction Loss: -0.4380403757095337
Iteration 881:
Training Loss: 5.702213764190674
Reconstruction Loss: -0.4380435049533844
Iteration 901:
Training Loss: 5.547464847564697
Reconstruction Loss: -0.4380490481853485
Iteration 921:
Training Loss: 5.294105052947998
Reconstruction Loss: -0.43805938959121704
Iteration 941:
Training Loss: 5.7255473136901855
Reconstruction Loss: -0.438080370426178
Iteration 961:
Training Loss: 5.427191734313965
Reconstruction Loss: -0.43813395500183105
Iteration 981:
Training Loss: 5.4478044509887695
Reconstruction Loss: -0.43831121921539307
Iteration 1001:
Training Loss: 5.748737812042236
Reconstruction Loss: -0.43933480978012085
Iteration 1021:
Training Loss: 5.416260242462158
Reconstruction Loss: -0.4700523614883423
Iteration 1041:
Training Loss: 4.758955478668213
Reconstruction Loss: -0.5714191794395447
Iteration 1061:
Training Loss: 5.1112060546875
Reconstruction Loss: -0.5706737637519836
Iteration 1081:
Training Loss: 5.08906888961792
Reconstruction Loss: -0.5713517069816589
Iteration 1101:
Training Loss: 5.043031692504883
Reconstruction Loss: -0.5649274587631226
Iteration 1121:
Training Loss: 4.990711688995361
Reconstruction Loss: -0.5689414739608765
Iteration 1141:
Training Loss: 5.083639621734619
Reconstruction Loss: -0.5731346011161804
Iteration 1161:
Training Loss: 4.976007461547852
Reconstruction Loss: -0.570591390132904
Iteration 1181:
Training Loss: 4.9372687339782715
Reconstruction Loss: -0.5721477270126343
Iteration 1201:
Training Loss: 5.070466041564941
Reconstruction Loss: -0.5818655490875244
Iteration 1221:
Training Loss: 4.969395637512207
Reconstruction Loss: -0.574394702911377
Iteration 1241:
Training Loss: 5.0106682777404785
Reconstruction Loss: -0.56476229429245
Iteration 1261:
Training Loss: 5.020052433013916
Reconstruction Loss: -0.576655387878418
Iteration 1281:
Training Loss: 5.008534908294678
Reconstruction Loss: -0.5700229406356812
Iteration 1301:
Training Loss: 5.119421005249023
Reconstruction Loss: -0.5622377395629883
Iteration 1321:
Training Loss: 4.944644927978516
Reconstruction Loss: -0.5678260922431946
Iteration 1341:
Training Loss: 5.181591033935547
Reconstruction Loss: -0.5806670188903809
Iteration 1361:
Training Loss: 4.987082004547119
Reconstruction Loss: -0.5743421316146851
Iteration 1381:
Training Loss: 4.822640419006348
Reconstruction Loss: -0.5774226784706116
Iteration 1401:
Training Loss: 4.937066078186035
Reconstruction Loss: -0.5879148244857788
Iteration 1421:
Training Loss: 4.479193210601807
Reconstruction Loss: -0.7253265976905823
Iteration 1441:
Training Loss: 4.416951656341553
Reconstruction Loss: -0.7341446280479431
Iteration 1461:
Training Loss: 4.322458267211914
Reconstruction Loss: -0.7338578104972839
Iteration 1481:
Training Loss: 4.605861186981201
Reconstruction Loss: -0.7308948040008545
Iteration 1501:
Training Loss: 4.442663192749023
Reconstruction Loss: -0.7330899834632874
Iteration 1521:
Training Loss: 4.811384201049805
Reconstruction Loss: -0.7366728186607361
Iteration 1541:
Training Loss: 4.714998245239258
Reconstruction Loss: -0.7285447120666504
Iteration 1561:
Training Loss: 4.542075157165527
Reconstruction Loss: -0.7267807722091675
Iteration 1581:
Training Loss: 4.431855201721191
Reconstruction Loss: -0.7239701151847839
Iteration 1601:
Training Loss: 4.41506290435791
Reconstruction Loss: -0.728840172290802
Iteration 1621:
Training Loss: 4.552209854125977
Reconstruction Loss: -0.7388865351676941
Iteration 1641:
Training Loss: 4.2950639724731445
Reconstruction Loss: -0.7338453531265259
Iteration 1661:
Training Loss: 4.741293907165527
Reconstruction Loss: -0.7406682968139648
Iteration 1681:
Training Loss: 4.738003253936768
Reconstruction Loss: -0.7298547625541687
Iteration 1701:
Training Loss: 4.441797256469727
Reconstruction Loss: -0.7440556883811951
Iteration 1721:
Training Loss: 4.644248962402344
Reconstruction Loss: -0.7502705454826355
Iteration 1741:
Training Loss: 4.578522682189941
Reconstruction Loss: -0.8277934789657593
Iteration 1761:
Training Loss: 3.9596199989318848
Reconstruction Loss: -0.8924582600593567
Iteration 1781:
Training Loss: 3.9415321350097656
Reconstruction Loss: -0.8747076988220215
Iteration 1801:
Training Loss: 3.8912367820739746
Reconstruction Loss: -0.8665228486061096
Iteration 1821:
Training Loss: 3.889380693435669
Reconstruction Loss: -0.8541660308837891
Iteration 1841:
Training Loss: 4.1037373542785645
Reconstruction Loss: -0.8562897443771362
Iteration 1861:
Training Loss: 4.039217948913574
Reconstruction Loss: -0.8488587737083435
Iteration 1881:
Training Loss: 4.014950275421143
Reconstruction Loss: -0.8562037348747253
Iteration 1901:
Training Loss: 3.863717555999756
Reconstruction Loss: -0.8425512909889221
Iteration 1921:
Training Loss: 4.165682792663574
Reconstruction Loss: -0.8401849269866943
Iteration 1941:
Training Loss: 3.849543571472168
Reconstruction Loss: -0.8497254252433777
Iteration 1961:
Training Loss: 3.833012104034424
Reconstruction Loss: -0.8630821704864502
Iteration 1981:
Training Loss: 3.8133888244628906
Reconstruction Loss: -0.8806029558181763
Iteration 2001:
Training Loss: 4.054914951324463
Reconstruction Loss: -0.9654797315597534
Iteration 2021:
Training Loss: 3.228973865509033
Reconstruction Loss: -1.2052249908447266
Iteration 2041:
Training Loss: 3.112166404724121
Reconstruction Loss: -1.2942330837249756
Iteration 2061:
Training Loss: 3.0653305053710938
Reconstruction Loss: -1.3205547332763672
Iteration 2081:
Training Loss: 3.1320018768310547
Reconstruction Loss: -1.337782382965088
Iteration 2101:
Training Loss: 3.227217435836792
Reconstruction Loss: -1.3391685485839844
Iteration 2121:
Training Loss: 3.0215511322021484
Reconstruction Loss: -1.3376425504684448
Iteration 2141:
Training Loss: 3.0602810382843018
Reconstruction Loss: -1.3238381147384644
Iteration 2161:
Training Loss: 2.937734842300415
Reconstruction Loss: -1.3241811990737915
Iteration 2181:
Training Loss: 3.016792058944702
Reconstruction Loss: -1.3203983306884766
Iteration 2201:
Training Loss: 3.270641326904297
Reconstruction Loss: -1.319003939628601
Iteration 2221:
Training Loss: 3.2246642112731934
Reconstruction Loss: -1.3249080181121826
Iteration 2241:
Training Loss: 3.1367404460906982
Reconstruction Loss: -1.3183048963546753
Iteration 2261:
Training Loss: 3.0404322147369385
Reconstruction Loss: -1.3222761154174805
Iteration 2281:
Training Loss: 3.401073455810547
Reconstruction Loss: -1.3143019676208496
Iteration 2301:
Training Loss: 3.286536931991577
Reconstruction Loss: -1.312842607498169
Iteration 2321:
Training Loss: 3.114297389984131
Reconstruction Loss: -1.3104792833328247
Iteration 2341:
Training Loss: 3.1164188385009766
Reconstruction Loss: -1.306929588317871
Iteration 2361:
Training Loss: 2.94943904876709
Reconstruction Loss: -1.315229892730713
Iteration 2381:
Training Loss: 3.082293748855591
Reconstruction Loss: -1.3033595085144043
Iteration 2401:
Training Loss: 3.063124179840088
Reconstruction Loss: -1.3004579544067383
Iteration 2421:
Training Loss: 2.7588157653808594
Reconstruction Loss: -1.3051643371582031
Iteration 2441:
Training Loss: 2.879239320755005
Reconstruction Loss: -1.2996307611465454
Iteration 2461:
Training Loss: 2.9485886096954346
Reconstruction Loss: -1.3002883195877075
Iteration 2481:
Training Loss: 2.9959492683410645
Reconstruction Loss: -1.3016855716705322
Iteration 2501:
Training Loss: 3.2762129306793213
Reconstruction Loss: -1.306785225868225
Iteration 2521:
Training Loss: 3.0575239658355713
Reconstruction Loss: -1.3001422882080078
Iteration 2541:
Training Loss: 3.0841636657714844
Reconstruction Loss: -1.304337739944458
Iteration 2561:
Training Loss: 2.969264507293701
Reconstruction Loss: -1.2999554872512817
Iteration 2581:
Training Loss: 3.0951151847839355
Reconstruction Loss: -1.3174666166305542
Iteration 2601:
Training Loss: 3.143542528152466
Reconstruction Loss: -1.3349230289459229
Iteration 2621:
Training Loss: 2.709895133972168
Reconstruction Loss: -1.4349265098571777
Iteration 2641:
Training Loss: 2.2630841732025146
Reconstruction Loss: -1.8089338541030884
Iteration 2661:
Training Loss: 1.688552975654602
Reconstruction Loss: -2.361567974090576
Iteration 2681:
Training Loss: 0.7737280130386353
Reconstruction Loss: -2.850595235824585
Iteration 2701:
Training Loss: 0.3776671886444092
Reconstruction Loss: -3.3019609451293945
Iteration 2721:
Training Loss: -0.09597881138324738
Reconstruction Loss: -3.736872434616089
Iteration 2741:
Training Loss: -0.4076121747493744
Reconstruction Loss: -4.157064914703369
Iteration 2761:
Training Loss: -0.7442412972450256
Reconstruction Loss: -4.568892955780029
Iteration 2781:
Training Loss: -1.2710039615631104
Reconstruction Loss: -4.986142635345459
Iteration 2801:
Training Loss: -2.0566115379333496
Reconstruction Loss: -5.3959455490112305
Iteration 2821:
Training Loss: -2.2121779918670654
Reconstruction Loss: -5.80248498916626
Iteration 2841:
Training Loss: -2.698767900466919
Reconstruction Loss: -6.211854934692383
Iteration 2861:
Training Loss: -2.8057303428649902
Reconstruction Loss: -6.617550373077393
Iteration 2881:
Training Loss: -3.5613770484924316
Reconstruction Loss: -7.031452178955078
Iteration 2901:
Training Loss: -3.606778144836426
Reconstruction Loss: -7.430815696716309
Iteration 2921:
Training Loss: -4.407184600830078
Reconstruction Loss: -7.834167003631592
Iteration 2941:
Training Loss: -4.783082008361816
Reconstruction Loss: -8.233091354370117
Iteration 2961:
Training Loss: -5.187546253204346
Reconstruction Loss: -8.627643585205078
Iteration 2981:
Training Loss: -5.512025833129883
Reconstruction Loss: -9.019736289978027
