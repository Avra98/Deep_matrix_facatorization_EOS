5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.588774681091309
Reconstruction Loss: -0.5217270851135254
Iteration 51:
Training Loss: 5.488060474395752
Reconstruction Loss: -0.5217270851135254
Iteration 101:
Training Loss: 5.503060817718506
Reconstruction Loss: -0.5217270851135254
Iteration 151:
Training Loss: 5.514731407165527
Reconstruction Loss: -0.5217270851135254
Iteration 201:
Training Loss: 5.345401287078857
Reconstruction Loss: -0.5217270851135254
Iteration 251:
Training Loss: 5.5367350578308105
Reconstruction Loss: -0.5217270851135254
Iteration 301:
Training Loss: 5.437167167663574
Reconstruction Loss: -0.5217272043228149
Iteration 351:
Training Loss: 5.499227523803711
Reconstruction Loss: -0.5217272043228149
Iteration 401:
Training Loss: 5.5023088455200195
Reconstruction Loss: -0.5217272043228149
Iteration 451:
Training Loss: 5.541409492492676
Reconstruction Loss: -0.5217272043228149
Iteration 501:
Training Loss: 5.635748386383057
Reconstruction Loss: -0.5217272043228149
Iteration 551:
Training Loss: 5.460989475250244
Reconstruction Loss: -0.5217272043228149
Iteration 601:
Training Loss: 5.428655624389648
Reconstruction Loss: -0.5217272639274597
Iteration 651:
Training Loss: 5.592606067657471
Reconstruction Loss: -0.5217272639274597
Iteration 701:
Training Loss: 5.582991600036621
Reconstruction Loss: -0.5217272639274597
Iteration 751:
Training Loss: 5.565417766571045
Reconstruction Loss: -0.5217273831367493
Iteration 801:
Training Loss: 5.447756290435791
Reconstruction Loss: -0.5217273831367493
Iteration 851:
Training Loss: 5.241000175476074
Reconstruction Loss: -0.5217273831367493
Iteration 901:
Training Loss: 5.525900363922119
Reconstruction Loss: -0.5217273831367493
Iteration 951:
Training Loss: 5.296048641204834
Reconstruction Loss: -0.5217273831367493
Iteration 1001:
Training Loss: 5.636600494384766
Reconstruction Loss: -0.5217273831367493
Iteration 1051:
Training Loss: 5.492291450500488
Reconstruction Loss: -0.521727442741394
Iteration 1101:
Training Loss: 5.498860836029053
Reconstruction Loss: -0.521727442741394
Iteration 1151:
Training Loss: 5.421864986419678
Reconstruction Loss: -0.5217275619506836
Iteration 1201:
Training Loss: 5.415583610534668
Reconstruction Loss: -0.5217275619506836
Iteration 1251:
Training Loss: 5.669544696807861
Reconstruction Loss: -0.5217276811599731
Iteration 1301:
Training Loss: 5.520229339599609
Reconstruction Loss: -0.5217276811599731
Iteration 1351:
Training Loss: 5.5497589111328125
Reconstruction Loss: -0.5217278003692627
Iteration 1401:
Training Loss: 5.579784870147705
Reconstruction Loss: -0.5217278599739075
Iteration 1451:
Training Loss: 5.507369041442871
Reconstruction Loss: -0.5217278599739075
Iteration 1501:
Training Loss: 5.436028957366943
Reconstruction Loss: -0.5217278599739075
Iteration 1551:
Training Loss: 5.395700931549072
Reconstruction Loss: -0.5217280983924866
Iteration 1601:
Training Loss: 5.556340217590332
Reconstruction Loss: -0.5217282772064209
Iteration 1651:
Training Loss: 5.465826988220215
Reconstruction Loss: -0.5217283964157104
Iteration 1701:
Training Loss: 5.379338264465332
Reconstruction Loss: -0.5217286944389343
Iteration 1751:
Training Loss: 5.588613510131836
Reconstruction Loss: -0.5217287540435791
Iteration 1801:
Training Loss: 5.60670280456543
Reconstruction Loss: -0.5217291116714478
Iteration 1851:
Training Loss: 5.499277114868164
Reconstruction Loss: -0.5217294692993164
Iteration 1901:
Training Loss: 5.3338141441345215
Reconstruction Loss: -0.5217300653457642
Iteration 1951:
Training Loss: 5.457697868347168
Reconstruction Loss: -0.5217306017875671
Iteration 2001:
Training Loss: 5.303157329559326
Reconstruction Loss: -0.5217312574386597
Iteration 2051:
Training Loss: 5.49194860458374
Reconstruction Loss: -0.5217323899269104
Iteration 2101:
Training Loss: 5.563093185424805
Reconstruction Loss: -0.5217337012290955
Iteration 2151:
Training Loss: 5.422231674194336
Reconstruction Loss: -0.5217354893684387
Iteration 2201:
Training Loss: 5.3492350578308105
Reconstruction Loss: -0.5217382907867432
Iteration 2251:
Training Loss: 5.551372051239014
Reconstruction Loss: -0.5217424631118774
Iteration 2301:
Training Loss: 5.574020862579346
Reconstruction Loss: -0.521748960018158
Iteration 2351:
Training Loss: 5.4249420166015625
Reconstruction Loss: -0.5217599272727966
Iteration 2401:
Training Loss: 5.42943811416626
Reconstruction Loss: -0.5217803120613098
Iteration 2451:
Training Loss: 5.511434078216553
Reconstruction Loss: -0.5218232870101929
Iteration 2501:
Training Loss: 5.482983112335205
Reconstruction Loss: -0.5219341516494751
Iteration 2551:
Training Loss: 5.501804828643799
Reconstruction Loss: -0.5223425626754761
Iteration 2601:
Training Loss: 5.453855991363525
Reconstruction Loss: -0.5255656242370605
Iteration 2651:
Training Loss: 5.1968865394592285
Reconstruction Loss: -0.6246089339256287
Iteration 2701:
Training Loss: 5.303133010864258
Reconstruction Loss: -0.6197047829627991
Iteration 2751:
Training Loss: 5.254743576049805
Reconstruction Loss: -0.6325675249099731
Iteration 2801:
Training Loss: 5.1124701499938965
Reconstruction Loss: -0.6566659808158875
Iteration 2851:
Training Loss: 4.75123405456543
Reconstruction Loss: -0.8337535262107849
Iteration 2901:
Training Loss: 4.749771595001221
Reconstruction Loss: -0.8529483675956726
Iteration 2951:
Training Loss: 4.7582478523254395
Reconstruction Loss: -0.8170658946037292
Iteration 3001:
Training Loss: 4.53958797454834
Reconstruction Loss: -0.7870008945465088
Iteration 3051:
Training Loss: 4.789697647094727
Reconstruction Loss: -0.7656221389770508
Iteration 3101:
Training Loss: 4.767379283905029
Reconstruction Loss: -0.7435829043388367
Iteration 3151:
Training Loss: 4.561692714691162
Reconstruction Loss: -0.7329193353652954
Iteration 3201:
Training Loss: 4.635770320892334
Reconstruction Loss: -0.7099835276603699
Iteration 3251:
Training Loss: 4.671976089477539
Reconstruction Loss: -0.692467451095581
Iteration 3301:
Training Loss: 4.656772136688232
Reconstruction Loss: -0.6804206371307373
Iteration 3351:
Training Loss: 4.5678839683532715
Reconstruction Loss: -0.6705136299133301
Iteration 3401:
Training Loss: 4.6959757804870605
Reconstruction Loss: -0.6668452024459839
Iteration 3451:
Training Loss: 4.514439582824707
Reconstruction Loss: -0.6582283973693848
Iteration 3501:
Training Loss: 4.60203742980957
Reconstruction Loss: -0.653298556804657
Iteration 3551:
Training Loss: 4.647904872894287
Reconstruction Loss: -0.6522936224937439
Iteration 3601:
Training Loss: 4.609796047210693
Reconstruction Loss: -0.6518580913543701
Iteration 3651:
Training Loss: 4.4054718017578125
Reconstruction Loss: -0.6545671224594116
Iteration 3701:
Training Loss: 4.6958208084106445
Reconstruction Loss: -0.646377682685852
Iteration 3751:
Training Loss: 4.584590911865234
Reconstruction Loss: -0.6479197144508362
Iteration 3801:
Training Loss: 4.511059761047363
Reconstruction Loss: -0.6511979103088379
Iteration 3851:
Training Loss: 4.435024738311768
Reconstruction Loss: -0.6664156913757324
Iteration 3901:
Training Loss: 4.287032127380371
Reconstruction Loss: -0.8512381315231323
Iteration 3951:
Training Loss: 4.071940898895264
Reconstruction Loss: -0.907233715057373
Iteration 4001:
Training Loss: 3.9668171405792236
Reconstruction Loss: -0.9101126194000244
Iteration 4051:
Training Loss: 4.1016716957092285
Reconstruction Loss: -0.9019780158996582
Iteration 4101:
Training Loss: 3.993539810180664
Reconstruction Loss: -0.9043880701065063
Iteration 4151:
Training Loss: 4.082429885864258
Reconstruction Loss: -0.9093701243400574
Iteration 4201:
Training Loss: 4.067461967468262
Reconstruction Loss: -0.9154937267303467
Iteration 4251:
Training Loss: 4.039717674255371
Reconstruction Loss: -0.9253678321838379
Iteration 4301:
Training Loss: 3.9473507404327393
Reconstruction Loss: -0.9421999454498291
Iteration 4351:
Training Loss: 4.057785987854004
Reconstruction Loss: -0.9551059603691101
Iteration 4401:
Training Loss: 3.9440672397613525
Reconstruction Loss: -0.96868497133255
Iteration 4451:
Training Loss: 3.987541437149048
Reconstruction Loss: -0.9802136421203613
Iteration 4501:
Training Loss: 3.863492488861084
Reconstruction Loss: -1.0175976753234863
Iteration 4551:
Training Loss: 3.373513698577881
Reconstruction Loss: -1.2343721389770508
Iteration 4601:
Training Loss: 3.066291570663452
Reconstruction Loss: -1.4343633651733398
Iteration 4651:
Training Loss: 3.04803466796875
Reconstruction Loss: -1.4922975301742554
Iteration 4701:
Training Loss: 3.057826042175293
Reconstruction Loss: -1.5093662738800049
Iteration 4751:
Training Loss: 3.061697006225586
Reconstruction Loss: -1.519209623336792
Iteration 4801:
Training Loss: 2.8709208965301514
Reconstruction Loss: -1.5288656949996948
Iteration 4851:
Training Loss: 2.948214530944824
Reconstruction Loss: -1.536897897720337
Iteration 4901:
Training Loss: 2.9776790142059326
Reconstruction Loss: -1.5422401428222656
Iteration 4951:
Training Loss: 2.8983120918273926
Reconstruction Loss: -1.55378258228302
Iteration 5001:
Training Loss: 2.8190712928771973
Reconstruction Loss: -1.5617485046386719
Iteration 5051:
Training Loss: 2.953826427459717
Reconstruction Loss: -1.5673205852508545
Iteration 5101:
Training Loss: 2.734893560409546
Reconstruction Loss: -1.5752217769622803
Iteration 5151:
Training Loss: 2.979323387145996
Reconstruction Loss: -1.581045150756836
Iteration 5201:
Training Loss: 2.7832705974578857
Reconstruction Loss: -1.5844473838806152
Iteration 5251:
Training Loss: 2.979240655899048
Reconstruction Loss: -1.5882060527801514
Iteration 5301:
Training Loss: 2.847707748413086
Reconstruction Loss: -1.5852851867675781
Iteration 5351:
Training Loss: 2.9371867179870605
Reconstruction Loss: -1.5873749256134033
Iteration 5401:
Training Loss: 2.798644781112671
Reconstruction Loss: -1.5832844972610474
Iteration 5451:
Training Loss: 2.873636245727539
Reconstruction Loss: -1.5830116271972656
Iteration 5501:
Training Loss: 2.825540781021118
Reconstruction Loss: -1.579417109489441
Iteration 5551:
Training Loss: 2.9372549057006836
Reconstruction Loss: -1.5782971382141113
Iteration 5601:
Training Loss: 2.7941958904266357
Reconstruction Loss: -1.5806519985198975
Iteration 5651:
Training Loss: 2.9079344272613525
Reconstruction Loss: -1.5718876123428345
Iteration 5701:
Training Loss: 2.8667774200439453
Reconstruction Loss: -1.5735102891921997
Iteration 5751:
Training Loss: 2.970120906829834
Reconstruction Loss: -1.5701897144317627
Iteration 5801:
Training Loss: 2.9411466121673584
Reconstruction Loss: -1.5653618574142456
Iteration 5851:
Training Loss: 2.793959379196167
Reconstruction Loss: -1.5627660751342773
Iteration 5901:
Training Loss: 2.763678789138794
Reconstruction Loss: -1.5631991624832153
Iteration 5951:
Training Loss: 2.7553863525390625
Reconstruction Loss: -1.5612341165542603
Iteration 6001:
Training Loss: 2.8899896144866943
Reconstruction Loss: -1.5588449239730835
Iteration 6051:
Training Loss: 2.850222110748291
Reconstruction Loss: -1.5581543445587158
Iteration 6101:
Training Loss: 2.726858139038086
Reconstruction Loss: -1.5582889318466187
Iteration 6151:
Training Loss: 2.8246593475341797
Reconstruction Loss: -1.5531504154205322
Iteration 6201:
Training Loss: 2.7723441123962402
Reconstruction Loss: -1.5582895278930664
Iteration 6251:
Training Loss: 2.80535888671875
Reconstruction Loss: -1.5536664724349976
Iteration 6301:
Training Loss: 2.897095203399658
Reconstruction Loss: -1.554810643196106
Iteration 6351:
Training Loss: 2.8882367610931396
Reconstruction Loss: -1.5492032766342163
Iteration 6401:
Training Loss: 2.7676899433135986
Reconstruction Loss: -1.546238660812378
Iteration 6451:
Training Loss: 2.9453163146972656
Reconstruction Loss: -1.5503703355789185
Iteration 6501:
Training Loss: 2.8280768394470215
Reconstruction Loss: -1.546779990196228
Iteration 6551:
Training Loss: 2.830460548400879
Reconstruction Loss: -1.5446456670761108
Iteration 6601:
Training Loss: 2.9375669956207275
Reconstruction Loss: -1.5490657091140747
Iteration 6651:
Training Loss: 2.9528310298919678
Reconstruction Loss: -1.545703411102295
Iteration 6701:
Training Loss: 2.7940495014190674
Reconstruction Loss: -1.5463581085205078
Iteration 6751:
Training Loss: 2.951728343963623
Reconstruction Loss: -1.5440784692764282
Iteration 6801:
Training Loss: 2.9266602993011475
Reconstruction Loss: -1.546274185180664
Iteration 6851:
Training Loss: 2.8695170879364014
Reconstruction Loss: -1.541702389717102
Iteration 6901:
Training Loss: 2.8191428184509277
Reconstruction Loss: -1.546324610710144
Iteration 6951:
Training Loss: 2.9398093223571777
Reconstruction Loss: -1.5389254093170166
Iteration 7001:
Training Loss: 2.8029091358184814
Reconstruction Loss: -1.5434489250183105
Iteration 7051:
Training Loss: 2.7039597034454346
Reconstruction Loss: -1.5422477722167969
Iteration 7101:
Training Loss: 2.934722661972046
Reconstruction Loss: -1.5416053533554077
Iteration 7151:
Training Loss: 2.9036996364593506
Reconstruction Loss: -1.5427119731903076
Iteration 7201:
Training Loss: 2.80936598777771
Reconstruction Loss: -1.5443357229232788
Iteration 7251:
Training Loss: 2.876368761062622
Reconstruction Loss: -1.5423791408538818
Iteration 7301:
Training Loss: 2.892458200454712
Reconstruction Loss: -1.5417804718017578
Iteration 7351:
Training Loss: 2.7988815307617188
Reconstruction Loss: -1.5412968397140503
Iteration 7401:
Training Loss: 2.762885570526123
Reconstruction Loss: -1.5427337884902954
Iteration 7451:
Training Loss: 2.902951955795288
Reconstruction Loss: -1.54350745677948
