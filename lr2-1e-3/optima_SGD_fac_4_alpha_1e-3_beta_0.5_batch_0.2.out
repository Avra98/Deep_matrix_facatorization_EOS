5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.628521919250488
Reconstruction Loss: -0.5978865027427673
Iteration 21:
Training Loss: 3.767336368560791
Reconstruction Loss: -1.3181864023208618
Iteration 41:
Training Loss: 3.033761501312256
Reconstruction Loss: -1.6640772819519043
Iteration 61:
Training Loss: 2.6303253173828125
Reconstruction Loss: -1.826504111289978
Iteration 81:
Training Loss: 1.8236933946609497
Reconstruction Loss: -2.1090455055236816
Iteration 101:
Training Loss: 1.3588457107543945
Reconstruction Loss: -2.5068106651306152
Iteration 121:
Training Loss: 0.3168119788169861
Reconstruction Loss: -2.899681329727173
Iteration 141:
Training Loss: 0.18901707231998444
Reconstruction Loss: -3.247433662414551
Iteration 161:
Training Loss: -0.5371028184890747
Reconstruction Loss: -3.5301215648651123
Iteration 181:
Training Loss: -0.6539131999015808
Reconstruction Loss: -3.754094362258911
Iteration 201:
Training Loss: -0.7342778444290161
Reconstruction Loss: -3.932072401046753
Iteration 221:
Training Loss: -1.2952463626861572
Reconstruction Loss: -4.0791521072387695
Iteration 241:
Training Loss: -1.2582066059112549
Reconstruction Loss: -4.197135925292969
Iteration 261:
Training Loss: -1.4117090702056885
Reconstruction Loss: -4.3076252937316895
Iteration 281:
Training Loss: -1.8313305377960205
Reconstruction Loss: -4.401416301727295
Iteration 301:
Training Loss: -1.764890193939209
Reconstruction Loss: -4.48253059387207
Iteration 321:
Training Loss: -1.897303819656372
Reconstruction Loss: -4.559354305267334
Iteration 341:
Training Loss: -2.1448264122009277
Reconstruction Loss: -4.626888275146484
Iteration 361:
Training Loss: -2.0564463138580322
Reconstruction Loss: -4.6889729499816895
Iteration 381:
Training Loss: -2.195713996887207
Reconstruction Loss: -4.7488861083984375
Iteration 401:
Training Loss: -2.2495386600494385
Reconstruction Loss: -4.800189018249512
Iteration 421:
Training Loss: -2.368360757827759
Reconstruction Loss: -4.852408409118652
Iteration 441:
Training Loss: -2.632894277572632
Reconstruction Loss: -4.900922775268555
Iteration 461:
Training Loss: -2.6735446453094482
Reconstruction Loss: -4.949052333831787
Iteration 481:
Training Loss: -2.8003416061401367
Reconstruction Loss: -4.989147186279297
Iteration 501:
Training Loss: -2.6098897457122803
Reconstruction Loss: -5.029505729675293
Iteration 521:
Training Loss: -2.615953207015991
Reconstruction Loss: -5.068292140960693
Iteration 541:
Training Loss: -2.658339023590088
Reconstruction Loss: -5.109231948852539
Iteration 561:
Training Loss: -2.6399004459381104
Reconstruction Loss: -5.141634464263916
Iteration 581:
Training Loss: -2.857935667037964
Reconstruction Loss: -5.1758928298950195
Iteration 601:
Training Loss: -2.685492992401123
Reconstruction Loss: -5.209246635437012
Iteration 621:
Training Loss: -2.8275339603424072
Reconstruction Loss: -5.241267204284668
Iteration 641:
Training Loss: -3.077850103378296
Reconstruction Loss: -5.270649433135986
Iteration 661:
Training Loss: -3.1422998905181885
Reconstruction Loss: -5.299941062927246
Iteration 681:
Training Loss: -3.002709150314331
Reconstruction Loss: -5.325033664703369
Iteration 701:
Training Loss: -3.2793827056884766
Reconstruction Loss: -5.3543243408203125
Iteration 721:
Training Loss: -3.0535058975219727
Reconstruction Loss: -5.381093978881836
Iteration 741:
Training Loss: -3.391383171081543
Reconstruction Loss: -5.408458709716797
Iteration 761:
Training Loss: -3.3937361240386963
Reconstruction Loss: -5.431487083435059
Iteration 781:
Training Loss: -3.161853313446045
Reconstruction Loss: -5.455962657928467
Iteration 801:
Training Loss: -3.4399304389953613
Reconstruction Loss: -5.479161262512207
Iteration 821:
Training Loss: -3.5512821674346924
Reconstruction Loss: -5.500156879425049
Iteration 841:
Training Loss: -3.5108895301818848
Reconstruction Loss: -5.520660400390625
Iteration 861:
Training Loss: -3.180055856704712
Reconstruction Loss: -5.543570518493652
Iteration 881:
Training Loss: -3.673140287399292
Reconstruction Loss: -5.564707279205322
Iteration 901:
Training Loss: -3.781883716583252
Reconstruction Loss: -5.583338260650635
Iteration 921:
Training Loss: -3.5528788566589355
Reconstruction Loss: -5.604519844055176
Iteration 941:
Training Loss: -3.600755453109741
Reconstruction Loss: -5.622255802154541
Iteration 961:
Training Loss: -3.659891366958618
Reconstruction Loss: -5.6407365798950195
Iteration 981:
Training Loss: -3.5798511505126953
Reconstruction Loss: -5.657809257507324
Iteration 1001:
Training Loss: -3.7728867530822754
Reconstruction Loss: -5.676663398742676
Iteration 1021:
Training Loss: -3.6548688411712646
Reconstruction Loss: -5.693592548370361
Iteration 1041:
Training Loss: -3.5193045139312744
Reconstruction Loss: -5.710031509399414
Iteration 1061:
Training Loss: -3.6592342853546143
Reconstruction Loss: -5.725799560546875
Iteration 1081:
Training Loss: -3.814333438873291
Reconstruction Loss: -5.743557929992676
Iteration 1101:
Training Loss: -3.88738751411438
Reconstruction Loss: -5.758784294128418
Iteration 1121:
Training Loss: -3.747001886367798
Reconstruction Loss: -5.772230625152588
Iteration 1141:
Training Loss: -3.884230852127075
Reconstruction Loss: -5.7862443923950195
Iteration 1161:
Training Loss: -3.967089891433716
Reconstruction Loss: -5.801448822021484
Iteration 1181:
Training Loss: -3.954500436782837
Reconstruction Loss: -5.816408157348633
Iteration 1201:
Training Loss: -3.9267568588256836
Reconstruction Loss: -5.830075740814209
Iteration 1221:
Training Loss: -4.063177108764648
Reconstruction Loss: -5.844595432281494
Iteration 1241:
Training Loss: -4.057760715484619
Reconstruction Loss: -5.857161045074463
Iteration 1261:
Training Loss: -4.162802219390869
Reconstruction Loss: -5.87097692489624
Iteration 1281:
Training Loss: -4.111562252044678
Reconstruction Loss: -5.882676601409912
Iteration 1301:
Training Loss: -4.315282344818115
Reconstruction Loss: -5.8961358070373535
Iteration 1321:
Training Loss: -3.9540586471557617
Reconstruction Loss: -5.905470371246338
Iteration 1341:
Training Loss: -4.367377281188965
Reconstruction Loss: -5.918355464935303
Iteration 1361:
Training Loss: -4.362419605255127
Reconstruction Loss: -5.932416915893555
Iteration 1381:
Training Loss: -4.345517158508301
Reconstruction Loss: -5.941521644592285
Iteration 1401:
Training Loss: -4.21934700012207
Reconstruction Loss: -5.954401016235352
Iteration 1421:
Training Loss: -4.3058342933654785
Reconstruction Loss: -5.9652228355407715
Iteration 1441:
Training Loss: -4.197573661804199
Reconstruction Loss: -5.974520206451416
Iteration 1461:
Training Loss: -4.312324523925781
Reconstruction Loss: -5.9869585037231445
Iteration 1481:
Training Loss: -4.103874206542969
Reconstruction Loss: -6.000294208526611
Iteration 1501:
Training Loss: -4.186825275421143
Reconstruction Loss: -6.007775783538818
Iteration 1521:
Training Loss: -4.480015277862549
Reconstruction Loss: -6.017419338226318
Iteration 1541:
Training Loss: -4.076403617858887
Reconstruction Loss: -6.02943229675293
Iteration 1561:
Training Loss: -4.485523700714111
Reconstruction Loss: -6.038685321807861
Iteration 1581:
Training Loss: -4.441597938537598
Reconstruction Loss: -6.048948764801025
Iteration 1601:
Training Loss: -4.4140496253967285
Reconstruction Loss: -6.057577610015869
Iteration 1621:
Training Loss: -4.439820289611816
Reconstruction Loss: -6.065491676330566
Iteration 1641:
Training Loss: -4.277402400970459
Reconstruction Loss: -6.076780796051025
Iteration 1661:
Training Loss: -4.375776767730713
Reconstruction Loss: -6.084502220153809
Iteration 1681:
Training Loss: -4.540987491607666
Reconstruction Loss: -6.091757297515869
Iteration 1701:
Training Loss: -4.271560192108154
Reconstruction Loss: -6.102416038513184
Iteration 1721:
Training Loss: -4.391279697418213
Reconstruction Loss: -6.11110782623291
Iteration 1741:
Training Loss: -4.5949296951293945
Reconstruction Loss: -6.120433330535889
Iteration 1761:
Training Loss: -4.9020256996154785
Reconstruction Loss: -6.128875732421875
Iteration 1781:
Training Loss: -4.501140117645264
Reconstruction Loss: -6.137133598327637
Iteration 1801:
Training Loss: -4.725272178649902
Reconstruction Loss: -6.14554500579834
Iteration 1821:
Training Loss: -4.518767356872559
Reconstruction Loss: -6.152019500732422
Iteration 1841:
Training Loss: -4.817902565002441
Reconstruction Loss: -6.16206693649292
Iteration 1861:
Training Loss: -4.818414688110352
Reconstruction Loss: -6.167886734008789
Iteration 1881:
Training Loss: -4.605688571929932
Reconstruction Loss: -6.175892353057861
Iteration 1901:
Training Loss: -4.7119317054748535
Reconstruction Loss: -6.183309555053711
Iteration 1921:
Training Loss: -4.7301201820373535
Reconstruction Loss: -6.191962242126465
Iteration 1941:
Training Loss: -4.844394683837891
Reconstruction Loss: -6.198519229888916
Iteration 1961:
Training Loss: -4.546041488647461
Reconstruction Loss: -6.205578327178955
Iteration 1981:
Training Loss: -4.509776592254639
Reconstruction Loss: -6.213292598724365
Iteration 2001:
Training Loss: -4.851648330688477
Reconstruction Loss: -6.219959735870361
Iteration 2021:
Training Loss: -4.990721702575684
Reconstruction Loss: -6.227462291717529
Iteration 2041:
Training Loss: -4.7886505126953125
Reconstruction Loss: -6.235110282897949
Iteration 2061:
Training Loss: -4.702030181884766
Reconstruction Loss: -6.241668701171875
Iteration 2081:
Training Loss: -5.045135974884033
Reconstruction Loss: -6.246956825256348
Iteration 2101:
Training Loss: -4.972438335418701
Reconstruction Loss: -6.2534685134887695
Iteration 2121:
Training Loss: -5.069851398468018
Reconstruction Loss: -6.260879993438721
Iteration 2141:
Training Loss: -5.098815441131592
Reconstruction Loss: -6.266782760620117
Iteration 2161:
Training Loss: -4.834312438964844
Reconstruction Loss: -6.275514602661133
Iteration 2181:
Training Loss: -4.781538009643555
Reconstruction Loss: -6.278081893920898
Iteration 2201:
Training Loss: -4.952378749847412
Reconstruction Loss: -6.284807205200195
Iteration 2221:
Training Loss: -5.16009521484375
Reconstruction Loss: -6.291404724121094
Iteration 2241:
Training Loss: -4.898263454437256
Reconstruction Loss: -6.29518985748291
Iteration 2261:
Training Loss: -4.854697227478027
Reconstruction Loss: -6.301318645477295
Iteration 2281:
Training Loss: -5.0453948974609375
Reconstruction Loss: -6.308933734893799
Iteration 2301:
Training Loss: -4.872286319732666
Reconstruction Loss: -6.314713954925537
Iteration 2321:
Training Loss: -5.130979061126709
Reconstruction Loss: -6.320613861083984
Iteration 2341:
Training Loss: -4.928387641906738
Reconstruction Loss: -6.326040267944336
Iteration 2361:
Training Loss: -4.833303451538086
Reconstruction Loss: -6.331138610839844
Iteration 2381:
Training Loss: -4.865865707397461
Reconstruction Loss: -6.335689067840576
Iteration 2401:
Training Loss: -5.038877964019775
Reconstruction Loss: -6.341189384460449
Iteration 2421:
Training Loss: -5.0849127769470215
Reconstruction Loss: -6.345736980438232
Iteration 2441:
Training Loss: -4.831479072570801
Reconstruction Loss: -6.351752281188965
Iteration 2461:
Training Loss: -5.109260082244873
Reconstruction Loss: -6.357101917266846
Iteration 2481:
Training Loss: -4.984314441680908
Reconstruction Loss: -6.363322734832764
Iteration 2501:
Training Loss: -5.080449104309082
Reconstruction Loss: -6.366815567016602
Iteration 2521:
Training Loss: -5.1505327224731445
Reconstruction Loss: -6.37184476852417
Iteration 2541:
Training Loss: -5.162429332733154
Reconstruction Loss: -6.377132415771484
Iteration 2561:
Training Loss: -5.387790679931641
Reconstruction Loss: -6.384629249572754
Iteration 2581:
Training Loss: -5.006979465484619
Reconstruction Loss: -6.386814117431641
Iteration 2601:
Training Loss: -5.068912506103516
Reconstruction Loss: -6.391655921936035
Iteration 2621:
Training Loss: -5.016706943511963
Reconstruction Loss: -6.395721435546875
Iteration 2641:
Training Loss: -5.612099647521973
Reconstruction Loss: -6.401697635650635
Iteration 2661:
Training Loss: -5.1863179206848145
Reconstruction Loss: -6.406062602996826
Iteration 2681:
Training Loss: -5.285264015197754
Reconstruction Loss: -6.410574913024902
Iteration 2701:
Training Loss: -5.244048595428467
Reconstruction Loss: -6.4132280349731445
Iteration 2721:
Training Loss: -5.482208728790283
Reconstruction Loss: -6.419873237609863
Iteration 2741:
Training Loss: -5.331665515899658
Reconstruction Loss: -6.4248151779174805
Iteration 2761:
Training Loss: -5.052661895751953
Reconstruction Loss: -6.427518844604492
Iteration 2781:
Training Loss: -5.177159786224365
Reconstruction Loss: -6.432559490203857
Iteration 2801:
Training Loss: -5.323262691497803
Reconstruction Loss: -6.4362897872924805
Iteration 2821:
Training Loss: -5.395998001098633
Reconstruction Loss: -6.442077159881592
Iteration 2841:
Training Loss: -5.181375026702881
Reconstruction Loss: -6.445532321929932
Iteration 2861:
Training Loss: -5.296391010284424
Reconstruction Loss: -6.449336051940918
Iteration 2881:
Training Loss: -5.491479396820068
Reconstruction Loss: -6.453645706176758
Iteration 2901:
Training Loss: -5.30621862411499
Reconstruction Loss: -6.45622444152832
Iteration 2921:
Training Loss: -5.384613513946533
Reconstruction Loss: -6.459219455718994
Iteration 2941:
Training Loss: -5.576559066772461
Reconstruction Loss: -6.4662699699401855
Iteration 2961:
Training Loss: -5.318938255310059
Reconstruction Loss: -6.46879768371582
Iteration 2981:
Training Loss: -5.773183822631836
Reconstruction Loss: -6.472691535949707
