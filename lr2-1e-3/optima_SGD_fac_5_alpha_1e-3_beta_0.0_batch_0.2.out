5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.7171478271484375
Reconstruction Loss: -0.4892677664756775
Iteration 21:
Training Loss: 2.408067226409912
Reconstruction Loss: -1.6577026844024658
Iteration 41:
Training Loss: 1.1324222087860107
Reconstruction Loss: -2.2503392696380615
Iteration 61:
Training Loss: 0.4508408308029175
Reconstruction Loss: -2.663311243057251
Iteration 81:
Training Loss: -0.33528757095336914
Reconstruction Loss: -2.959105968475342
Iteration 101:
Training Loss: -0.49048274755477905
Reconstruction Loss: -3.1566851139068604
Iteration 121:
Training Loss: -0.9805110692977905
Reconstruction Loss: -3.319305658340454
Iteration 141:
Training Loss: -1.0476107597351074
Reconstruction Loss: -3.436008930206299
Iteration 161:
Training Loss: -1.4171507358551025
Reconstruction Loss: -3.529303789138794
Iteration 181:
Training Loss: -1.7417187690734863
Reconstruction Loss: -3.6102097034454346
Iteration 201:
Training Loss: -1.9923169612884521
Reconstruction Loss: -3.674699068069458
Iteration 221:
Training Loss: -1.8150948286056519
Reconstruction Loss: -3.7301204204559326
Iteration 241:
Training Loss: -1.8933956623077393
Reconstruction Loss: -3.7804319858551025
Iteration 261:
Training Loss: -2.251138210296631
Reconstruction Loss: -3.8286683559417725
Iteration 281:
Training Loss: -2.1595616340637207
Reconstruction Loss: -3.8722736835479736
Iteration 301:
Training Loss: -2.1837871074676514
Reconstruction Loss: -3.9068782329559326
Iteration 321:
Training Loss: -2.4219212532043457
Reconstruction Loss: -3.940161943435669
Iteration 341:
Training Loss: -2.562588691711426
Reconstruction Loss: -3.9772887229919434
Iteration 361:
Training Loss: -2.5407421588897705
Reconstruction Loss: -4.004807472229004
Iteration 381:
Training Loss: -2.340897798538208
Reconstruction Loss: -4.02943229675293
Iteration 401:
Training Loss: -2.895435333251953
Reconstruction Loss: -4.060842514038086
Iteration 421:
Training Loss: -3.002092123031616
Reconstruction Loss: -4.082268238067627
Iteration 441:
Training Loss: -2.905095100402832
Reconstruction Loss: -4.106966018676758
Iteration 461:
Training Loss: -2.838562250137329
Reconstruction Loss: -4.127366542816162
Iteration 481:
Training Loss: -2.929626941680908
Reconstruction Loss: -4.147218227386475
Iteration 501:
Training Loss: -3.0223631858825684
Reconstruction Loss: -4.171229839324951
Iteration 521:
Training Loss: -3.1638357639312744
Reconstruction Loss: -4.1909661293029785
Iteration 541:
Training Loss: -2.9783291816711426
Reconstruction Loss: -4.209399223327637
Iteration 561:
Training Loss: -3.230884075164795
Reconstruction Loss: -4.227172374725342
Iteration 581:
Training Loss: -2.8598432540893555
Reconstruction Loss: -4.242317199707031
Iteration 601:
Training Loss: -3.0904910564422607
Reconstruction Loss: -4.255887508392334
Iteration 621:
Training Loss: -3.349315881729126
Reconstruction Loss: -4.2724738121032715
Iteration 641:
Training Loss: -3.1858444213867188
Reconstruction Loss: -4.288336277008057
Iteration 661:
Training Loss: -3.661196708679199
Reconstruction Loss: -4.302910327911377
Iteration 681:
Training Loss: -3.39099383354187
Reconstruction Loss: -4.317800521850586
Iteration 701:
Training Loss: -3.4789814949035645
Reconstruction Loss: -4.3292975425720215
Iteration 721:
Training Loss: -3.354053020477295
Reconstruction Loss: -4.3427557945251465
Iteration 741:
Training Loss: -3.5548295974731445
Reconstruction Loss: -4.356035232543945
Iteration 761:
Training Loss: -3.597198486328125
Reconstruction Loss: -4.367733001708984
Iteration 781:
Training Loss: -3.8433499336242676
Reconstruction Loss: -4.379640102386475
Iteration 801:
Training Loss: -3.6345317363739014
Reconstruction Loss: -4.393170356750488
Iteration 821:
Training Loss: -3.4815785884857178
Reconstruction Loss: -4.400961875915527
Iteration 841:
Training Loss: -3.6548476219177246
Reconstruction Loss: -4.413651943206787
Iteration 861:
Training Loss: -3.7015912532806396
Reconstruction Loss: -4.424656867980957
Iteration 881:
Training Loss: -4.000827312469482
Reconstruction Loss: -4.431654453277588
Iteration 901:
Training Loss: -3.793060302734375
Reconstruction Loss: -4.443575382232666
Iteration 921:
Training Loss: -3.54833722114563
Reconstruction Loss: -4.452171325683594
Iteration 941:
Training Loss: -3.9450910091400146
Reconstruction Loss: -4.463785648345947
Iteration 961:
Training Loss: -3.709111452102661
Reconstruction Loss: -4.470855236053467
Iteration 981:
Training Loss: -3.9403276443481445
Reconstruction Loss: -4.48133659362793
Iteration 1001:
Training Loss: -4.114430904388428
Reconstruction Loss: -4.492105484008789
Iteration 1021:
Training Loss: -3.996110439300537
Reconstruction Loss: -4.497989654541016
Iteration 1041:
Training Loss: -3.8171725273132324
Reconstruction Loss: -4.507063388824463
Iteration 1061:
Training Loss: -4.1703267097473145
Reconstruction Loss: -4.5143585205078125
Iteration 1081:
Training Loss: -3.968447685241699
Reconstruction Loss: -4.523641109466553
Iteration 1101:
Training Loss: -3.985023260116577
Reconstruction Loss: -4.532170295715332
Iteration 1121:
Training Loss: -4.077582359313965
Reconstruction Loss: -4.540185451507568
Iteration 1141:
Training Loss: -4.178995609283447
Reconstruction Loss: -4.546806335449219
Iteration 1161:
Training Loss: -4.033064365386963
Reconstruction Loss: -4.553676605224609
Iteration 1181:
Training Loss: -4.1854095458984375
Reconstruction Loss: -4.5600433349609375
Iteration 1201:
Training Loss: -4.1722846031188965
Reconstruction Loss: -4.566394329071045
Iteration 1221:
Training Loss: -4.17510461807251
Reconstruction Loss: -4.574860572814941
Iteration 1241:
Training Loss: -4.0727949142456055
Reconstruction Loss: -4.5807085037231445
Iteration 1261:
Training Loss: -4.443363666534424
Reconstruction Loss: -4.588394641876221
Iteration 1281:
Training Loss: -4.322002410888672
Reconstruction Loss: -4.595210552215576
Iteration 1301:
Training Loss: -4.364929676055908
Reconstruction Loss: -4.601408958435059
Iteration 1321:
Training Loss: -4.1607255935668945
Reconstruction Loss: -4.605842113494873
Iteration 1341:
Training Loss: -4.113205909729004
Reconstruction Loss: -4.610716342926025
Iteration 1361:
Training Loss: -4.577212333679199
Reconstruction Loss: -4.620236396789551
Iteration 1381:
Training Loss: -4.637414932250977
Reconstruction Loss: -4.623695373535156
Iteration 1401:
Training Loss: -4.655500411987305
Reconstruction Loss: -4.628691673278809
Iteration 1421:
Training Loss: -4.393028736114502
Reconstruction Loss: -4.636068344116211
Iteration 1441:
Training Loss: -4.331916332244873
Reconstruction Loss: -4.641707420349121
Iteration 1461:
Training Loss: -4.211237907409668
Reconstruction Loss: -4.648037910461426
Iteration 1481:
Training Loss: -4.747480392456055
Reconstruction Loss: -4.650340557098389
Iteration 1501:
Training Loss: -4.425642013549805
Reconstruction Loss: -4.657176971435547
Iteration 1521:
Training Loss: -4.637669563293457
Reconstruction Loss: -4.663009166717529
Iteration 1541:
Training Loss: -4.466831684112549
Reconstruction Loss: -4.668313026428223
Iteration 1561:
Training Loss: -4.602331638336182
Reconstruction Loss: -4.672887802124023
Iteration 1581:
Training Loss: -4.253143787384033
Reconstruction Loss: -4.67738151550293
Iteration 1601:
Training Loss: -4.5166425704956055
Reconstruction Loss: -4.684021472930908
Iteration 1621:
Training Loss: -4.675737380981445
Reconstruction Loss: -4.687175750732422
Iteration 1641:
Training Loss: -4.701717853546143
Reconstruction Loss: -4.692315578460693
Iteration 1661:
Training Loss: -4.561759948730469
Reconstruction Loss: -4.696643352508545
Iteration 1681:
Training Loss: -4.635817527770996
Reconstruction Loss: -4.700232028961182
Iteration 1701:
Training Loss: -4.708804130554199
Reconstruction Loss: -4.704288005828857
Iteration 1721:
Training Loss: -4.714056491851807
Reconstruction Loss: -4.708701133728027
Iteration 1741:
Training Loss: -4.588374614715576
Reconstruction Loss: -4.713376045227051
Iteration 1761:
Training Loss: -4.771872043609619
Reconstruction Loss: -4.718166351318359
Iteration 1781:
Training Loss: -4.651603698730469
Reconstruction Loss: -4.723031997680664
Iteration 1801:
Training Loss: -4.493078708648682
Reconstruction Loss: -4.727775573730469
Iteration 1821:
Training Loss: -4.850761890411377
Reconstruction Loss: -4.730292797088623
Iteration 1841:
Training Loss: -4.5547966957092285
Reconstruction Loss: -4.734071254730225
Iteration 1861:
Training Loss: -5.091164588928223
Reconstruction Loss: -4.7401838302612305
Iteration 1881:
Training Loss: -5.0583672523498535
Reconstruction Loss: -4.743067741394043
Iteration 1901:
Training Loss: -4.9426069259643555
Reconstruction Loss: -4.745843410491943
Iteration 1921:
Training Loss: -4.778735160827637
Reconstruction Loss: -4.750560283660889
Iteration 1941:
Training Loss: -4.556597709655762
Reconstruction Loss: -4.753589153289795
Iteration 1961:
Training Loss: -4.8036394119262695
Reconstruction Loss: -4.755852699279785
Iteration 1981:
Training Loss: -4.879359722137451
Reconstruction Loss: -4.7595672607421875
Iteration 2001:
Training Loss: -4.910301685333252
Reconstruction Loss: -4.763299465179443
Iteration 2021:
Training Loss: -4.8053154945373535
Reconstruction Loss: -4.76800012588501
Iteration 2041:
Training Loss: -4.840123176574707
Reconstruction Loss: -4.769633769989014
Iteration 2061:
Training Loss: -4.983915328979492
Reconstruction Loss: -4.773568630218506
Iteration 2081:
Training Loss: -4.960201263427734
Reconstruction Loss: -4.78013277053833
Iteration 2101:
Training Loss: -5.037259101867676
Reconstruction Loss: -4.782694339752197
Iteration 2121:
Training Loss: -4.955517292022705
Reconstruction Loss: -4.784391403198242
Iteration 2141:
Training Loss: -5.059286117553711
Reconstruction Loss: -4.786873817443848
Iteration 2161:
Training Loss: -5.109408855438232
Reconstruction Loss: -4.790281295776367
Iteration 2181:
Training Loss: -5.115010738372803
Reconstruction Loss: -4.794148921966553
Iteration 2201:
Training Loss: -4.9539079666137695
Reconstruction Loss: -4.797518253326416
Iteration 2221:
Training Loss: -5.077122688293457
Reconstruction Loss: -4.799126148223877
Iteration 2241:
Training Loss: -5.2068986892700195
Reconstruction Loss: -4.803448677062988
Iteration 2261:
Training Loss: -5.151130199432373
Reconstruction Loss: -4.805423736572266
Iteration 2281:
Training Loss: -4.9796552658081055
Reconstruction Loss: -4.808911323547363
Iteration 2301:
Training Loss: -4.873032093048096
Reconstruction Loss: -4.810764312744141
Iteration 2321:
Training Loss: -5.152774333953857
Reconstruction Loss: -4.8147664070129395
Iteration 2341:
Training Loss: -5.1569437980651855
Reconstruction Loss: -4.81921911239624
Iteration 2361:
Training Loss: -5.397388935089111
Reconstruction Loss: -4.820385932922363
Iteration 2381:
Training Loss: -5.1728010177612305
Reconstruction Loss: -4.822502613067627
Iteration 2401:
Training Loss: -4.987495422363281
Reconstruction Loss: -4.823721885681152
Iteration 2421:
Training Loss: -5.364681720733643
Reconstruction Loss: -4.827162742614746
Iteration 2441:
Training Loss: -5.074687957763672
Reconstruction Loss: -4.830175876617432
Iteration 2461:
Training Loss: -5.1548991203308105
Reconstruction Loss: -4.832310199737549
Iteration 2481:
Training Loss: -5.400806903839111
Reconstruction Loss: -4.835352897644043
Iteration 2501:
Training Loss: -5.266074180603027
Reconstruction Loss: -4.838570594787598
Iteration 2521:
Training Loss: -5.324163436889648
Reconstruction Loss: -4.841465950012207
Iteration 2541:
Training Loss: -5.176433086395264
Reconstruction Loss: -4.843471527099609
Iteration 2561:
Training Loss: -5.322816848754883
Reconstruction Loss: -4.845291614532471
Iteration 2581:
Training Loss: -5.364840030670166
Reconstruction Loss: -4.848379611968994
Iteration 2601:
Training Loss: -5.0967912673950195
Reconstruction Loss: -4.851447582244873
Iteration 2621:
Training Loss: -5.154443264007568
Reconstruction Loss: -4.852279186248779
Iteration 2641:
Training Loss: -5.735796928405762
Reconstruction Loss: -4.8546295166015625
Iteration 2661:
Training Loss: -5.288443565368652
Reconstruction Loss: -4.857346057891846
Iteration 2681:
Training Loss: -5.41263484954834
Reconstruction Loss: -4.859496116638184
Iteration 2701:
Training Loss: -5.3515849113464355
Reconstruction Loss: -4.862765312194824
Iteration 2721:
Training Loss: -5.1460723876953125
Reconstruction Loss: -4.865824222564697
Iteration 2741:
Training Loss: -5.039043426513672
Reconstruction Loss: -4.8677263259887695
Iteration 2761:
Training Loss: -5.555323600769043
Reconstruction Loss: -4.868308067321777
Iteration 2781:
Training Loss: -5.4602437019348145
Reconstruction Loss: -4.870715141296387
Iteration 2801:
Training Loss: -5.481682300567627
Reconstruction Loss: -4.871934413909912
Iteration 2821:
Training Loss: -5.360803127288818
Reconstruction Loss: -4.875652313232422
Iteration 2841:
Training Loss: -5.623007774353027
Reconstruction Loss: -4.876949310302734
Iteration 2861:
Training Loss: -5.464479923248291
Reconstruction Loss: -4.879428386688232
Iteration 2881:
Training Loss: -5.491299152374268
Reconstruction Loss: -4.881607532501221
Iteration 2901:
Training Loss: -5.650556564331055
Reconstruction Loss: -4.883728981018066
Iteration 2921:
Training Loss: -5.3153791427612305
Reconstruction Loss: -4.8841938972473145
Iteration 2941:
Training Loss: -5.441092491149902
Reconstruction Loss: -4.887228488922119
Iteration 2961:
Training Loss: -5.512486934661865
Reconstruction Loss: -4.8900465965271
Iteration 2981:
Training Loss: -5.5219197273254395
Reconstruction Loss: -4.889666557312012
Iteration 3001:
Training Loss: -5.33464241027832
Reconstruction Loss: -4.893752574920654
Iteration 3021:
Training Loss: -5.252862930297852
Reconstruction Loss: -4.8940558433532715
Iteration 3041:
Training Loss: -5.442190647125244
Reconstruction Loss: -4.896636009216309
Iteration 3061:
Training Loss: -5.366532325744629
Reconstruction Loss: -4.899748802185059
Iteration 3081:
Training Loss: -5.73087739944458
Reconstruction Loss: -4.8997344970703125
Iteration 3101:
Training Loss: -5.578990936279297
Reconstruction Loss: -4.9033637046813965
Iteration 3121:
Training Loss: -5.752838611602783
Reconstruction Loss: -4.902867794036865
Iteration 3141:
Training Loss: -5.536639213562012
Reconstruction Loss: -4.904834747314453
Iteration 3161:
Training Loss: -5.627457141876221
Reconstruction Loss: -4.907674789428711
Iteration 3181:
Training Loss: -5.668515682220459
Reconstruction Loss: -4.90876579284668
Iteration 3201:
Training Loss: -5.6079864501953125
Reconstruction Loss: -4.910255432128906
Iteration 3221:
Training Loss: -5.734067916870117
Reconstruction Loss: -4.912106513977051
Iteration 3241:
Training Loss: -5.592569351196289
Reconstruction Loss: -4.914297580718994
Iteration 3261:
Training Loss: -5.492975234985352
Reconstruction Loss: -4.915902137756348
Iteration 3281:
Training Loss: -5.625354290008545
Reconstruction Loss: -4.917544841766357
Iteration 3301:
Training Loss: -5.746681213378906
Reconstruction Loss: -4.919361591339111
Iteration 3321:
Training Loss: -5.684399604797363
Reconstruction Loss: -4.919894218444824
Iteration 3341:
Training Loss: -5.592061996459961
Reconstruction Loss: -4.921560764312744
Iteration 3361:
Training Loss: -5.75284481048584
Reconstruction Loss: -4.923671245574951
Iteration 3381:
Training Loss: -5.766531944274902
Reconstruction Loss: -4.924712657928467
Iteration 3401:
Training Loss: -5.900235652923584
Reconstruction Loss: -4.926425933837891
Iteration 3421:
Training Loss: -5.765411376953125
Reconstruction Loss: -4.928050518035889
Iteration 3441:
Training Loss: -5.920607566833496
Reconstruction Loss: -4.930247783660889
Iteration 3461:
Training Loss: -5.797475337982178
Reconstruction Loss: -4.9316792488098145
Iteration 3481:
Training Loss: -6.0550537109375
Reconstruction Loss: -4.931797027587891
Iteration 3501:
Training Loss: -5.967574119567871
Reconstruction Loss: -4.933335304260254
Iteration 3521:
Training Loss: -5.980786323547363
Reconstruction Loss: -4.935004234313965
Iteration 3541:
Training Loss: -5.741709232330322
Reconstruction Loss: -4.936355113983154
Iteration 3561:
Training Loss: -5.571407794952393
Reconstruction Loss: -4.937711238861084
Iteration 3581:
Training Loss: -5.714827537536621
Reconstruction Loss: -4.94045877456665
Iteration 3601:
Training Loss: -5.940581321716309
Reconstruction Loss: -4.9404120445251465
Iteration 3621:
Training Loss: -6.062507152557373
Reconstruction Loss: -4.942601203918457
Iteration 3641:
Training Loss: -6.022357940673828
Reconstruction Loss: -4.942774772644043
Iteration 3661:
Training Loss: -5.803726673126221
Reconstruction Loss: -4.944455146789551
Iteration 3681:
Training Loss: -6.1081132888793945
Reconstruction Loss: -4.945481777191162
Iteration 3701:
Training Loss: -5.739224910736084
Reconstruction Loss: -4.946212291717529
Iteration 3721:
Training Loss: -5.9522271156311035
Reconstruction Loss: -4.9494829177856445
Iteration 3741:
Training Loss: -5.714816570281982
Reconstruction Loss: -4.950167179107666
Iteration 3761:
Training Loss: -6.082883358001709
Reconstruction Loss: -4.951159477233887
Iteration 3781:
Training Loss: -5.878384590148926
Reconstruction Loss: -4.952461242675781
Iteration 3801:
Training Loss: -5.989762306213379
Reconstruction Loss: -4.954046249389648
Iteration 3821:
Training Loss: -5.942629337310791
Reconstruction Loss: -4.955567359924316
Iteration 3841:
Training Loss: -5.800484657287598
Reconstruction Loss: -4.956701755523682
Iteration 3861:
Training Loss: -6.0510172843933105
Reconstruction Loss: -4.957170486450195
Iteration 3881:
Training Loss: -5.903397083282471
Reconstruction Loss: -4.958917140960693
Iteration 3901:
Training Loss: -6.295197010040283
Reconstruction Loss: -4.959718704223633
Iteration 3921:
Training Loss: -5.881439685821533
Reconstruction Loss: -4.9610700607299805
Iteration 3941:
Training Loss: -6.12182092666626
Reconstruction Loss: -4.961917877197266
Iteration 3961:
Training Loss: -6.220343112945557
Reconstruction Loss: -4.96401834487915
Iteration 3981:
Training Loss: -5.920104503631592
Reconstruction Loss: -4.96368932723999
Iteration 4001:
Training Loss: -5.975833892822266
Reconstruction Loss: -4.965327739715576
Iteration 4021:
Training Loss: -5.890573024749756
Reconstruction Loss: -4.966378688812256
Iteration 4041:
Training Loss: -6.169022083282471
Reconstruction Loss: -4.967803478240967
Iteration 4061:
Training Loss: -6.18313455581665
Reconstruction Loss: -4.968208312988281
Iteration 4081:
Training Loss: -6.103135585784912
Reconstruction Loss: -4.9701457023620605
Iteration 4101:
Training Loss: -6.211841106414795
Reconstruction Loss: -4.972134590148926
Iteration 4121:
Training Loss: -6.305398941040039
Reconstruction Loss: -4.972263336181641
Iteration 4141:
Training Loss: -5.956275463104248
Reconstruction Loss: -4.972387313842773
Iteration 4161:
Training Loss: -6.223475456237793
Reconstruction Loss: -4.973515510559082
Iteration 4181:
Training Loss: -6.295587539672852
Reconstruction Loss: -4.974554538726807
Iteration 4201:
Training Loss: -6.076103210449219
Reconstruction Loss: -4.9755706787109375
Iteration 4221:
Training Loss: -6.065643787384033
Reconstruction Loss: -4.976336479187012
Iteration 4241:
Training Loss: -6.108962535858154
Reconstruction Loss: -4.978363513946533
Iteration 4261:
Training Loss: -5.988166809082031
Reconstruction Loss: -4.978853702545166
Iteration 4281:
Training Loss: -6.277440547943115
Reconstruction Loss: -4.980136394500732
Iteration 4301:
Training Loss: -6.415719509124756
Reconstruction Loss: -4.981390476226807
Iteration 4321:
Training Loss: -6.405864238739014
Reconstruction Loss: -4.982099533081055
Iteration 4341:
Training Loss: -6.276481628417969
Reconstruction Loss: -4.983006477355957
Iteration 4361:
Training Loss: -6.075112342834473
Reconstruction Loss: -4.984109401702881
Iteration 4381:
Training Loss: -6.053860187530518
Reconstruction Loss: -4.983950138092041
Iteration 4401:
Training Loss: -6.06362771987915
Reconstruction Loss: -4.985776424407959
Iteration 4421:
Training Loss: -6.339469909667969
Reconstruction Loss: -4.985322952270508
Iteration 4441:
Training Loss: -6.092508316040039
Reconstruction Loss: -4.987542629241943
Iteration 4461:
Training Loss: -6.341680526733398
Reconstruction Loss: -4.989027976989746
Iteration 4481:
Training Loss: -6.375247001647949
Reconstruction Loss: -4.989089012145996
Iteration 4501:
Training Loss: -6.410315036773682
Reconstruction Loss: -4.990215301513672
Iteration 4521:
Training Loss: -6.340293884277344
Reconstruction Loss: -4.991120338439941
Iteration 4541:
Training Loss: -6.104593276977539
Reconstruction Loss: -4.991495132446289
Iteration 4561:
Training Loss: -6.3902788162231445
Reconstruction Loss: -4.992764949798584
Iteration 4581:
Training Loss: -6.274181842803955
Reconstruction Loss: -4.994406700134277
Iteration 4601:
Training Loss: -6.202540397644043
Reconstruction Loss: -4.9955291748046875
Iteration 4621:
Training Loss: -6.424393653869629
Reconstruction Loss: -4.996304512023926
Iteration 4641:
Training Loss: -6.414035797119141
Reconstruction Loss: -4.996792793273926
Iteration 4661:
Training Loss: -6.710160255432129
Reconstruction Loss: -4.997429370880127
Iteration 4681:
Training Loss: -6.294203758239746
Reconstruction Loss: -4.9989237785339355
Iteration 4701:
Training Loss: -6.39573860168457
Reconstruction Loss: -4.998460292816162
Iteration 4721:
Training Loss: -6.127993583679199
Reconstruction Loss: -4.999417781829834
Iteration 4741:
Training Loss: -6.26855993270874
Reconstruction Loss: -5.001216411590576
Iteration 4761:
Training Loss: -6.16086483001709
Reconstruction Loss: -5.001760482788086
Iteration 4781:
Training Loss: -6.269784450531006
Reconstruction Loss: -5.001640796661377
Iteration 4801:
Training Loss: -6.476125240325928
Reconstruction Loss: -5.003357887268066
Iteration 4821:
Training Loss: -6.491255760192871
Reconstruction Loss: -5.003786087036133
Iteration 4841:
Training Loss: -6.452741622924805
Reconstruction Loss: -5.004148960113525
Iteration 4861:
Training Loss: -6.576151371002197
Reconstruction Loss: -5.0058274269104
Iteration 4881:
Training Loss: -6.338014602661133
Reconstruction Loss: -5.005880832672119
Iteration 4901:
Training Loss: -6.537524700164795
Reconstruction Loss: -5.007139205932617
Iteration 4921:
Training Loss: -6.358514308929443
Reconstruction Loss: -5.006667613983154
Iteration 4941:
Training Loss: -6.41089391708374
Reconstruction Loss: -5.008724689483643
Iteration 4961:
Training Loss: -6.496399879455566
Reconstruction Loss: -5.008949279785156
Iteration 4981:
Training Loss: -6.721259593963623
Reconstruction Loss: -5.008368015289307
Iteration 5001:
Training Loss: -6.662461280822754
Reconstruction Loss: -5.010562896728516
Iteration 5021:
Training Loss: -6.305078506469727
Reconstruction Loss: -5.0097880363464355
Iteration 5041:
Training Loss: -6.473599433898926
Reconstruction Loss: -5.01136589050293
Iteration 5061:
Training Loss: -6.533725738525391
Reconstruction Loss: -5.011458396911621
Iteration 5081:
Training Loss: -6.863292217254639
Reconstruction Loss: -5.012468338012695
Iteration 5101:
Training Loss: -6.648787498474121
Reconstruction Loss: -5.012966156005859
Iteration 5121:
Training Loss: -6.798308372497559
Reconstruction Loss: -5.014460563659668
Iteration 5141:
Training Loss: -6.392931938171387
Reconstruction Loss: -5.01505184173584
Iteration 5161:
Training Loss: -6.535781383514404
Reconstruction Loss: -5.015386581420898
Iteration 5181:
Training Loss: -6.648528099060059
Reconstruction Loss: -5.017982482910156
Iteration 5201:
Training Loss: -6.529285907745361
Reconstruction Loss: -5.017237663269043
Iteration 5221:
Training Loss: -6.5584635734558105
Reconstruction Loss: -5.018033981323242
Iteration 5241:
Training Loss: -6.317393779754639
Reconstruction Loss: -5.01894474029541
Iteration 5261:
Training Loss: -6.645654201507568
Reconstruction Loss: -5.018661975860596
Iteration 5281:
Training Loss: -6.609745502471924
Reconstruction Loss: -5.019536972045898
Iteration 5301:
Training Loss: -6.816079616546631
Reconstruction Loss: -5.020432472229004
Iteration 5321:
Training Loss: -6.886350631713867
Reconstruction Loss: -5.020368576049805
Iteration 5341:
Training Loss: -6.715531349182129
Reconstruction Loss: -5.021343231201172
Iteration 5361:
Training Loss: -6.804846286773682
Reconstruction Loss: -5.021940231323242
Iteration 5381:
Training Loss: -6.840369701385498
Reconstruction Loss: -5.0228424072265625
Iteration 5401:
Training Loss: -6.708978652954102
Reconstruction Loss: -5.0225510597229
Iteration 5421:
Training Loss: -6.731659412384033
Reconstruction Loss: -5.024566173553467
Iteration 5441:
Training Loss: -6.717671871185303
Reconstruction Loss: -5.0249199867248535
Iteration 5461:
Training Loss: -6.782600402832031
Reconstruction Loss: -5.023905277252197
Iteration 5481:
Training Loss: -6.7492170333862305
Reconstruction Loss: -5.026008605957031
Iteration 5501:
Training Loss: -6.707465648651123
Reconstruction Loss: -5.02561092376709
Iteration 5521:
Training Loss: -6.812058925628662
Reconstruction Loss: -5.026801586151123
Iteration 5541:
Training Loss: -6.6890950202941895
Reconstruction Loss: -5.027321815490723
Iteration 5561:
Training Loss: -7.056342601776123
Reconstruction Loss: -5.028358459472656
Iteration 5581:
Training Loss: -6.831759929656982
Reconstruction Loss: -5.029015064239502
Iteration 5601:
Training Loss: -6.60802698135376
Reconstruction Loss: -5.029056549072266
Iteration 5621:
Training Loss: -6.681976318359375
Reconstruction Loss: -5.029873371124268
Iteration 5641:
Training Loss: -6.666249752044678
Reconstruction Loss: -5.029573917388916
Iteration 5661:
Training Loss: -6.855736255645752
Reconstruction Loss: -5.03041934967041
Iteration 5681:
Training Loss: -6.805176258087158
Reconstruction Loss: -5.031189918518066
Iteration 5701:
Training Loss: -7.00033712387085
Reconstruction Loss: -5.0322184562683105
Iteration 5721:
Training Loss: -6.80247688293457
Reconstruction Loss: -5.0324811935424805
Iteration 5741:
Training Loss: -6.99548864364624
Reconstruction Loss: -5.032618999481201
Iteration 5761:
Training Loss: -6.713181972503662
Reconstruction Loss: -5.03354549407959
Iteration 5781:
Training Loss: -6.931868076324463
Reconstruction Loss: -5.033506393432617
Iteration 5801:
Training Loss: -6.83289909362793
Reconstruction Loss: -5.033562183380127
Iteration 5821:
Training Loss: -6.853574275970459
Reconstruction Loss: -5.03527307510376
Iteration 5841:
Training Loss: -7.015280723571777
Reconstruction Loss: -5.034851551055908
Iteration 5861:
Training Loss: -6.949621677398682
Reconstruction Loss: -5.036014080047607
Iteration 5881:
Training Loss: -6.887362003326416
Reconstruction Loss: -5.036506175994873
Iteration 5901:
Training Loss: -6.771729946136475
Reconstruction Loss: -5.036927700042725
Iteration 5921:
Training Loss: -6.92710018157959
Reconstruction Loss: -5.037484645843506
Iteration 5941:
Training Loss: -6.859311580657959
Reconstruction Loss: -5.037545204162598
Iteration 5961:
Training Loss: -7.02461051940918
Reconstruction Loss: -5.038152694702148
Iteration 5981:
Training Loss: -6.864231586456299
Reconstruction Loss: -5.038610935211182
Iteration 6001:
Training Loss: -7.126979351043701
Reconstruction Loss: -5.039024829864502
Iteration 6021:
Training Loss: -6.864518165588379
Reconstruction Loss: -5.039835453033447
Iteration 6041:
Training Loss: -7.516902446746826
Reconstruction Loss: -5.039772033691406
Iteration 6061:
Training Loss: -7.016409873962402
Reconstruction Loss: -5.040353298187256
Iteration 6081:
Training Loss: -7.273565292358398
Reconstruction Loss: -5.040814399719238
Iteration 6101:
Training Loss: -6.84965181350708
Reconstruction Loss: -5.041505336761475
Iteration 6121:
Training Loss: -6.854565143585205
Reconstruction Loss: -5.042314052581787
Iteration 6141:
Training Loss: -7.018691062927246
Reconstruction Loss: -5.042525768280029
Iteration 6161:
Training Loss: -6.845891952514648
Reconstruction Loss: -5.042856216430664
Iteration 6181:
Training Loss: -7.004124164581299
Reconstruction Loss: -5.043288707733154
Iteration 6201:
Training Loss: -7.115940093994141
Reconstruction Loss: -5.043395519256592
Iteration 6221:
Training Loss: -6.897416591644287
Reconstruction Loss: -5.043981552124023
Iteration 6241:
Training Loss: -7.106367588043213
Reconstruction Loss: -5.044210910797119
Iteration 6261:
Training Loss: -7.3072509765625
Reconstruction Loss: -5.045170783996582
Iteration 6281:
Training Loss: -7.051996231079102
Reconstruction Loss: -5.0447773933410645
Iteration 6301:
Training Loss: -7.241093158721924
Reconstruction Loss: -5.045256614685059
Iteration 6321:
Training Loss: -7.169579029083252
Reconstruction Loss: -5.045797824859619
Iteration 6341:
Training Loss: -6.939209461212158
Reconstruction Loss: -5.046969890594482
Iteration 6361:
Training Loss: -7.156065940856934
Reconstruction Loss: -5.046750068664551
Iteration 6381:
Training Loss: -7.296995639801025
Reconstruction Loss: -5.047807216644287
Iteration 6401:
Training Loss: -7.080258846282959
Reconstruction Loss: -5.048085689544678
Iteration 6421:
Training Loss: -6.99601411819458
Reconstruction Loss: -5.048581123352051
Iteration 6441:
Training Loss: -7.138251304626465
Reconstruction Loss: -5.0482025146484375
Iteration 6461:
Training Loss: -6.962237358093262
Reconstruction Loss: -5.048834800720215
Iteration 6481:
Training Loss: -7.283015727996826
Reconstruction Loss: -5.0493621826171875
Iteration 6501:
Training Loss: -7.201003074645996
Reconstruction Loss: -5.050450801849365
Iteration 6521:
Training Loss: -6.866096019744873
Reconstruction Loss: -5.050099849700928
Iteration 6541:
Training Loss: -7.039358139038086
Reconstruction Loss: -5.050015926361084
Iteration 6561:
Training Loss: -7.100815773010254
Reconstruction Loss: -5.051486015319824
Iteration 6581:
Training Loss: -7.096913814544678
Reconstruction Loss: -5.051146507263184
Iteration 6601:
Training Loss: -7.177426815032959
Reconstruction Loss: -5.05139684677124
Iteration 6621:
Training Loss: -7.286190986633301
Reconstruction Loss: -5.051615238189697
Iteration 6641:
Training Loss: -7.069053649902344
Reconstruction Loss: -5.052035331726074
Iteration 6661:
Training Loss: -7.447265148162842
Reconstruction Loss: -5.0525593757629395
Iteration 6681:
Training Loss: -7.319896221160889
Reconstruction Loss: -5.052530288696289
Iteration 6701:
Training Loss: -7.136602401733398
Reconstruction Loss: -5.053143501281738
Iteration 6721:
Training Loss: -7.340754508972168
Reconstruction Loss: -5.053474426269531
Iteration 6741:
Training Loss: -7.2281341552734375
Reconstruction Loss: -5.053488254547119
Iteration 6761:
Training Loss: -7.104167461395264
Reconstruction Loss: -5.054590702056885
Iteration 6781:
Training Loss: -7.277901649475098
Reconstruction Loss: -5.054192066192627
Iteration 6801:
Training Loss: -7.587210178375244
Reconstruction Loss: -5.055017948150635
Iteration 6821:
Training Loss: -7.277446269989014
Reconstruction Loss: -5.05540132522583
Iteration 6841:
Training Loss: -7.342372894287109
Reconstruction Loss: -5.055471897125244
Iteration 6861:
Training Loss: -7.2799811363220215
Reconstruction Loss: -5.055739402770996
Iteration 6881:
Training Loss: -7.385070323944092
Reconstruction Loss: -5.056378364562988
Iteration 6901:
Training Loss: -7.148006439208984
Reconstruction Loss: -5.055826187133789
Iteration 6921:
Training Loss: -7.410290241241455
Reconstruction Loss: -5.056676387786865
Iteration 6941:
Training Loss: -7.6663970947265625
Reconstruction Loss: -5.0568108558654785
Iteration 6961:
Training Loss: -7.39407205581665
Reconstruction Loss: -5.057468891143799
Iteration 6981:
Training Loss: -7.241352081298828
Reconstruction Loss: -5.058261871337891
Iteration 7001:
Training Loss: -7.3670573234558105
Reconstruction Loss: -5.058444023132324
Iteration 7021:
Training Loss: -7.23552131652832
Reconstruction Loss: -5.0582404136657715
Iteration 7041:
Training Loss: -7.115542888641357
Reconstruction Loss: -5.058983325958252
Iteration 7061:
Training Loss: -7.569097518920898
Reconstruction Loss: -5.059159755706787
Iteration 7081:
Training Loss: -7.629307270050049
Reconstruction Loss: -5.058962345123291
Iteration 7101:
Training Loss: -7.5333428382873535
Reconstruction Loss: -5.060042858123779
Iteration 7121:
Training Loss: -7.641952991485596
Reconstruction Loss: -5.059727191925049
Iteration 7141:
Training Loss: -7.651195526123047
Reconstruction Loss: -5.060235023498535
Iteration 7161:
Training Loss: -7.697354793548584
Reconstruction Loss: -5.060217380523682
Iteration 7181:
Training Loss: -7.502777576446533
Reconstruction Loss: -5.060732364654541
Iteration 7201:
Training Loss: -7.42343282699585
Reconstruction Loss: -5.06159782409668
Iteration 7221:
Training Loss: -7.4698805809021
Reconstruction Loss: -5.061708450317383
Iteration 7241:
Training Loss: -7.279083728790283
Reconstruction Loss: -5.062088489532471
Iteration 7261:
Training Loss: -7.668148517608643
Reconstruction Loss: -5.061853408813477
Iteration 7281:
Training Loss: -7.570438861846924
Reconstruction Loss: -5.062258720397949
Iteration 7301:
Training Loss: -7.425058364868164
Reconstruction Loss: -5.062258720397949
Iteration 7321:
Training Loss: -7.569878101348877
Reconstruction Loss: -5.062555313110352
Iteration 7341:
Training Loss: -7.454036712646484
Reconstruction Loss: -5.062945365905762
Iteration 7361:
Training Loss: -7.543685436248779
Reconstruction Loss: -5.063362121582031
Iteration 7381:
Training Loss: -7.606625556945801
Reconstruction Loss: -5.0633463859558105
Iteration 7401:
Training Loss: -7.433402061462402
Reconstruction Loss: -5.064105033874512
Iteration 7421:
Training Loss: -7.297555446624756
Reconstruction Loss: -5.064209938049316
Iteration 7441:
Training Loss: -7.447287082672119
Reconstruction Loss: -5.0638227462768555
Iteration 7461:
Training Loss: -7.1669416427612305
Reconstruction Loss: -5.06522274017334
Iteration 7481:
Training Loss: -7.459354877471924
Reconstruction Loss: -5.0654296875
Iteration 7501:
Training Loss: -7.431689739227295
Reconstruction Loss: -5.065878868103027
Iteration 7521:
Training Loss: -7.799871921539307
Reconstruction Loss: -5.0654988288879395
Iteration 7541:
Training Loss: -7.382961750030518
Reconstruction Loss: -5.06572151184082
Iteration 7561:
Training Loss: -7.732625484466553
Reconstruction Loss: -5.066386699676514
Iteration 7581:
Training Loss: -7.442137241363525
Reconstruction Loss: -5.066586494445801
Iteration 7601:
Training Loss: -7.4929962158203125
Reconstruction Loss: -5.066509246826172
Iteration 7621:
Training Loss: -7.584349155426025
Reconstruction Loss: -5.066873073577881
Iteration 7641:
Training Loss: -7.765361309051514
Reconstruction Loss: -5.06696081161499
Iteration 7661:
Training Loss: -7.5339436531066895
Reconstruction Loss: -5.06741189956665
Iteration 7681:
Training Loss: -7.773661136627197
Reconstruction Loss: -5.067511558532715
Iteration 7701:
Training Loss: -7.666151523590088
Reconstruction Loss: -5.067287921905518
Iteration 7721:
Training Loss: -7.391256809234619
Reconstruction Loss: -5.068051815032959
Iteration 7741:
Training Loss: -7.638969421386719
Reconstruction Loss: -5.068980693817139
Iteration 7761:
Training Loss: -7.610951900482178
Reconstruction Loss: -5.06826114654541
Iteration 7781:
Training Loss: -7.592233180999756
Reconstruction Loss: -5.06886625289917
Iteration 7801:
Training Loss: -7.452739715576172
Reconstruction Loss: -5.069141864776611
Iteration 7821:
Training Loss: -7.505229473114014
Reconstruction Loss: -5.069093227386475
Iteration 7841:
Training Loss: -7.583615779876709
Reconstruction Loss: -5.069384574890137
Iteration 7861:
Training Loss: -7.846717834472656
Reconstruction Loss: -5.06995964050293
Iteration 7881:
Training Loss: -7.776809215545654
Reconstruction Loss: -5.069470405578613
Iteration 7901:
Training Loss: -7.854946136474609
Reconstruction Loss: -5.069977760314941
Iteration 7921:
Training Loss: -7.801980972290039
Reconstruction Loss: -5.069873809814453
Iteration 7941:
Training Loss: -7.605434417724609
Reconstruction Loss: -5.070525169372559
Iteration 7961:
Training Loss: -7.629162311553955
Reconstruction Loss: -5.071086883544922
Iteration 7981:
Training Loss: -8.039841651916504
Reconstruction Loss: -5.0711188316345215
Iteration 8001:
Training Loss: -7.579456806182861
Reconstruction Loss: -5.070961952209473
Iteration 8021:
Training Loss: -7.410608291625977
Reconstruction Loss: -5.070932388305664
Iteration 8041:
Training Loss: -7.974677562713623
Reconstruction Loss: -5.071661949157715
Iteration 8061:
Training Loss: -7.76197624206543
Reconstruction Loss: -5.072412490844727
Iteration 8081:
Training Loss: -7.7942891120910645
Reconstruction Loss: -5.072183132171631
Iteration 8101:
Training Loss: -7.678928375244141
Reconstruction Loss: -5.072353363037109
Iteration 8121:
Training Loss: -7.605527400970459
Reconstruction Loss: -5.0723185539245605
Iteration 8141:
Training Loss: -7.710086345672607
Reconstruction Loss: -5.072442054748535
Iteration 8161:
Training Loss: -8.037524223327637
Reconstruction Loss: -5.07271671295166
Iteration 8181:
Training Loss: -7.998451232910156
Reconstruction Loss: -5.07290506362915
Iteration 8201:
Training Loss: -7.789000034332275
Reconstruction Loss: -5.073117733001709
Iteration 8221:
Training Loss: -8.142396926879883
Reconstruction Loss: -5.073211669921875
Iteration 8241:
Training Loss: -7.64651346206665
Reconstruction Loss: -5.073614597320557
Iteration 8261:
Training Loss: -7.663692474365234
Reconstruction Loss: -5.073877811431885
Iteration 8281:
Training Loss: -7.548264980316162
Reconstruction Loss: -5.073726177215576
Iteration 8301:
Training Loss: -7.959097862243652
Reconstruction Loss: -5.073824405670166
Iteration 8321:
Training Loss: -7.677885055541992
Reconstruction Loss: -5.07436466217041
Iteration 8341:
Training Loss: -8.004351615905762
Reconstruction Loss: -5.074577331542969
Iteration 8361:
Training Loss: -7.840713024139404
Reconstruction Loss: -5.074499607086182
Iteration 8381:
Training Loss: -7.870867729187012
Reconstruction Loss: -5.074976444244385
Iteration 8401:
Training Loss: -8.09061050415039
Reconstruction Loss: -5.0747833251953125
Iteration 8421:
Training Loss: -8.020670890808105
Reconstruction Loss: -5.075390338897705
Iteration 8441:
Training Loss: -7.979645252227783
Reconstruction Loss: -5.075567245483398
Iteration 8461:
Training Loss: -7.754092693328857
Reconstruction Loss: -5.075685977935791
Iteration 8481:
Training Loss: -8.067327499389648
Reconstruction Loss: -5.07541561126709
Iteration 8501:
Training Loss: -8.000797271728516
Reconstruction Loss: -5.076221942901611
Iteration 8521:
Training Loss: -8.119269371032715
Reconstruction Loss: -5.076186656951904
Iteration 8541:
Training Loss: -7.77979850769043
Reconstruction Loss: -5.07606315612793
Iteration 8561:
Training Loss: -7.966687202453613
Reconstruction Loss: -5.0768256187438965
Iteration 8581:
Training Loss: -8.027902603149414
Reconstruction Loss: -5.0767130851745605
Iteration 8601:
Training Loss: -7.857607364654541
Reconstruction Loss: -5.076885223388672
Iteration 8621:
Training Loss: -7.825139045715332
Reconstruction Loss: -5.0771164894104
Iteration 8641:
Training Loss: -8.192625999450684
Reconstruction Loss: -5.076963901519775
Iteration 8661:
Training Loss: -7.766329765319824
Reconstruction Loss: -5.076942443847656
Iteration 8681:
Training Loss: -8.093703269958496
Reconstruction Loss: -5.077382564544678
Iteration 8701:
Training Loss: -8.183487892150879
Reconstruction Loss: -5.077817916870117
Iteration 8721:
Training Loss: -7.994262218475342
Reconstruction Loss: -5.077676296234131
Iteration 8741:
Training Loss: -7.831440448760986
Reconstruction Loss: -5.07745361328125
Iteration 8761:
Training Loss: -7.939585208892822
Reconstruction Loss: -5.078261852264404
Iteration 8781:
Training Loss: -7.931853771209717
Reconstruction Loss: -5.078494548797607
Iteration 8801:
Training Loss: -7.933424949645996
Reconstruction Loss: -5.078476905822754
Iteration 8821:
Training Loss: -8.022994995117188
Reconstruction Loss: -5.078433036804199
Iteration 8841:
Training Loss: -8.088268280029297
Reconstruction Loss: -5.078927040100098
Iteration 8861:
Training Loss: -8.152422904968262
Reconstruction Loss: -5.078801155090332
Iteration 8881:
Training Loss: -7.790334701538086
Reconstruction Loss: -5.079118251800537
Iteration 8901:
Training Loss: -8.006709098815918
Reconstruction Loss: -5.0792765617370605
Iteration 8921:
Training Loss: -7.86570930480957
Reconstruction Loss: -5.079193592071533
Iteration 8941:
Training Loss: -8.099410057067871
Reconstruction Loss: -5.079818248748779
Iteration 8961:
Training Loss: -8.1849365234375
Reconstruction Loss: -5.079568862915039
Iteration 8981:
Training Loss: -7.85153865814209
Reconstruction Loss: -5.079690456390381
Iteration 9001:
Training Loss: -8.148791313171387
Reconstruction Loss: -5.080201625823975
Iteration 9021:
Training Loss: -7.953345775604248
Reconstruction Loss: -5.0801239013671875
Iteration 9041:
Training Loss: -7.950264930725098
Reconstruction Loss: -5.079824924468994
Iteration 9061:
Training Loss: -8.140361785888672
Reconstruction Loss: -5.080253601074219
Iteration 9081:
Training Loss: -7.989555835723877
Reconstruction Loss: -5.080542087554932
Iteration 9101:
Training Loss: -8.319890975952148
Reconstruction Loss: -5.080542087554932
Iteration 9121:
Training Loss: -8.181159019470215
Reconstruction Loss: -5.080265522003174
Iteration 9141:
Training Loss: -8.172088623046875
Reconstruction Loss: -5.080785274505615
Iteration 9161:
Training Loss: -8.12621021270752
Reconstruction Loss: -5.08060884475708
Iteration 9181:
Training Loss: -7.997008800506592
Reconstruction Loss: -5.081119060516357
Iteration 9201:
Training Loss: -8.128460884094238
Reconstruction Loss: -5.081334590911865
Iteration 9221:
Training Loss: -8.156578063964844
Reconstruction Loss: -5.081563472747803
Iteration 9241:
Training Loss: -8.491292953491211
Reconstruction Loss: -5.081707954406738
Iteration 9261:
Training Loss: -8.245142936706543
Reconstruction Loss: -5.081733703613281
Iteration 9281:
Training Loss: -8.294639587402344
Reconstruction Loss: -5.081778049468994
Iteration 9301:
Training Loss: -8.19113540649414
Reconstruction Loss: -5.08200216293335
Iteration 9321:
Training Loss: -8.14484691619873
Reconstruction Loss: -5.081984996795654
Iteration 9341:
Training Loss: -8.19703483581543
Reconstruction Loss: -5.081852436065674
Iteration 9361:
Training Loss: -8.066337585449219
Reconstruction Loss: -5.082430362701416
Iteration 9381:
Training Loss: -8.280248641967773
Reconstruction Loss: -5.082499980926514
Iteration 9401:
Training Loss: -8.15487003326416
Reconstruction Loss: -5.082793235778809
Iteration 9421:
Training Loss: -8.208273887634277
Reconstruction Loss: -5.082857608795166
Iteration 9441:
Training Loss: -8.236433029174805
Reconstruction Loss: -5.082420349121094
Iteration 9461:
Training Loss: -8.238544464111328
Reconstruction Loss: -5.083368301391602
Iteration 9481:
Training Loss: -8.240699768066406
Reconstruction Loss: -5.0832037925720215
Iteration 9501:
Training Loss: -8.105634689331055
Reconstruction Loss: -5.083309650421143
Iteration 9521:
Training Loss: -8.263906478881836
Reconstruction Loss: -5.083292484283447
Iteration 9541:
Training Loss: -8.059680938720703
Reconstruction Loss: -5.08353853225708
Iteration 9561:
Training Loss: -8.136563301086426
Reconstruction Loss: -5.083619117736816
Iteration 9581:
Training Loss: -8.412168502807617
Reconstruction Loss: -5.08376932144165
Iteration 9601:
Training Loss: -8.228447914123535
Reconstruction Loss: -5.083804607391357
Iteration 9621:
Training Loss: -8.307806015014648
Reconstruction Loss: -5.084117412567139
Iteration 9641:
Training Loss: -8.1165132522583
Reconstruction Loss: -5.0841755867004395
Iteration 9661:
Training Loss: -8.195313453674316
Reconstruction Loss: -5.084075927734375
Iteration 9681:
Training Loss: -8.0087308883667
Reconstruction Loss: -5.083985328674316
Iteration 9701:
Training Loss: -8.281654357910156
Reconstruction Loss: -5.084383010864258
Iteration 9721:
Training Loss: -8.134824752807617
Reconstruction Loss: -5.084585189819336
Iteration 9741:
Training Loss: -8.268983840942383
Reconstruction Loss: -5.084540843963623
Iteration 9761:
Training Loss: -8.395326614379883
Reconstruction Loss: -5.084559917449951
Iteration 9781:
Training Loss: -8.199846267700195
Reconstruction Loss: -5.084352970123291
Iteration 9801:
Training Loss: -8.104833602905273
Reconstruction Loss: -5.0847930908203125
Iteration 9821:
Training Loss: -8.482941627502441
Reconstruction Loss: -5.085164546966553
Iteration 9841:
Training Loss: -8.453391075134277
Reconstruction Loss: -5.085422992706299
Iteration 9861:
Training Loss: -8.475475311279297
Reconstruction Loss: -5.085294723510742
Iteration 9881:
Training Loss: -8.430978775024414
Reconstruction Loss: -5.085596084594727
Iteration 9901:
Training Loss: -8.265213012695312
Reconstruction Loss: -5.085057258605957
Iteration 9921:
Training Loss: -8.645137786865234
Reconstruction Loss: -5.085456371307373
Iteration 9941:
Training Loss: -8.332820892333984
Reconstruction Loss: -5.085664749145508
Iteration 9961:
Training Loss: -8.236496925354004
Reconstruction Loss: -5.0860514640808105
Iteration 9981:
Training Loss: -8.484618186950684
Reconstruction Loss: -5.086494445800781
