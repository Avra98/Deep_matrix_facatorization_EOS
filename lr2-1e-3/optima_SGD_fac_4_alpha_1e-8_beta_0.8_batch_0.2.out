5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.547167778015137
Reconstruction Loss: -0.47404181957244873
Iteration 21:
Training Loss: 5.374547481536865
Reconstruction Loss: -0.47404181957244873
Iteration 41:
Training Loss: 5.376227378845215
Reconstruction Loss: -0.47404181957244873
Iteration 61:
Training Loss: 5.681107044219971
Reconstruction Loss: -0.47404181957244873
Iteration 81:
Training Loss: 5.749460220336914
Reconstruction Loss: -0.4740419089794159
Iteration 101:
Training Loss: 5.76240873336792
Reconstruction Loss: -0.4740419089794159
Iteration 121:
Training Loss: 5.4542765617370605
Reconstruction Loss: -0.4740419089794159
Iteration 141:
Training Loss: 5.3382439613342285
Reconstruction Loss: -0.4740419089794159
Iteration 161:
Training Loss: 5.541221618652344
Reconstruction Loss: -0.4740419089794159
Iteration 181:
Training Loss: 5.448637008666992
Reconstruction Loss: -0.47404181957244873
Iteration 201:
Training Loss: 5.500372409820557
Reconstruction Loss: -0.4740419089794159
Iteration 221:
Training Loss: 5.5837016105651855
Reconstruction Loss: -0.4740419089794159
Iteration 241:
Training Loss: 5.33242654800415
Reconstruction Loss: -0.4740419089794159
Iteration 261:
Training Loss: 5.467701435089111
Reconstruction Loss: -0.4740419089794159
Iteration 281:
Training Loss: 5.542559623718262
Reconstruction Loss: -0.4740419089794159
Iteration 301:
Training Loss: 5.700122833251953
Reconstruction Loss: -0.4740419089794159
Iteration 321:
Training Loss: 5.6164231300354
Reconstruction Loss: -0.47404199838638306
Iteration 341:
Training Loss: 5.452215671539307
Reconstruction Loss: -0.47404199838638306
Iteration 361:
Training Loss: 5.767895221710205
Reconstruction Loss: -0.47404199838638306
Iteration 381:
Training Loss: 5.590059757232666
Reconstruction Loss: -0.47404199838638306
Iteration 401:
Training Loss: 5.375233173370361
Reconstruction Loss: -0.47404199838638306
Iteration 421:
Training Loss: 5.337483882904053
Reconstruction Loss: -0.47404199838638306
Iteration 441:
Training Loss: 5.365869522094727
Reconstruction Loss: -0.4740421175956726
Iteration 461:
Training Loss: 5.516798973083496
Reconstruction Loss: -0.4740421175956726
Iteration 481:
Training Loss: 5.5865302085876465
Reconstruction Loss: -0.4740421175956726
Iteration 501:
Training Loss: 5.547753810882568
Reconstruction Loss: -0.4740421175956726
Iteration 521:
Training Loss: 5.566145896911621
Reconstruction Loss: -0.4740421772003174
Iteration 541:
Training Loss: 5.339787483215332
Reconstruction Loss: -0.4740421772003174
Iteration 561:
Training Loss: 5.705424785614014
Reconstruction Loss: -0.4740421772003174
Iteration 581:
Training Loss: 5.657584190368652
Reconstruction Loss: -0.4740421772003174
Iteration 601:
Training Loss: 5.621641159057617
Reconstruction Loss: -0.4740421772003174
Iteration 621:
Training Loss: 5.331810474395752
Reconstruction Loss: -0.4740421772003174
Iteration 641:
Training Loss: 5.267218589782715
Reconstruction Loss: -0.4740421772003174
Iteration 661:
Training Loss: 5.49307107925415
Reconstruction Loss: -0.4740421772003174
Iteration 681:
Training Loss: 5.518554210662842
Reconstruction Loss: -0.4740421772003174
Iteration 701:
Training Loss: 5.2038140296936035
Reconstruction Loss: -0.4740421772003174
Iteration 721:
Training Loss: 5.468160152435303
Reconstruction Loss: -0.47404229640960693
Iteration 741:
Training Loss: 5.464081764221191
Reconstruction Loss: -0.47404229640960693
Iteration 761:
Training Loss: 5.576601982116699
Reconstruction Loss: -0.47404229640960693
Iteration 781:
Training Loss: 5.466680526733398
Reconstruction Loss: -0.47404229640960693
Iteration 801:
Training Loss: 5.5683794021606445
Reconstruction Loss: -0.47404229640960693
Iteration 821:
Training Loss: 5.713550567626953
Reconstruction Loss: -0.47404229640960693
Iteration 841:
Training Loss: 5.663920879364014
Reconstruction Loss: -0.47404229640960693
Iteration 861:
Training Loss: 5.537182331085205
Reconstruction Loss: -0.47404229640960693
Iteration 881:
Training Loss: 5.322027206420898
Reconstruction Loss: -0.47404229640960693
Iteration 901:
Training Loss: 5.625953674316406
Reconstruction Loss: -0.4740423858165741
Iteration 921:
Training Loss: 5.687130928039551
Reconstruction Loss: -0.4740423858165741
Iteration 941:
Training Loss: 5.742160320281982
Reconstruction Loss: -0.4740423858165741
Iteration 961:
Training Loss: 5.070669651031494
Reconstruction Loss: -0.4740423858165741
Iteration 981:
Training Loss: 5.605968475341797
Reconstruction Loss: -0.4740423858165741
Iteration 1001:
Training Loss: 5.647040843963623
Reconstruction Loss: -0.47404247522354126
Iteration 1021:
Training Loss: 5.357111930847168
Reconstruction Loss: -0.47404247522354126
Iteration 1041:
Training Loss: 5.647444248199463
Reconstruction Loss: -0.47404247522354126
Iteration 1061:
Training Loss: 5.260207176208496
Reconstruction Loss: -0.4740425944328308
Iteration 1081:
Training Loss: 5.645629405975342
Reconstruction Loss: -0.4740425944328308
Iteration 1101:
Training Loss: 5.718700885772705
Reconstruction Loss: -0.4740425944328308
Iteration 1121:
Training Loss: 5.193399906158447
Reconstruction Loss: -0.4740425944328308
Iteration 1141:
Training Loss: 5.607882022857666
Reconstruction Loss: -0.4740426540374756
Iteration 1161:
Training Loss: 5.552942276000977
Reconstruction Loss: -0.4740425944328308
Iteration 1181:
Training Loss: 5.606451034545898
Reconstruction Loss: -0.4740426540374756
Iteration 1201:
Training Loss: 5.572519302368164
Reconstruction Loss: -0.47404277324676514
Iteration 1221:
Training Loss: 5.6319122314453125
Reconstruction Loss: -0.47404277324676514
Iteration 1241:
Training Loss: 5.449060440063477
Reconstruction Loss: -0.47404277324676514
Iteration 1261:
Training Loss: 5.673596382141113
Reconstruction Loss: -0.47404277324676514
Iteration 1281:
Training Loss: 5.661437034606934
Reconstruction Loss: -0.4740428626537323
Iteration 1301:
Training Loss: 5.351306438446045
Reconstruction Loss: -0.47404295206069946
Iteration 1321:
Training Loss: 5.622229099273682
Reconstruction Loss: -0.4740428626537323
Iteration 1341:
Training Loss: 5.4295148849487305
Reconstruction Loss: -0.47404295206069946
Iteration 1361:
Training Loss: 5.431416988372803
Reconstruction Loss: -0.47404295206069946
Iteration 1381:
Training Loss: 5.802094459533691
Reconstruction Loss: -0.474043071269989
Iteration 1401:
Training Loss: 5.656932353973389
Reconstruction Loss: -0.474043071269989
Iteration 1421:
Training Loss: 5.627241134643555
Reconstruction Loss: -0.474043071269989
Iteration 1441:
Training Loss: 5.683529376983643
Reconstruction Loss: -0.47404325008392334
Iteration 1461:
Training Loss: 5.376309394836426
Reconstruction Loss: -0.47404325008392334
Iteration 1481:
Training Loss: 5.747505187988281
Reconstruction Loss: -0.47404325008392334
Iteration 1501:
Training Loss: 5.3650054931640625
Reconstruction Loss: -0.47404342889785767
Iteration 1521:
Training Loss: 5.576039791107178
Reconstruction Loss: -0.47404342889785767
Iteration 1541:
Training Loss: 5.399868488311768
Reconstruction Loss: -0.47404342889785767
Iteration 1561:
Training Loss: 5.646902084350586
Reconstruction Loss: -0.4740435481071472
Iteration 1581:
Training Loss: 5.595082759857178
Reconstruction Loss: -0.4740435481071472
Iteration 1601:
Training Loss: 5.514260768890381
Reconstruction Loss: -0.47404372692108154
Iteration 1621:
Training Loss: 5.645132541656494
Reconstruction Loss: -0.47404372692108154
Iteration 1641:
Training Loss: 5.592743396759033
Reconstruction Loss: -0.47404372692108154
Iteration 1661:
Training Loss: 5.323812484741211
Reconstruction Loss: -0.47404390573501587
Iteration 1681:
Training Loss: 5.801651954650879
Reconstruction Loss: -0.4740440249443054
Iteration 1701:
Training Loss: 5.308192729949951
Reconstruction Loss: -0.4740441143512726
Iteration 1721:
Training Loss: 5.379873275756836
Reconstruction Loss: -0.4740441143512726
Iteration 1741:
Training Loss: 5.493841171264648
Reconstruction Loss: -0.47404420375823975
Iteration 1761:
Training Loss: 5.370399475097656
Reconstruction Loss: -0.4740443825721741
Iteration 1781:
Training Loss: 5.6304168701171875
Reconstruction Loss: -0.4740445911884308
Iteration 1801:
Training Loss: 5.52020788192749
Reconstruction Loss: -0.4740445911884308
Iteration 1821:
Training Loss: 5.527982234954834
Reconstruction Loss: -0.4740447998046875
Iteration 1841:
Training Loss: 5.311970233917236
Reconstruction Loss: -0.4740448594093323
Iteration 1861:
Training Loss: 5.613776206970215
Reconstruction Loss: -0.474045068025589
Iteration 1881:
Training Loss: 5.534003257751465
Reconstruction Loss: -0.47404515743255615
Iteration 1901:
Training Loss: 5.470604419708252
Reconstruction Loss: -0.4740453362464905
Iteration 1921:
Training Loss: 5.314992904663086
Reconstruction Loss: -0.47404563426971436
Iteration 1941:
Training Loss: 5.568603515625
Reconstruction Loss: -0.4740458130836487
Iteration 1961:
Training Loss: 5.078796863555908
Reconstruction Loss: -0.47404593229293823
Iteration 1981:
Training Loss: 5.681886672973633
Reconstruction Loss: -0.47404611110687256
Iteration 2001:
Training Loss: 5.7716803550720215
Reconstruction Loss: -0.47404658794403076
Iteration 2021:
Training Loss: 5.3405351638793945
Reconstruction Loss: -0.4740467965602875
Iteration 2041:
Training Loss: 5.634289741516113
Reconstruction Loss: -0.4740471839904785
Iteration 2061:
Training Loss: 5.361633777618408
Reconstruction Loss: -0.4740474820137024
Iteration 2081:
Training Loss: 5.647088527679443
Reconstruction Loss: -0.4740479588508606
Iteration 2101:
Training Loss: 5.142538547515869
Reconstruction Loss: -0.47404831647872925
Iteration 2121:
Training Loss: 5.526895523071289
Reconstruction Loss: -0.4740487039089203
Iteration 2141:
Training Loss: 5.7302937507629395
Reconstruction Loss: -0.4740493893623352
Iteration 2161:
Training Loss: 5.607422828674316
Reconstruction Loss: -0.47405004501342773
Iteration 2181:
Training Loss: 5.436517238616943
Reconstruction Loss: -0.4740506410598755
Iteration 2201:
Training Loss: 5.424301624298096
Reconstruction Loss: -0.47405147552490234
Iteration 2221:
Training Loss: 5.616187572479248
Reconstruction Loss: -0.4740523397922516
Iteration 2241:
Training Loss: 5.573386192321777
Reconstruction Loss: -0.47405338287353516
Iteration 2261:
Training Loss: 5.28679084777832
Reconstruction Loss: -0.47405463457107544
Iteration 2281:
Training Loss: 5.537557125091553
Reconstruction Loss: -0.4740561842918396
Iteration 2301:
Training Loss: 5.456315994262695
Reconstruction Loss: -0.4740579128265381
Iteration 2321:
Training Loss: 5.458175182342529
Reconstruction Loss: -0.47405990958213806
Iteration 2341:
Training Loss: 5.694973945617676
Reconstruction Loss: -0.4740625023841858
Iteration 2361:
Training Loss: 5.228153705596924
Reconstruction Loss: -0.47406575083732605
Iteration 2381:
Training Loss: 5.536422252655029
Reconstruction Loss: -0.4740696847438812
Iteration 2401:
Training Loss: 5.444063186645508
Reconstruction Loss: -0.47407495975494385
Iteration 2421:
Training Loss: 5.273880958557129
Reconstruction Loss: -0.4740821123123169
Iteration 2441:
Training Loss: 5.518153190612793
Reconstruction Loss: -0.4740917980670929
Iteration 2461:
Training Loss: 5.5824875831604
Reconstruction Loss: -0.47410568594932556
Iteration 2481:
Training Loss: 5.68297004699707
Reconstruction Loss: -0.47412732243537903
Iteration 2501:
Training Loss: 5.3423919677734375
Reconstruction Loss: -0.4741630554199219
Iteration 2521:
Training Loss: 5.500678539276123
Reconstruction Loss: -0.4742289185523987
Iteration 2541:
Training Loss: 5.194733619689941
Reconstruction Loss: -0.4743744134902954
Iteration 2561:
Training Loss: 5.540307998657227
Reconstruction Loss: -0.47479718923568726
Iteration 2581:
Training Loss: 5.662543773651123
Reconstruction Loss: -0.47729504108428955
Iteration 2601:
Training Loss: 5.086082458496094
Reconstruction Loss: -0.6033861637115479
Iteration 2621:
Training Loss: 5.018117904663086
Reconstruction Loss: -0.5880467295646667
Iteration 2641:
Training Loss: 5.152434349060059
Reconstruction Loss: -0.593787431716919
Iteration 2661:
Training Loss: 5.077781677246094
Reconstruction Loss: -0.5782842040061951
Iteration 2681:
Training Loss: 4.924083232879639
Reconstruction Loss: -0.5687135457992554
Iteration 2701:
Training Loss: 5.076870918273926
Reconstruction Loss: -0.5266094207763672
Iteration 2721:
Training Loss: 4.7383294105529785
Reconstruction Loss: -0.5171159505844116
Iteration 2741:
Training Loss: 4.901655673980713
Reconstruction Loss: -0.5361146330833435
Iteration 2761:
Training Loss: 5.208376407623291
Reconstruction Loss: -0.5368661284446716
Iteration 2781:
Training Loss: 5.02879524230957
Reconstruction Loss: -0.5544018745422363
Iteration 2801:
Training Loss: 5.060079574584961
Reconstruction Loss: -0.5318426489830017
Iteration 2821:
Training Loss: 4.957873821258545
Reconstruction Loss: -0.5489670038223267
Iteration 2841:
Training Loss: 5.038599491119385
Reconstruction Loss: -0.539610743522644
Iteration 2861:
Training Loss: 5.147704124450684
Reconstruction Loss: -0.5334038734436035
Iteration 2881:
Training Loss: 4.895687103271484
Reconstruction Loss: -0.5389199256896973
Iteration 2901:
Training Loss: 4.576690673828125
Reconstruction Loss: -0.7569363117218018
Iteration 2921:
Training Loss: 4.482706069946289
Reconstruction Loss: -0.8390830755233765
Iteration 2941:
Training Loss: 4.627385139465332
Reconstruction Loss: -0.8472254276275635
Iteration 2961:
Training Loss: 4.480473041534424
Reconstruction Loss: -0.8360153436660767
Iteration 2981:
Training Loss: 4.610470294952393
Reconstruction Loss: -0.8265568017959595
