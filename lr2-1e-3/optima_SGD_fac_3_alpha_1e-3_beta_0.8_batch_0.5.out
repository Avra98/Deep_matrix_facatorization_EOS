5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.450307369232178
Reconstruction Loss: -0.5300759077072144
Iteration 51:
Training Loss: 4.609058380126953
Reconstruction Loss: -0.7473734617233276
Iteration 101:
Training Loss: 3.703197956085205
Reconstruction Loss: -1.1499987840652466
Iteration 151:
Training Loss: 2.7285733222961426
Reconstruction Loss: -1.553590178489685
Iteration 201:
Training Loss: 2.137852191925049
Reconstruction Loss: -1.9152392148971558
Iteration 251:
Training Loss: 1.4538705348968506
Reconstruction Loss: -2.3674604892730713
Iteration 301:
Training Loss: 0.7669214010238647
Reconstruction Loss: -2.8369028568267822
Iteration 351:
Training Loss: 0.3776165246963501
Reconstruction Loss: -3.2647464275360107
Iteration 401:
Training Loss: -0.21328355371952057
Reconstruction Loss: -3.640564441680908
Iteration 451:
Training Loss: -0.7060964107513428
Reconstruction Loss: -3.9598424434661865
Iteration 501:
Training Loss: -1.073638677597046
Reconstruction Loss: -4.222051620483398
Iteration 551:
Training Loss: -1.2803517580032349
Reconstruction Loss: -4.437536716461182
Iteration 601:
Training Loss: -1.4553476572036743
Reconstruction Loss: -4.614862442016602
Iteration 651:
Training Loss: -1.7267320156097412
Reconstruction Loss: -4.76340389251709
Iteration 701:
Training Loss: -1.9328293800354004
Reconstruction Loss: -4.889186859130859
Iteration 751:
Training Loss: -2.1016347408294678
Reconstruction Loss: -4.995223045349121
Iteration 801:
Training Loss: -2.103158712387085
Reconstruction Loss: -5.0890607833862305
Iteration 851:
Training Loss: -2.2326228618621826
Reconstruction Loss: -5.170122146606445
Iteration 901:
Training Loss: -2.436373472213745
Reconstruction Loss: -5.243011951446533
Iteration 951:
Training Loss: -2.5807628631591797
Reconstruction Loss: -5.310144424438477
Iteration 1001:
Training Loss: -2.603158712387085
Reconstruction Loss: -5.369137287139893
Iteration 1051:
Training Loss: -2.7715096473693848
Reconstruction Loss: -5.424812316894531
Iteration 1101:
Training Loss: -2.7654287815093994
Reconstruction Loss: -5.476102828979492
Iteration 1151:
Training Loss: -2.6892600059509277
Reconstruction Loss: -5.522416114807129
Iteration 1201:
Training Loss: -2.853583335876465
Reconstruction Loss: -5.566656589508057
Iteration 1251:
Training Loss: -2.854691743850708
Reconstruction Loss: -5.608860015869141
Iteration 1301:
Training Loss: -2.894900321960449
Reconstruction Loss: -5.647421836853027
Iteration 1351:
Training Loss: -3.0604605674743652
Reconstruction Loss: -5.684080600738525
Iteration 1401:
Training Loss: -3.038813591003418
Reconstruction Loss: -5.7209954261779785
Iteration 1451:
Training Loss: -3.1931209564208984
Reconstruction Loss: -5.754116535186768
Iteration 1501:
Training Loss: -3.176602840423584
Reconstruction Loss: -5.785639762878418
Iteration 1551:
Training Loss: -3.2439379692077637
Reconstruction Loss: -5.815786838531494
Iteration 1601:
Training Loss: -3.278451442718506
Reconstruction Loss: -5.844988822937012
Iteration 1651:
Training Loss: -3.412031888961792
Reconstruction Loss: -5.873317241668701
Iteration 1701:
Training Loss: -3.325593948364258
Reconstruction Loss: -5.8988165855407715
Iteration 1751:
Training Loss: -3.5117077827453613
Reconstruction Loss: -5.924015045166016
Iteration 1801:
Training Loss: -3.4353272914886475
Reconstruction Loss: -5.949028968811035
Iteration 1851:
Training Loss: -3.594886302947998
Reconstruction Loss: -5.973093032836914
Iteration 1901:
Training Loss: -3.613096237182617
Reconstruction Loss: -5.995797634124756
Iteration 1951:
Training Loss: -3.577193260192871
Reconstruction Loss: -6.018192768096924
Iteration 2001:
Training Loss: -3.7186830043792725
Reconstruction Loss: -6.039222240447998
Iteration 2051:
Training Loss: -3.6242990493774414
Reconstruction Loss: -6.060083389282227
Iteration 2101:
Training Loss: -3.787703275680542
Reconstruction Loss: -6.0799880027771
Iteration 2151:
Training Loss: -3.6550374031066895
Reconstruction Loss: -6.100013732910156
Iteration 2201:
Training Loss: -3.80318546295166
Reconstruction Loss: -6.119025707244873
Iteration 2251:
Training Loss: -3.8727121353149414
Reconstruction Loss: -6.137113094329834
Iteration 2301:
Training Loss: -3.89689564704895
Reconstruction Loss: -6.155142784118652
Iteration 2351:
Training Loss: -3.821324348449707
Reconstruction Loss: -6.172191143035889
Iteration 2401:
Training Loss: -3.96944260597229
Reconstruction Loss: -6.188625335693359
Iteration 2451:
Training Loss: -3.9827563762664795
Reconstruction Loss: -6.205416679382324
Iteration 2501:
Training Loss: -3.9945757389068604
Reconstruction Loss: -6.221219062805176
Iteration 2551:
Training Loss: -3.9888756275177
Reconstruction Loss: -6.236617088317871
Iteration 2601:
Training Loss: -4.048450946807861
Reconstruction Loss: -6.251338005065918
Iteration 2651:
Training Loss: -4.165740966796875
Reconstruction Loss: -6.265166282653809
Iteration 2701:
Training Loss: -4.105576992034912
Reconstruction Loss: -6.280021667480469
Iteration 2751:
Training Loss: -4.0042619705200195
Reconstruction Loss: -6.294220447540283
Iteration 2801:
Training Loss: -4.0738911628723145
Reconstruction Loss: -6.30820894241333
Iteration 2851:
Training Loss: -4.138522148132324
Reconstruction Loss: -6.322275638580322
Iteration 2901:
Training Loss: -4.186646461486816
Reconstruction Loss: -6.333467960357666
Iteration 2951:
Training Loss: -4.311233997344971
Reconstruction Loss: -6.347116947174072
Iteration 3001:
Training Loss: -4.199217319488525
Reconstruction Loss: -6.359339714050293
Iteration 3051:
Training Loss: -4.296032905578613
Reconstruction Loss: -6.371872425079346
Iteration 3101:
Training Loss: -4.240030288696289
Reconstruction Loss: -6.382541179656982
Iteration 3151:
Training Loss: -4.26619291305542
Reconstruction Loss: -6.394340515136719
Iteration 3201:
Training Loss: -4.374868392944336
Reconstruction Loss: -6.4057297706604
Iteration 3251:
Training Loss: -4.394944667816162
Reconstruction Loss: -6.4177165031433105
Iteration 3301:
Training Loss: -4.4016923904418945
Reconstruction Loss: -6.42838716506958
Iteration 3351:
Training Loss: -4.4503607749938965
Reconstruction Loss: -6.439055919647217
Iteration 3401:
Training Loss: -4.4108805656433105
Reconstruction Loss: -6.449398517608643
Iteration 3451:
Training Loss: -4.617708206176758
Reconstruction Loss: -6.460025787353516
Iteration 3501:
Training Loss: -4.451213836669922
Reconstruction Loss: -6.4693922996521
Iteration 3551:
Training Loss: -4.465658187866211
Reconstruction Loss: -6.479507923126221
Iteration 3601:
Training Loss: -4.5440592765808105
Reconstruction Loss: -6.488447189331055
Iteration 3651:
Training Loss: -4.531897068023682
Reconstruction Loss: -6.499148368835449
Iteration 3701:
Training Loss: -4.76919412612915
Reconstruction Loss: -6.507718086242676
Iteration 3751:
Training Loss: -4.5314507484436035
Reconstruction Loss: -6.5173726081848145
Iteration 3801:
Training Loss: -4.673940658569336
Reconstruction Loss: -6.526662826538086
Iteration 3851:
Training Loss: -4.642544269561768
Reconstruction Loss: -6.53501033782959
Iteration 3901:
Training Loss: -4.757939338684082
Reconstruction Loss: -6.543915271759033
Iteration 3951:
Training Loss: -4.631702423095703
Reconstruction Loss: -6.553120136260986
Iteration 4001:
Training Loss: -4.703104019165039
Reconstruction Loss: -6.56088399887085
Iteration 4051:
Training Loss: -4.7282562255859375
Reconstruction Loss: -6.56923770904541
Iteration 4101:
Training Loss: -4.706874847412109
Reconstruction Loss: -6.577198028564453
Iteration 4151:
Training Loss: -4.6574015617370605
Reconstruction Loss: -6.5852861404418945
Iteration 4201:
Training Loss: -4.59251594543457
Reconstruction Loss: -6.593400955200195
Iteration 4251:
Training Loss: -4.746020317077637
Reconstruction Loss: -6.601132869720459
Iteration 4301:
Training Loss: -4.754157066345215
Reconstruction Loss: -6.6087822914123535
Iteration 4351:
Training Loss: -4.754904270172119
Reconstruction Loss: -6.615895748138428
Iteration 4401:
Training Loss: -4.820450782775879
Reconstruction Loss: -6.623010158538818
Iteration 4451:
Training Loss: -4.789156436920166
Reconstruction Loss: -6.630688667297363
Iteration 4501:
Training Loss: -4.885008335113525
Reconstruction Loss: -6.637249946594238
Iteration 4551:
Training Loss: -4.9195075035095215
Reconstruction Loss: -6.645402431488037
Iteration 4601:
Training Loss: -4.850185394287109
Reconstruction Loss: -6.6523871421813965
Iteration 4651:
Training Loss: -4.98494815826416
Reconstruction Loss: -6.65819787979126
Iteration 4701:
Training Loss: -4.851072311401367
Reconstruction Loss: -6.6663007736206055
Iteration 4751:
Training Loss: -4.923858642578125
Reconstruction Loss: -6.6728386878967285
Iteration 4801:
Training Loss: -4.883652687072754
Reconstruction Loss: -6.6795783042907715
Iteration 4851:
Training Loss: -4.875802516937256
Reconstruction Loss: -6.685561180114746
Iteration 4901:
Training Loss: -4.859612464904785
Reconstruction Loss: -6.6917219161987305
Iteration 4951:
Training Loss: -4.943120002746582
Reconstruction Loss: -6.698655605316162
Iteration 5001:
Training Loss: -5.118030548095703
Reconstruction Loss: -6.705118656158447
Iteration 5051:
Training Loss: -4.998396873474121
Reconstruction Loss: -6.711059093475342
Iteration 5101:
Training Loss: -4.9836745262146
Reconstruction Loss: -6.717568874359131
Iteration 5151:
Training Loss: -4.972607612609863
Reconstruction Loss: -6.72318696975708
Iteration 5201:
Training Loss: -4.976611137390137
Reconstruction Loss: -6.7290778160095215
Iteration 5251:
Training Loss: -5.0250091552734375
Reconstruction Loss: -6.735591411590576
Iteration 5301:
Training Loss: -5.173966407775879
Reconstruction Loss: -6.741500377655029
Iteration 5351:
Training Loss: -5.091879844665527
Reconstruction Loss: -6.746859550476074
Iteration 5401:
Training Loss: -5.067609786987305
Reconstruction Loss: -6.7528157234191895
Iteration 5451:
Training Loss: -5.041421413421631
Reconstruction Loss: -6.758007526397705
Iteration 5501:
Training Loss: -5.034889221191406
Reconstruction Loss: -6.763824939727783
Iteration 5551:
Training Loss: -5.172423362731934
Reconstruction Loss: -6.768685817718506
Iteration 5601:
Training Loss: -5.142333030700684
Reconstruction Loss: -6.774191379547119
Iteration 5651:
Training Loss: -5.178291320800781
Reconstruction Loss: -6.7790069580078125
Iteration 5701:
Training Loss: -5.245429039001465
Reconstruction Loss: -6.784283638000488
Iteration 5751:
Training Loss: -5.302021026611328
Reconstruction Loss: -6.790777206420898
Iteration 5801:
Training Loss: -5.231103420257568
Reconstruction Loss: -6.79479455947876
Iteration 5851:
Training Loss: -5.115295886993408
Reconstruction Loss: -6.8002471923828125
Iteration 5901:
Training Loss: -5.117724895477295
Reconstruction Loss: -6.80593729019165
Iteration 5951:
Training Loss: -5.273951530456543
Reconstruction Loss: -6.810719013214111
Iteration 6001:
Training Loss: -5.228804111480713
Reconstruction Loss: -6.81521463394165
Iteration 6051:
Training Loss: -5.207332611083984
Reconstruction Loss: -6.820636749267578
Iteration 6101:
Training Loss: -5.515480041503906
Reconstruction Loss: -6.8247456550598145
Iteration 6151:
Training Loss: -5.278260707855225
Reconstruction Loss: -6.830074787139893
Iteration 6201:
Training Loss: -5.243951797485352
Reconstruction Loss: -6.834653377532959
Iteration 6251:
Training Loss: -5.3188347816467285
Reconstruction Loss: -6.8391828536987305
Iteration 6301:
Training Loss: -5.332094192504883
Reconstruction Loss: -6.843837261199951
Iteration 6351:
Training Loss: -5.486878871917725
Reconstruction Loss: -6.848289489746094
Iteration 6401:
Training Loss: -5.30027961730957
Reconstruction Loss: -6.852946758270264
Iteration 6451:
Training Loss: -5.313528060913086
Reconstruction Loss: -6.857246398925781
Iteration 6501:
Training Loss: -5.329982757568359
Reconstruction Loss: -6.861306190490723
Iteration 6551:
Training Loss: -5.349695682525635
Reconstruction Loss: -6.866373538970947
Iteration 6601:
Training Loss: -5.3227972984313965
Reconstruction Loss: -6.870389938354492
Iteration 6651:
Training Loss: -5.4850969314575195
Reconstruction Loss: -6.875206470489502
Iteration 6701:
Training Loss: -5.3110575675964355
Reconstruction Loss: -6.879627227783203
Iteration 6751:
Training Loss: -5.286005973815918
Reconstruction Loss: -6.883373260498047
Iteration 6801:
Training Loss: -5.371295928955078
Reconstruction Loss: -6.887499809265137
Iteration 6851:
Training Loss: -5.447909355163574
Reconstruction Loss: -6.892354488372803
Iteration 6901:
Training Loss: -5.3940510749816895
Reconstruction Loss: -6.896113395690918
Iteration 6951:
Training Loss: -5.4207305908203125
Reconstruction Loss: -6.900002956390381
Iteration 7001:
Training Loss: -5.254429340362549
Reconstruction Loss: -6.904176235198975
Iteration 7051:
Training Loss: -5.380428791046143
Reconstruction Loss: -6.9083404541015625
Iteration 7101:
Training Loss: -5.387207508087158
Reconstruction Loss: -6.911750316619873
Iteration 7151:
Training Loss: -5.580543518066406
Reconstruction Loss: -6.915933132171631
Iteration 7201:
Training Loss: -5.485235691070557
Reconstruction Loss: -6.918962478637695
Iteration 7251:
Training Loss: -5.628005027770996
Reconstruction Loss: -6.923624038696289
Iteration 7301:
Training Loss: -5.617203712463379
Reconstruction Loss: -6.927572250366211
Iteration 7351:
Training Loss: -5.400473594665527
Reconstruction Loss: -6.931360721588135
Iteration 7401:
Training Loss: -5.464726448059082
Reconstruction Loss: -6.934634208679199
Iteration 7451:
Training Loss: -5.522899627685547
Reconstruction Loss: -6.938662052154541
