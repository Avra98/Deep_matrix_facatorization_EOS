5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.737432956695557
Reconstruction Loss: -0.43930143117904663
Iteration 11:
Training Loss: 5.7952375411987305
Reconstruction Loss: -0.43930134177207947
Iteration 21:
Training Loss: 5.762139320373535
Reconstruction Loss: -0.43930143117904663
Iteration 31:
Training Loss: 5.619210243225098
Reconstruction Loss: -0.43930160999298096
Iteration 41:
Training Loss: 5.510194778442383
Reconstruction Loss: -0.43930160999298096
Iteration 51:
Training Loss: 5.480156421661377
Reconstruction Loss: -0.43930160999298096
Iteration 61:
Training Loss: 5.559499263763428
Reconstruction Loss: -0.4393016993999481
Iteration 71:
Training Loss: 5.6091437339782715
Reconstruction Loss: -0.4393016993999481
Iteration 81:
Training Loss: 4.916435718536377
Reconstruction Loss: -0.4393016993999481
Iteration 91:
Training Loss: 5.478192329406738
Reconstruction Loss: -0.4393017888069153
Iteration 101:
Training Loss: 5.218440055847168
Reconstruction Loss: -0.4393017888069153
Iteration 111:
Training Loss: 5.172313213348389
Reconstruction Loss: -0.4393017888069153
Iteration 121:
Training Loss: 5.752562522888184
Reconstruction Loss: -0.43930187821388245
Iteration 131:
Training Loss: 6.006768226623535
Reconstruction Loss: -0.43930187821388245
Iteration 141:
Training Loss: 5.671092987060547
Reconstruction Loss: -0.43930187821388245
Iteration 151:
Training Loss: 5.703134059906006
Reconstruction Loss: -0.43930208683013916
Iteration 161:
Training Loss: 5.746192932128906
Reconstruction Loss: -0.43930208683013916
Iteration 171:
Training Loss: 5.618040084838867
Reconstruction Loss: -0.43930208683013916
Iteration 181:
Training Loss: 5.444657802581787
Reconstruction Loss: -0.4393021762371063
Iteration 191:
Training Loss: 5.457368850708008
Reconstruction Loss: -0.4393022656440735
Iteration 201:
Training Loss: 5.709481716156006
Reconstruction Loss: -0.4393024444580078
Iteration 211:
Training Loss: 5.578230381011963
Reconstruction Loss: -0.4393024444580078
Iteration 221:
Training Loss: 5.650073051452637
Reconstruction Loss: -0.4393024444580078
Iteration 231:
Training Loss: 5.688351154327393
Reconstruction Loss: -0.4393024444580078
Iteration 241:
Training Loss: 5.861472129821777
Reconstruction Loss: -0.439302533864975
Iteration 251:
Training Loss: 5.703540802001953
Reconstruction Loss: -0.439302533864975
Iteration 261:
Training Loss: 5.582618236541748
Reconstruction Loss: -0.43930262327194214
Iteration 271:
Training Loss: 5.237511157989502
Reconstruction Loss: -0.4393027126789093
Iteration 281:
Training Loss: 5.387194633483887
Reconstruction Loss: -0.439302921295166
Iteration 291:
Training Loss: 5.563882827758789
Reconstruction Loss: -0.439302921295166
Iteration 301:
Training Loss: 5.861486911773682
Reconstruction Loss: -0.439302921295166
Iteration 311:
Training Loss: 5.786721229553223
Reconstruction Loss: -0.4393029808998108
Iteration 321:
Training Loss: 5.8529558181762695
Reconstruction Loss: -0.4393029808998108
Iteration 331:
Training Loss: 5.479726314544678
Reconstruction Loss: -0.4393029808998108
Iteration 341:
Training Loss: 5.364921569824219
Reconstruction Loss: -0.43930310010910034
Iteration 351:
Training Loss: 5.7087202072143555
Reconstruction Loss: -0.43930327892303467
Iteration 361:
Training Loss: 4.999864101409912
Reconstruction Loss: -0.43930327892303467
Iteration 371:
Training Loss: 5.409778118133545
Reconstruction Loss: -0.439303457736969
Iteration 381:
Training Loss: 5.929098129272461
Reconstruction Loss: -0.439303457736969
Iteration 391:
Training Loss: 5.553866863250732
Reconstruction Loss: -0.439303457736969
Iteration 401:
Training Loss: 5.656250953674316
Reconstruction Loss: -0.43930354714393616
Iteration 411:
Training Loss: 5.729328155517578
Reconstruction Loss: -0.43930375576019287
Iteration 421:
Training Loss: 5.338874816894531
Reconstruction Loss: -0.43930381536483765
Iteration 431:
Training Loss: 5.200496673583984
Reconstruction Loss: -0.43930381536483765
Iteration 441:
Training Loss: 5.912722587585449
Reconstruction Loss: -0.4393041133880615
Iteration 451:
Training Loss: 5.310441493988037
Reconstruction Loss: -0.4393041133880615
Iteration 461:
Training Loss: 6.059545040130615
Reconstruction Loss: -0.4393042027950287
Iteration 471:
Training Loss: 5.716794490814209
Reconstruction Loss: -0.43930429220199585
Iteration 481:
Training Loss: 5.70884895324707
Reconstruction Loss: -0.43930429220199585
Iteration 491:
Training Loss: 5.33650541305542
Reconstruction Loss: -0.439304381608963
Iteration 501:
Training Loss: 6.006254196166992
Reconstruction Loss: -0.4393046498298645
Iteration 511:
Training Loss: 5.935590744018555
Reconstruction Loss: -0.4393046498298645
Iteration 521:
Training Loss: 5.37425422668457
Reconstruction Loss: -0.4393048584461212
Iteration 531:
Training Loss: 6.17030143737793
Reconstruction Loss: -0.4393048584461212
Iteration 541:
Training Loss: 5.598212242126465
Reconstruction Loss: -0.4393051266670227
Iteration 551:
Training Loss: 5.3192524909973145
Reconstruction Loss: -0.43930521607398987
Iteration 561:
Training Loss: 5.689244747161865
Reconstruction Loss: -0.43930521607398987
Iteration 571:
Training Loss: 4.995096683502197
Reconstruction Loss: -0.43930548429489136
Iteration 581:
Training Loss: 5.6803460121154785
Reconstruction Loss: -0.4393056035041809
Iteration 591:
Training Loss: 5.736462116241455
Reconstruction Loss: -0.4393058717250824
Iteration 601:
Training Loss: 5.473724365234375
Reconstruction Loss: -0.43930596113204956
Iteration 611:
Training Loss: 5.545642375946045
Reconstruction Loss: -0.43930622935295105
Iteration 621:
Training Loss: 5.638611793518066
Reconstruction Loss: -0.4393063187599182
Iteration 631:
Training Loss: 5.513377666473389
Reconstruction Loss: -0.43930649757385254
Iteration 641:
Training Loss: 6.010994911193848
Reconstruction Loss: -0.43930670619010925
Iteration 651:
Training Loss: 5.6611714363098145
Reconstruction Loss: -0.4393068850040436
Iteration 661:
Training Loss: 5.45403528213501
Reconstruction Loss: -0.43930724263191223
Iteration 671:
Training Loss: 5.0448737144470215
Reconstruction Loss: -0.4393075406551361
Iteration 681:
Training Loss: 5.544187545776367
Reconstruction Loss: -0.43930771946907043
Iteration 691:
Training Loss: 5.511155605316162
Reconstruction Loss: -0.4393080770969391
Iteration 701:
Training Loss: 5.500367164611816
Reconstruction Loss: -0.4393083453178406
Iteration 711:
Training Loss: 5.99082612991333
Reconstruction Loss: -0.43930864334106445
Iteration 721:
Training Loss: 5.5555524826049805
Reconstruction Loss: -0.4393090009689331
Iteration 731:
Training Loss: 5.573520660400391
Reconstruction Loss: -0.43930938839912415
Iteration 741:
Training Loss: 5.339688777923584
Reconstruction Loss: -0.4393097460269928
Iteration 751:
Training Loss: 5.766996383666992
Reconstruction Loss: -0.439310222864151
Iteration 761:
Training Loss: 5.871919631958008
Reconstruction Loss: -0.4393106698989868
Iteration 771:
Training Loss: 5.249323844909668
Reconstruction Loss: -0.439311146736145
Iteration 781:
Training Loss: 5.745481491088867
Reconstruction Loss: -0.43931177258491516
Iteration 791:
Training Loss: 5.599120140075684
Reconstruction Loss: -0.4393123388290405
Iteration 801:
Training Loss: 5.56319522857666
Reconstruction Loss: -0.4393130838871002
Iteration 811:
Training Loss: 5.846651554107666
Reconstruction Loss: -0.4393139183521271
Iteration 821:
Training Loss: 5.482054710388184
Reconstruction Loss: -0.4393145442008972
Iteration 831:
Training Loss: 5.557883262634277
Reconstruction Loss: -0.4393155872821808
Iteration 841:
Training Loss: 5.487696170806885
Reconstruction Loss: -0.43931668996810913
Iteration 851:
Training Loss: 5.904762268066406
Reconstruction Loss: -0.4393177926540375
Iteration 861:
Training Loss: 5.341067314147949
Reconstruction Loss: -0.4393191933631897
Iteration 871:
Training Loss: 5.956052780151367
Reconstruction Loss: -0.4393206536769867
Iteration 881:
Training Loss: 5.608301639556885
Reconstruction Loss: -0.43932250142097473
Iteration 891:
Training Loss: 5.412200927734375
Reconstruction Loss: -0.43932464718818665
Iteration 901:
Training Loss: 4.93209981918335
Reconstruction Loss: -0.4393271505832672
Iteration 911:
Training Loss: 4.9858856201171875
Reconstruction Loss: -0.43933019042015076
Iteration 921:
Training Loss: 6.177661418914795
Reconstruction Loss: -0.43933388590812683
Iteration 931:
Training Loss: 5.715685844421387
Reconstruction Loss: -0.4393385946750641
Iteration 941:
Training Loss: 5.293536186218262
Reconstruction Loss: -0.4393444359302521
Iteration 951:
Training Loss: 5.749312400817871
Reconstruction Loss: -0.43935200572013855
Iteration 961:
Training Loss: 5.462337017059326
Reconstruction Loss: -0.43936246633529663
Iteration 971:
Training Loss: 5.514167308807373
Reconstruction Loss: -0.43937671184539795
Iteration 981:
Training Loss: 5.34977912902832
Reconstruction Loss: -0.4393981695175171
Iteration 991:
Training Loss: 5.255522727966309
Reconstruction Loss: -0.4394320249557495
Iteration 1001:
Training Loss: 5.710244178771973
Reconstruction Loss: -0.43949103355407715
Iteration 1011:
Training Loss: 5.619395732879639
Reconstruction Loss: -0.4396115839481354
Iteration 1021:
Training Loss: 5.5835137367248535
Reconstruction Loss: -0.43993765115737915
Iteration 1031:
Training Loss: 5.150270462036133
Reconstruction Loss: -0.44166773557662964
Iteration 1041:
Training Loss: 5.440951347351074
Reconstruction Loss: -0.5773168206214905
Iteration 1051:
Training Loss: 5.639160633087158
Reconstruction Loss: -0.305227667093277
Iteration 1061:
Training Loss: 4.675375938415527
Reconstruction Loss: -0.5741164684295654
Iteration 1071:
Training Loss: 4.774921417236328
Reconstruction Loss: -0.5882484912872314
Iteration 1081:
Training Loss: 5.1420392990112305
Reconstruction Loss: -0.5497116446495056
Iteration 1091:
Training Loss: 4.969099998474121
Reconstruction Loss: -0.5852554440498352
Iteration 1101:
Training Loss: 5.274674892425537
Reconstruction Loss: -0.6352778673171997
Iteration 1111:
Training Loss: 5.26505708694458
Reconstruction Loss: -0.4790070652961731
Iteration 1121:
Training Loss: 4.711541175842285
Reconstruction Loss: -0.5524675846099854
Iteration 1131:
Training Loss: 5.315377712249756
Reconstruction Loss: -0.6525428295135498
Iteration 1141:
Training Loss: 4.800093173980713
Reconstruction Loss: -0.561827540397644
Iteration 1151:
Training Loss: 4.422802925109863
Reconstruction Loss: -0.5838946104049683
Iteration 1161:
Training Loss: 5.158447265625
Reconstruction Loss: -0.6253764033317566
Iteration 1171:
Training Loss: 5.1613311767578125
Reconstruction Loss: -0.5993151664733887
Iteration 1181:
Training Loss: 4.6129584312438965
Reconstruction Loss: -0.6226906776428223
Iteration 1191:
Training Loss: 5.028001308441162
Reconstruction Loss: -0.628030002117157
Iteration 1201:
Training Loss: 4.950202465057373
Reconstruction Loss: -0.6062545776367188
Iteration 1211:
Training Loss: 5.332857608795166
Reconstruction Loss: -0.5855734348297119
Iteration 1221:
Training Loss: 4.929116249084473
Reconstruction Loss: -0.5797677040100098
Iteration 1231:
Training Loss: 5.33008337020874
Reconstruction Loss: -0.5982368588447571
Iteration 1241:
Training Loss: 5.260819911956787
Reconstruction Loss: -0.4449416399002075
Iteration 1251:
Training Loss: 4.875733375549316
Reconstruction Loss: -0.6136550307273865
Iteration 1261:
Training Loss: 4.868324279785156
Reconstruction Loss: -0.6052601337432861
Iteration 1271:
Training Loss: 5.326226711273193
Reconstruction Loss: -0.41855815052986145
Iteration 1281:
Training Loss: 4.933937072753906
Reconstruction Loss: -0.5549679398536682
Iteration 1291:
Training Loss: 5.098916053771973
Reconstruction Loss: -0.5787882804870605
Iteration 1301:
Training Loss: 4.8118896484375
Reconstruction Loss: -0.5443238019943237
Iteration 1311:
Training Loss: 5.09059476852417
Reconstruction Loss: -0.5181109309196472
Iteration 1321:
Training Loss: 5.2789483070373535
Reconstruction Loss: -0.43879789113998413
Iteration 1331:
Training Loss: 5.312972545623779
Reconstruction Loss: -0.581173837184906
Iteration 1341:
Training Loss: 4.69399881362915
Reconstruction Loss: -0.5278985500335693
Iteration 1351:
Training Loss: 4.721883773803711
Reconstruction Loss: -0.627689003944397
Iteration 1361:
Training Loss: 4.88102388381958
Reconstruction Loss: -0.6164770126342773
Iteration 1371:
Training Loss: 4.940479755401611
Reconstruction Loss: -0.5962390899658203
Iteration 1381:
Training Loss: 4.681075096130371
Reconstruction Loss: -0.47777050733566284
Iteration 1391:
Training Loss: 5.006434917449951
Reconstruction Loss: -0.5747086405754089
Iteration 1401:
Training Loss: 4.9469122886657715
Reconstruction Loss: -0.5615195035934448
Iteration 1411:
Training Loss: 5.354843616485596
Reconstruction Loss: -0.5864989161491394
Iteration 1421:
Training Loss: 5.022752285003662
Reconstruction Loss: -0.620049774646759
Iteration 1431:
Training Loss: 4.925061225891113
Reconstruction Loss: -0.6478873491287231
Iteration 1441:
Training Loss: 4.537553787231445
Reconstruction Loss: -0.6266932487487793
Iteration 1451:
Training Loss: 4.951335906982422
Reconstruction Loss: -0.5247820615768433
Iteration 1461:
Training Loss: 5.1164960861206055
Reconstruction Loss: -0.6016168594360352
Iteration 1471:
Training Loss: 4.731162071228027
Reconstruction Loss: -0.5503849983215332
Iteration 1481:
Training Loss: 4.919217586517334
Reconstruction Loss: -0.6400185227394104
Iteration 1491:
Training Loss: 4.81734037399292
Reconstruction Loss: -0.513105571269989
Iteration 1501:
Training Loss: 4.853585243225098
Reconstruction Loss: -0.6139824986457825
Iteration 1511:
Training Loss: 4.819892406463623
Reconstruction Loss: -0.6328314542770386
Iteration 1521:
Training Loss: 4.3135809898376465
Reconstruction Loss: -0.5272542238235474
Iteration 1531:
Training Loss: 4.795045852661133
Reconstruction Loss: -0.5488404631614685
Iteration 1541:
Training Loss: 5.323032379150391
Reconstruction Loss: -0.6208156943321228
Iteration 1551:
Training Loss: 5.010858058929443
Reconstruction Loss: -0.5763318538665771
Iteration 1561:
Training Loss: 5.045251846313477
Reconstruction Loss: -0.6153497695922852
Iteration 1571:
Training Loss: 5.509732246398926
Reconstruction Loss: -0.6458644270896912
Iteration 1581:
Training Loss: 5.143979549407959
Reconstruction Loss: -0.5819780826568604
Iteration 1591:
Training Loss: 5.185391902923584
Reconstruction Loss: -0.5876491069793701
Iteration 1601:
Training Loss: 5.125695705413818
Reconstruction Loss: -0.5474483966827393
Iteration 1611:
Training Loss: 4.8642401695251465
Reconstruction Loss: -0.5079377293586731
Iteration 1621:
Training Loss: 5.156586647033691
Reconstruction Loss: -0.4973108768463135
Iteration 1631:
Training Loss: 4.924948692321777
Reconstruction Loss: -0.619468092918396
Iteration 1641:
Training Loss: 4.822399139404297
Reconstruction Loss: -0.49202167987823486
Iteration 1651:
Training Loss: 5.0146355628967285
Reconstruction Loss: -0.5195460319519043
Iteration 1661:
Training Loss: 4.89836311340332
Reconstruction Loss: -0.5408975481987
Iteration 1671:
Training Loss: 4.659822940826416
Reconstruction Loss: -0.23872530460357666
Iteration 1681:
Training Loss: 5.1058173179626465
Reconstruction Loss: -0.58660489320755
Iteration 1691:
Training Loss: 5.171865463256836
Reconstruction Loss: -0.5849389433860779
Iteration 1701:
Training Loss: 4.850683212280273
Reconstruction Loss: -0.47156229615211487
Iteration 1711:
Training Loss: 5.182741165161133
Reconstruction Loss: -0.5250051617622375
Iteration 1721:
Training Loss: 5.216562747955322
Reconstruction Loss: -0.5813250541687012
Iteration 1731:
Training Loss: 4.919989109039307
Reconstruction Loss: -0.561920702457428
Iteration 1741:
Training Loss: 4.9880475997924805
Reconstruction Loss: -0.5902409553527832
Iteration 1751:
Training Loss: 4.75428581237793
Reconstruction Loss: -0.5819218754768372
Iteration 1761:
Training Loss: 5.161550998687744
Reconstruction Loss: -0.5915327072143555
Iteration 1771:
Training Loss: 5.325176239013672
Reconstruction Loss: -0.6113023161888123
Iteration 1781:
Training Loss: 5.0066046714782715
Reconstruction Loss: -0.5396603345870972
Iteration 1791:
Training Loss: 4.8026275634765625
Reconstruction Loss: -0.5469767451286316
Iteration 1801:
Training Loss: 5.314631938934326
Reconstruction Loss: -0.6157723665237427
Iteration 1811:
Training Loss: 5.1850905418396
Reconstruction Loss: -0.6075230240821838
Iteration 1821:
Training Loss: 4.8998122215271
Reconstruction Loss: -0.5693956613540649
Iteration 1831:
Training Loss: 5.557718753814697
Reconstruction Loss: -0.5823936462402344
Iteration 1841:
Training Loss: 4.853203296661377
Reconstruction Loss: -0.6436882615089417
Iteration 1851:
Training Loss: 5.026716709136963
Reconstruction Loss: -0.6018392443656921
Iteration 1861:
Training Loss: 4.977376937866211
Reconstruction Loss: -0.5736814737319946
Iteration 1871:
Training Loss: 4.986049652099609
Reconstruction Loss: -0.6376671195030212
Iteration 1881:
Training Loss: 5.201297760009766
Reconstruction Loss: -0.4384472370147705
Iteration 1891:
Training Loss: 5.153432846069336
Reconstruction Loss: -0.6537874937057495
Iteration 1901:
Training Loss: 4.7428059577941895
Reconstruction Loss: -0.6350626945495605
Iteration 1911:
Training Loss: 5.184990406036377
Reconstruction Loss: -0.5402032136917114
Iteration 1921:
Training Loss: 5.1137824058532715
Reconstruction Loss: -0.44492581486701965
Iteration 1931:
Training Loss: 5.019399642944336
Reconstruction Loss: -0.6351802349090576
Iteration 1941:
Training Loss: 5.359162330627441
Reconstruction Loss: -0.5652515292167664
Iteration 1951:
Training Loss: 5.250635147094727
Reconstruction Loss: -0.519564151763916
Iteration 1961:
Training Loss: 4.971009254455566
Reconstruction Loss: -0.5645005106925964
Iteration 1971:
Training Loss: 4.74757719039917
Reconstruction Loss: -0.7752678394317627
Iteration 1981:
Training Loss: 4.0874104499816895
Reconstruction Loss: -0.7338767647743225
Iteration 1991:
Training Loss: 4.863393306732178
Reconstruction Loss: -0.5793983936309814
Iteration 2001:
Training Loss: 4.698262691497803
Reconstruction Loss: -0.749952495098114
Iteration 2011:
Training Loss: 4.406229019165039
Reconstruction Loss: -0.6871318817138672
Iteration 2021:
Training Loss: 4.370203971862793
Reconstruction Loss: -0.7074560523033142
Iteration 2031:
Training Loss: 4.456909656524658
Reconstruction Loss: -0.5983542799949646
Iteration 2041:
Training Loss: 4.558782577514648
Reconstruction Loss: -0.6196334362030029
Iteration 2051:
Training Loss: 4.767932891845703
Reconstruction Loss: -0.6068399548530579
Iteration 2061:
Training Loss: 4.621293067932129
Reconstruction Loss: -0.6824902296066284
Iteration 2071:
Training Loss: 4.220882415771484
Reconstruction Loss: -0.6699087023735046
Iteration 2081:
Training Loss: 4.639577865600586
Reconstruction Loss: -0.6371018886566162
Iteration 2091:
Training Loss: 4.547637462615967
Reconstruction Loss: -0.6360829472541809
Iteration 2101:
Training Loss: 4.337225437164307
Reconstruction Loss: -0.7140061259269714
Iteration 2111:
Training Loss: 4.648963451385498
Reconstruction Loss: -0.7441058158874512
Iteration 2121:
Training Loss: 4.200192928314209
Reconstruction Loss: -0.7338283061981201
Iteration 2131:
Training Loss: 4.494834899902344
Reconstruction Loss: -0.6362484693527222
Iteration 2141:
Training Loss: 4.280949115753174
Reconstruction Loss: -0.6838404536247253
Iteration 2151:
Training Loss: 4.660768032073975
Reconstruction Loss: -0.6830682158470154
Iteration 2161:
Training Loss: 4.796596527099609
Reconstruction Loss: -0.7243496179580688
Iteration 2171:
Training Loss: 4.239198207855225
Reconstruction Loss: -0.6166729927062988
Iteration 2181:
Training Loss: 4.549733638763428
Reconstruction Loss: -0.599719762802124
Iteration 2191:
Training Loss: 4.233798980712891
Reconstruction Loss: -0.6533374786376953
Iteration 2201:
Training Loss: 4.268145561218262
Reconstruction Loss: -0.6730467081069946
Iteration 2211:
Training Loss: 4.052481174468994
Reconstruction Loss: -0.6211296916007996
Iteration 2221:
Training Loss: 4.75250244140625
Reconstruction Loss: -0.6924436092376709
Iteration 2231:
Training Loss: 4.168254375457764
Reconstruction Loss: -0.6770290732383728
Iteration 2241:
Training Loss: 4.735538959503174
Reconstruction Loss: -0.6338058114051819
Iteration 2251:
Training Loss: 4.381753921508789
Reconstruction Loss: -0.6438586711883545
Iteration 2261:
Training Loss: 4.559414863586426
Reconstruction Loss: -0.6806856393814087
Iteration 2271:
Training Loss: 4.093976974487305
Reconstruction Loss: -0.7302850484848022
Iteration 2281:
Training Loss: 4.484832763671875
Reconstruction Loss: -0.4507094919681549
Iteration 2291:
Training Loss: 4.346588134765625
Reconstruction Loss: -0.6339331865310669
Iteration 2301:
Training Loss: 4.077128887176514
Reconstruction Loss: -0.6556387543678284
Iteration 2311:
Training Loss: 4.642757415771484
Reconstruction Loss: -0.6727516055107117
Iteration 2321:
Training Loss: 4.799861907958984
Reconstruction Loss: -0.6933628916740417
Iteration 2331:
Training Loss: 4.153618335723877
Reconstruction Loss: -0.7074904441833496
Iteration 2341:
Training Loss: 4.048127174377441
Reconstruction Loss: -0.6641886830329895
Iteration 2351:
Training Loss: 4.3676371574401855
Reconstruction Loss: -0.7372925281524658
Iteration 2361:
Training Loss: 4.523527145385742
Reconstruction Loss: -0.7011678218841553
Iteration 2371:
Training Loss: 4.363715171813965
Reconstruction Loss: -0.6298768520355225
Iteration 2381:
Training Loss: 4.666048049926758
Reconstruction Loss: -0.575698971748352
Iteration 2391:
Training Loss: 4.290161609649658
Reconstruction Loss: -0.6074040532112122
Iteration 2401:
Training Loss: 4.861297607421875
Reconstruction Loss: -0.5810465812683105
Iteration 2411:
Training Loss: 4.529088973999023
Reconstruction Loss: -0.6684311032295227
Iteration 2421:
Training Loss: 4.233651638031006
Reconstruction Loss: -0.7054851055145264
Iteration 2431:
Training Loss: 3.9453465938568115
Reconstruction Loss: -0.6784164309501648
Iteration 2441:
Training Loss: 4.277060508728027
Reconstruction Loss: -0.720586895942688
Iteration 2451:
Training Loss: 4.916947841644287
Reconstruction Loss: -0.7684621810913086
Iteration 2461:
Training Loss: 4.369513034820557
Reconstruction Loss: -0.6266477108001709
Iteration 2471:
Training Loss: 4.371123313903809
Reconstruction Loss: -0.7246888875961304
Iteration 2481:
Training Loss: 4.325446605682373
Reconstruction Loss: -0.7488633394241333
Iteration 2491:
Training Loss: 4.3037519454956055
Reconstruction Loss: -0.697340190410614
Iteration 2501:
Training Loss: 4.396750450134277
Reconstruction Loss: -0.6633913516998291
Iteration 2511:
Training Loss: 4.202338695526123
Reconstruction Loss: -0.719639003276825
Iteration 2521:
Training Loss: 4.816607475280762
Reconstruction Loss: -0.5642755031585693
Iteration 2531:
Training Loss: 4.671989440917969
Reconstruction Loss: -0.647404134273529
Iteration 2541:
Training Loss: 4.313940048217773
Reconstruction Loss: -0.7178812026977539
Iteration 2551:
Training Loss: 4.23119592666626
Reconstruction Loss: -0.6310210227966309
Iteration 2561:
Training Loss: 4.545748233795166
Reconstruction Loss: -0.6701948046684265
Iteration 2571:
Training Loss: 4.9187798500061035
Reconstruction Loss: -0.6414087414741516
Iteration 2581:
Training Loss: 4.092285633087158
Reconstruction Loss: -0.6683830618858337
Iteration 2591:
Training Loss: 4.427858352661133
Reconstruction Loss: -0.7328746318817139
Iteration 2601:
Training Loss: 4.511890411376953
Reconstruction Loss: -0.6896612644195557
Iteration 2611:
Training Loss: 4.586899280548096
Reconstruction Loss: -0.6634240746498108
Iteration 2621:
Training Loss: 4.4572858810424805
Reconstruction Loss: -0.7167676687240601
Iteration 2631:
Training Loss: 4.151591777801514
Reconstruction Loss: -0.720992386341095
Iteration 2641:
Training Loss: 4.2112603187561035
Reconstruction Loss: -0.7353606224060059
Iteration 2651:
Training Loss: 4.469796657562256
Reconstruction Loss: -0.721200704574585
Iteration 2661:
Training Loss: 4.421022415161133
Reconstruction Loss: -0.6303716897964478
Iteration 2671:
Training Loss: 4.363682270050049
Reconstruction Loss: -0.5995664596557617
Iteration 2681:
Training Loss: 4.404003620147705
Reconstruction Loss: -0.6444743275642395
Iteration 2691:
Training Loss: 4.247622966766357
Reconstruction Loss: -0.6828845739364624
Iteration 2701:
Training Loss: 4.2554192543029785
Reconstruction Loss: -0.6806327700614929
Iteration 2711:
Training Loss: 4.653956413269043
Reconstruction Loss: -0.6699258685112
Iteration 2721:
Training Loss: 4.606667995452881
Reconstruction Loss: -0.6760311126708984
Iteration 2731:
Training Loss: 4.621156215667725
Reconstruction Loss: -0.6934083104133606
Iteration 2741:
Training Loss: 4.3181047439575195
Reconstruction Loss: -0.6636335253715515
Iteration 2751:
Training Loss: 4.323988437652588
Reconstruction Loss: -0.6922532916069031
Iteration 2761:
Training Loss: 4.525237560272217
Reconstruction Loss: -0.6553995013237
Iteration 2771:
Training Loss: 4.480022430419922
Reconstruction Loss: -0.698428213596344
Iteration 2781:
Training Loss: 4.61043119430542
Reconstruction Loss: -0.6582973599433899
Iteration 2791:
Training Loss: 4.115795135498047
Reconstruction Loss: -0.6665330529212952
Iteration 2801:
Training Loss: 4.069445610046387
Reconstruction Loss: -0.6920380592346191
Iteration 2811:
Training Loss: 4.473583221435547
Reconstruction Loss: -0.693736732006073
Iteration 2821:
Training Loss: 4.840018272399902
Reconstruction Loss: -0.6627801060676575
Iteration 2831:
Training Loss: 4.459240436553955
Reconstruction Loss: -0.6403098702430725
Iteration 2841:
Training Loss: 4.634796619415283
Reconstruction Loss: -0.7172966003417969
Iteration 2851:
Training Loss: 4.467519283294678
Reconstruction Loss: -0.6305322647094727
Iteration 2861:
Training Loss: 4.339313507080078
Reconstruction Loss: -0.6595289707183838
Iteration 2871:
Training Loss: 4.5329694747924805
Reconstruction Loss: -0.6448289155960083
Iteration 2881:
Training Loss: 4.552190780639648
Reconstruction Loss: -0.666812002658844
Iteration 2891:
Training Loss: 4.558071613311768
Reconstruction Loss: -0.6639009714126587
Iteration 2901:
Training Loss: 4.563360691070557
Reconstruction Loss: -0.66445392370224
Iteration 2911:
Training Loss: 4.572876930236816
Reconstruction Loss: -0.6647545695304871
Iteration 2921:
Training Loss: 5.138683319091797
Reconstruction Loss: -0.6800109148025513
Iteration 2931:
Training Loss: 4.572802543640137
Reconstruction Loss: -0.6580073237419128
Iteration 2941:
Training Loss: 4.522665023803711
Reconstruction Loss: -0.6852692365646362
Iteration 2951:
Training Loss: 4.2158966064453125
Reconstruction Loss: -0.6930719614028931
Iteration 2961:
Training Loss: 4.460546970367432
Reconstruction Loss: -0.6306868195533752
Iteration 2971:
Training Loss: 4.308154106140137
Reconstruction Loss: -0.5756266713142395
Iteration 2981:
Training Loss: 4.404425144195557
Reconstruction Loss: -0.6134109497070312
Iteration 2991:
Training Loss: 4.75261926651001
Reconstruction Loss: -0.6969125866889954
Iteration 3001:
Training Loss: 4.734921932220459
Reconstruction Loss: -0.6460882425308228
Iteration 3011:
Training Loss: 3.8424830436706543
Reconstruction Loss: -0.723261296749115
Iteration 3021:
Training Loss: 4.615977764129639
Reconstruction Loss: -0.7426548600196838
Iteration 3031:
Training Loss: 4.3230719566345215
Reconstruction Loss: -0.6804060935974121
Iteration 3041:
Training Loss: 4.503927230834961
Reconstruction Loss: -0.6699267625808716
Iteration 3051:
Training Loss: 4.744796276092529
Reconstruction Loss: -0.7420763969421387
Iteration 3061:
Training Loss: 4.661304473876953
Reconstruction Loss: -0.6850937604904175
Iteration 3071:
Training Loss: 4.290034770965576
Reconstruction Loss: -0.6545742154121399
Iteration 3081:
Training Loss: 4.450575351715088
Reconstruction Loss: -0.6889330744743347
Iteration 3091:
Training Loss: 4.547241687774658
Reconstruction Loss: -0.6796820163726807
Iteration 3101:
Training Loss: 3.9278640747070312
Reconstruction Loss: -0.7031037211418152
Iteration 3111:
Training Loss: 3.8378100395202637
Reconstruction Loss: -0.586127519607544
Iteration 3121:
Training Loss: 4.537015914916992
Reconstruction Loss: -0.6831802129745483
Iteration 3131:
Training Loss: 4.700690269470215
Reconstruction Loss: -0.6650877594947815
Iteration 3141:
Training Loss: 4.154970169067383
Reconstruction Loss: -0.6881164908409119
Iteration 3151:
Training Loss: 4.268562316894531
Reconstruction Loss: -0.7043572068214417
Iteration 3161:
Training Loss: 4.723947525024414
Reconstruction Loss: -0.6667616367340088
Iteration 3171:
Training Loss: 4.347874641418457
Reconstruction Loss: -0.6392877697944641
Iteration 3181:
Training Loss: 4.594560623168945
Reconstruction Loss: -0.5802278518676758
Iteration 3191:
Training Loss: 4.401463031768799
Reconstruction Loss: -0.5626415014266968
Iteration 3201:
Training Loss: 4.506292343139648
Reconstruction Loss: -0.5946903824806213
Iteration 3211:
Training Loss: 4.476447582244873
Reconstruction Loss: -0.645812451839447
Iteration 3221:
Training Loss: 4.639611721038818
Reconstruction Loss: -0.5620983242988586
Iteration 3231:
Training Loss: 3.9422848224639893
Reconstruction Loss: -0.716471791267395
Iteration 3241:
Training Loss: 4.219583988189697
Reconstruction Loss: -0.6748183369636536
Iteration 3251:
Training Loss: 4.75749397277832
Reconstruction Loss: -0.6842952370643616
Iteration 3261:
Training Loss: 4.49954891204834
Reconstruction Loss: -0.6571331024169922
Iteration 3271:
Training Loss: 4.644117832183838
Reconstruction Loss: -0.6164932250976562
Iteration 3281:
Training Loss: 4.500002384185791
Reconstruction Loss: -0.6729823350906372
Iteration 3291:
Training Loss: 4.1800689697265625
Reconstruction Loss: -0.6973019242286682
Iteration 3301:
Training Loss: 4.764973163604736
Reconstruction Loss: -0.666776716709137
Iteration 3311:
Training Loss: 3.685091257095337
Reconstruction Loss: -0.6653899550437927
Iteration 3321:
Training Loss: 4.7371745109558105
Reconstruction Loss: -0.7398544549942017
Iteration 3331:
Training Loss: 4.623272895812988
Reconstruction Loss: -0.7189615368843079
Iteration 3341:
Training Loss: 4.277900218963623
Reconstruction Loss: -0.691248893737793
Iteration 3351:
Training Loss: 4.98705530166626
Reconstruction Loss: -0.5575003027915955
Iteration 3361:
Training Loss: 4.4590277671813965
Reconstruction Loss: -0.608009397983551
Iteration 3371:
Training Loss: 4.843567848205566
Reconstruction Loss: -0.6229056715965271
Iteration 3381:
Training Loss: 4.867915153503418
Reconstruction Loss: -0.7024064064025879
Iteration 3391:
Training Loss: 4.436172962188721
Reconstruction Loss: -0.6748377680778503
Iteration 3401:
Training Loss: 5.033700942993164
Reconstruction Loss: -0.663631796836853
Iteration 3411:
Training Loss: 4.614203453063965
Reconstruction Loss: -0.6924716234207153
Iteration 3421:
Training Loss: 4.412015438079834
Reconstruction Loss: -0.6760528683662415
Iteration 3431:
Training Loss: 3.991471529006958
Reconstruction Loss: -0.6884037852287292
Iteration 3441:
Training Loss: 4.662271499633789
Reconstruction Loss: -0.7106950879096985
Iteration 3451:
Training Loss: 3.9687554836273193
Reconstruction Loss: -0.719389796257019
Iteration 3461:
Training Loss: 4.41550874710083
Reconstruction Loss: -0.6842790842056274
Iteration 3471:
Training Loss: 4.15373420715332
Reconstruction Loss: -0.6678037643432617
Iteration 3481:
Training Loss: 4.341822624206543
Reconstruction Loss: -0.6702488660812378
Iteration 3491:
Training Loss: 4.660727024078369
Reconstruction Loss: -0.6743158102035522
Iteration 3501:
Training Loss: 4.297636032104492
Reconstruction Loss: -0.6882297396659851
Iteration 3511:
Training Loss: 4.53256893157959
Reconstruction Loss: -0.6426543593406677
Iteration 3521:
Training Loss: 4.279129981994629
Reconstruction Loss: -0.6837425231933594
Iteration 3531:
Training Loss: 4.671095371246338
Reconstruction Loss: -0.6221989393234253
Iteration 3541:
Training Loss: 4.1834635734558105
Reconstruction Loss: -0.6726441979408264
Iteration 3551:
Training Loss: 4.614551544189453
Reconstruction Loss: -0.6178595423698425
Iteration 3561:
Training Loss: 4.523331642150879
Reconstruction Loss: -0.6735297441482544
Iteration 3571:
Training Loss: 4.676234722137451
Reconstruction Loss: -0.6014438271522522
Iteration 3581:
Training Loss: 4.485770225524902
Reconstruction Loss: -0.7172991037368774
Iteration 3591:
Training Loss: 4.436392307281494
Reconstruction Loss: -0.7276888489723206
Iteration 3601:
Training Loss: 4.4826884269714355
Reconstruction Loss: -0.5857576131820679
Iteration 3611:
Training Loss: 4.7203288078308105
Reconstruction Loss: -0.6948139071464539
Iteration 3621:
Training Loss: 4.757602691650391
Reconstruction Loss: -0.6463820338249207
Iteration 3631:
Training Loss: 4.859213829040527
Reconstruction Loss: -0.5154379606246948
Iteration 3641:
Training Loss: 4.279078960418701
Reconstruction Loss: -0.6479285955429077
Iteration 3651:
Training Loss: 4.3361711502075195
Reconstruction Loss: -0.6211223602294922
Iteration 3661:
Training Loss: 4.565072536468506
Reconstruction Loss: -0.7351561188697815
Iteration 3671:
Training Loss: 4.543421268463135
Reconstruction Loss: -0.5489320755004883
Iteration 3681:
Training Loss: 4.508872985839844
Reconstruction Loss: -0.6470659971237183
Iteration 3691:
Training Loss: 4.322086334228516
Reconstruction Loss: -0.688258945941925
Iteration 3701:
Training Loss: 4.059680938720703
Reconstruction Loss: -0.6806391477584839
Iteration 3711:
Training Loss: 4.4529337882995605
Reconstruction Loss: -0.6963544487953186
Iteration 3721:
Training Loss: 4.386606693267822
Reconstruction Loss: -0.7115910649299622
Iteration 3731:
Training Loss: 4.488902568817139
Reconstruction Loss: -0.6792455315589905
Iteration 3741:
Training Loss: 4.210036277770996
Reconstruction Loss: -0.6771714687347412
Iteration 3751:
Training Loss: 4.449712753295898
Reconstruction Loss: -0.6056262254714966
Iteration 3761:
Training Loss: 4.552816390991211
Reconstruction Loss: -0.6153666377067566
Iteration 3771:
Training Loss: 4.300900936126709
Reconstruction Loss: -0.6074870228767395
Iteration 3781:
Training Loss: 4.562964916229248
Reconstruction Loss: -0.6278026103973389
Iteration 3791:
Training Loss: 4.678255081176758
Reconstruction Loss: -0.6433495283126831
Iteration 3801:
Training Loss: 4.623631000518799
Reconstruction Loss: -0.6970973014831543
Iteration 3811:
Training Loss: 4.247241020202637
Reconstruction Loss: -0.6154801249504089
Iteration 3821:
Training Loss: 4.838274002075195
Reconstruction Loss: -0.5540804266929626
Iteration 3831:
Training Loss: 4.354494094848633
Reconstruction Loss: -0.6262643337249756
Iteration 3841:
Training Loss: 4.395721912384033
Reconstruction Loss: -0.6789535284042358
Iteration 3851:
Training Loss: 4.623087406158447
Reconstruction Loss: -0.6691960692405701
Iteration 3861:
Training Loss: 4.414846897125244
Reconstruction Loss: -0.676000714302063
Iteration 3871:
Training Loss: 4.594111919403076
Reconstruction Loss: -0.6384167671203613
Iteration 3881:
Training Loss: 4.4195356369018555
Reconstruction Loss: -0.6901636719703674
Iteration 3891:
Training Loss: 4.75266170501709
Reconstruction Loss: -0.6915608048439026
Iteration 3901:
Training Loss: 4.579982757568359
Reconstruction Loss: -0.6660538911819458
Iteration 3911:
Training Loss: 4.633864879608154
Reconstruction Loss: -0.6770510077476501
Iteration 3921:
Training Loss: 4.601201057434082
Reconstruction Loss: -0.7343683838844299
Iteration 3931:
Training Loss: 4.405385494232178
Reconstruction Loss: -0.6901124119758606
Iteration 3941:
Training Loss: 4.275851726531982
Reconstruction Loss: -0.7377334237098694
Iteration 3951:
Training Loss: 4.321577072143555
Reconstruction Loss: -0.6990853548049927
Iteration 3961:
Training Loss: 4.471493244171143
Reconstruction Loss: -0.7293234467506409
Iteration 3971:
Training Loss: 4.532889366149902
Reconstruction Loss: -0.7099127769470215
Iteration 3981:
Training Loss: 4.205658912658691
Reconstruction Loss: -0.7212643623352051
Iteration 3991:
Training Loss: 4.766873359680176
Reconstruction Loss: -0.6307134628295898
Iteration 4001:
Training Loss: 4.657587051391602
Reconstruction Loss: -0.6906101703643799
Iteration 4011:
Training Loss: 4.508335113525391
Reconstruction Loss: -0.6053365468978882
Iteration 4021:
Training Loss: 4.60189151763916
Reconstruction Loss: -0.6340650916099548
Iteration 4031:
Training Loss: 4.299509525299072
Reconstruction Loss: -0.6485121250152588
Iteration 4041:
Training Loss: 4.330437660217285
Reconstruction Loss: -0.6288129091262817
Iteration 4051:
Training Loss: 4.0809760093688965
Reconstruction Loss: -0.6390047073364258
Iteration 4061:
Training Loss: 4.2952656745910645
Reconstruction Loss: -0.662159264087677
Iteration 4071:
Training Loss: 4.258114337921143
Reconstruction Loss: -0.6852015852928162
Iteration 4081:
Training Loss: 4.355895042419434
Reconstruction Loss: -0.6442074179649353
Iteration 4091:
Training Loss: 4.346482276916504
Reconstruction Loss: -0.6399181485176086
Iteration 4101:
Training Loss: 4.352039337158203
Reconstruction Loss: -0.6173247694969177
Iteration 4111:
Training Loss: 4.44113826751709
Reconstruction Loss: -0.7041289210319519
Iteration 4121:
Training Loss: 4.6040730476379395
Reconstruction Loss: -0.7318003177642822
Iteration 4131:
Training Loss: 4.600223064422607
Reconstruction Loss: -0.6811611652374268
Iteration 4141:
Training Loss: 4.4068145751953125
Reconstruction Loss: -0.6477744579315186
Iteration 4151:
Training Loss: 4.27047872543335
Reconstruction Loss: -0.5380836725234985
Iteration 4161:
Training Loss: 4.582324028015137
Reconstruction Loss: -0.6961118578910828
Iteration 4171:
Training Loss: 4.297322750091553
Reconstruction Loss: -0.7097422480583191
Iteration 4181:
Training Loss: 4.446700572967529
Reconstruction Loss: -0.7144862413406372
Iteration 4191:
Training Loss: 4.200733661651611
Reconstruction Loss: -0.6399098038673401
Iteration 4201:
Training Loss: 4.193117618560791
Reconstruction Loss: -0.6219087243080139
Iteration 4211:
Training Loss: 4.560091972351074
Reconstruction Loss: -0.6799577474594116
Iteration 4221:
Training Loss: 3.765228509902954
Reconstruction Loss: -0.6328353881835938
Iteration 4231:
Training Loss: 4.388983726501465
Reconstruction Loss: -0.653169572353363
Iteration 4241:
Training Loss: 4.054600238800049
Reconstruction Loss: -0.6496313810348511
Iteration 4251:
Training Loss: 4.6414475440979
Reconstruction Loss: -0.6695261597633362
Iteration 4261:
Training Loss: 4.4286088943481445
Reconstruction Loss: -0.7394983172416687
Iteration 4271:
Training Loss: 4.153619289398193
Reconstruction Loss: -0.6489264965057373
Iteration 4281:
Training Loss: 4.385813236236572
Reconstruction Loss: -0.718293309211731
Iteration 4291:
Training Loss: 4.588809490203857
Reconstruction Loss: -0.6515713334083557
Iteration 4301:
Training Loss: 4.5889692306518555
Reconstruction Loss: -0.6027340292930603
Iteration 4311:
Training Loss: 4.405725002288818
Reconstruction Loss: -0.6410262584686279
Iteration 4321:
Training Loss: 4.493250370025635
Reconstruction Loss: -0.6588580012321472
Iteration 4331:
Training Loss: 4.506834030151367
Reconstruction Loss: -0.6041041016578674
Iteration 4341:
Training Loss: 4.832944869995117
Reconstruction Loss: -0.6451065540313721
Iteration 4351:
Training Loss: 4.166351318359375
Reconstruction Loss: -0.6185725927352905
Iteration 4361:
Training Loss: 4.635882377624512
Reconstruction Loss: -0.681261420249939
Iteration 4371:
Training Loss: 4.24566650390625
Reconstruction Loss: -0.59466153383255
Iteration 4381:
Training Loss: 3.938680410385132
Reconstruction Loss: -0.6646488904953003
Iteration 4391:
Training Loss: 4.686037063598633
Reconstruction Loss: -0.7223171591758728
Iteration 4401:
Training Loss: 4.657476902008057
Reconstruction Loss: -0.7630900740623474
Iteration 4411:
Training Loss: 4.302327632904053
Reconstruction Loss: -0.6268903613090515
Iteration 4421:
Training Loss: 4.225640773773193
Reconstruction Loss: -0.6205581426620483
Iteration 4431:
Training Loss: 4.268863201141357
Reconstruction Loss: -0.6792181134223938
Iteration 4441:
Training Loss: 5.127298831939697
Reconstruction Loss: -0.6759734153747559
Iteration 4451:
Training Loss: 4.574092388153076
Reconstruction Loss: -0.644874095916748
Iteration 4461:
Training Loss: 4.381772041320801
Reconstruction Loss: -0.6858685612678528
Iteration 4471:
Training Loss: 4.417120933532715
Reconstruction Loss: -0.6078332662582397
Iteration 4481:
Training Loss: 3.9691054821014404
Reconstruction Loss: -0.678555428981781
Iteration 4491:
Training Loss: 4.47086238861084
Reconstruction Loss: -0.6612567901611328
Iteration 4501:
Training Loss: 4.324753761291504
Reconstruction Loss: -0.7128430604934692
Iteration 4511:
Training Loss: 4.679261684417725
Reconstruction Loss: -0.6661707162857056
Iteration 4521:
Training Loss: 4.596826076507568
Reconstruction Loss: -0.6790038347244263
Iteration 4531:
Training Loss: 4.19838285446167
Reconstruction Loss: -0.6952512860298157
Iteration 4541:
Training Loss: 4.4498748779296875
Reconstruction Loss: -0.6589635014533997
Iteration 4551:
Training Loss: 4.60837459564209
Reconstruction Loss: -0.5036367177963257
Iteration 4561:
Training Loss: 4.425482273101807
Reconstruction Loss: -0.703689694404602
Iteration 4571:
Training Loss: 4.3229289054870605
Reconstruction Loss: -0.7197422385215759
Iteration 4581:
Training Loss: 4.651484489440918
Reconstruction Loss: -0.6878938674926758
Iteration 4591:
Training Loss: 4.557579040527344
Reconstruction Loss: -0.5969264507293701
Iteration 4601:
Training Loss: 4.4845452308654785
Reconstruction Loss: -0.6698032021522522
Iteration 4611:
Training Loss: 4.335564136505127
Reconstruction Loss: -0.6847389340400696
Iteration 4621:
Training Loss: 4.779092788696289
Reconstruction Loss: -0.6827883124351501
Iteration 4631:
Training Loss: 4.312896728515625
Reconstruction Loss: -0.6536373496055603
Iteration 4641:
Training Loss: 3.644296646118164
Reconstruction Loss: -0.6997385621070862
Iteration 4651:
Training Loss: 4.236118793487549
Reconstruction Loss: -0.6295409202575684
Iteration 4661:
Training Loss: 4.187254428863525
Reconstruction Loss: -0.6721658110618591
Iteration 4671:
Training Loss: 4.456661224365234
Reconstruction Loss: -0.6566826701164246
Iteration 4681:
Training Loss: 4.34493350982666
Reconstruction Loss: -0.7191741466522217
Iteration 4691:
Training Loss: 4.638575553894043
Reconstruction Loss: -0.6390244960784912
Iteration 4701:
Training Loss: 4.360894680023193
Reconstruction Loss: -0.6755938529968262
Iteration 4711:
Training Loss: 4.100910663604736
Reconstruction Loss: -0.6602199077606201
Iteration 4721:
Training Loss: 4.221153259277344
Reconstruction Loss: -0.6530697345733643
Iteration 4731:
Training Loss: 4.595435619354248
Reconstruction Loss: -0.7121921181678772
Iteration 4741:
Training Loss: 4.495707035064697
Reconstruction Loss: -0.5600688457489014
Iteration 4751:
Training Loss: 4.6093950271606445
Reconstruction Loss: -0.6910969614982605
Iteration 4761:
Training Loss: 4.420964241027832
Reconstruction Loss: -0.6686737537384033
Iteration 4771:
Training Loss: 4.646471977233887
Reconstruction Loss: -0.6206286549568176
Iteration 4781:
Training Loss: 4.713968753814697
Reconstruction Loss: -0.7052757740020752
Iteration 4791:
Training Loss: 4.695685863494873
Reconstruction Loss: -0.531440258026123
Iteration 4801:
Training Loss: 4.9187140464782715
Reconstruction Loss: -0.6956301927566528
Iteration 4811:
Training Loss: 4.463979244232178
Reconstruction Loss: -0.6010599732398987
Iteration 4821:
Training Loss: 4.69124174118042
Reconstruction Loss: -0.6143213510513306
Iteration 4831:
Training Loss: 4.578611373901367
Reconstruction Loss: -0.6540554761886597
Iteration 4841:
Training Loss: 4.434482574462891
Reconstruction Loss: -0.7180637121200562
Iteration 4851:
Training Loss: 4.614728927612305
Reconstruction Loss: -0.6265400648117065
Iteration 4861:
Training Loss: 4.695311069488525
Reconstruction Loss: -0.6393950581550598
Iteration 4871:
Training Loss: 4.64543342590332
Reconstruction Loss: -0.6730709075927734
Iteration 4881:
Training Loss: 4.572772979736328
Reconstruction Loss: -0.631075918674469
Iteration 4891:
Training Loss: 4.373994827270508
Reconstruction Loss: -0.7038425803184509
Iteration 4901:
Training Loss: 4.687880039215088
Reconstruction Loss: -0.7179713845252991
Iteration 4911:
Training Loss: 4.213839530944824
Reconstruction Loss: -0.6229801177978516
Iteration 4921:
Training Loss: 4.733789443969727
Reconstruction Loss: -0.6800836324691772
Iteration 4931:
Training Loss: 4.25414514541626
Reconstruction Loss: -0.6391777396202087
Iteration 4941:
Training Loss: 4.468607425689697
Reconstruction Loss: -0.7045443058013916
Iteration 4951:
Training Loss: 4.30584192276001
Reconstruction Loss: -0.7046980261802673
Iteration 4961:
Training Loss: 4.513461589813232
Reconstruction Loss: -0.5387783050537109
Iteration 4971:
Training Loss: 4.425295829772949
Reconstruction Loss: -0.6050252914428711
Iteration 4981:
Training Loss: 4.317049503326416
Reconstruction Loss: -0.6077474355697632
Iteration 4991:
Training Loss: 4.426826000213623
Reconstruction Loss: -0.7015306949615479
