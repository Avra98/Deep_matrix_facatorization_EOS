5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.530964374542236
Reconstruction Loss: -0.3916729688644409
Iteration 21:
Training Loss: 4.859014511108398
Reconstruction Loss: -0.7050935626029968
Iteration 41:
Training Loss: 3.597857713699341
Reconstruction Loss: -1.4501458406448364
Iteration 61:
Training Loss: 2.2112200260162354
Reconstruction Loss: -1.9612867832183838
Iteration 81:
Training Loss: 1.4870274066925049
Reconstruction Loss: -2.5353519916534424
Iteration 101:
Training Loss: 0.4628416895866394
Reconstruction Loss: -3.0821690559387207
Iteration 121:
Training Loss: 0.2047647088766098
Reconstruction Loss: -3.5785539150238037
Iteration 141:
Training Loss: -0.6678487062454224
Reconstruction Loss: -4.017770767211914
Iteration 161:
Training Loss: -1.213424801826477
Reconstruction Loss: -4.38278865814209
Iteration 181:
Training Loss: -1.449614405632019
Reconstruction Loss: -4.690099239349365
Iteration 201:
Training Loss: -1.6537636518478394
Reconstruction Loss: -4.929221153259277
Iteration 221:
Training Loss: -1.7247270345687866
Reconstruction Loss: -5.121383190155029
Iteration 241:
Training Loss: -2.0165648460388184
Reconstruction Loss: -5.2779316902160645
Iteration 261:
Training Loss: -1.9789174795150757
Reconstruction Loss: -5.401423931121826
Iteration 281:
Training Loss: -2.567727565765381
Reconstruction Loss: -5.504999160766602
Iteration 301:
Training Loss: -2.523806095123291
Reconstruction Loss: -5.593949794769287
Iteration 321:
Training Loss: -2.5796446800231934
Reconstruction Loss: -5.663843631744385
Iteration 341:
Training Loss: -2.466597318649292
Reconstruction Loss: -5.725915431976318
Iteration 361:
Training Loss: -2.524320125579834
Reconstruction Loss: -5.783196449279785
Iteration 381:
Training Loss: -2.769098997116089
Reconstruction Loss: -5.830288410186768
Iteration 401:
Training Loss: -2.666935682296753
Reconstruction Loss: -5.877487659454346
Iteration 421:
Training Loss: -3.231231689453125
Reconstruction Loss: -5.921325206756592
Iteration 441:
Training Loss: -2.995337963104248
Reconstruction Loss: -5.958393096923828
Iteration 461:
Training Loss: -3.0623598098754883
Reconstruction Loss: -5.9913859367370605
Iteration 481:
Training Loss: -3.060386896133423
Reconstruction Loss: -6.0229268074035645
Iteration 501:
Training Loss: -3.016803026199341
Reconstruction Loss: -6.052990436553955
Iteration 521:
Training Loss: -3.2195119857788086
Reconstruction Loss: -6.083563804626465
Iteration 541:
Training Loss: -3.4910221099853516
Reconstruction Loss: -6.110671043395996
Iteration 561:
Training Loss: -3.1720147132873535
Reconstruction Loss: -6.136848449707031
Iteration 581:
Training Loss: -3.3784971237182617
Reconstruction Loss: -6.161038398742676
Iteration 601:
Training Loss: -3.5456864833831787
Reconstruction Loss: -6.182521820068359
Iteration 621:
Training Loss: -3.585512161254883
Reconstruction Loss: -6.205605983734131
Iteration 641:
Training Loss: -3.39165997505188
Reconstruction Loss: -6.227148532867432
Iteration 661:
Training Loss: -3.7961020469665527
Reconstruction Loss: -6.247162818908691
Iteration 681:
Training Loss: -3.6673805713653564
Reconstruction Loss: -6.2650532722473145
Iteration 701:
Training Loss: -3.9287331104278564
Reconstruction Loss: -6.285396099090576
Iteration 721:
Training Loss: -3.9339632987976074
Reconstruction Loss: -6.303949356079102
Iteration 741:
Training Loss: -3.7562966346740723
Reconstruction Loss: -6.321666717529297
Iteration 761:
Training Loss: -3.8634378910064697
Reconstruction Loss: -6.337398052215576
Iteration 781:
Training Loss: -4.113384246826172
Reconstruction Loss: -6.354597568511963
Iteration 801:
Training Loss: -3.7596447467803955
Reconstruction Loss: -6.367893695831299
Iteration 821:
Training Loss: -3.759335994720459
Reconstruction Loss: -6.383331298828125
Iteration 841:
Training Loss: -4.019296646118164
Reconstruction Loss: -6.399924278259277
Iteration 861:
Training Loss: -3.7085065841674805
Reconstruction Loss: -6.414226531982422
Iteration 881:
Training Loss: -3.8974266052246094
Reconstruction Loss: -6.42659854888916
Iteration 901:
Training Loss: -4.067711353302002
Reconstruction Loss: -6.441922187805176
Iteration 921:
Training Loss: -4.122137546539307
Reconstruction Loss: -6.4552321434021
Iteration 941:
Training Loss: -4.1263957023620605
Reconstruction Loss: -6.466601371765137
Iteration 961:
Training Loss: -4.031822204589844
Reconstruction Loss: -6.480606555938721
Iteration 981:
Training Loss: -4.640858173370361
Reconstruction Loss: -6.491474628448486
Iteration 1001:
Training Loss: -4.024041175842285
Reconstruction Loss: -6.503129005432129
Iteration 1021:
Training Loss: -4.051202297210693
Reconstruction Loss: -6.515220642089844
Iteration 1041:
Training Loss: -4.011678218841553
Reconstruction Loss: -6.527378559112549
Iteration 1061:
Training Loss: -4.430124282836914
Reconstruction Loss: -6.538557052612305
Iteration 1081:
Training Loss: -4.2191901206970215
Reconstruction Loss: -6.549081802368164
Iteration 1101:
Training Loss: -4.140100955963135
Reconstruction Loss: -6.559741973876953
Iteration 1121:
Training Loss: -4.233245849609375
Reconstruction Loss: -6.5684356689453125
Iteration 1141:
Training Loss: -4.474399089813232
Reconstruction Loss: -6.579219818115234
Iteration 1161:
Training Loss: -4.428659439086914
Reconstruction Loss: -6.590174198150635
Iteration 1181:
Training Loss: -4.405176162719727
Reconstruction Loss: -6.599263668060303
Iteration 1201:
Training Loss: -4.475462913513184
Reconstruction Loss: -6.608920574188232
Iteration 1221:
Training Loss: -4.5174560546875
Reconstruction Loss: -6.61557674407959
Iteration 1241:
Training Loss: -4.398181915283203
Reconstruction Loss: -6.626861095428467
Iteration 1261:
Training Loss: -4.396139144897461
Reconstruction Loss: -6.638538360595703
Iteration 1281:
Training Loss: -4.526291847229004
Reconstruction Loss: -6.646602153778076
Iteration 1301:
Training Loss: -4.589374542236328
Reconstruction Loss: -6.65256404876709
Iteration 1321:
Training Loss: -4.606873035430908
Reconstruction Loss: -6.662408351898193
Iteration 1341:
Training Loss: -4.5954694747924805
Reconstruction Loss: -6.66888952255249
Iteration 1361:
Training Loss: -4.44813346862793
Reconstruction Loss: -6.678037643432617
Iteration 1381:
Training Loss: -4.5161895751953125
Reconstruction Loss: -6.685726642608643
Iteration 1401:
Training Loss: -5.009036064147949
Reconstruction Loss: -6.693363666534424
Iteration 1421:
Training Loss: -4.474244594573975
Reconstruction Loss: -6.701014041900635
Iteration 1441:
Training Loss: -4.803576946258545
Reconstruction Loss: -6.709758281707764
Iteration 1461:
Training Loss: -4.5705246925354
Reconstruction Loss: -6.717848777770996
Iteration 1481:
Training Loss: -4.7343621253967285
Reconstruction Loss: -6.723655700683594
Iteration 1501:
Training Loss: -4.39949893951416
Reconstruction Loss: -6.73087215423584
Iteration 1521:
Training Loss: -4.530896186828613
Reconstruction Loss: -6.738128662109375
Iteration 1541:
Training Loss: -4.537550926208496
Reconstruction Loss: -6.744836330413818
Iteration 1561:
Training Loss: -5.05645227432251
Reconstruction Loss: -6.752059459686279
Iteration 1581:
Training Loss: -4.5716328620910645
Reconstruction Loss: -6.758849143981934
Iteration 1601:
Training Loss: -4.765073299407959
Reconstruction Loss: -6.76549768447876
Iteration 1621:
Training Loss: -4.800533294677734
Reconstruction Loss: -6.772138595581055
Iteration 1641:
Training Loss: -4.925237655639648
Reconstruction Loss: -6.779633045196533
Iteration 1661:
Training Loss: -4.877730846405029
Reconstruction Loss: -6.783403396606445
Iteration 1681:
Training Loss: -4.7522873878479
Reconstruction Loss: -6.790024757385254
Iteration 1701:
Training Loss: -5.0473480224609375
Reconstruction Loss: -6.796352863311768
Iteration 1721:
Training Loss: -4.860150337219238
Reconstruction Loss: -6.802834510803223
Iteration 1741:
Training Loss: -4.881637096405029
Reconstruction Loss: -6.810404300689697
Iteration 1761:
Training Loss: -4.927341461181641
Reconstruction Loss: -6.814073085784912
Iteration 1781:
Training Loss: -5.227952480316162
Reconstruction Loss: -6.821117401123047
Iteration 1801:
Training Loss: -5.136826515197754
Reconstruction Loss: -6.826934814453125
Iteration 1821:
Training Loss: -5.448742389678955
Reconstruction Loss: -6.831713676452637
Iteration 1841:
Training Loss: -5.1936845779418945
Reconstruction Loss: -6.83798360824585
Iteration 1861:
Training Loss: -4.848857879638672
Reconstruction Loss: -6.843023300170898
Iteration 1881:
Training Loss: -5.358160495758057
Reconstruction Loss: -6.849215030670166
Iteration 1901:
Training Loss: -5.252705097198486
Reconstruction Loss: -6.8540754318237305
Iteration 1921:
Training Loss: -5.2107768058776855
Reconstruction Loss: -6.858444690704346
Iteration 1941:
Training Loss: -4.958660125732422
Reconstruction Loss: -6.865112781524658
Iteration 1961:
Training Loss: -5.281437873840332
Reconstruction Loss: -6.869710445404053
Iteration 1981:
Training Loss: -5.126213550567627
Reconstruction Loss: -6.87455940246582
Iteration 2001:
Training Loss: -5.138896465301514
Reconstruction Loss: -6.87890625
Iteration 2021:
Training Loss: -5.172441482543945
Reconstruction Loss: -6.884583473205566
Iteration 2041:
Training Loss: -5.153205394744873
Reconstruction Loss: -6.889898777008057
Iteration 2061:
Training Loss: -5.122981071472168
Reconstruction Loss: -6.89360237121582
Iteration 2081:
Training Loss: -5.301540851593018
Reconstruction Loss: -6.898865699768066
Iteration 2101:
Training Loss: -5.170455455780029
Reconstruction Loss: -6.904347896575928
Iteration 2121:
Training Loss: -5.183706283569336
Reconstruction Loss: -6.908398628234863
Iteration 2141:
Training Loss: -5.359281539916992
Reconstruction Loss: -6.912966251373291
Iteration 2161:
Training Loss: -5.299931049346924
Reconstruction Loss: -6.91630220413208
Iteration 2181:
Training Loss: -5.247447967529297
Reconstruction Loss: -6.921261310577393
Iteration 2201:
Training Loss: -5.131780624389648
Reconstruction Loss: -6.926580429077148
Iteration 2221:
Training Loss: -5.612995624542236
Reconstruction Loss: -6.930729866027832
Iteration 2241:
Training Loss: -5.2572102546691895
Reconstruction Loss: -6.934884071350098
Iteration 2261:
Training Loss: -5.260343551635742
Reconstruction Loss: -6.939016342163086
Iteration 2281:
Training Loss: -5.583461761474609
Reconstruction Loss: -6.945332050323486
Iteration 2301:
Training Loss: -5.457430839538574
Reconstruction Loss: -6.947834014892578
Iteration 2321:
Training Loss: -5.3716936111450195
Reconstruction Loss: -6.951505184173584
Iteration 2341:
Training Loss: -5.297591686248779
Reconstruction Loss: -6.957263946533203
Iteration 2361:
Training Loss: -5.399091720581055
Reconstruction Loss: -6.959681034088135
Iteration 2381:
Training Loss: -5.431232452392578
Reconstruction Loss: -6.963510990142822
Iteration 2401:
Training Loss: -5.51180362701416
Reconstruction Loss: -6.967764854431152
Iteration 2421:
Training Loss: -5.660059928894043
Reconstruction Loss: -6.971884250640869
Iteration 2441:
Training Loss: -5.669557094573975
Reconstruction Loss: -6.976230621337891
Iteration 2461:
Training Loss: -5.76338005065918
Reconstruction Loss: -6.980970859527588
Iteration 2481:
Training Loss: -5.569349765777588
Reconstruction Loss: -6.982748031616211
Iteration 2501:
Training Loss: -5.554953575134277
Reconstruction Loss: -6.986572742462158
Iteration 2521:
Training Loss: -5.709349155426025
Reconstruction Loss: -6.991176128387451
Iteration 2541:
Training Loss: -5.848178386688232
Reconstruction Loss: -6.994867324829102
Iteration 2561:
Training Loss: -5.293470859527588
Reconstruction Loss: -6.998701095581055
Iteration 2581:
Training Loss: -5.62530517578125
Reconstruction Loss: -7.001934051513672
Iteration 2601:
Training Loss: -5.6352715492248535
Reconstruction Loss: -7.005475997924805
Iteration 2621:
Training Loss: -5.494417190551758
Reconstruction Loss: -7.009802341461182
Iteration 2641:
Training Loss: -5.443020343780518
Reconstruction Loss: -7.012845039367676
Iteration 2661:
Training Loss: -5.649519920349121
Reconstruction Loss: -7.015613555908203
Iteration 2681:
Training Loss: -5.7111616134643555
Reconstruction Loss: -7.019578456878662
Iteration 2701:
Training Loss: -5.627348899841309
Reconstruction Loss: -7.02330207824707
Iteration 2721:
Training Loss: -5.6628289222717285
Reconstruction Loss: -7.026679515838623
Iteration 2741:
Training Loss: -5.5323686599731445
Reconstruction Loss: -7.029219627380371
Iteration 2761:
Training Loss: -5.3785719871521
Reconstruction Loss: -7.033224105834961
Iteration 2781:
Training Loss: -5.698608875274658
Reconstruction Loss: -7.035977363586426
Iteration 2801:
Training Loss: -5.470038414001465
Reconstruction Loss: -7.039942264556885
Iteration 2821:
Training Loss: -5.677083969116211
Reconstruction Loss: -7.042976379394531
Iteration 2841:
Training Loss: -5.989644527435303
Reconstruction Loss: -7.046250343322754
Iteration 2861:
Training Loss: -6.035061359405518
Reconstruction Loss: -7.050090312957764
Iteration 2881:
Training Loss: -6.004251956939697
Reconstruction Loss: -7.05206298828125
Iteration 2901:
Training Loss: -5.626649379730225
Reconstruction Loss: -7.054765701293945
Iteration 2921:
Training Loss: -5.858928680419922
Reconstruction Loss: -7.058557033538818
Iteration 2941:
Training Loss: -6.429412841796875
Reconstruction Loss: -7.061636924743652
Iteration 2961:
Training Loss: -5.968798637390137
Reconstruction Loss: -7.064300537109375
Iteration 2981:
Training Loss: -5.616012096405029
Reconstruction Loss: -7.067542552947998
