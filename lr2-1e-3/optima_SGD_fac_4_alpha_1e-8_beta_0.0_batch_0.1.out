5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.622891902923584
Reconstruction Loss: -0.4663717746734619
Iteration 11:
Training Loss: 5.232041835784912
Reconstruction Loss: -0.4663717746734619
Iteration 21:
Training Loss: 5.610340595245361
Reconstruction Loss: -0.4663717746734619
Iteration 31:
Training Loss: 5.718361854553223
Reconstruction Loss: -0.4663717746734619
Iteration 41:
Training Loss: 5.246389865875244
Reconstruction Loss: -0.4663717746734619
Iteration 51:
Training Loss: 5.346552848815918
Reconstruction Loss: -0.46637189388275146
Iteration 61:
Training Loss: 5.217588424682617
Reconstruction Loss: -0.46637189388275146
Iteration 71:
Training Loss: 5.566076755523682
Reconstruction Loss: -0.46637189388275146
Iteration 81:
Training Loss: 6.00651216506958
Reconstruction Loss: -0.46637189388275146
Iteration 91:
Training Loss: 5.5614447593688965
Reconstruction Loss: -0.46637189388275146
Iteration 101:
Training Loss: 5.873770713806152
Reconstruction Loss: -0.46637195348739624
Iteration 111:
Training Loss: 5.7077813148498535
Reconstruction Loss: -0.46637195348739624
Iteration 121:
Training Loss: 5.503411769866943
Reconstruction Loss: -0.46637195348739624
Iteration 131:
Training Loss: 5.455478668212891
Reconstruction Loss: -0.46637195348739624
Iteration 141:
Training Loss: 5.908352375030518
Reconstruction Loss: -0.4663720726966858
Iteration 151:
Training Loss: 5.108066558837891
Reconstruction Loss: -0.46637195348739624
Iteration 161:
Training Loss: 5.4980998039245605
Reconstruction Loss: -0.4663720726966858
Iteration 171:
Training Loss: 5.66497278213501
Reconstruction Loss: -0.4663720726966858
Iteration 181:
Training Loss: 5.394001007080078
Reconstruction Loss: -0.4663720726966858
Iteration 191:
Training Loss: 5.195497035980225
Reconstruction Loss: -0.4663720726966858
Iteration 201:
Training Loss: 5.462099552154541
Reconstruction Loss: -0.4663720726966858
Iteration 211:
Training Loss: 6.018645286560059
Reconstruction Loss: -0.46637216210365295
Iteration 221:
Training Loss: 5.095654010772705
Reconstruction Loss: -0.46637216210365295
Iteration 231:
Training Loss: 5.366188049316406
Reconstruction Loss: -0.46637216210365295
Iteration 241:
Training Loss: 5.43833589553833
Reconstruction Loss: -0.46637216210365295
Iteration 251:
Training Loss: 5.533935546875
Reconstruction Loss: -0.4663722515106201
Iteration 261:
Training Loss: 5.489059925079346
Reconstruction Loss: -0.4663722515106201
Iteration 271:
Training Loss: 5.656923770904541
Reconstruction Loss: -0.4663722515106201
Iteration 281:
Training Loss: 5.577392578125
Reconstruction Loss: -0.4663722515106201
Iteration 291:
Training Loss: 5.222513198852539
Reconstruction Loss: -0.4663723409175873
Iteration 301:
Training Loss: 5.43275260925293
Reconstruction Loss: -0.4663723409175873
Iteration 311:
Training Loss: 5.456144332885742
Reconstruction Loss: -0.4663723409175873
Iteration 321:
Training Loss: 5.477019786834717
Reconstruction Loss: -0.46637243032455444
Iteration 331:
Training Loss: 5.141677379608154
Reconstruction Loss: -0.46637243032455444
Iteration 341:
Training Loss: 5.211019039154053
Reconstruction Loss: -0.46637243032455444
Iteration 351:
Training Loss: 5.6962738037109375
Reconstruction Loss: -0.46637243032455444
Iteration 361:
Training Loss: 5.390239715576172
Reconstruction Loss: -0.466372549533844
Iteration 371:
Training Loss: 5.504011154174805
Reconstruction Loss: -0.466372549533844
Iteration 381:
Training Loss: 5.1866230964660645
Reconstruction Loss: -0.46637263894081116
Iteration 391:
Training Loss: 5.604182243347168
Reconstruction Loss: -0.46637263894081116
Iteration 401:
Training Loss: 5.661439895629883
Reconstruction Loss: -0.46637263894081116
Iteration 411:
Training Loss: 5.31078577041626
Reconstruction Loss: -0.46637263894081116
Iteration 421:
Training Loss: 5.58742094039917
Reconstruction Loss: -0.46637263894081116
Iteration 431:
Training Loss: 5.3011064529418945
Reconstruction Loss: -0.4663727283477783
Iteration 441:
Training Loss: 4.869224548339844
Reconstruction Loss: -0.4663727283477783
Iteration 451:
Training Loss: 5.627312183380127
Reconstruction Loss: -0.4663727283477783
Iteration 461:
Training Loss: 6.202487945556641
Reconstruction Loss: -0.4663728177547455
Iteration 471:
Training Loss: 5.213317394256592
Reconstruction Loss: -0.46637290716171265
Iteration 481:
Training Loss: 5.749659538269043
Reconstruction Loss: -0.46637290716171265
Iteration 491:
Training Loss: 5.45321798324585
Reconstruction Loss: -0.4663730263710022
Iteration 501:
Training Loss: 5.752417087554932
Reconstruction Loss: -0.4663730263710022
Iteration 511:
Training Loss: 5.576627254486084
Reconstruction Loss: -0.466373085975647
Iteration 521:
Training Loss: 5.007044315338135
Reconstruction Loss: -0.4663732051849365
Iteration 531:
Training Loss: 5.776758193969727
Reconstruction Loss: -0.4663732051849365
Iteration 541:
Training Loss: 5.8419084548950195
Reconstruction Loss: -0.4663732051849365
Iteration 551:
Training Loss: 4.980684280395508
Reconstruction Loss: -0.4663732945919037
Iteration 561:
Training Loss: 5.191039085388184
Reconstruction Loss: -0.4663735032081604
Iteration 571:
Training Loss: 5.777921676635742
Reconstruction Loss: -0.4663735628128052
Iteration 581:
Training Loss: 5.5480875968933105
Reconstruction Loss: -0.4663735628128052
Iteration 591:
Training Loss: 5.456568241119385
Reconstruction Loss: -0.4663735628128052
Iteration 601:
Training Loss: 5.366480827331543
Reconstruction Loss: -0.4663736820220947
Iteration 611:
Training Loss: 5.601853370666504
Reconstruction Loss: -0.4663737714290619
Iteration 621:
Training Loss: 5.726312637329102
Reconstruction Loss: -0.46637386083602905
Iteration 631:
Training Loss: 5.945273399353027
Reconstruction Loss: -0.4663739800453186
Iteration 641:
Training Loss: 5.533238410949707
Reconstruction Loss: -0.4663739800453186
Iteration 651:
Training Loss: 5.614904403686523
Reconstruction Loss: -0.46637415885925293
Iteration 661:
Training Loss: 6.150545597076416
Reconstruction Loss: -0.4663742482662201
Iteration 671:
Training Loss: 5.425358295440674
Reconstruction Loss: -0.4663744568824768
Iteration 681:
Training Loss: 5.839385032653809
Reconstruction Loss: -0.4663744568824768
Iteration 691:
Training Loss: 5.590193748474121
Reconstruction Loss: -0.46637463569641113
Iteration 701:
Training Loss: 5.3785080909729
Reconstruction Loss: -0.46637481451034546
Iteration 711:
Training Loss: 5.5049309730529785
Reconstruction Loss: -0.466374933719635
Iteration 721:
Training Loss: 5.543107986450195
Reconstruction Loss: -0.4663752019405365
Iteration 731:
Training Loss: 5.156209945678711
Reconstruction Loss: -0.4663753807544708
Iteration 741:
Training Loss: 5.926898956298828
Reconstruction Loss: -0.466375470161438
Iteration 751:
Training Loss: 5.675103664398193
Reconstruction Loss: -0.46637576818466187
Iteration 761:
Training Loss: 5.625441074371338
Reconstruction Loss: -0.4663759469985962
Iteration 771:
Training Loss: 5.620863914489746
Reconstruction Loss: -0.4663761556148529
Iteration 781:
Training Loss: 5.801993370056152
Reconstruction Loss: -0.4663764238357544
Iteration 791:
Training Loss: 5.600564479827881
Reconstruction Loss: -0.46637672185897827
Iteration 801:
Training Loss: 5.704308032989502
Reconstruction Loss: -0.46637701988220215
Iteration 811:
Training Loss: 6.0864763259887695
Reconstruction Loss: -0.4663773775100708
Iteration 821:
Training Loss: 5.344906806945801
Reconstruction Loss: -0.46637776494026184
Iteration 831:
Training Loss: 6.163005828857422
Reconstruction Loss: -0.46637824177742004
Iteration 841:
Training Loss: 5.497234344482422
Reconstruction Loss: -0.46637871861457825
Iteration 851:
Training Loss: 5.614596366882324
Reconstruction Loss: -0.46637919545173645
Iteration 861:
Training Loss: 6.1700334548950195
Reconstruction Loss: -0.4663797616958618
Iteration 871:
Training Loss: 5.660223960876465
Reconstruction Loss: -0.4663803279399872
Iteration 881:
Training Loss: 5.74233341217041
Reconstruction Loss: -0.4663810729980469
Iteration 891:
Training Loss: 5.532383441925049
Reconstruction Loss: -0.4663819372653961
Iteration 901:
Training Loss: 5.360476493835449
Reconstruction Loss: -0.4663828909397125
Iteration 911:
Training Loss: 5.7034807205200195
Reconstruction Loss: -0.4663839340209961
Iteration 921:
Training Loss: 5.704898834228516
Reconstruction Loss: -0.4663850665092468
Iteration 931:
Training Loss: 5.78605318069458
Reconstruction Loss: -0.46638649702072144
Iteration 941:
Training Loss: 5.491900444030762
Reconstruction Loss: -0.4663880467414856
Iteration 951:
Training Loss: 5.428572177886963
Reconstruction Loss: -0.4663900136947632
Iteration 961:
Training Loss: 5.5535688400268555
Reconstruction Loss: -0.46639230847358704
Iteration 971:
Training Loss: 5.673970699310303
Reconstruction Loss: -0.46639496088027954
Iteration 981:
Training Loss: 5.727100849151611
Reconstruction Loss: -0.4663981795310974
Iteration 991:
Training Loss: 5.612227916717529
Reconstruction Loss: -0.4664023816585541
Iteration 1001:
Training Loss: 5.609543800354004
Reconstruction Loss: -0.4664074182510376
Iteration 1011:
Training Loss: 5.977181911468506
Reconstruction Loss: -0.46641379594802856
Iteration 1021:
Training Loss: 5.091937065124512
Reconstruction Loss: -0.466422438621521
Iteration 1031:
Training Loss: 5.725048542022705
Reconstruction Loss: -0.4664338231086731
Iteration 1041:
Training Loss: 5.372302055358887
Reconstruction Loss: -0.46644967794418335
Iteration 1051:
Training Loss: 5.459826469421387
Reconstruction Loss: -0.4664725065231323
Iteration 1061:
Training Loss: 5.786539554595947
Reconstruction Loss: -0.46650755405426025
Iteration 1071:
Training Loss: 5.291025161743164
Reconstruction Loss: -0.46656590700149536
Iteration 1081:
Training Loss: 5.233339309692383
Reconstruction Loss: -0.4666730463504791
Iteration 1091:
Training Loss: 5.697417259216309
Reconstruction Loss: -0.46690744161605835
Iteration 1101:
Training Loss: 5.729303359985352
Reconstruction Loss: -0.46763503551483154
Iteration 1111:
Training Loss: 5.890472888946533
Reconstruction Loss: -0.4731902480125427
Iteration 1121:
Training Loss: 5.561405658721924
Reconstruction Loss: -0.5493274331092834
Iteration 1131:
Training Loss: 4.887908935546875
Reconstruction Loss: -0.48568689823150635
Iteration 1141:
Training Loss: 4.576901912689209
Reconstruction Loss: -0.6781246066093445
Iteration 1151:
Training Loss: 4.368401527404785
Reconstruction Loss: -0.6967564821243286
Iteration 1161:
Training Loss: 4.6249566078186035
Reconstruction Loss: -0.7005000114440918
Iteration 1171:
Training Loss: 4.8177385330200195
Reconstruction Loss: -0.7213539481163025
Iteration 1181:
Training Loss: 4.168462753295898
Reconstruction Loss: -0.707822859287262
Iteration 1191:
Training Loss: 4.718384265899658
Reconstruction Loss: -0.6728757619857788
Iteration 1201:
Training Loss: 4.081974983215332
Reconstruction Loss: -0.6971316933631897
Iteration 1211:
Training Loss: 4.7139081954956055
Reconstruction Loss: -0.6360587477684021
Iteration 1221:
Training Loss: 4.595987796783447
Reconstruction Loss: -0.6917182803153992
Iteration 1231:
Training Loss: 4.111820220947266
Reconstruction Loss: -0.6905454993247986
Iteration 1241:
Training Loss: 3.8578615188598633
Reconstruction Loss: -0.6607253551483154
Iteration 1251:
Training Loss: 4.793921947479248
Reconstruction Loss: -0.7190737724304199
Iteration 1261:
Training Loss: 4.361880779266357
Reconstruction Loss: -0.7291953563690186
Iteration 1271:
Training Loss: 4.54703950881958
Reconstruction Loss: -0.7404082417488098
Iteration 1281:
Training Loss: 4.755197048187256
Reconstruction Loss: -0.6615141034126282
Iteration 1291:
Training Loss: 4.561302661895752
Reconstruction Loss: -0.6772355437278748
Iteration 1301:
Training Loss: 4.4509501457214355
Reconstruction Loss: -0.723510205745697
Iteration 1311:
Training Loss: 4.680137634277344
Reconstruction Loss: -0.6950007677078247
Iteration 1321:
Training Loss: 4.651057243347168
Reconstruction Loss: -0.7236211895942688
Iteration 1331:
Training Loss: 4.556964874267578
Reconstruction Loss: -0.7099897861480713
Iteration 1341:
Training Loss: 4.846156120300293
Reconstruction Loss: -0.6799319386482239
Iteration 1351:
Training Loss: 4.555924415588379
Reconstruction Loss: -0.6696076989173889
Iteration 1361:
Training Loss: 4.458147048950195
Reconstruction Loss: -0.6501608490943909
Iteration 1371:
Training Loss: 4.168357849121094
Reconstruction Loss: -0.6724641919136047
Iteration 1381:
Training Loss: 4.537339687347412
Reconstruction Loss: -0.677057683467865
Iteration 1391:
Training Loss: 4.446142673492432
Reconstruction Loss: -0.6728972792625427
Iteration 1401:
Training Loss: 4.362761497497559
Reconstruction Loss: -0.6935126781463623
Iteration 1411:
Training Loss: 4.134397506713867
Reconstruction Loss: -0.6733400225639343
Iteration 1421:
Training Loss: 4.370792865753174
Reconstruction Loss: -0.6821209192276001
Iteration 1431:
Training Loss: 4.5958781242370605
Reconstruction Loss: -0.6605125665664673
Iteration 1441:
Training Loss: 4.5330491065979
Reconstruction Loss: -0.7324354648590088
Iteration 1451:
Training Loss: 4.420489311218262
Reconstruction Loss: -0.662623941898346
Iteration 1461:
Training Loss: 4.15920352935791
Reconstruction Loss: -0.6711490750312805
Iteration 1471:
Training Loss: 4.487137794494629
Reconstruction Loss: -0.6877217888832092
Iteration 1481:
Training Loss: 4.10381555557251
Reconstruction Loss: -0.626479983329773
Iteration 1491:
Training Loss: 4.632218360900879
Reconstruction Loss: -0.716112494468689
