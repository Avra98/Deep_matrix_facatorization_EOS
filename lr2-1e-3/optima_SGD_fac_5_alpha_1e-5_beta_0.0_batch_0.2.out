5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.548096179962158
Reconstruction Loss: -0.38877588510513306
Iteration 21:
Training Loss: 5.573035717010498
Reconstruction Loss: -0.39386704564094543
Iteration 41:
Training Loss: 4.997270107269287
Reconstruction Loss: -0.4649280905723572
Iteration 61:
Training Loss: 5.187938213348389
Reconstruction Loss: -0.6880198121070862
Iteration 81:
Training Loss: 4.518604278564453
Reconstruction Loss: -0.7668370008468628
Iteration 101:
Training Loss: 4.349947929382324
Reconstruction Loss: -0.9320344924926758
Iteration 121:
Training Loss: 3.252397298812866
Reconstruction Loss: -1.4484308958053589
Iteration 141:
Training Loss: 3.1562068462371826
Reconstruction Loss: -1.5439008474349976
Iteration 161:
Training Loss: 3.4374797344207764
Reconstruction Loss: -1.5948268175125122
Iteration 181:
Training Loss: 2.5791115760803223
Reconstruction Loss: -2.0213534832000732
Iteration 201:
Training Loss: 1.061641812324524
Reconstruction Loss: -3.0016400814056396
Iteration 221:
Training Loss: -0.21521596610546112
Reconstruction Loss: -3.9494285583496094
Iteration 241:
Training Loss: -1.0510730743408203
Reconstruction Loss: -4.858520984649658
Iteration 261:
Training Loss: -1.7261967658996582
Reconstruction Loss: -5.710011959075928
Iteration 281:
Training Loss: -2.80147123336792
Reconstruction Loss: -6.490908145904541
Iteration 301:
Training Loss: -3.223724365234375
Reconstruction Loss: -7.185763359069824
Iteration 321:
Training Loss: -3.9074199199676514
Reconstruction Loss: -7.783380508422852
Iteration 341:
Training Loss: -4.242546081542969
Reconstruction Loss: -8.251212120056152
Iteration 361:
Training Loss: -4.485415458679199
Reconstruction Loss: -8.588138580322266
Iteration 381:
Training Loss: -4.667056083679199
Reconstruction Loss: -8.823827743530273
Iteration 401:
Training Loss: -4.685533046722412
Reconstruction Loss: -8.980083465576172
Iteration 421:
Training Loss: -4.7570905685424805
Reconstruction Loss: -9.08206558227539
Iteration 441:
Training Loss: -4.84158182144165
Reconstruction Loss: -9.142327308654785
Iteration 461:
Training Loss: -4.601951599121094
Reconstruction Loss: -9.196444511413574
Iteration 481:
Training Loss: -4.865335941314697
Reconstruction Loss: -9.237212181091309
Iteration 501:
Training Loss: -4.786696910858154
Reconstruction Loss: -9.26580810546875
Iteration 521:
Training Loss: -4.789188385009766
Reconstruction Loss: -9.314800262451172
Iteration 541:
Training Loss: -4.6137847900390625
Reconstruction Loss: -9.329111099243164
Iteration 561:
Training Loss: -4.831827640533447
Reconstruction Loss: -9.3627347946167
Iteration 581:
Training Loss: -4.846941947937012
Reconstruction Loss: -9.387580871582031
Iteration 601:
Training Loss: -4.957858085632324
Reconstruction Loss: -9.400710105895996
Iteration 621:
Training Loss: -5.148963928222656
Reconstruction Loss: -9.435539245605469
Iteration 641:
Training Loss: -5.115273952484131
Reconstruction Loss: -9.458544731140137
Iteration 661:
Training Loss: -4.926823139190674
Reconstruction Loss: -9.474732398986816
Iteration 681:
Training Loss: -5.077335357666016
Reconstruction Loss: -9.491373062133789
Iteration 701:
Training Loss: -4.980513572692871
Reconstruction Loss: -9.510130882263184
Iteration 721:
Training Loss: -5.092645645141602
Reconstruction Loss: -9.541440963745117
Iteration 741:
Training Loss: -5.110136032104492
Reconstruction Loss: -9.554269790649414
Iteration 761:
Training Loss: -5.339282989501953
Reconstruction Loss: -9.577146530151367
Iteration 781:
Training Loss: -5.265715599060059
Reconstruction Loss: -9.583864212036133
Iteration 801:
Training Loss: -5.396232604980469
Reconstruction Loss: -9.618839263916016
Iteration 821:
Training Loss: -5.104025840759277
Reconstruction Loss: -9.633543014526367
Iteration 841:
Training Loss: -5.259979248046875
Reconstruction Loss: -9.644220352172852
Iteration 861:
Training Loss: -5.397250652313232
Reconstruction Loss: -9.657715797424316
Iteration 881:
Training Loss: -5.121082782745361
Reconstruction Loss: -9.66489315032959
Iteration 901:
Training Loss: -5.3004069328308105
Reconstruction Loss: -9.694246292114258
Iteration 921:
Training Loss: -5.415369033813477
Reconstruction Loss: -9.707826614379883
Iteration 941:
Training Loss: -5.338618755340576
Reconstruction Loss: -9.732826232910156
Iteration 961:
Training Loss: -5.368696212768555
Reconstruction Loss: -9.732096672058105
Iteration 981:
Training Loss: -5.425405025482178
Reconstruction Loss: -9.769552230834961
Iteration 1001:
Training Loss: -5.294633865356445
Reconstruction Loss: -9.762943267822266
Iteration 1021:
Training Loss: -5.16691780090332
Reconstruction Loss: -9.786125183105469
Iteration 1041:
Training Loss: -5.3903985023498535
Reconstruction Loss: -9.808588981628418
Iteration 1061:
Training Loss: -5.365972995758057
Reconstruction Loss: -9.824967384338379
Iteration 1081:
Training Loss: -5.409265995025635
Reconstruction Loss: -9.828116416931152
Iteration 1101:
Training Loss: -5.384341716766357
Reconstruction Loss: -9.82770824432373
Iteration 1121:
Training Loss: -5.696280002593994
Reconstruction Loss: -9.850619316101074
Iteration 1141:
Training Loss: -5.391623497009277
Reconstruction Loss: -9.862250328063965
Iteration 1161:
Training Loss: -5.35762357711792
Reconstruction Loss: -9.888750076293945
Iteration 1181:
Training Loss: -5.509897232055664
Reconstruction Loss: -9.892937660217285
Iteration 1201:
Training Loss: -5.46591329574585
Reconstruction Loss: -9.906885147094727
Iteration 1221:
Training Loss: -5.5652875900268555
Reconstruction Loss: -9.913895606994629
Iteration 1241:
Training Loss: -5.511687278747559
Reconstruction Loss: -9.933650970458984
Iteration 1261:
Training Loss: -5.878664016723633
Reconstruction Loss: -9.93388843536377
Iteration 1281:
Training Loss: -5.321037292480469
Reconstruction Loss: -9.955572128295898
Iteration 1301:
Training Loss: -5.424778938293457
Reconstruction Loss: -9.967909812927246
Iteration 1321:
Training Loss: -5.830270767211914
Reconstruction Loss: -9.98449993133545
Iteration 1341:
Training Loss: -5.511154651641846
Reconstruction Loss: -9.987060546875
Iteration 1361:
Training Loss: -5.5701751708984375
Reconstruction Loss: -9.999332427978516
Iteration 1381:
Training Loss: -5.652921676635742
Reconstruction Loss: -10.020069122314453
Iteration 1401:
Training Loss: -5.539266109466553
Reconstruction Loss: -10.019011497497559
Iteration 1421:
Training Loss: -5.355167388916016
Reconstruction Loss: -10.006990432739258
Iteration 1441:
Training Loss: -5.5475239753723145
Reconstruction Loss: -10.033172607421875
Iteration 1461:
Training Loss: -5.673286437988281
Reconstruction Loss: -10.043811798095703
Iteration 1481:
Training Loss: -5.853610038757324
Reconstruction Loss: -10.06898307800293
Iteration 1501:
Training Loss: -5.901078701019287
Reconstruction Loss: -10.074458122253418
Iteration 1521:
Training Loss: -5.6326375007629395
Reconstruction Loss: -10.095599174499512
Iteration 1541:
Training Loss: -5.815047740936279
Reconstruction Loss: -10.099514961242676
Iteration 1561:
Training Loss: -5.597993850708008
Reconstruction Loss: -10.086577415466309
Iteration 1581:
Training Loss: -5.705231666564941
Reconstruction Loss: -10.11784553527832
Iteration 1601:
Training Loss: -5.765286922454834
Reconstruction Loss: -10.137250900268555
Iteration 1621:
Training Loss: -5.610897541046143
Reconstruction Loss: -10.139467239379883
Iteration 1641:
Training Loss: -5.787184715270996
Reconstruction Loss: -10.13929557800293
Iteration 1661:
Training Loss: -5.715287685394287
Reconstruction Loss: -10.160552978515625
Iteration 1681:
Training Loss: -6.15883207321167
Reconstruction Loss: -10.175436973571777
Iteration 1701:
Training Loss: -5.731400012969971
Reconstruction Loss: -10.181303024291992
Iteration 1721:
Training Loss: -5.8179402351379395
Reconstruction Loss: -10.191073417663574
Iteration 1741:
Training Loss: -5.84665060043335
Reconstruction Loss: -10.194599151611328
Iteration 1761:
Training Loss: -5.523688316345215
Reconstruction Loss: -10.20310115814209
Iteration 1781:
Training Loss: -5.841955661773682
Reconstruction Loss: -10.219983100891113
Iteration 1801:
Training Loss: -5.6846160888671875
Reconstruction Loss: -10.213154792785645
Iteration 1821:
Training Loss: -5.8596649169921875
Reconstruction Loss: -10.239721298217773
Iteration 1841:
Training Loss: -5.846599578857422
Reconstruction Loss: -10.241472244262695
Iteration 1861:
Training Loss: -5.837727069854736
Reconstruction Loss: -10.247220993041992
Iteration 1881:
Training Loss: -6.131348133087158
Reconstruction Loss: -10.273260116577148
Iteration 1901:
Training Loss: -6.02208137512207
Reconstruction Loss: -10.26949405670166
Iteration 1921:
Training Loss: -5.7244157791137695
Reconstruction Loss: -10.292430877685547
Iteration 1941:
Training Loss: -6.115300178527832
Reconstruction Loss: -10.289735794067383
Iteration 1961:
Training Loss: -5.828482151031494
Reconstruction Loss: -10.3074312210083
Iteration 1981:
Training Loss: -6.060498237609863
Reconstruction Loss: -10.302724838256836
Iteration 2001:
Training Loss: -6.132841110229492
Reconstruction Loss: -10.314569473266602
Iteration 2021:
Training Loss: -5.8393473625183105
Reconstruction Loss: -10.309629440307617
Iteration 2041:
Training Loss: -6.180549621582031
Reconstruction Loss: -10.325610160827637
Iteration 2061:
Training Loss: -6.220911026000977
Reconstruction Loss: -10.331084251403809
Iteration 2081:
Training Loss: -5.866048812866211
Reconstruction Loss: -10.337814331054688
Iteration 2101:
Training Loss: -6.003259658813477
Reconstruction Loss: -10.350773811340332
Iteration 2121:
Training Loss: -6.0534186363220215
Reconstruction Loss: -10.350913047790527
Iteration 2141:
Training Loss: -6.3473968505859375
Reconstruction Loss: -10.390686988830566
Iteration 2161:
Training Loss: -6.266321659088135
Reconstruction Loss: -10.394059181213379
Iteration 2181:
Training Loss: -5.873205661773682
Reconstruction Loss: -10.387130737304688
Iteration 2201:
Training Loss: -5.983340263366699
Reconstruction Loss: -10.387740135192871
Iteration 2221:
Training Loss: -5.968202114105225
Reconstruction Loss: -10.398545265197754
Iteration 2241:
Training Loss: -6.133918762207031
Reconstruction Loss: -10.41721248626709
Iteration 2261:
Training Loss: -6.128361225128174
Reconstruction Loss: -10.408403396606445
Iteration 2281:
Training Loss: -6.2556610107421875
Reconstruction Loss: -10.425007820129395
Iteration 2301:
Training Loss: -6.236218452453613
Reconstruction Loss: -10.416717529296875
Iteration 2321:
Training Loss: -6.007319450378418
Reconstruction Loss: -10.440293312072754
Iteration 2341:
Training Loss: -6.216246128082275
Reconstruction Loss: -10.440705299377441
Iteration 2361:
Training Loss: -6.226573467254639
Reconstruction Loss: -10.45434856414795
Iteration 2381:
Training Loss: -6.07276725769043
Reconstruction Loss: -10.443604469299316
Iteration 2401:
Training Loss: -5.8098225593566895
Reconstruction Loss: -10.452874183654785
Iteration 2421:
Training Loss: -6.184430122375488
Reconstruction Loss: -10.45837116241455
Iteration 2441:
Training Loss: -6.279670715332031
Reconstruction Loss: -10.474662780761719
Iteration 2461:
Training Loss: -6.07537317276001
Reconstruction Loss: -10.482490539550781
Iteration 2481:
Training Loss: -6.119975566864014
Reconstruction Loss: -10.487415313720703
Iteration 2501:
Training Loss: -6.215419769287109
Reconstruction Loss: -10.501290321350098
Iteration 2521:
Training Loss: -6.653247833251953
Reconstruction Loss: -10.5057954788208
Iteration 2541:
Training Loss: -6.17793083190918
Reconstruction Loss: -10.505523681640625
Iteration 2561:
Training Loss: -6.315516948699951
Reconstruction Loss: -10.526878356933594
Iteration 2581:
Training Loss: -6.393887519836426
Reconstruction Loss: -10.530457496643066
Iteration 2601:
Training Loss: -6.723254203796387
Reconstruction Loss: -10.522397994995117
Iteration 2621:
Training Loss: -6.259223937988281
Reconstruction Loss: -10.52843189239502
Iteration 2641:
Training Loss: -6.109318256378174
Reconstruction Loss: -10.546501159667969
Iteration 2661:
Training Loss: -6.380499362945557
Reconstruction Loss: -10.555109977722168
Iteration 2681:
Training Loss: -6.268669605255127
Reconstruction Loss: -10.560572624206543
Iteration 2701:
Training Loss: -6.26436185836792
Reconstruction Loss: -10.567289352416992
Iteration 2721:
Training Loss: -6.343958377838135
Reconstruction Loss: -10.567024230957031
Iteration 2741:
Training Loss: -6.244376182556152
Reconstruction Loss: -10.581367492675781
Iteration 2761:
Training Loss: -6.1526079177856445
Reconstruction Loss: -10.59280776977539
Iteration 2781:
Training Loss: -6.327459335327148
Reconstruction Loss: -10.582623481750488
Iteration 2801:
Training Loss: -6.35228157043457
Reconstruction Loss: -10.588683128356934
Iteration 2821:
Training Loss: -6.432764530181885
Reconstruction Loss: -10.602035522460938
Iteration 2841:
Training Loss: -6.249860763549805
Reconstruction Loss: -10.622154235839844
Iteration 2861:
Training Loss: -6.37027645111084
Reconstruction Loss: -10.615665435791016
Iteration 2881:
Training Loss: -6.465292930603027
Reconstruction Loss: -10.60515308380127
Iteration 2901:
Training Loss: -6.738114833831787
Reconstruction Loss: -10.619059562683105
Iteration 2921:
Training Loss: -5.918984889984131
Reconstruction Loss: -10.615245819091797
Iteration 2941:
Training Loss: -6.175429344177246
Reconstruction Loss: -10.639436721801758
Iteration 2961:
Training Loss: -6.528134822845459
Reconstruction Loss: -10.644673347473145
Iteration 2981:
Training Loss: -6.2066521644592285
Reconstruction Loss: -10.643343925476074
Iteration 3001:
Training Loss: -5.981183052062988
Reconstruction Loss: -10.640549659729004
Iteration 3021:
Training Loss: -6.4070963859558105
Reconstruction Loss: -10.672204971313477
Iteration 3041:
Training Loss: -6.2865166664123535
Reconstruction Loss: -10.659544944763184
Iteration 3061:
Training Loss: -6.3298258781433105
Reconstruction Loss: -10.667184829711914
Iteration 3081:
Training Loss: -6.114366054534912
Reconstruction Loss: -10.674614906311035
Iteration 3101:
Training Loss: -6.262171745300293
Reconstruction Loss: -10.682868003845215
Iteration 3121:
Training Loss: -6.48322868347168
Reconstruction Loss: -10.682157516479492
Iteration 3141:
Training Loss: -6.47935676574707
Reconstruction Loss: -10.703031539916992
Iteration 3161:
Training Loss: -6.296926498413086
Reconstruction Loss: -10.686629295349121
Iteration 3181:
Training Loss: -6.380962371826172
Reconstruction Loss: -10.69078540802002
Iteration 3201:
Training Loss: -6.35281229019165
Reconstruction Loss: -10.706802368164062
Iteration 3221:
Training Loss: -6.571385383605957
Reconstruction Loss: -10.725213050842285
Iteration 3241:
Training Loss: -6.485002517700195
Reconstruction Loss: -10.716793060302734
Iteration 3261:
Training Loss: -6.570610523223877
Reconstruction Loss: -10.71739673614502
Iteration 3281:
Training Loss: -6.480490207672119
Reconstruction Loss: -10.731682777404785
Iteration 3301:
Training Loss: -6.726461410522461
Reconstruction Loss: -10.744011878967285
Iteration 3321:
Training Loss: -6.333276271820068
Reconstruction Loss: -10.738243103027344
Iteration 3341:
Training Loss: -6.461822032928467
Reconstruction Loss: -10.74283504486084
Iteration 3361:
Training Loss: -6.346705913543701
Reconstruction Loss: -10.753226280212402
Iteration 3381:
Training Loss: -6.247662544250488
Reconstruction Loss: -10.746129035949707
Iteration 3401:
Training Loss: -6.741218090057373
Reconstruction Loss: -10.766169548034668
Iteration 3421:
Training Loss: -6.470620632171631
Reconstruction Loss: -10.76844596862793
Iteration 3441:
Training Loss: -6.700937747955322
Reconstruction Loss: -10.778539657592773
Iteration 3461:
Training Loss: -6.276342868804932
Reconstruction Loss: -10.777946472167969
Iteration 3481:
Training Loss: -6.62134313583374
Reconstruction Loss: -10.78448486328125
Iteration 3501:
Training Loss: -6.508624076843262
Reconstruction Loss: -10.790993690490723
Iteration 3521:
Training Loss: -6.787368297576904
Reconstruction Loss: -10.790066719055176
Iteration 3541:
Training Loss: -6.622813701629639
Reconstruction Loss: -10.815780639648438
Iteration 3561:
Training Loss: -6.691963195800781
Reconstruction Loss: -10.807940483093262
Iteration 3581:
Training Loss: -6.816518306732178
Reconstruction Loss: -10.80838394165039
Iteration 3601:
Training Loss: -6.619412899017334
Reconstruction Loss: -10.814803123474121
Iteration 3621:
Training Loss: -6.813106536865234
Reconstruction Loss: -10.822380065917969
Iteration 3641:
Training Loss: -6.279623031616211
Reconstruction Loss: -10.820484161376953
Iteration 3661:
Training Loss: -6.570000648498535
Reconstruction Loss: -10.826192855834961
Iteration 3681:
Training Loss: -6.761511325836182
Reconstruction Loss: -10.833574295043945
Iteration 3701:
Training Loss: -6.5897536277771
Reconstruction Loss: -10.83508014678955
Iteration 3721:
Training Loss: -6.572248935699463
Reconstruction Loss: -10.845264434814453
Iteration 3741:
Training Loss: -6.596582889556885
Reconstruction Loss: -10.836297035217285
Iteration 3761:
Training Loss: -6.67847204208374
Reconstruction Loss: -10.84741497039795
Iteration 3781:
Training Loss: -6.499294281005859
Reconstruction Loss: -10.853838920593262
Iteration 3801:
Training Loss: -6.68538236618042
Reconstruction Loss: -10.872283935546875
Iteration 3821:
Training Loss: -6.542777061462402
Reconstruction Loss: -10.85656452178955
Iteration 3841:
Training Loss: -6.4093828201293945
Reconstruction Loss: -10.87735652923584
Iteration 3861:
Training Loss: -6.53275728225708
Reconstruction Loss: -10.885143280029297
Iteration 3881:
Training Loss: -6.532774925231934
Reconstruction Loss: -10.881723403930664
Iteration 3901:
Training Loss: -6.7284746170043945
Reconstruction Loss: -10.883671760559082
Iteration 3921:
Training Loss: -6.7317399978637695
Reconstruction Loss: -10.883116722106934
Iteration 3941:
Training Loss: -6.5056843757629395
Reconstruction Loss: -10.89706039428711
Iteration 3961:
Training Loss: -7.026221752166748
Reconstruction Loss: -10.904292106628418
Iteration 3981:
Training Loss: -6.47197151184082
Reconstruction Loss: -10.910929679870605
Iteration 4001:
Training Loss: -6.807484149932861
Reconstruction Loss: -10.909036636352539
Iteration 4021:
Training Loss: -6.5180134773254395
Reconstruction Loss: -10.91382122039795
Iteration 4041:
Training Loss: -6.501867294311523
Reconstruction Loss: -10.914852142333984
Iteration 4061:
Training Loss: -6.676357269287109
Reconstruction Loss: -10.91761589050293
Iteration 4081:
Training Loss: -6.8183159828186035
Reconstruction Loss: -10.93018913269043
Iteration 4101:
Training Loss: -6.810239315032959
Reconstruction Loss: -10.930354118347168
Iteration 4121:
Training Loss: -6.9250569343566895
Reconstruction Loss: -10.93818473815918
Iteration 4141:
Training Loss: -6.671818733215332
Reconstruction Loss: -10.936155319213867
Iteration 4161:
Training Loss: -6.691817760467529
Reconstruction Loss: -10.951510429382324
Iteration 4181:
Training Loss: -6.563677787780762
Reconstruction Loss: -10.942944526672363
Iteration 4201:
Training Loss: -6.5438714027404785
Reconstruction Loss: -10.944713592529297
Iteration 4221:
Training Loss: -6.608818054199219
Reconstruction Loss: -10.9542818069458
Iteration 4241:
Training Loss: -6.900540828704834
Reconstruction Loss: -10.959734916687012
Iteration 4261:
Training Loss: -6.819106578826904
Reconstruction Loss: -10.96782398223877
Iteration 4281:
Training Loss: -6.583710670471191
Reconstruction Loss: -10.962943077087402
Iteration 4301:
Training Loss: -6.863001346588135
Reconstruction Loss: -10.976720809936523
Iteration 4321:
Training Loss: -6.902245998382568
Reconstruction Loss: -10.974275588989258
Iteration 4341:
Training Loss: -6.731334209442139
Reconstruction Loss: -10.986615180969238
Iteration 4361:
Training Loss: -6.659437656402588
Reconstruction Loss: -10.98969554901123
Iteration 4381:
Training Loss: -6.520069599151611
Reconstruction Loss: -10.988531112670898
Iteration 4401:
Training Loss: -6.962642192840576
Reconstruction Loss: -10.996248245239258
Iteration 4421:
Training Loss: -6.648917198181152
Reconstruction Loss: -10.994390487670898
Iteration 4441:
Training Loss: -6.669833660125732
Reconstruction Loss: -11.001571655273438
Iteration 4461:
Training Loss: -6.766182899475098
Reconstruction Loss: -11.0006103515625
Iteration 4481:
Training Loss: -7.000222682952881
Reconstruction Loss: -11.020549774169922
Iteration 4501:
Training Loss: -7.0213823318481445
Reconstruction Loss: -11.007052421569824
Iteration 4521:
Training Loss: -6.872807025909424
Reconstruction Loss: -11.01404857635498
Iteration 4541:
Training Loss: -6.822690010070801
Reconstruction Loss: -11.016461372375488
Iteration 4561:
Training Loss: -6.67448091506958
Reconstruction Loss: -11.024259567260742
Iteration 4581:
Training Loss: -6.538416862487793
Reconstruction Loss: -11.020793914794922
Iteration 4601:
Training Loss: -6.977902889251709
Reconstruction Loss: -11.0364990234375
Iteration 4621:
Training Loss: -6.852559566497803
Reconstruction Loss: -11.044858932495117
Iteration 4641:
Training Loss: -6.901550769805908
Reconstruction Loss: -11.022643089294434
Iteration 4661:
Training Loss: -7.140660285949707
Reconstruction Loss: -11.0488862991333
Iteration 4681:
Training Loss: -6.907200813293457
Reconstruction Loss: -11.055217742919922
Iteration 4701:
Training Loss: -6.863659858703613
Reconstruction Loss: -11.05029010772705
Iteration 4721:
Training Loss: -6.814158916473389
Reconstruction Loss: -11.051292419433594
Iteration 4741:
Training Loss: -6.768601894378662
Reconstruction Loss: -11.067728042602539
Iteration 4761:
Training Loss: -6.912715911865234
Reconstruction Loss: -11.063376426696777
Iteration 4781:
Training Loss: -6.727437496185303
Reconstruction Loss: -11.046890258789062
Iteration 4801:
Training Loss: -6.858388423919678
Reconstruction Loss: -11.066654205322266
Iteration 4821:
Training Loss: -7.005063056945801
Reconstruction Loss: -11.08532428741455
Iteration 4841:
Training Loss: -6.954052925109863
Reconstruction Loss: -11.080838203430176
Iteration 4861:
Training Loss: -6.930436134338379
Reconstruction Loss: -11.087430000305176
Iteration 4881:
Training Loss: -6.784668445587158
Reconstruction Loss: -11.07616901397705
Iteration 4901:
Training Loss: -7.085668087005615
Reconstruction Loss: -11.08405876159668
Iteration 4921:
Training Loss: -6.962021827697754
Reconstruction Loss: -11.095128059387207
Iteration 4941:
Training Loss: -6.7504777908325195
Reconstruction Loss: -11.091533660888672
Iteration 4961:
Training Loss: -6.821847915649414
Reconstruction Loss: -11.110663414001465
Iteration 4981:
Training Loss: -7.26469087600708
Reconstruction Loss: -11.110088348388672
Iteration 5001:
Training Loss: -6.848906517028809
Reconstruction Loss: -11.10359001159668
Iteration 5021:
Training Loss: -7.072963237762451
Reconstruction Loss: -11.109149932861328
Iteration 5041:
Training Loss: -6.929120063781738
Reconstruction Loss: -11.124200820922852
Iteration 5061:
Training Loss: -6.7444257736206055
Reconstruction Loss: -11.10378646850586
Iteration 5081:
Training Loss: -6.979222297668457
Reconstruction Loss: -11.126903533935547
Iteration 5101:
Training Loss: -7.021796703338623
Reconstruction Loss: -11.123064994812012
Iteration 5121:
Training Loss: -6.781955718994141
Reconstruction Loss: -11.131291389465332
Iteration 5141:
Training Loss: -6.868052005767822
Reconstruction Loss: -11.137791633605957
Iteration 5161:
Training Loss: -6.833802223205566
Reconstruction Loss: -11.137320518493652
Iteration 5181:
Training Loss: -7.1623005867004395
Reconstruction Loss: -11.133934020996094
Iteration 5201:
Training Loss: -7.0066118240356445
Reconstruction Loss: -11.148903846740723
Iteration 5221:
Training Loss: -7.044548511505127
Reconstruction Loss: -11.145293235778809
Iteration 5241:
Training Loss: -6.8362321853637695
Reconstruction Loss: -11.15611457824707
Iteration 5261:
Training Loss: -6.992631435394287
Reconstruction Loss: -11.16125202178955
Iteration 5281:
Training Loss: -7.051045894622803
Reconstruction Loss: -11.157851219177246
Iteration 5301:
Training Loss: -7.443742275238037
Reconstruction Loss: -11.161548614501953
Iteration 5321:
Training Loss: -6.890766620635986
Reconstruction Loss: -11.171357154846191
Iteration 5341:
Training Loss: -6.707208633422852
Reconstruction Loss: -11.152120590209961
Iteration 5361:
Training Loss: -6.970975875854492
Reconstruction Loss: -11.179041862487793
Iteration 5381:
Training Loss: -7.183374881744385
Reconstruction Loss: -11.173114776611328
Iteration 5401:
Training Loss: -7.119142532348633
Reconstruction Loss: -11.184674263000488
Iteration 5421:
Training Loss: -6.89655876159668
Reconstruction Loss: -11.175578117370605
Iteration 5441:
Training Loss: -7.27118444442749
Reconstruction Loss: -11.185073852539062
Iteration 5461:
Training Loss: -6.817979335784912
Reconstruction Loss: -11.192383766174316
Iteration 5481:
Training Loss: -7.031857967376709
Reconstruction Loss: -11.199155807495117
Iteration 5501:
Training Loss: -7.06899881362915
Reconstruction Loss: -11.193521499633789
Iteration 5521:
Training Loss: -6.756588459014893
Reconstruction Loss: -11.199224472045898
Iteration 5541:
Training Loss: -7.096198081970215
Reconstruction Loss: -11.194997787475586
Iteration 5561:
Training Loss: -6.944592475891113
Reconstruction Loss: -11.201115608215332
Iteration 5581:
Training Loss: -7.376428127288818
Reconstruction Loss: -11.202760696411133
Iteration 5601:
Training Loss: -6.969043254852295
Reconstruction Loss: -11.21488094329834
Iteration 5621:
Training Loss: -6.933945655822754
Reconstruction Loss: -11.218483924865723
Iteration 5641:
Training Loss: -6.924294471740723
Reconstruction Loss: -11.227598190307617
Iteration 5661:
Training Loss: -6.902352809906006
Reconstruction Loss: -11.212146759033203
Iteration 5681:
Training Loss: -7.200901031494141
Reconstruction Loss: -11.226696968078613
Iteration 5701:
Training Loss: -7.157516002655029
Reconstruction Loss: -11.230060577392578
Iteration 5721:
Training Loss: -7.057920932769775
Reconstruction Loss: -11.23752498626709
Iteration 5741:
Training Loss: -7.372588157653809
Reconstruction Loss: -11.240200996398926
Iteration 5761:
Training Loss: -6.8972601890563965
Reconstruction Loss: -11.246150970458984
Iteration 5781:
Training Loss: -7.120785236358643
Reconstruction Loss: -11.248869895935059
Iteration 5801:
Training Loss: -6.93886137008667
Reconstruction Loss: -11.2588529586792
Iteration 5821:
Training Loss: -7.373895168304443
Reconstruction Loss: -11.253690719604492
Iteration 5841:
Training Loss: -7.220597267150879
Reconstruction Loss: -11.254213333129883
Iteration 5861:
Training Loss: -7.183121681213379
Reconstruction Loss: -11.272522926330566
Iteration 5881:
Training Loss: -7.077861785888672
Reconstruction Loss: -11.259553909301758
Iteration 5901:
Training Loss: -6.891085147857666
Reconstruction Loss: -11.257766723632812
Iteration 5921:
Training Loss: -7.12026834487915
Reconstruction Loss: -11.26134204864502
Iteration 5941:
Training Loss: -6.993058681488037
Reconstruction Loss: -11.275086402893066
Iteration 5961:
Training Loss: -6.903890132904053
Reconstruction Loss: -11.26526927947998
Iteration 5981:
Training Loss: -7.211324691772461
Reconstruction Loss: -11.27106761932373
Iteration 6001:
Training Loss: -7.06231164932251
Reconstruction Loss: -11.279199600219727
Iteration 6021:
Training Loss: -6.867030620574951
Reconstruction Loss: -11.274481773376465
Iteration 6041:
Training Loss: -7.168720245361328
Reconstruction Loss: -11.272082328796387
Iteration 6061:
Training Loss: -7.172674179077148
Reconstruction Loss: -11.281933784484863
Iteration 6081:
Training Loss: -7.135645389556885
Reconstruction Loss: -11.293004035949707
Iteration 6101:
Training Loss: -7.216545104980469
Reconstruction Loss: -11.293943405151367
Iteration 6121:
Training Loss: -7.254234790802002
Reconstruction Loss: -11.287064552307129
Iteration 6141:
Training Loss: -7.044085502624512
Reconstruction Loss: -11.291363716125488
Iteration 6161:
Training Loss: -7.3800048828125
Reconstruction Loss: -11.295397758483887
Iteration 6181:
Training Loss: -6.985921382904053
Reconstruction Loss: -11.304792404174805
Iteration 6201:
Training Loss: -7.084805965423584
Reconstruction Loss: -11.31118106842041
Iteration 6221:
Training Loss: -7.077454566955566
Reconstruction Loss: -11.306168556213379
Iteration 6241:
Training Loss: -7.00043249130249
Reconstruction Loss: -11.312371253967285
Iteration 6261:
Training Loss: -7.0370378494262695
Reconstruction Loss: -11.316051483154297
Iteration 6281:
Training Loss: -7.420865535736084
Reconstruction Loss: -11.329821586608887
Iteration 6301:
Training Loss: -6.939192295074463
Reconstruction Loss: -11.318521499633789
Iteration 6321:
Training Loss: -7.2868146896362305
Reconstruction Loss: -11.323238372802734
Iteration 6341:
Training Loss: -6.94154167175293
Reconstruction Loss: -11.335395812988281
Iteration 6361:
Training Loss: -7.371870994567871
Reconstruction Loss: -11.338709831237793
Iteration 6381:
Training Loss: -7.169149875640869
Reconstruction Loss: -11.319723129272461
Iteration 6401:
Training Loss: -6.981050968170166
Reconstruction Loss: -11.338703155517578
Iteration 6421:
Training Loss: -7.241352081298828
Reconstruction Loss: -11.343731880187988
Iteration 6441:
Training Loss: -7.2469096183776855
Reconstruction Loss: -11.343552589416504
Iteration 6461:
Training Loss: -7.422725200653076
Reconstruction Loss: -11.353630065917969
Iteration 6481:
Training Loss: -7.165552139282227
Reconstruction Loss: -11.34388256072998
Iteration 6501:
Training Loss: -7.394853591918945
Reconstruction Loss: -11.35748291015625
Iteration 6521:
Training Loss: -7.379486560821533
Reconstruction Loss: -11.348255157470703
Iteration 6541:
Training Loss: -7.31377649307251
Reconstruction Loss: -11.35937213897705
Iteration 6561:
Training Loss: -7.214254379272461
Reconstruction Loss: -11.358479499816895
Iteration 6581:
Training Loss: -7.189423084259033
Reconstruction Loss: -11.357105255126953
Iteration 6601:
Training Loss: -7.029667854309082
Reconstruction Loss: -11.375131607055664
Iteration 6621:
Training Loss: -7.1737141609191895
Reconstruction Loss: -11.362211227416992
Iteration 6641:
Training Loss: -7.693790435791016
Reconstruction Loss: -11.373456001281738
Iteration 6661:
Training Loss: -7.0266432762146
Reconstruction Loss: -11.36649227142334
Iteration 6681:
Training Loss: -7.34396505355835
Reconstruction Loss: -11.375221252441406
Iteration 6701:
Training Loss: -7.332787036895752
Reconstruction Loss: -11.379400253295898
Iteration 6721:
Training Loss: -7.258163928985596
Reconstruction Loss: -11.383545875549316
Iteration 6741:
Training Loss: -7.126659870147705
Reconstruction Loss: -11.392447471618652
Iteration 6761:
Training Loss: -7.11647367477417
Reconstruction Loss: -11.378280639648438
Iteration 6781:
Training Loss: -7.301725387573242
Reconstruction Loss: -11.391063690185547
Iteration 6801:
Training Loss: -7.166597843170166
Reconstruction Loss: -11.395934104919434
Iteration 6821:
Training Loss: -7.298414707183838
Reconstruction Loss: -11.395151138305664
Iteration 6841:
Training Loss: -7.247873306274414
Reconstruction Loss: -11.399185180664062
Iteration 6861:
Training Loss: -7.52745246887207
Reconstruction Loss: -11.399369239807129
Iteration 6881:
Training Loss: -7.17821741104126
Reconstruction Loss: -11.407759666442871
Iteration 6901:
Training Loss: -6.972141742706299
Reconstruction Loss: -11.414252281188965
Iteration 6921:
Training Loss: -7.208520889282227
Reconstruction Loss: -11.406427383422852
Iteration 6941:
Training Loss: -7.434443473815918
Reconstruction Loss: -11.405346870422363
Iteration 6961:
Training Loss: -7.151515960693359
Reconstruction Loss: -11.428579330444336
Iteration 6981:
Training Loss: -6.970990180969238
Reconstruction Loss: -11.410889625549316
Iteration 7001:
Training Loss: -7.376809597015381
Reconstruction Loss: -11.414141654968262
Iteration 7021:
Training Loss: -7.028414726257324
Reconstruction Loss: -11.419648170471191
Iteration 7041:
Training Loss: -7.2596282958984375
Reconstruction Loss: -11.421426773071289
Iteration 7061:
Training Loss: -7.586204528808594
Reconstruction Loss: -11.440118789672852
Iteration 7081:
Training Loss: -7.299618721008301
Reconstruction Loss: -11.432438850402832
Iteration 7101:
Training Loss: -7.362421989440918
Reconstruction Loss: -11.436236381530762
Iteration 7121:
Training Loss: -7.297893047332764
Reconstruction Loss: -11.445967674255371
Iteration 7141:
Training Loss: -7.395335674285889
Reconstruction Loss: -11.43227481842041
Iteration 7161:
Training Loss: -7.324386119842529
Reconstruction Loss: -11.439926147460938
Iteration 7181:
Training Loss: -7.573263168334961
Reconstruction Loss: -11.447173118591309
Iteration 7201:
Training Loss: -7.4380693435668945
Reconstruction Loss: -11.440767288208008
Iteration 7221:
Training Loss: -7.316874980926514
Reconstruction Loss: -11.448902130126953
Iteration 7241:
Training Loss: -7.2334699630737305
Reconstruction Loss: -11.45376968383789
Iteration 7261:
Training Loss: -7.269801616668701
Reconstruction Loss: -11.468344688415527
Iteration 7281:
Training Loss: -7.159675598144531
Reconstruction Loss: -11.458295822143555
Iteration 7301:
Training Loss: -7.41280460357666
Reconstruction Loss: -11.461769104003906
Iteration 7321:
Training Loss: -7.182680606842041
Reconstruction Loss: -11.461862564086914
Iteration 7341:
Training Loss: -7.200239658355713
Reconstruction Loss: -11.461715698242188
Iteration 7361:
Training Loss: -7.429119110107422
Reconstruction Loss: -11.480256080627441
Iteration 7381:
Training Loss: -7.20973014831543
Reconstruction Loss: -11.475380897521973
Iteration 7401:
Training Loss: -7.518578052520752
Reconstruction Loss: -11.479137420654297
Iteration 7421:
Training Loss: -7.239177227020264
Reconstruction Loss: -11.479279518127441
Iteration 7441:
Training Loss: -7.236958980560303
Reconstruction Loss: -11.490739822387695
Iteration 7461:
Training Loss: -7.2735676765441895
Reconstruction Loss: -11.485172271728516
Iteration 7481:
Training Loss: -7.860577583312988
Reconstruction Loss: -11.496295928955078
Iteration 7501:
Training Loss: -7.18021821975708
Reconstruction Loss: -11.492693901062012
Iteration 7521:
Training Loss: -7.316086769104004
Reconstruction Loss: -11.486278533935547
Iteration 7541:
Training Loss: -7.351297378540039
Reconstruction Loss: -11.49515438079834
Iteration 7561:
Training Loss: -7.303333759307861
Reconstruction Loss: -11.498663902282715
Iteration 7581:
Training Loss: -7.588646411895752
Reconstruction Loss: -11.49892520904541
Iteration 7601:
Training Loss: -7.192630290985107
Reconstruction Loss: -11.499231338500977
Iteration 7621:
Training Loss: -7.146789073944092
Reconstruction Loss: -11.498054504394531
Iteration 7641:
Training Loss: -7.416655540466309
Reconstruction Loss: -11.507184028625488
Iteration 7661:
Training Loss: -7.385421276092529
Reconstruction Loss: -11.505906105041504
Iteration 7681:
Training Loss: -7.5491108894348145
Reconstruction Loss: -11.511345863342285
Iteration 7701:
Training Loss: -7.1800079345703125
Reconstruction Loss: -11.517619132995605
Iteration 7721:
Training Loss: -7.205104351043701
Reconstruction Loss: -11.505453109741211
Iteration 7741:
Training Loss: -7.284715175628662
Reconstruction Loss: -11.527286529541016
Iteration 7761:
Training Loss: -7.258796691894531
Reconstruction Loss: -11.518749237060547
Iteration 7781:
Training Loss: -7.549531936645508
Reconstruction Loss: -11.522719383239746
Iteration 7801:
Training Loss: -7.4992146492004395
Reconstruction Loss: -11.521820068359375
Iteration 7821:
Training Loss: -7.256847858428955
Reconstruction Loss: -11.525897979736328
Iteration 7841:
Training Loss: -7.380335330963135
Reconstruction Loss: -11.537623405456543
Iteration 7861:
Training Loss: -7.341464519500732
Reconstruction Loss: -11.536055564880371
Iteration 7881:
Training Loss: -7.544436454772949
Reconstruction Loss: -11.532587051391602
Iteration 7901:
Training Loss: -7.253624439239502
Reconstruction Loss: -11.535226821899414
Iteration 7921:
Training Loss: -7.535678386688232
Reconstruction Loss: -11.550863265991211
Iteration 7941:
Training Loss: -7.439216136932373
Reconstruction Loss: -11.552701950073242
Iteration 7961:
Training Loss: -7.6248860359191895
Reconstruction Loss: -11.534706115722656
Iteration 7981:
Training Loss: -7.571455955505371
Reconstruction Loss: -11.544023513793945
Iteration 8001:
Training Loss: -7.536803722381592
Reconstruction Loss: -11.550312042236328
Iteration 8021:
Training Loss: -7.3551201820373535
Reconstruction Loss: -11.548980712890625
Iteration 8041:
Training Loss: -7.6502156257629395
Reconstruction Loss: -11.550335884094238
Iteration 8061:
Training Loss: -7.568007946014404
Reconstruction Loss: -11.550586700439453
Iteration 8081:
Training Loss: -7.7462639808654785
Reconstruction Loss: -11.542572021484375
Iteration 8101:
Training Loss: -7.475924491882324
Reconstruction Loss: -11.568724632263184
Iteration 8121:
Training Loss: -7.2055511474609375
Reconstruction Loss: -11.562572479248047
Iteration 8141:
Training Loss: -7.551018238067627
Reconstruction Loss: -11.573749542236328
Iteration 8161:
Training Loss: -7.385685920715332
Reconstruction Loss: -11.576492309570312
Iteration 8181:
Training Loss: -7.462045669555664
Reconstruction Loss: -11.571572303771973
Iteration 8201:
Training Loss: -7.384828567504883
Reconstruction Loss: -11.574909210205078
Iteration 8221:
Training Loss: -7.408979415893555
Reconstruction Loss: -11.57680606842041
Iteration 8241:
Training Loss: -7.703172206878662
Reconstruction Loss: -11.578070640563965
Iteration 8261:
Training Loss: -7.675729751586914
Reconstruction Loss: -11.580546379089355
Iteration 8281:
Training Loss: -7.082620143890381
Reconstruction Loss: -11.585763931274414
Iteration 8301:
Training Loss: -7.306041240692139
Reconstruction Loss: -11.575149536132812
Iteration 8321:
Training Loss: -7.389767646789551
Reconstruction Loss: -11.599876403808594
Iteration 8341:
Training Loss: -7.743438243865967
Reconstruction Loss: -11.591571807861328
Iteration 8361:
Training Loss: -7.5856404304504395
Reconstruction Loss: -11.586874961853027
Iteration 8381:
Training Loss: -7.640866756439209
Reconstruction Loss: -11.605740547180176
Iteration 8401:
Training Loss: -7.382296085357666
Reconstruction Loss: -11.596807479858398
Iteration 8421:
Training Loss: -7.531221389770508
Reconstruction Loss: -11.593562126159668
Iteration 8441:
Training Loss: -7.69120454788208
Reconstruction Loss: -11.60409164428711
Iteration 8461:
Training Loss: -7.7879838943481445
Reconstruction Loss: -11.606557846069336
Iteration 8481:
Training Loss: -7.447507858276367
Reconstruction Loss: -11.609391212463379
Iteration 8501:
Training Loss: -7.611634731292725
Reconstruction Loss: -11.614012718200684
Iteration 8521:
Training Loss: -7.448739528656006
Reconstruction Loss: -11.59553337097168
Iteration 8541:
Training Loss: -7.38014030456543
Reconstruction Loss: -11.603981971740723
Iteration 8561:
Training Loss: -7.694986343383789
Reconstruction Loss: -11.626678466796875
Iteration 8581:
Training Loss: -7.354953765869141
Reconstruction Loss: -11.61225414276123
Iteration 8601:
Training Loss: -7.6476898193359375
Reconstruction Loss: -11.617280006408691
Iteration 8621:
Training Loss: -7.621744155883789
Reconstruction Loss: -11.614516258239746
Iteration 8641:
Training Loss: -7.661022663116455
Reconstruction Loss: -11.622142791748047
Iteration 8661:
Training Loss: -7.505760192871094
Reconstruction Loss: -11.610198974609375
Iteration 8681:
Training Loss: -7.5046916007995605
Reconstruction Loss: -11.6259126663208
Iteration 8701:
Training Loss: -7.832784652709961
Reconstruction Loss: -11.634442329406738
Iteration 8721:
Training Loss: -7.4821319580078125
Reconstruction Loss: -11.639241218566895
Iteration 8741:
Training Loss: -7.5277252197265625
Reconstruction Loss: -11.633597373962402
Iteration 8761:
Training Loss: -7.694650173187256
Reconstruction Loss: -11.644349098205566
Iteration 8781:
Training Loss: -7.624880790710449
Reconstruction Loss: -11.644780158996582
Iteration 8801:
Training Loss: -7.547584533691406
Reconstruction Loss: -11.646162986755371
Iteration 8821:
Training Loss: -7.5472283363342285
Reconstruction Loss: -11.640523910522461
Iteration 8841:
Training Loss: -7.391798496246338
Reconstruction Loss: -11.65110969543457
Iteration 8861:
Training Loss: -7.465770721435547
Reconstruction Loss: -11.638301849365234
Iteration 8881:
Training Loss: -7.340615749359131
Reconstruction Loss: -11.62922477722168
Iteration 8901:
Training Loss: -7.8106255531311035
Reconstruction Loss: -11.649864196777344
Iteration 8921:
Training Loss: -7.760720729827881
Reconstruction Loss: -11.656672477722168
Iteration 8941:
Training Loss: -7.501715183258057
Reconstruction Loss: -11.652966499328613
Iteration 8961:
Training Loss: -8.055594444274902
Reconstruction Loss: -11.653599739074707
Iteration 8981:
Training Loss: -7.7389092445373535
Reconstruction Loss: -11.667014122009277
Iteration 9001:
Training Loss: -7.585413932800293
Reconstruction Loss: -11.659974098205566
Iteration 9021:
Training Loss: -7.360734462738037
Reconstruction Loss: -11.669628143310547
Iteration 9041:
Training Loss: -7.408432483673096
Reconstruction Loss: -11.655801773071289
Iteration 9061:
Training Loss: -7.352779865264893
Reconstruction Loss: -11.675492286682129
Iteration 9081:
Training Loss: -7.745790481567383
Reconstruction Loss: -11.661602973937988
Iteration 9101:
Training Loss: -7.588849067687988
Reconstruction Loss: -11.672394752502441
Iteration 9121:
Training Loss: -7.726560115814209
Reconstruction Loss: -11.675735473632812
Iteration 9141:
Training Loss: -7.376973628997803
Reconstruction Loss: -11.676987648010254
Iteration 9161:
Training Loss: -7.814388275146484
Reconstruction Loss: -11.673199653625488
Iteration 9181:
Training Loss: -7.8860697746276855
Reconstruction Loss: -11.678306579589844
Iteration 9201:
Training Loss: -7.507970809936523
Reconstruction Loss: -11.677316665649414
Iteration 9221:
Training Loss: -8.025611877441406
Reconstruction Loss: -11.682634353637695
Iteration 9241:
Training Loss: -7.290947437286377
Reconstruction Loss: -11.69091510772705
Iteration 9261:
Training Loss: -7.544112205505371
Reconstruction Loss: -11.692073822021484
Iteration 9281:
Training Loss: -7.722216606140137
Reconstruction Loss: -11.709319114685059
Iteration 9301:
Training Loss: -7.841443061828613
Reconstruction Loss: -11.692858695983887
Iteration 9321:
Training Loss: -7.653049945831299
Reconstruction Loss: -11.706721305847168
Iteration 9341:
Training Loss: -7.638123989105225
Reconstruction Loss: -11.69643497467041
Iteration 9361:
Training Loss: -7.747495651245117
Reconstruction Loss: -11.693961143493652
Iteration 9381:
Training Loss: -7.921403884887695
Reconstruction Loss: -11.702649116516113
Iteration 9401:
Training Loss: -7.46596097946167
Reconstruction Loss: -11.703970909118652
Iteration 9421:
Training Loss: -7.509453296661377
Reconstruction Loss: -11.704584121704102
Iteration 9441:
Training Loss: -7.877138614654541
Reconstruction Loss: -11.709948539733887
Iteration 9461:
Training Loss: -7.459050178527832
Reconstruction Loss: -11.71013355255127
Iteration 9481:
Training Loss: -7.339676380157471
Reconstruction Loss: -11.704505920410156
Iteration 9501:
Training Loss: -7.18440055847168
Reconstruction Loss: -11.727326393127441
Iteration 9521:
Training Loss: -7.8263115882873535
Reconstruction Loss: -11.721531867980957
Iteration 9541:
Training Loss: -7.6596550941467285
Reconstruction Loss: -11.7347993850708
Iteration 9561:
Training Loss: -7.411950588226318
Reconstruction Loss: -11.71495246887207
Iteration 9581:
Training Loss: -8.089021682739258
Reconstruction Loss: -11.720640182495117
Iteration 9601:
Training Loss: -7.635274887084961
Reconstruction Loss: -11.72951602935791
Iteration 9621:
Training Loss: -7.867589950561523
Reconstruction Loss: -11.727750778198242
Iteration 9641:
Training Loss: -7.789607524871826
Reconstruction Loss: -11.735252380371094
Iteration 9661:
Training Loss: -7.571009635925293
Reconstruction Loss: -11.729721069335938
Iteration 9681:
Training Loss: -7.695016860961914
Reconstruction Loss: -11.739941596984863
Iteration 9701:
Training Loss: -7.712508201599121
Reconstruction Loss: -11.749004364013672
Iteration 9721:
Training Loss: -7.657975196838379
Reconstruction Loss: -11.738842964172363
Iteration 9741:
Training Loss: -7.969532489776611
Reconstruction Loss: -11.743324279785156
Iteration 9761:
Training Loss: -7.633884429931641
Reconstruction Loss: -11.739242553710938
Iteration 9781:
Training Loss: -7.570279121398926
Reconstruction Loss: -11.743831634521484
Iteration 9801:
Training Loss: -7.701297283172607
Reconstruction Loss: -11.738293647766113
Iteration 9821:
Training Loss: -7.973507881164551
Reconstruction Loss: -11.747418403625488
Iteration 9841:
Training Loss: -7.772449970245361
Reconstruction Loss: -11.75256633758545
Iteration 9861:
Training Loss: -7.771189212799072
Reconstruction Loss: -11.754341125488281
Iteration 9881:
Training Loss: -7.508597373962402
Reconstruction Loss: -11.76740550994873
Iteration 9901:
Training Loss: -7.686278343200684
Reconstruction Loss: -11.755149841308594
Iteration 9921:
Training Loss: -7.623319149017334
Reconstruction Loss: -11.752150535583496
Iteration 9941:
Training Loss: -7.5932135581970215
Reconstruction Loss: -11.76624870300293
Iteration 9961:
Training Loss: -7.734162330627441
Reconstruction Loss: -11.757694244384766
Iteration 9981:
Training Loss: -7.61065149307251
Reconstruction Loss: -11.774258613586426
