5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.509716987609863
Reconstruction Loss: -0.5099350810050964
Iteration 101:
Training Loss: 3.533311128616333
Reconstruction Loss: -1.1895486116409302
Iteration 201:
Training Loss: 1.7752902507781982
Reconstruction Loss: -1.90742826461792
Iteration 301:
Training Loss: 0.7906706929206848
Reconstruction Loss: -2.4427430629730225
Iteration 401:
Training Loss: 0.2133268415927887
Reconstruction Loss: -2.8446977138519287
Iteration 501:
Training Loss: -0.23568910360336304
Reconstruction Loss: -3.1762173175811768
Iteration 601:
Training Loss: -0.6121611595153809
Reconstruction Loss: -3.455803155899048
Iteration 701:
Training Loss: -0.9316015839576721
Reconstruction Loss: -3.692440986633301
Iteration 801:
Training Loss: -1.2034436464309692
Reconstruction Loss: -3.893012046813965
Iteration 901:
Training Loss: -1.4359687566757202
Reconstruction Loss: -4.063755512237549
Iteration 1001:
Training Loss: -1.6366137266159058
Reconstruction Loss: -4.210268497467041
Iteration 1101:
Training Loss: -1.8117247819900513
Reconstruction Loss: -4.337323188781738
Iteration 1201:
Training Loss: -1.9664525985717773
Reconstruction Loss: -4.448805332183838
Iteration 1301:
Training Loss: -2.1048269271850586
Reconstruction Loss: -4.547785758972168
Iteration 1401:
Training Loss: -2.229940414428711
Reconstruction Loss: -4.636646747589111
Iteration 1501:
Training Loss: -2.3441474437713623
Reconstruction Loss: -4.717228412628174
Iteration 1601:
Training Loss: -2.449252128601074
Reconstruction Loss: -4.790948390960693
Iteration 1701:
Training Loss: -2.5466437339782715
Reconstruction Loss: -4.8589067459106445
Iteration 1801:
Training Loss: -2.6374154090881348
Reconstruction Loss: -4.921966075897217
Iteration 1901:
Training Loss: -2.722428560256958
Reconstruction Loss: -4.980807781219482
Iteration 2001:
Training Loss: -2.8023884296417236
Reconstruction Loss: -5.035979747772217
Iteration 2101:
Training Loss: -2.8778645992279053
Reconstruction Loss: -5.087925434112549
Iteration 2201:
Training Loss: -2.9493331909179688
Reconstruction Loss: -5.137009620666504
Iteration 2301:
Training Loss: -3.0171940326690674
Reconstruction Loss: -5.183536052703857
Iteration 2401:
Training Loss: -3.0817911624908447
Reconstruction Loss: -5.227757930755615
Iteration 2501:
Training Loss: -3.143418312072754
Reconstruction Loss: -5.269894123077393
Iteration 2601:
Training Loss: -3.2023329734802246
Reconstruction Loss: -5.3101301193237305
Iteration 2701:
Training Loss: -3.2587525844573975
Reconstruction Loss: -5.348626613616943
Iteration 2801:
Training Loss: -3.3128833770751953
Reconstruction Loss: -5.385525226593018
Iteration 2901:
Training Loss: -3.3648908138275146
Reconstruction Loss: -5.420949935913086
Iteration 3001:
Training Loss: -3.414940357208252
Reconstruction Loss: -5.455008506774902
Iteration 3101:
Training Loss: -3.463167905807495
Reconstruction Loss: -5.487800121307373
Iteration 3201:
Training Loss: -3.509699821472168
Reconstruction Loss: -5.519408226013184
Iteration 3301:
Training Loss: -3.554647922515869
Reconstruction Loss: -5.549914360046387
Iteration 3401:
Training Loss: -3.5981242656707764
Reconstruction Loss: -5.579388618469238
Iteration 3501:
Training Loss: -3.640218496322632
Reconstruction Loss: -5.607896327972412
Iteration 3601:
Training Loss: -3.6810152530670166
Reconstruction Loss: -5.635492324829102
Iteration 3701:
Training Loss: -3.7205963134765625
Reconstruction Loss: -5.662230014801025
Iteration 3801:
Training Loss: -3.7590386867523193
Reconstruction Loss: -5.68815803527832
Iteration 3901:
Training Loss: -3.7964000701904297
Reconstruction Loss: -5.713324069976807
Iteration 4001:
Training Loss: -3.832751989364624
Reconstruction Loss: -5.737765789031982
Iteration 4101:
Training Loss: -3.868136167526245
Reconstruction Loss: -5.761520862579346
Iteration 4201:
Training Loss: -3.9026236534118652
Reconstruction Loss: -5.7846245765686035
Iteration 4301:
Training Loss: -3.9362497329711914
Reconstruction Loss: -5.8071112632751465
Iteration 4401:
Training Loss: -3.9690706729888916
Reconstruction Loss: -5.829007625579834
Iteration 4501:
Training Loss: -4.0011186599731445
Reconstruction Loss: -5.850340843200684
Iteration 4601:
Training Loss: -4.032436370849609
Reconstruction Loss: -5.8711442947387695
Iteration 4701:
Training Loss: -4.063061237335205
Reconstruction Loss: -5.891433238983154
Iteration 4801:
Training Loss: -4.093027114868164
Reconstruction Loss: -5.9112324714660645
Iteration 4901:
Training Loss: -4.122365474700928
Reconstruction Loss: -5.93056583404541
Iteration 5001:
Training Loss: -4.151102066040039
Reconstruction Loss: -5.949446201324463
Iteration 5101:
Training Loss: -4.179271697998047
Reconstruction Loss: -5.9678955078125
Iteration 5201:
Training Loss: -4.206891059875488
Reconstruction Loss: -5.985939979553223
Iteration 5301:
Training Loss: -4.233997344970703
Reconstruction Loss: -6.003581523895264
Iteration 5401:
Training Loss: -4.26060152053833
Reconstruction Loss: -6.020846366882324
Iteration 5501:
Training Loss: -4.28672981262207
Reconstruction Loss: -6.037746906280518
Iteration 5601:
Training Loss: -4.3124003410339355
Reconstruction Loss: -6.0542893409729
Iteration 5701:
Training Loss: -4.3376312255859375
Reconstruction Loss: -6.070491790771484
Iteration 5801:
Training Loss: -4.362446308135986
Reconstruction Loss: -6.086369037628174
Iteration 5901:
Training Loss: -4.38685941696167
Reconstruction Loss: -6.101925373077393
Iteration 6001:
Training Loss: -4.410882472991943
Reconstruction Loss: -6.117180347442627
Iteration 6101:
Training Loss: -4.434528827667236
Reconstruction Loss: -6.13214111328125
Iteration 6201:
Training Loss: -4.457827091217041
Reconstruction Loss: -6.1468186378479
Iteration 6301:
Training Loss: -4.48077392578125
Reconstruction Loss: -6.161216735839844
Iteration 6401:
Training Loss: -4.5033860206604
Reconstruction Loss: -6.175348281860352
Iteration 6501:
Training Loss: -4.525681972503662
Reconstruction Loss: -6.189223289489746
Iteration 6601:
Training Loss: -4.547667503356934
Reconstruction Loss: -6.202846050262451
Iteration 6701:
Training Loss: -4.569357395172119
Reconstruction Loss: -6.216222763061523
Iteration 6801:
Training Loss: -4.590754985809326
Reconstruction Loss: -6.229366779327393
Iteration 6901:
Training Loss: -4.611875057220459
Reconstruction Loss: -6.242288589477539
Iteration 7001:
Training Loss: -4.632727146148682
Reconstruction Loss: -6.254985332489014
Iteration 7101:
Training Loss: -4.653316974639893
Reconstruction Loss: -6.267465591430664
Iteration 7201:
Training Loss: -4.6736578941345215
Reconstruction Loss: -6.279733657836914
Iteration 7301:
Training Loss: -4.693748950958252
Reconstruction Loss: -6.2918009757995605
Iteration 7401:
Training Loss: -4.713606834411621
Reconstruction Loss: -6.303675651550293
Iteration 7501:
Training Loss: -4.733234405517578
Reconstruction Loss: -6.3153557777404785
Iteration 7601:
Training Loss: -4.7526350021362305
Reconstruction Loss: -6.326848030090332
Iteration 7701:
Training Loss: -4.771820068359375
Reconstruction Loss: -6.338162422180176
Iteration 7801:
Training Loss: -4.790793418884277
Reconstruction Loss: -6.349300861358643
Iteration 7901:
Training Loss: -4.809567451477051
Reconstruction Loss: -6.360263824462891
Iteration 8001:
Training Loss: -4.82813835144043
Reconstruction Loss: -6.371059417724609
Iteration 8101:
Training Loss: -4.8465118408203125
Reconstruction Loss: -6.381691932678223
Iteration 8201:
Training Loss: -4.864708423614502
Reconstruction Loss: -6.392158031463623
Iteration 8301:
Training Loss: -4.8827128410339355
Reconstruction Loss: -6.402469635009766
Iteration 8401:
Training Loss: -4.90054178237915
Reconstruction Loss: -6.412630081176758
Iteration 8501:
Training Loss: -4.918194770812988
Reconstruction Loss: -6.422647953033447
Iteration 8601:
Training Loss: -4.935677528381348
Reconstruction Loss: -6.432514190673828
Iteration 8701:
Training Loss: -4.953000068664551
Reconstruction Loss: -6.442248344421387
Iteration 8801:
Training Loss: -4.970152854919434
Reconstruction Loss: -6.4518351554870605
Iteration 8901:
Training Loss: -4.987154006958008
Reconstruction Loss: -6.461287975311279
Iteration 9001:
Training Loss: -5.004005432128906
Reconstruction Loss: -6.470613479614258
Iteration 9101:
Training Loss: -5.020697593688965
Reconstruction Loss: -6.479806423187256
Iteration 9201:
Training Loss: -5.03725004196167
Reconstruction Loss: -6.488875865936279
Iteration 9301:
Training Loss: -5.053648471832275
Reconstruction Loss: -6.497811794281006
Iteration 9401:
Training Loss: -5.069921970367432
Reconstruction Loss: -6.506636142730713
Iteration 9501:
Training Loss: -5.086050510406494
Reconstruction Loss: -6.515337944030762
Iteration 9601:
Training Loss: -5.102048397064209
Reconstruction Loss: -6.523928642272949
Iteration 9701:
Training Loss: -5.117917537689209
Reconstruction Loss: -6.532402515411377
Iteration 9801:
Training Loss: -5.133650779724121
Reconstruction Loss: -6.540769577026367
Iteration 9901:
Training Loss: -5.149259567260742
Reconstruction Loss: -6.549032688140869
Iteration 10001:
Training Loss: -5.164740085601807
Reconstruction Loss: -6.55718469619751
Iteration 10101:
Training Loss: -5.180108070373535
Reconstruction Loss: -6.565229892730713
Iteration 10201:
Training Loss: -5.1953535079956055
Reconstruction Loss: -6.573177337646484
Iteration 10301:
Training Loss: -5.210484981536865
Reconstruction Loss: -6.581019401550293
Iteration 10401:
Training Loss: -5.225500583648682
Reconstruction Loss: -6.588766574859619
Iteration 10501:
Training Loss: -5.240401744842529
Reconstruction Loss: -6.596414566040039
Iteration 10601:
Training Loss: -5.2552008628845215
Reconstruction Loss: -6.603971004486084
Iteration 10701:
Training Loss: -5.269883632659912
Reconstruction Loss: -6.611433506011963
Iteration 10801:
Training Loss: -5.284467697143555
Reconstruction Loss: -6.618809700012207
Iteration 10901:
Training Loss: -5.298946380615234
Reconstruction Loss: -6.626094341278076
Iteration 11001:
Training Loss: -5.313320159912109
Reconstruction Loss: -6.633294105529785
Iteration 11101:
Training Loss: -5.327597618103027
Reconstruction Loss: -6.640406131744385
Iteration 11201:
Training Loss: -5.341782569885254
Reconstruction Loss: -6.647434234619141
Iteration 11301:
Training Loss: -5.355861663818359
Reconstruction Loss: -6.654384613037109
Iteration 11401:
Training Loss: -5.369855880737305
Reconstruction Loss: -6.661252498626709
Iteration 11501:
Training Loss: -5.383746147155762
Reconstruction Loss: -6.668036937713623
Iteration 11601:
Training Loss: -5.397548675537109
Reconstruction Loss: -6.674745082855225
Iteration 11701:
Training Loss: -5.411266326904297
Reconstruction Loss: -6.68137788772583
Iteration 11801:
Training Loss: -5.42488431930542
Reconstruction Loss: -6.687935829162598
Iteration 11901:
Training Loss: -5.438420295715332
Reconstruction Loss: -6.694419860839844
Iteration 12001:
Training Loss: -5.451870441436768
Reconstruction Loss: -6.700819492340088
Iteration 12101:
Training Loss: -5.465237140655518
Reconstruction Loss: -6.707156658172607
Iteration 12201:
Training Loss: -5.478520393371582
Reconstruction Loss: -6.713424205780029
Iteration 12301:
Training Loss: -5.4917192459106445
Reconstruction Loss: -6.719625949859619
Iteration 12401:
Training Loss: -5.504838943481445
Reconstruction Loss: -6.725762367248535
Iteration 12501:
Training Loss: -5.517871379852295
Reconstruction Loss: -6.731828212738037
Iteration 12601:
Training Loss: -5.5308308601379395
Reconstruction Loss: -6.737828731536865
Iteration 12701:
Training Loss: -5.543711185455322
Reconstruction Loss: -6.743765354156494
Iteration 12801:
Training Loss: -5.55651330947876
Reconstruction Loss: -6.749639987945557
Iteration 12901:
Training Loss: -5.5692458152771
Reconstruction Loss: -6.7554497718811035
Iteration 13001:
Training Loss: -5.581899642944336
Reconstruction Loss: -6.76119327545166
Iteration 13101:
Training Loss: -5.594477653503418
Reconstruction Loss: -6.766875267028809
Iteration 13201:
Training Loss: -5.6069841384887695
Reconstruction Loss: -6.772499084472656
Iteration 13301:
Training Loss: -5.6194281578063965
Reconstruction Loss: -6.778064727783203
Iteration 13401:
Training Loss: -5.63179349899292
Reconstruction Loss: -6.783573627471924
Iteration 13501:
Training Loss: -5.644092559814453
Reconstruction Loss: -6.789028644561768
Iteration 13601:
Training Loss: -5.656324863433838
Reconstruction Loss: -6.794425010681152
Iteration 13701:
Training Loss: -5.668484210968018
Reconstruction Loss: -6.799768447875977
Iteration 13801:
Training Loss: -5.680577754974365
Reconstruction Loss: -6.8050537109375
Iteration 13901:
Training Loss: -5.692612648010254
Reconstruction Loss: -6.810286521911621
Iteration 14001:
Training Loss: -5.704580307006836
Reconstruction Loss: -6.815465927124023
Iteration 14101:
Training Loss: -5.71647834777832
Reconstruction Loss: -6.820597171783447
Iteration 14201:
Training Loss: -5.72831916809082
Reconstruction Loss: -6.82567834854126
Iteration 14301:
Training Loss: -5.740095138549805
Reconstruction Loss: -6.830708026885986
Iteration 14401:
Training Loss: -5.751798152923584
Reconstruction Loss: -6.835695743560791
Iteration 14501:
Training Loss: -5.763452053070068
Reconstruction Loss: -6.840633392333984
Iteration 14601:
Training Loss: -5.775052070617676
Reconstruction Loss: -6.845520973205566
Iteration 14701:
Training Loss: -5.786581039428711
Reconstruction Loss: -6.8503546714782715
Iteration 14801:
Training Loss: -5.7980546951293945
Reconstruction Loss: -6.855140686035156
Iteration 14901:
Training Loss: -5.809463024139404
Reconstruction Loss: -6.859879016876221
