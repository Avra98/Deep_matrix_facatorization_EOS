5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.845126152038574
Reconstruction Loss: -0.07765182852745056
Iteration 101:
Training Loss: 2.6216986179351807
Reconstruction Loss: -0.8852624297142029
Iteration 201:
Training Loss: 1.5460951328277588
Reconstruction Loss: -1.30745530128479
Iteration 301:
Training Loss: 0.8628746271133423
Reconstruction Loss: -1.5648863315582275
Iteration 401:
Training Loss: 0.34942033886909485
Reconstruction Loss: -1.7588622570037842
Iteration 501:
Training Loss: -0.0564618781208992
Reconstruction Loss: -1.9182560443878174
Iteration 601:
Training Loss: -0.38747769594192505
Reconstruction Loss: -2.0520148277282715
Iteration 701:
Training Loss: -0.6660622358322144
Reconstruction Loss: -2.1657357215881348
Iteration 801:
Training Loss: -0.9070848822593689
Reconstruction Loss: -2.263824224472046
Iteration 901:
Training Loss: -1.120450496673584
Reconstruction Loss: -2.3495616912841797
Iteration 1001:
Training Loss: -1.3128284215927124
Reconstruction Loss: -2.425313711166382
Iteration 1101:
Training Loss: -1.48874032497406
Reconstruction Loss: -2.4928019046783447
Iteration 1201:
Training Loss: -1.6512882709503174
Reconstruction Loss: -2.553311824798584
Iteration 1301:
Training Loss: -1.8026604652404785
Reconstruction Loss: -2.6078383922576904
Iteration 1401:
Training Loss: -1.9444447755813599
Reconstruction Loss: -2.6571736335754395
Iteration 1501:
Training Loss: -2.077845335006714
Reconstruction Loss: -2.7019646167755127
Iteration 1601:
Training Loss: -2.203800678253174
Reconstruction Loss: -2.742753028869629
Iteration 1701:
Training Loss: -2.3230671882629395
Reconstruction Loss: -2.7799978256225586
Iteration 1801:
Training Loss: -2.436269760131836
Reconstruction Loss: -2.8140923976898193
Iteration 1901:
Training Loss: -2.5439341068267822
Reconstruction Loss: -2.84537935256958
Iteration 2001:
Training Loss: -2.6465203762054443
Reconstruction Loss: -2.8741562366485596
Iteration 2101:
Training Loss: -2.744418144226074
Reconstruction Loss: -2.9006850719451904
Iteration 2201:
Training Loss: -2.8379831314086914
Reconstruction Loss: -2.9251956939697266
Iteration 2301:
Training Loss: -2.927522897720337
Reconstruction Loss: -2.9478912353515625
Iteration 2401:
Training Loss: -3.0133213996887207
Reconstruction Loss: -2.9689512252807617
Iteration 2501:
Training Loss: -3.095633029937744
Reconstruction Loss: -2.9885339736938477
Iteration 2601:
Training Loss: -3.1746954917907715
Reconstruction Loss: -3.0067801475524902
Iteration 2701:
Training Loss: -3.2507169246673584
Reconstruction Loss: -3.0238161087036133
Iteration 2801:
Training Loss: -3.3239023685455322
Reconstruction Loss: -3.03975248336792
Iteration 2901:
Training Loss: -3.3944332599639893
Reconstruction Loss: -3.0546882152557373
Iteration 3001:
Training Loss: -3.46248197555542
Reconstruction Loss: -3.068711042404175
Iteration 3101:
Training Loss: -3.528207540512085
Reconstruction Loss: -3.0819013118743896
Iteration 3201:
Training Loss: -3.5917563438415527
Reconstruction Loss: -3.094329833984375
Iteration 3301:
Training Loss: -3.653266668319702
Reconstruction Loss: -3.106058120727539
Iteration 3401:
Training Loss: -3.7128636837005615
Reconstruction Loss: -3.1171436309814453
Iteration 3501:
Training Loss: -3.77067494392395
Reconstruction Loss: -3.127638578414917
Iteration 3601:
Training Loss: -3.8268096446990967
Reconstruction Loss: -3.137587308883667
Iteration 3701:
Training Loss: -3.8813693523406982
Reconstruction Loss: -3.1470320224761963
Iteration 3801:
Training Loss: -3.934450626373291
Reconstruction Loss: -3.156010150909424
Iteration 3901:
Training Loss: -3.986147165298462
Reconstruction Loss: -3.1645541191101074
Iteration 4001:
Training Loss: -4.036537170410156
Reconstruction Loss: -3.1726949214935303
Iteration 4101:
Training Loss: -4.085710525512695
Reconstruction Loss: -3.180462121963501
Iteration 4201:
Training Loss: -4.133731365203857
Reconstruction Loss: -3.1878788471221924
Iteration 4301:
Training Loss: -4.180664539337158
Reconstruction Loss: -3.194969415664673
Iteration 4401:
Training Loss: -4.226579189300537
Reconstruction Loss: -3.201754093170166
Iteration 4501:
Training Loss: -4.2715277671813965
Reconstruction Loss: -3.208251953125
Iteration 4601:
Training Loss: -4.315566539764404
Reconstruction Loss: -3.2144815921783447
Iteration 4701:
Training Loss: -4.3587493896484375
Reconstruction Loss: -3.220459222793579
Iteration 4801:
Training Loss: -4.401119232177734
Reconstruction Loss: -3.2261979579925537
Iteration 4901:
Training Loss: -4.442720413208008
Reconstruction Loss: -3.2317135334014893
Iteration 5001:
Training Loss: -4.483589172363281
Reconstruction Loss: -3.2370173931121826
Iteration 5101:
Training Loss: -4.523770809173584
Reconstruction Loss: -3.242119312286377
Iteration 5201:
Training Loss: -4.563292980194092
Reconstruction Loss: -3.247032642364502
Iteration 5301:
Training Loss: -4.602193832397461
Reconstruction Loss: -3.2517666816711426
Iteration 5401:
Training Loss: -4.640499114990234
Reconstruction Loss: -3.2563302516937256
Iteration 5501:
Training Loss: -4.678238868713379
Reconstruction Loss: -3.260732889175415
Iteration 5601:
Training Loss: -4.7154436111450195
Reconstruction Loss: -3.2649805545806885
Iteration 5701:
Training Loss: -4.752127647399902
Reconstruction Loss: -3.2690834999084473
Iteration 5801:
Training Loss: -4.788326740264893
Reconstruction Loss: -3.2730460166931152
Iteration 5901:
Training Loss: -4.824049949645996
Reconstruction Loss: -3.2768774032592773
Iteration 6001:
Training Loss: -4.8593220710754395
Reconstruction Loss: -3.28058123588562
Iteration 6101:
Training Loss: -4.894166469573975
Reconstruction Loss: -3.2841668128967285
Iteration 6201:
Training Loss: -4.928594589233398
Reconstruction Loss: -3.28763484954834
Iteration 6301:
Training Loss: -4.962625503540039
Reconstruction Loss: -3.2909936904907227
Iteration 6401:
Training Loss: -4.996273517608643
Reconstruction Loss: -3.2942445278167725
Iteration 6501:
Training Loss: -5.02955436706543
Reconstruction Loss: -3.297396183013916
Iteration 6601:
Training Loss: -5.062483310699463
Reconstruction Loss: -3.300450563430786
Iteration 6701:
Training Loss: -5.095076084136963
Reconstruction Loss: -3.3034117221832275
Iteration 6801:
Training Loss: -5.127330303192139
Reconstruction Loss: -3.3062832355499268
Iteration 6901:
Training Loss: -5.159272193908691
Reconstruction Loss: -3.309068202972412
Iteration 7001:
Training Loss: -5.190893650054932
Reconstruction Loss: -3.311772346496582
Iteration 7101:
Training Loss: -5.222227096557617
Reconstruction Loss: -3.314396858215332
Iteration 7201:
Training Loss: -5.253281593322754
Reconstruction Loss: -3.316945791244507
Iteration 7301:
Training Loss: -5.284053802490234
Reconstruction Loss: -3.3194196224212646
Iteration 7401:
Training Loss: -5.314549922943115
Reconstruction Loss: -3.321824312210083
Iteration 7501:
Training Loss: -5.344794750213623
Reconstruction Loss: -3.3241591453552246
Iteration 7601:
Training Loss: -5.374777793884277
Reconstruction Loss: -3.326429843902588
Iteration 7701:
Training Loss: -5.40451717376709
Reconstruction Loss: -3.3286361694335938
Iteration 7801:
Training Loss: -5.434017181396484
Reconstruction Loss: -3.3307840824127197
Iteration 7901:
Training Loss: -5.463276386260986
Reconstruction Loss: -3.332871913909912
Iteration 8001:
Training Loss: -5.492316722869873
Reconstruction Loss: -3.3349039554595947
Iteration 8101:
Training Loss: -5.521128177642822
Reconstruction Loss: -3.3368828296661377
Iteration 8201:
Training Loss: -5.549727439880371
Reconstruction Loss: -3.3388073444366455
Iteration 8301:
Training Loss: -5.578115463256836
Reconstruction Loss: -3.340679407119751
Iteration 8401:
Training Loss: -5.606302738189697
Reconstruction Loss: -3.3425018787384033
Iteration 8501:
Training Loss: -5.634292125701904
Reconstruction Loss: -3.344275951385498
Iteration 8601:
Training Loss: -5.662080764770508
Reconstruction Loss: -3.3460042476654053
Iteration 8701:
Training Loss: -5.6896748542785645
Reconstruction Loss: -3.3476877212524414
Iteration 8801:
Training Loss: -5.717092037200928
Reconstruction Loss: -3.3493261337280273
Iteration 8901:
Training Loss: -5.744330883026123
Reconstruction Loss: -3.3509268760681152
Iteration 9001:
Training Loss: -5.771381378173828
Reconstruction Loss: -3.352484941482544
Iteration 9101:
Training Loss: -5.798258304595947
Reconstruction Loss: -3.3540029525756836
Iteration 9201:
Training Loss: -5.8249735832214355
Reconstruction Loss: -3.3554842472076416
Iteration 9301:
Training Loss: -5.851510047912598
Reconstruction Loss: -3.356926441192627
Iteration 9401:
Training Loss: -5.877890586853027
Reconstruction Loss: -3.358334541320801
Iteration 9501:
Training Loss: -5.904096603393555
Reconstruction Loss: -3.359706163406372
Iteration 9601:
Training Loss: -5.930172443389893
Reconstruction Loss: -3.36104416847229
Iteration 9701:
Training Loss: -5.956074237823486
Reconstruction Loss: -3.3623483180999756
Iteration 9801:
Training Loss: -5.981821060180664
Reconstruction Loss: -3.3636224269866943
Iteration 9901:
Training Loss: -6.007411956787109
Reconstruction Loss: -3.3648672103881836
Iteration 10001:
Training Loss: -6.032872200012207
Reconstruction Loss: -3.366082191467285
Iteration 10101:
Training Loss: -6.058187484741211
Reconstruction Loss: -3.3672666549682617
Iteration 10201:
Training Loss: -6.083364486694336
Reconstruction Loss: -3.3684239387512207
Iteration 10301:
Training Loss: -6.108404636383057
Reconstruction Loss: -3.369551658630371
Iteration 10401:
Training Loss: -6.133308410644531
Reconstruction Loss: -3.370654821395874
Iteration 10501:
Training Loss: -6.158084869384766
Reconstruction Loss: -3.3717308044433594
Iteration 10601:
Training Loss: -6.182712078094482
Reconstruction Loss: -3.3727805614471436
Iteration 10701:
Training Loss: -6.20722770690918
Reconstruction Loss: -3.3738081455230713
Iteration 10801:
Training Loss: -6.231605052947998
Reconstruction Loss: -3.3748109340667725
Iteration 10901:
Training Loss: -6.255848407745361
Reconstruction Loss: -3.3757882118225098
Iteration 11001:
Training Loss: -6.279989242553711
Reconstruction Loss: -3.376742362976074
Iteration 11101:
Training Loss: -6.3039960861206055
Reconstruction Loss: -3.3776776790618896
Iteration 11201:
Training Loss: -6.32788610458374
Reconstruction Loss: -3.378591775894165
Iteration 11301:
Training Loss: -6.351662635803223
Reconstruction Loss: -3.3794867992401123
Iteration 11401:
Training Loss: -6.3753252029418945
Reconstruction Loss: -3.3803606033325195
Iteration 11501:
Training Loss: -6.398870944976807
Reconstruction Loss: -3.381214141845703
Iteration 11601:
Training Loss: -6.422300815582275
Reconstruction Loss: -3.3820483684539795
Iteration 11701:
Training Loss: -6.445621967315674
Reconstruction Loss: -3.382864475250244
Iteration 11801:
Training Loss: -6.468839168548584
Reconstruction Loss: -3.383662700653076
Iteration 11901:
Training Loss: -6.491938591003418
Reconstruction Loss: -3.3844411373138428
Iteration 12001:
Training Loss: -6.514928340911865
Reconstruction Loss: -3.385202646255493
Iteration 12101:
Training Loss: -6.537827968597412
Reconstruction Loss: -3.38594651222229
Iteration 12201:
Training Loss: -6.560631275177002
Reconstruction Loss: -3.3866751194000244
Iteration 12301:
Training Loss: -6.583344459533691
Reconstruction Loss: -3.3873891830444336
Iteration 12401:
Training Loss: -6.605920314788818
Reconstruction Loss: -3.3880863189697266
Iteration 12501:
Training Loss: -6.628411293029785
Reconstruction Loss: -3.3887691497802734
Iteration 12601:
Training Loss: -6.650790214538574
Reconstruction Loss: -3.3894357681274414
Iteration 12701:
Training Loss: -6.673089504241943
Reconstruction Loss: -3.390087604522705
Iteration 12801:
Training Loss: -6.695303440093994
Reconstruction Loss: -3.390726327896118
Iteration 12901:
Training Loss: -6.717384338378906
Reconstruction Loss: -3.391350746154785
Iteration 13001:
Training Loss: -6.739390850067139
Reconstruction Loss: -3.3919625282287598
Iteration 13101:
Training Loss: -6.761310577392578
Reconstruction Loss: -3.3925621509552
Iteration 13201:
Training Loss: -6.783148765563965
Reconstruction Loss: -3.393148183822632
Iteration 13301:
Training Loss: -6.804897785186768
Reconstruction Loss: -3.393723249435425
Iteration 13401:
Training Loss: -6.826560974121094
Reconstruction Loss: -3.394286870956421
Iteration 13501:
Training Loss: -6.848128318786621
Reconstruction Loss: -3.394838333129883
Iteration 13601:
Training Loss: -6.869601726531982
Reconstruction Loss: -3.3953781127929688
Iteration 13701:
Training Loss: -6.890997409820557
Reconstruction Loss: -3.395905017852783
Iteration 13801:
Training Loss: -6.912311553955078
Reconstruction Loss: -3.39642071723938
Iteration 13901:
Training Loss: -6.933539390563965
Reconstruction Loss: -3.3969271183013916
Iteration 14001:
Training Loss: -6.954690933227539
Reconstruction Loss: -3.3974218368530273
Iteration 14101:
Training Loss: -6.975746154785156
Reconstruction Loss: -3.3979060649871826
Iteration 14201:
Training Loss: -6.9967193603515625
Reconstruction Loss: -3.3983821868896484
Iteration 14301:
Training Loss: -7.01763391494751
Reconstruction Loss: -3.398848533630371
Iteration 14401:
Training Loss: -7.03847074508667
Reconstruction Loss: -3.3993053436279297
Iteration 14501:
Training Loss: -7.059202194213867
Reconstruction Loss: -3.399752140045166
Iteration 14601:
Training Loss: -7.079856872558594
Reconstruction Loss: -3.4001882076263428
Iteration 14701:
Training Loss: -7.100437164306641
Reconstruction Loss: -3.4006152153015137
Iteration 14801:
Training Loss: -7.120955944061279
Reconstruction Loss: -3.401033639907837
Iteration 14901:
Training Loss: -7.1413798332214355
Reconstruction Loss: -3.4014434814453125
Iteration 15001:
Training Loss: -7.161760330200195
Reconstruction Loss: -3.401845932006836
Iteration 15101:
Training Loss: -7.182062149047852
Reconstruction Loss: -3.4022421836853027
Iteration 15201:
Training Loss: -7.20228910446167
Reconstruction Loss: -3.4026291370391846
Iteration 15301:
Training Loss: -7.222449779510498
Reconstruction Loss: -3.4030067920684814
Iteration 15401:
Training Loss: -7.242529392242432
Reconstruction Loss: -3.40337872505188
Iteration 15501:
Training Loss: -7.262538433074951
Reconstruction Loss: -3.403744697570801
Iteration 15601:
Training Loss: -7.282471179962158
Reconstruction Loss: -3.4041025638580322
Iteration 15701:
Training Loss: -7.3023247718811035
Reconstruction Loss: -3.4044530391693115
Iteration 15801:
Training Loss: -7.322131633758545
Reconstruction Loss: -3.404796600341797
Iteration 15901:
Training Loss: -7.341860294342041
Reconstruction Loss: -3.405134439468384
Iteration 16001:
Training Loss: -7.361512184143066
Reconstruction Loss: -3.405465841293335
Iteration 16101:
Training Loss: -7.381103038787842
Reconstruction Loss: -3.405790328979492
Iteration 16201:
Training Loss: -7.400613307952881
Reconstruction Loss: -3.4061081409454346
Iteration 16301:
Training Loss: -7.420054912567139
Reconstruction Loss: -3.4064197540283203
Iteration 16401:
Training Loss: -7.439421653747559
Reconstruction Loss: -3.4067256450653076
Iteration 16501:
Training Loss: -7.458739280700684
Reconstruction Loss: -3.407024621963501
Iteration 16601:
Training Loss: -7.4780402183532715
Reconstruction Loss: -3.4073166847229004
Iteration 16701:
Training Loss: -7.497260093688965
Reconstruction Loss: -3.407604217529297
Iteration 16801:
Training Loss: -7.516421318054199
Reconstruction Loss: -3.4078872203826904
Iteration 16901:
Training Loss: -7.535520553588867
Reconstruction Loss: -3.4081637859344482
Iteration 17001:
Training Loss: -7.554579257965088
Reconstruction Loss: -3.4084346294403076
Iteration 17101:
Training Loss: -7.573555946350098
Reconstruction Loss: -3.4086999893188477
Iteration 17201:
Training Loss: -7.592474937438965
Reconstruction Loss: -3.408961534500122
Iteration 17301:
Training Loss: -7.611341953277588
Reconstruction Loss: -3.409217357635498
Iteration 17401:
Training Loss: -7.630131721496582
Reconstruction Loss: -3.4094676971435547
Iteration 17501:
Training Loss: -7.648841857910156
Reconstruction Loss: -3.4097142219543457
Iteration 17601:
Training Loss: -7.667523384094238
Reconstruction Loss: -3.4099557399749756
Iteration 17701:
Training Loss: -7.686108589172363
Reconstruction Loss: -3.410191774368286
Iteration 17801:
Training Loss: -7.7046732902526855
Reconstruction Loss: -3.410423517227173
Iteration 17901:
Training Loss: -7.723158359527588
Reconstruction Loss: -3.410651683807373
Iteration 18001:
Training Loss: -7.741593360900879
Reconstruction Loss: -3.4108760356903076
Iteration 18101:
Training Loss: -7.759978771209717
Reconstruction Loss: -3.4110970497131348
Iteration 18201:
Training Loss: -7.778311729431152
Reconstruction Loss: -3.4113149642944336
Iteration 18301:
Training Loss: -7.796605110168457
Reconstruction Loss: -3.4115285873413086
Iteration 18401:
Training Loss: -7.8148393630981445
Reconstruction Loss: -3.4117379188537598
Iteration 18501:
Training Loss: -7.832984447479248
Reconstruction Loss: -3.4119436740875244
Iteration 18601:
Training Loss: -7.851078987121582
Reconstruction Loss: -3.412144184112549
Iteration 18701:
Training Loss: -7.869193077087402
Reconstruction Loss: -3.412339687347412
Iteration 18801:
Training Loss: -7.887188911437988
Reconstruction Loss: -3.4125306606292725
Iteration 18901:
Training Loss: -7.905155658721924
Reconstruction Loss: -3.4127182960510254
Iteration 19001:
Training Loss: -7.923076629638672
Reconstruction Loss: -3.412902355194092
Iteration 19101:
Training Loss: -7.940934658050537
Reconstruction Loss: -3.413083076477051
Iteration 19201:
Training Loss: -7.9587507247924805
Reconstruction Loss: -3.4132609367370605
Iteration 19301:
Training Loss: -7.976536273956299
Reconstruction Loss: -3.413435459136963
Iteration 19401:
Training Loss: -7.994260787963867
Reconstruction Loss: -3.413606643676758
Iteration 19501:
Training Loss: -8.011944770812988
Reconstruction Loss: -3.4137749671936035
Iteration 19601:
Training Loss: -8.029563903808594
Reconstruction Loss: -3.413940668106079
Iteration 19701:
Training Loss: -8.04713249206543
Reconstruction Loss: -3.4141032695770264
Iteration 19801:
Training Loss: -8.06466293334961
Reconstruction Loss: -3.4142627716064453
Iteration 19901:
Training Loss: -8.082161903381348
Reconstruction Loss: -3.4144186973571777
Iteration 20001:
Training Loss: -8.099594116210938
Reconstruction Loss: -3.414572238922119
Iteration 20101:
Training Loss: -8.116968154907227
Reconstruction Loss: -3.414721965789795
Iteration 20201:
Training Loss: -8.134328842163086
Reconstruction Loss: -3.4148683547973633
Iteration 20301:
Training Loss: -8.151590347290039
Reconstruction Loss: -3.415011167526245
Iteration 20401:
Training Loss: -8.168828964233398
Reconstruction Loss: -3.4151511192321777
Iteration 20501:
Training Loss: -8.185992240905762
Reconstruction Loss: -3.4152889251708984
Iteration 20601:
Training Loss: -8.203165054321289
Reconstruction Loss: -3.415424108505249
Iteration 20701:
Training Loss: -8.22027587890625
Reconstruction Loss: -3.415555477142334
Iteration 20801:
Training Loss: -8.237337112426758
Reconstruction Loss: -3.415684223175049
Iteration 20901:
Training Loss: -8.254340171813965
Reconstruction Loss: -3.4158103466033936
Iteration 21001:
Training Loss: -8.271331787109375
Reconstruction Loss: -3.4159348011016846
Iteration 21101:
Training Loss: -8.288297653198242
Reconstruction Loss: -3.416057586669922
Iteration 21201:
Training Loss: -8.30518627166748
Reconstruction Loss: -3.416177749633789
Iteration 21301:
Training Loss: -8.32205867767334
Reconstruction Loss: -3.4162962436676025
Iteration 21401:
Training Loss: -8.338871002197266
Reconstruction Loss: -3.416412591934204
Iteration 21501:
Training Loss: -8.355680465698242
Reconstruction Loss: -3.4165265560150146
Iteration 21601:
Training Loss: -8.372424125671387
Reconstruction Loss: -3.416637897491455
Iteration 21701:
Training Loss: -8.389122009277344
Reconstruction Loss: -3.4167466163635254
Iteration 21801:
Training Loss: -8.405791282653809
Reconstruction Loss: -3.416853427886963
Iteration 21901:
Training Loss: -8.42243480682373
Reconstruction Loss: -3.4169583320617676
Iteration 22001:
Training Loss: -8.438995361328125
Reconstruction Loss: -3.417060375213623
Iteration 22101:
Training Loss: -8.455526351928711
Reconstruction Loss: -3.417161703109741
Iteration 22201:
Training Loss: -8.472058296203613
Reconstruction Loss: -3.417262077331543
Iteration 22301:
Training Loss: -8.48851490020752
Reconstruction Loss: -3.4173614978790283
Iteration 22401:
Training Loss: -8.50494384765625
Reconstruction Loss: -3.4174587726593018
Iteration 22501:
Training Loss: -8.521347045898438
Reconstruction Loss: -3.4175539016723633
Iteration 22601:
Training Loss: -8.53768253326416
Reconstruction Loss: -3.41764760017395
Iteration 22701:
Training Loss: -8.553955078125
Reconstruction Loss: -3.417738437652588
Iteration 22801:
Training Loss: -8.570236206054688
Reconstruction Loss: -3.4178273677825928
Iteration 22901:
Training Loss: -8.586490631103516
Reconstruction Loss: -3.417914628982544
Iteration 23001:
Training Loss: -8.6027193069458
Reconstruction Loss: -3.4180006980895996
Iteration 23101:
Training Loss: -8.618854522705078
Reconstruction Loss: -3.4180848598480225
Iteration 23201:
Training Loss: -8.634987831115723
Reconstruction Loss: -3.418168306350708
Iteration 23301:
Training Loss: -8.651045799255371
Reconstruction Loss: -3.418250560760498
Iteration 23401:
Training Loss: -8.667110443115234
Reconstruction Loss: -3.4183316230773926
Iteration 23501:
Training Loss: -8.68313980102539
Reconstruction Loss: -3.4184114933013916
Iteration 23601:
Training Loss: -8.699128150939941
Reconstruction Loss: -3.418490171432495
Iteration 23701:
Training Loss: -8.715113639831543
Reconstruction Loss: -3.418567657470703
Iteration 23801:
Training Loss: -8.731038093566895
Reconstruction Loss: -3.418644428253174
Iteration 23901:
Training Loss: -8.74694538116455
Reconstruction Loss: -3.4187204837799072
Iteration 24001:
Training Loss: -8.762794494628906
Reconstruction Loss: -3.418795108795166
Iteration 24101:
Training Loss: -8.778631210327148
Reconstruction Loss: -3.4188687801361084
Iteration 24201:
Training Loss: -8.794429779052734
Reconstruction Loss: -3.418940782546997
Iteration 24301:
Training Loss: -8.810172080993652
Reconstruction Loss: -3.419010877609253
Iteration 24401:
Training Loss: -8.825868606567383
Reconstruction Loss: -3.4190797805786133
Iteration 24501:
Training Loss: -8.841601371765137
Reconstruction Loss: -3.419147491455078
Iteration 24601:
Training Loss: -8.857269287109375
Reconstruction Loss: -3.4192140102386475
Iteration 24701:
Training Loss: -8.872916221618652
Reconstruction Loss: -3.4192795753479004
Iteration 24801:
Training Loss: -8.888531684875488
Reconstruction Loss: -3.419344186782837
Iteration 24901:
Training Loss: -8.90414047241211
Reconstruction Loss: -3.419407606124878
Iteration 25001:
Training Loss: -8.91968822479248
Reconstruction Loss: -3.4194695949554443
Iteration 25101:
Training Loss: -8.935164451599121
Reconstruction Loss: -3.419529676437378
Iteration 25201:
Training Loss: -8.95071792602539
Reconstruction Loss: -3.419588088989258
Iteration 25301:
Training Loss: -8.966126441955566
Reconstruction Loss: -3.4196457862854004
Iteration 25401:
Training Loss: -8.981588363647461
Reconstruction Loss: -3.4197025299072266
Iteration 25501:
Training Loss: -8.996971130371094
Reconstruction Loss: -3.4197585582733154
Iteration 25601:
Training Loss: -9.01233959197998
Reconstruction Loss: -3.4198129177093506
Iteration 25701:
Training Loss: -9.027710914611816
Reconstruction Loss: -3.419865608215332
Iteration 25801:
Training Loss: -9.04305362701416
Reconstruction Loss: -3.419917106628418
Iteration 25901:
Training Loss: -9.058296203613281
Reconstruction Loss: -3.4199671745300293
Iteration 26001:
Training Loss: -9.073582649230957
Reconstruction Loss: -3.420016288757324
Iteration 26101:
Training Loss: -9.088756561279297
Reconstruction Loss: -3.420064687728882
Iteration 26201:
Training Loss: -9.103970527648926
Reconstruction Loss: -3.420112371444702
Iteration 26301:
Training Loss: -9.11907958984375
Reconstruction Loss: -3.4201598167419434
Iteration 26401:
Training Loss: -9.13414478302002
Reconstruction Loss: -3.420206308364868
Iteration 26501:
Training Loss: -9.149251937866211
Reconstruction Loss: -3.4202520847320557
Iteration 26601:
Training Loss: -9.164308547973633
Reconstruction Loss: -3.420297145843506
Iteration 26701:
Training Loss: -9.179363250732422
Reconstruction Loss: -3.420341730117798
Iteration 26801:
Training Loss: -9.19430160522461
Reconstruction Loss: -3.4203855991363525
Iteration 26901:
Training Loss: -9.20934009552002
Reconstruction Loss: -3.420428991317749
Iteration 27001:
Training Loss: -9.224325180053711
Reconstruction Loss: -3.420471668243408
Iteration 27101:
Training Loss: -9.239267349243164
Reconstruction Loss: -3.420513391494751
Iteration 27201:
Training Loss: -9.254202842712402
Reconstruction Loss: -3.4205543994903564
Iteration 27301:
Training Loss: -9.269083976745605
Reconstruction Loss: -3.4205949306488037
Iteration 27401:
Training Loss: -9.283989906311035
Reconstruction Loss: -3.4206342697143555
Iteration 27501:
Training Loss: -9.29882526397705
Reconstruction Loss: -3.420673370361328
Iteration 27601:
Training Loss: -9.313732147216797
Reconstruction Loss: -3.4207115173339844
Iteration 27701:
Training Loss: -9.328544616699219
Reconstruction Loss: -3.420748233795166
Iteration 27801:
Training Loss: -9.343326568603516
Reconstruction Loss: -3.420783758163452
Iteration 27901:
Training Loss: -9.358075141906738
Reconstruction Loss: -3.4208180904388428
Iteration 28001:
Training Loss: -9.37276840209961
Reconstruction Loss: -3.4208507537841797
Iteration 28101:
Training Loss: -9.387469291687012
Reconstruction Loss: -3.4208824634552
Iteration 28201:
Training Loss: -9.402193069458008
Reconstruction Loss: -3.4209134578704834
Iteration 28301:
Training Loss: -9.416837692260742
Reconstruction Loss: -3.4209437370300293
Iteration 28401:
Training Loss: -9.431509971618652
Reconstruction Loss: -3.420973777770996
Iteration 28501:
Training Loss: -9.446111679077148
Reconstruction Loss: -3.4210033416748047
Iteration 28601:
Training Loss: -9.460732460021973
Reconstruction Loss: -3.421031951904297
Iteration 28701:
Training Loss: -9.475309371948242
Reconstruction Loss: -3.421060562133789
Iteration 28801:
Training Loss: -9.489886283874512
Reconstruction Loss: -3.421088218688965
Iteration 28901:
Training Loss: -9.504411697387695
Reconstruction Loss: -3.4211158752441406
Iteration 29001:
Training Loss: -9.518957138061523
Reconstruction Loss: -3.421142578125
Iteration 29101:
Training Loss: -9.533513069152832
Reconstruction Loss: -3.4211690425872803
Iteration 29201:
Training Loss: -9.548003196716309
Reconstruction Loss: -3.421194553375244
Iteration 29301:
Training Loss: -9.562458992004395
Reconstruction Loss: -3.42121958732605
Iteration 29401:
Training Loss: -9.576887130737305
Reconstruction Loss: -3.4212441444396973
Iteration 29501:
Training Loss: -9.591355323791504
Reconstruction Loss: -3.4212682247161865
Iteration 29601:
Training Loss: -9.605718612670898
Reconstruction Loss: -3.421292543411255
Iteration 29701:
Training Loss: -9.620105743408203
Reconstruction Loss: -3.421315908432007
Iteration 29801:
Training Loss: -9.634455680847168
Reconstruction Loss: -3.4213390350341797
Iteration 29901:
Training Loss: -9.64881420135498
Reconstruction Loss: -3.4213619232177734
Iteration 30001:
Training Loss: -9.66312313079834
Reconstruction Loss: -3.421383857727051
Iteration 30101:
Training Loss: -9.677396774291992
Reconstruction Loss: -3.421405553817749
Iteration 30201:
Training Loss: -9.6917085647583
Reconstruction Loss: -3.421426773071289
Iteration 30301:
Training Loss: -9.70589542388916
Reconstruction Loss: -3.42144775390625
Iteration 30401:
Training Loss: -9.720151901245117
Reconstruction Loss: -3.4214682579040527
Iteration 30501:
Training Loss: -9.734320640563965
Reconstruction Loss: -3.4214885234832764
Iteration 30601:
Training Loss: -9.74851131439209
Reconstruction Loss: -3.421508312225342
Iteration 30701:
Training Loss: -9.762665748596191
Reconstruction Loss: -3.421527862548828
Iteration 30801:
Training Loss: -9.776775360107422
Reconstruction Loss: -3.421546220779419
Iteration 30901:
Training Loss: -9.790899276733398
Reconstruction Loss: -3.4215641021728516
Iteration 31001:
Training Loss: -9.805030822753906
Reconstruction Loss: -3.4215822219848633
Iteration 31101:
Training Loss: -9.819067001342773
Reconstruction Loss: -3.4215993881225586
Iteration 31201:
Training Loss: -9.833159446716309
Reconstruction Loss: -3.421617031097412
Iteration 31301:
Training Loss: -9.847288131713867
Reconstruction Loss: -3.4216341972351074
Iteration 31401:
Training Loss: -9.861307144165039
Reconstruction Loss: -3.4216513633728027
Iteration 31501:
Training Loss: -9.875384330749512
Reconstruction Loss: -3.421668291091919
Iteration 31601:
Training Loss: -9.88943099975586
Reconstruction Loss: -3.421684741973877
Iteration 31701:
Training Loss: -9.903406143188477
Reconstruction Loss: -3.421701192855835
Iteration 31801:
Training Loss: -9.917314529418945
Reconstruction Loss: -3.4217166900634766
Iteration 31901:
Training Loss: -9.931315422058105
Reconstruction Loss: -3.421732187271118
Iteration 32001:
Training Loss: -9.94528579711914
Reconstruction Loss: -3.4217476844787598
Iteration 32101:
Training Loss: -9.959161758422852
Reconstruction Loss: -3.4217631816864014
Iteration 32201:
Training Loss: -9.973061561584473
Reconstruction Loss: -3.421778440475464
Iteration 32301:
Training Loss: -9.987070083618164
Reconstruction Loss: -3.4217934608459473
Iteration 32401:
Training Loss: -10.000929832458496
Reconstruction Loss: -3.4218084812164307
Iteration 32501:
Training Loss: -10.014752388000488
Reconstruction Loss: -3.421823263168335
Iteration 32601:
Training Loss: -10.02869987487793
Reconstruction Loss: -3.42183780670166
Iteration 32701:
Training Loss: -10.042515754699707
Reconstruction Loss: -3.4218525886535645
Iteration 32801:
Training Loss: -10.056336402893066
Reconstruction Loss: -3.4218666553497314
Iteration 32901:
Training Loss: -10.070210456848145
Reconstruction Loss: -3.4218807220458984
Iteration 33001:
Training Loss: -10.08398723602295
Reconstruction Loss: -3.4218945503234863
Iteration 33101:
Training Loss: -10.097724914550781
Reconstruction Loss: -3.421907663345337
Iteration 33201:
Training Loss: -10.111565589904785
Reconstruction Loss: -3.4219205379486084
Iteration 33301:
Training Loss: -10.125304222106934
Reconstruction Loss: -3.421933174133301
Iteration 33401:
Training Loss: -10.139087677001953
Reconstruction Loss: -3.421945571899414
Iteration 33501:
Training Loss: -10.15269660949707
Reconstruction Loss: -3.421957492828369
Iteration 33601:
Training Loss: -10.1664457321167
Reconstruction Loss: -3.421969413757324
Iteration 33701:
Training Loss: -10.180168151855469
Reconstruction Loss: -3.4219810962677
Iteration 33801:
Training Loss: -10.193753242492676
Reconstruction Loss: -3.421992301940918
Iteration 33901:
Training Loss: -10.207395553588867
Reconstruction Loss: -3.4220027923583984
Iteration 34001:
Training Loss: -10.221049308776855
Reconstruction Loss: -3.422013282775879
Iteration 34101:
Training Loss: -10.234715461730957
Reconstruction Loss: -3.4220235347747803
Iteration 34201:
Training Loss: -10.248237609863281
Reconstruction Loss: -3.4220335483551025
Iteration 34301:
Training Loss: -10.2618408203125
Reconstruction Loss: -3.422043800354004
Iteration 34401:
Training Loss: -10.275481224060059
Reconstruction Loss: -3.422053575515747
Iteration 34501:
Training Loss: -10.289093971252441
Reconstruction Loss: -3.422063112258911
Iteration 34601:
Training Loss: -10.302656173706055
Reconstruction Loss: -3.422072410583496
Iteration 34701:
Training Loss: -10.316303253173828
Reconstruction Loss: -3.422081232070923
Iteration 34801:
Training Loss: -10.329766273498535
Reconstruction Loss: -3.4220900535583496
Iteration 34901:
Training Loss: -10.343276977539062
Reconstruction Loss: -3.4220986366271973
Iteration 35001:
Training Loss: -10.356836318969727
Reconstruction Loss: -3.4221065044403076
Iteration 35101:
Training Loss: -10.370328903198242
Reconstruction Loss: -3.4221136569976807
Iteration 35201:
Training Loss: -10.38383674621582
Reconstruction Loss: -3.4221208095550537
Iteration 35301:
Training Loss: -10.397239685058594
Reconstruction Loss: -3.4221274852752686
Iteration 35401:
Training Loss: -10.410712242126465
Reconstruction Loss: -3.422133684158325
Iteration 35501:
Training Loss: -10.424159049987793
Reconstruction Loss: -3.4221396446228027
Iteration 35601:
Training Loss: -10.43759536743164
Reconstruction Loss: -3.4221456050872803
Iteration 35701:
Training Loss: -10.450901985168457
Reconstruction Loss: -3.422151803970337
Iteration 35801:
Training Loss: -10.464363098144531
Reconstruction Loss: -3.422157049179077
Iteration 35901:
Training Loss: -10.477696418762207
Reconstruction Loss: -3.4221625328063965
Iteration 36001:
Training Loss: -10.491093635559082
Reconstruction Loss: -3.422168016433716
Iteration 36101:
Training Loss: -10.504413604736328
Reconstruction Loss: -3.422173261642456
Iteration 36201:
Training Loss: -10.51775074005127
Reconstruction Loss: -3.4221785068511963
Iteration 36301:
Training Loss: -10.531048774719238
Reconstruction Loss: -3.4221835136413574
Iteration 36401:
Training Loss: -10.544358253479004
Reconstruction Loss: -3.4221887588500977
Iteration 36501:
Training Loss: -10.557605743408203
Reconstruction Loss: -3.4221935272216797
Iteration 36601:
Training Loss: -10.57091236114502
Reconstruction Loss: -3.422198534011841
Iteration 36701:
Training Loss: -10.584179878234863
Reconstruction Loss: -3.4222028255462646
Iteration 36801:
Training Loss: -10.59744930267334
Reconstruction Loss: -3.4222073554992676
Iteration 36901:
Training Loss: -10.610796928405762
Reconstruction Loss: -3.4222116470336914
Iteration 37001:
Training Loss: -10.624002456665039
Reconstruction Loss: -3.4222159385681152
Iteration 37101:
Training Loss: -10.637213706970215
Reconstruction Loss: -3.422220468521118
Iteration 37201:
Training Loss: -10.65040111541748
Reconstruction Loss: -3.422224521636963
Iteration 37301:
Training Loss: -10.663544654846191
Reconstruction Loss: -3.4222283363342285
Iteration 37401:
Training Loss: -10.676811218261719
Reconstruction Loss: -3.4222328662872314
Iteration 37501:
Training Loss: -10.689922332763672
Reconstruction Loss: -3.4222371578216553
Iteration 37601:
Training Loss: -10.703051567077637
Reconstruction Loss: -3.4222419261932373
Iteration 37701:
Training Loss: -10.716203689575195
Reconstruction Loss: -3.422245979309082
Iteration 37801:
Training Loss: -10.729293823242188
Reconstruction Loss: -3.422250270843506
Iteration 37901:
Training Loss: -10.742366790771484
Reconstruction Loss: -3.4222543239593506
Iteration 38001:
Training Loss: -10.7554931640625
Reconstruction Loss: -3.422258138656616
Iteration 38101:
Training Loss: -10.768499374389648
Reconstruction Loss: -3.422261953353882
Iteration 38201:
Training Loss: -10.781597137451172
Reconstruction Loss: -3.4222655296325684
Iteration 38301:
Training Loss: -10.794540405273438
Reconstruction Loss: -3.422269344329834
Iteration 38401:
Training Loss: -10.807586669921875
Reconstruction Loss: -3.4222724437713623
Iteration 38501:
Training Loss: -10.820643424987793
Reconstruction Loss: -3.4222757816314697
Iteration 38601:
Training Loss: -10.833574295043945
Reconstruction Loss: -3.422278642654419
Iteration 38701:
Training Loss: -10.846527099609375
Reconstruction Loss: -3.422281265258789
Iteration 38801:
Training Loss: -10.859457015991211
Reconstruction Loss: -3.4222843647003174
Iteration 38901:
Training Loss: -10.872466087341309
Reconstruction Loss: -3.4222867488861084
Iteration 39001:
Training Loss: -10.885346412658691
Reconstruction Loss: -3.4222891330718994
Iteration 39101:
Training Loss: -10.898350715637207
Reconstruction Loss: -3.4222919940948486
Iteration 39201:
Training Loss: -10.911294937133789
Reconstruction Loss: -3.4222943782806396
Iteration 39301:
Training Loss: -10.924245834350586
Reconstruction Loss: -3.4222965240478516
Iteration 39401:
Training Loss: -10.937089920043945
Reconstruction Loss: -3.4222986698150635
Iteration 39501:
Training Loss: -10.949954986572266
Reconstruction Loss: -3.422300338745117
Iteration 39601:
Training Loss: -10.962789535522461
Reconstruction Loss: -3.422302007675171
Iteration 39701:
Training Loss: -10.975750923156738
Reconstruction Loss: -3.4223034381866455
Iteration 39801:
Training Loss: -10.98855209350586
Reconstruction Loss: -3.42230486869812
Iteration 39901:
Training Loss: -11.001425743103027
Reconstruction Loss: -3.4223062992095947
Iteration 40001:
Training Loss: -11.014220237731934
Reconstruction Loss: -3.422307252883911
Iteration 40101:
Training Loss: -11.026962280273438
Reconstruction Loss: -3.4223084449768066
Iteration 40201:
Training Loss: -11.039806365966797
Reconstruction Loss: -3.422309398651123
Iteration 40301:
Training Loss: -11.052587509155273
Reconstruction Loss: -3.4223103523254395
Iteration 40401:
Training Loss: -11.065323829650879
Reconstruction Loss: -3.4223110675811768
Iteration 40501:
Training Loss: -11.078082084655762
Reconstruction Loss: -3.422312021255493
Iteration 40601:
Training Loss: -11.090813636779785
Reconstruction Loss: -3.4223127365112305
Iteration 40701:
Training Loss: -11.103586196899414
Reconstruction Loss: -3.422313690185547
Iteration 40801:
Training Loss: -11.116321563720703
Reconstruction Loss: -3.4223146438598633
Iteration 40901:
Training Loss: -11.129061698913574
Reconstruction Loss: -3.4223155975341797
Iteration 41001:
Training Loss: -11.141751289367676
Reconstruction Loss: -3.422316551208496
Iteration 41101:
Training Loss: -11.154435157775879
Reconstruction Loss: -3.4223179817199707
Iteration 41201:
Training Loss: -11.167157173156738
Reconstruction Loss: -3.422319173812866
Iteration 41301:
Training Loss: -11.179755210876465
Reconstruction Loss: -3.4223201274871826
Iteration 41401:
Training Loss: -11.192422866821289
Reconstruction Loss: -3.4223215579986572
Iteration 41501:
Training Loss: -11.204986572265625
Reconstruction Loss: -3.4223227500915527
Iteration 41601:
Training Loss: -11.217604637145996
Reconstruction Loss: -3.4223239421844482
Iteration 41701:
Training Loss: -11.230313301086426
Reconstruction Loss: -3.4223251342773438
Iteration 41801:
Training Loss: -11.242803573608398
Reconstruction Loss: -3.42232608795166
Iteration 41901:
Training Loss: -11.255426406860352
Reconstruction Loss: -3.4223270416259766
Iteration 42001:
Training Loss: -11.267970085144043
Reconstruction Loss: -3.422328233718872
Iteration 42101:
Training Loss: -11.280652046203613
Reconstruction Loss: -3.4223289489746094
Iteration 42201:
Training Loss: -11.292989730834961
Reconstruction Loss: -3.422330141067505
Iteration 42301:
Training Loss: -11.305508613586426
Reconstruction Loss: -3.422330617904663
Iteration 42401:
Training Loss: -11.31808090209961
Reconstruction Loss: -3.4223310947418213
Iteration 42501:
Training Loss: -11.330558776855469
Reconstruction Loss: -3.422332286834717
Iteration 42601:
Training Loss: -11.343119621276855
Reconstruction Loss: -3.422332763671875
Iteration 42701:
Training Loss: -11.35546588897705
Reconstruction Loss: -3.4223334789276123
Iteration 42801:
Training Loss: -11.368026733398438
Reconstruction Loss: -3.4223341941833496
Iteration 42901:
Training Loss: -11.380435943603516
Reconstruction Loss: -3.422334909439087
Iteration 43001:
Training Loss: -11.392876625061035
Reconstruction Loss: -3.422335386276245
Iteration 43101:
Training Loss: -11.405194282531738
Reconstruction Loss: -3.4223361015319824
Iteration 43201:
Training Loss: -11.417557716369629
Reconstruction Loss: -3.4223368167877197
Iteration 43301:
Training Loss: -11.429903030395508
Reconstruction Loss: -3.422337770462036
Iteration 43401:
Training Loss: -11.442516326904297
Reconstruction Loss: -3.4223382472991943
Iteration 43501:
Training Loss: -11.454645156860352
Reconstruction Loss: -3.4223392009735107
Iteration 43601:
Training Loss: -11.467097282409668
Reconstruction Loss: -3.422339677810669
Iteration 43701:
Training Loss: -11.479401588439941
Reconstruction Loss: -3.4223408699035645
Iteration 43801:
Training Loss: -11.491888046264648
Reconstruction Loss: -3.4223411083221436
Iteration 43901:
Training Loss: -11.504133224487305
Reconstruction Loss: -3.422342538833618
Iteration 44001:
Training Loss: -11.516504287719727
Reconstruction Loss: -3.4223430156707764
Iteration 44101:
Training Loss: -11.528754234313965
Reconstruction Loss: -3.4223432540893555
Iteration 44201:
Training Loss: -11.540997505187988
Reconstruction Loss: -3.4223437309265137
Iteration 44301:
Training Loss: -11.553360939025879
Reconstruction Loss: -3.422344207763672
Iteration 44401:
Training Loss: -11.565471649169922
Reconstruction Loss: -3.42234468460083
Iteration 44501:
Training Loss: -11.577783584594727
Reconstruction Loss: -3.422344923019409
Iteration 44601:
Training Loss: -11.589897155761719
Reconstruction Loss: -3.4223456382751465
Iteration 44701:
Training Loss: -11.602269172668457
Reconstruction Loss: -3.4223461151123047
Iteration 44801:
Training Loss: -11.614463806152344
Reconstruction Loss: -3.422346591949463
Iteration 44901:
Training Loss: -11.626630783081055
Reconstruction Loss: -3.4223473072052
Iteration 45001:
Training Loss: -11.63874626159668
Reconstruction Loss: -3.4223480224609375
Iteration 45101:
Training Loss: -11.650726318359375
Reconstruction Loss: -3.4223484992980957
Iteration 45201:
Training Loss: -11.662922859191895
Reconstruction Loss: -3.422349214553833
Iteration 45301:
Training Loss: -11.675124168395996
Reconstruction Loss: -3.422349691390991
Iteration 45401:
Training Loss: -11.68726921081543
Reconstruction Loss: -3.4223504066467285
Iteration 45501:
Training Loss: -11.699419021606445
Reconstruction Loss: -3.4223508834838867
Iteration 45601:
Training Loss: -11.711522102355957
Reconstruction Loss: -3.422351360321045
Iteration 45701:
Training Loss: -11.723612785339355
Reconstruction Loss: -3.422351598739624
Iteration 45801:
Training Loss: -11.735790252685547
Reconstruction Loss: -3.4223520755767822
Iteration 45901:
Training Loss: -11.747904777526855
Reconstruction Loss: -3.4223520755767822
Iteration 46001:
Training Loss: -11.760040283203125
Reconstruction Loss: -3.4223525524139404
Iteration 46101:
Training Loss: -11.772130966186523
Reconstruction Loss: -3.4223527908325195
Iteration 46201:
Training Loss: -11.78441333770752
Reconstruction Loss: -3.4223527908325195
Iteration 46301:
Training Loss: -11.796329498291016
Reconstruction Loss: -3.4223527908325195
Iteration 46401:
Training Loss: -11.808420181274414
Reconstruction Loss: -3.4223530292510986
Iteration 46501:
Training Loss: -11.820283889770508
Reconstruction Loss: -3.4223532676696777
Iteration 46601:
Training Loss: -11.83243179321289
Reconstruction Loss: -3.4223530292510986
Iteration 46701:
Training Loss: -11.844562530517578
Reconstruction Loss: -3.4223530292510986
Iteration 46801:
Training Loss: -11.856708526611328
Reconstruction Loss: -3.4223530292510986
Iteration 46901:
Training Loss: -11.868744850158691
Reconstruction Loss: -3.4223527908325195
Iteration 47001:
Training Loss: -11.880646705627441
Reconstruction Loss: -3.4223520755767822
Iteration 47101:
Training Loss: -11.89253044128418
Reconstruction Loss: -3.422351837158203
Iteration 47201:
Training Loss: -11.904454231262207
Reconstruction Loss: -3.422351360321045
Iteration 47301:
Training Loss: -11.916396141052246
Reconstruction Loss: -3.4223506450653076
Iteration 47401:
Training Loss: -11.928218841552734
Reconstruction Loss: -3.4223499298095703
Iteration 47501:
Training Loss: -11.940058708190918
Reconstruction Loss: -3.422349214553833
Iteration 47601:
Training Loss: -11.951899528503418
Reconstruction Loss: -3.4223484992980957
Iteration 47701:
Training Loss: -11.963845252990723
Reconstruction Loss: -3.4223475456237793
Iteration 47801:
Training Loss: -11.97554874420166
Reconstruction Loss: -3.422346591949463
Iteration 47901:
Training Loss: -11.987283706665039
Reconstruction Loss: -3.4223458766937256
Iteration 48001:
Training Loss: -11.999408721923828
Reconstruction Loss: -3.422344923019409
Iteration 48101:
Training Loss: -12.010926246643066
Reconstruction Loss: -3.4223437309265137
Iteration 48201:
Training Loss: -12.023061752319336
Reconstruction Loss: -3.4223430156707764
Iteration 48301:
Training Loss: -12.034465789794922
Reconstruction Loss: -3.42234206199646
Iteration 48401:
Training Loss: -12.046194076538086
Reconstruction Loss: -3.4223413467407227
Iteration 48501:
Training Loss: -12.057910919189453
Reconstruction Loss: -3.422340154647827
Iteration 48601:
Training Loss: -12.069597244262695
Reconstruction Loss: -3.4223392009735107
Iteration 48701:
Training Loss: -12.08130931854248
Reconstruction Loss: -3.4223382472991943
Iteration 48801:
Training Loss: -12.093132019042969
Reconstruction Loss: -3.422337532043457
Iteration 48901:
Training Loss: -12.1045560836792
Reconstruction Loss: -3.4223365783691406
Iteration 49001:
Training Loss: -12.116128921508789
Reconstruction Loss: -3.422335624694824
Iteration 49101:
Training Loss: -12.127843856811523
Reconstruction Loss: -3.422334909439087
Iteration 49201:
Training Loss: -12.139599800109863
Reconstruction Loss: -3.4223339557647705
Iteration 49301:
Training Loss: -12.151155471801758
Reconstruction Loss: -3.422333002090454
Iteration 49401:
Training Loss: -12.16263198852539
Reconstruction Loss: -3.422332286834717
Iteration 49501:
Training Loss: -12.17432975769043
Reconstruction Loss: -3.4223313331604004
Iteration 49601:
Training Loss: -12.185871124267578
Reconstruction Loss: -3.422330379486084
Iteration 49701:
Training Loss: -12.197778701782227
Reconstruction Loss: -3.4223294258117676
Iteration 49801:
Training Loss: -12.20898723602295
Reconstruction Loss: -3.422328233718872
Iteration 49901:
Training Loss: -12.220765113830566
Reconstruction Loss: -3.4223272800445557
