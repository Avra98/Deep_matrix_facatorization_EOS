5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.148070335388184
Reconstruction Loss: -0.7327531576156616
Iteration 101:
Training Loss: 5.147355079650879
Reconstruction Loss: -0.7330242395401001
Iteration 201:
Training Loss: 5.146425724029541
Reconstruction Loss: -0.7334079742431641
Iteration 301:
Training Loss: 5.144789218902588
Reconstruction Loss: -0.7341469526290894
Iteration 401:
Training Loss: 5.140478134155273
Reconstruction Loss: -0.7362530827522278
Iteration 501:
Training Loss: 5.109975814819336
Reconstruction Loss: -0.7517271637916565
Iteration 601:
Training Loss: 4.710684776306152
Reconstruction Loss: -0.8079542517662048
Iteration 701:
Training Loss: 4.1631855964660645
Reconstruction Loss: -0.984505295753479
Iteration 801:
Training Loss: 3.8465843200683594
Reconstruction Loss: -1.1497018337249756
Iteration 901:
Training Loss: 3.6713571548461914
Reconstruction Loss: -1.2816513776779175
Iteration 1001:
Training Loss: 3.503030776977539
Reconstruction Loss: -1.395184874534607
Iteration 1101:
Training Loss: 3.1049489974975586
Reconstruction Loss: -1.6524198055267334
Iteration 1201:
Training Loss: 2.897007703781128
Reconstruction Loss: -1.8392592668533325
Iteration 1301:
Training Loss: 2.792651891708374
Reconstruction Loss: -1.9204416275024414
Iteration 1401:
Training Loss: 2.724838972091675
Reconstruction Loss: -1.9222842454910278
Iteration 1501:
Training Loss: 2.677565574645996
Reconstruction Loss: -1.8906900882720947
Iteration 1601:
Training Loss: 2.6312270164489746
Reconstruction Loss: -1.8644282817840576
Iteration 1701:
Training Loss: 2.5348353385925293
Reconstruction Loss: -1.8837441205978394
Iteration 1801:
Training Loss: 2.1080894470214844
Reconstruction Loss: -2.1213831901550293
Iteration 1901:
Training Loss: 1.0681676864624023
Reconstruction Loss: -2.8727200031280518
Iteration 2001:
Training Loss: 0.13105596601963043
Reconstruction Loss: -3.652705430984497
Iteration 2101:
Training Loss: -0.6912885308265686
Reconstruction Loss: -4.3485212326049805
Iteration 2201:
Training Loss: -1.418751835823059
Reconstruction Loss: -4.970125675201416
Iteration 2301:
Training Loss: -2.064070701599121
Reconstruction Loss: -5.533895015716553
Iteration 2401:
Training Loss: -2.6436610221862793
Reconstruction Loss: -6.055860996246338
Iteration 2501:
Training Loss: -3.17362904548645
Reconstruction Loss: -6.548235893249512
Iteration 2601:
Training Loss: -3.6659464836120605
Reconstruction Loss: -7.019125938415527
Iteration 2701:
Training Loss: -4.127891540527344
Reconstruction Loss: -7.473448753356934
Iteration 2801:
Training Loss: -4.562786102294922
Reconstruction Loss: -7.913763046264648
Iteration 2901:
Training Loss: -4.970826625823975
Reconstruction Loss: -8.340730667114258
Iteration 3001:
Training Loss: -5.349785327911377
Reconstruction Loss: -8.7532377243042
Iteration 3101:
Training Loss: -5.69564962387085
Reconstruction Loss: -9.148348808288574
Iteration 3201:
Training Loss: -6.003633975982666
Reconstruction Loss: -9.52125358581543
Iteration 3301:
Training Loss: -6.269480228424072
Reconstruction Loss: -9.865426063537598
Iteration 3401:
Training Loss: -6.4909281730651855
Reconstruction Loss: -10.173405647277832
Iteration 3501:
Training Loss: -6.668668746948242
Reconstruction Loss: -10.43828296661377
Iteration 3601:
Training Loss: -6.806464195251465
Reconstruction Loss: -10.655689239501953
Iteration 3701:
Training Loss: -6.910183906555176
Reconstruction Loss: -10.825331687927246
Iteration 3801:
Training Loss: -6.9866180419921875
Reconstruction Loss: -10.951272964477539
Iteration 3901:
Training Loss: -7.042203903198242
Reconstruction Loss: -11.0405912399292
Iteration 4001:
Training Loss: -7.082523345947266
Reconstruction Loss: -11.101544380187988
Iteration 4101:
Training Loss: -7.111942291259766
Reconstruction Loss: -11.141857147216797
Iteration 4201:
Training Loss: -7.133738040924072
Reconstruction Loss: -11.167917251586914
Iteration 4301:
Training Loss: -7.150256633758545
Reconstruction Loss: -11.184540748596191
Iteration 4401:
Training Loss: -7.163178443908691
Reconstruction Loss: -11.1951265335083
Iteration 4501:
Training Loss: -7.173668384552002
Reconstruction Loss: -11.20199966430664
Iteration 4601:
Training Loss: -7.182466506958008
Reconstruction Loss: -11.206704139709473
Iteration 4701:
Training Loss: -7.190147399902344
Reconstruction Loss: -11.210184097290039
Iteration 4801:
Training Loss: -7.197048187255859
Reconstruction Loss: -11.213086128234863
Iteration 4901:
Training Loss: -7.203433036804199
Reconstruction Loss: -11.215764999389648
Iteration 5001:
Training Loss: -7.209444999694824
Reconstruction Loss: -11.218438148498535
Iteration 5101:
Training Loss: -7.215196132659912
Reconstruction Loss: -11.221240997314453
Iteration 5201:
Training Loss: -7.22078275680542
Reconstruction Loss: -11.224213600158691
Iteration 5301:
Training Loss: -7.226264953613281
Reconstruction Loss: -11.227374076843262
Iteration 5401:
Training Loss: -7.231629848480225
Reconstruction Loss: -11.230661392211914
Iteration 5501:
Training Loss: -7.236917495727539
Reconstruction Loss: -11.234125137329102
Iteration 5601:
Training Loss: -7.242146015167236
Reconstruction Loss: -11.237777709960938
Iteration 5701:
Training Loss: -7.247352123260498
Reconstruction Loss: -11.241546630859375
Iteration 5801:
Training Loss: -7.2525153160095215
Reconstruction Loss: -11.245438575744629
Iteration 5901:
Training Loss: -7.25763463973999
Reconstruction Loss: -11.249399185180664
Iteration 6001:
Training Loss: -7.262736797332764
Reconstruction Loss: -11.253406524658203
Iteration 6101:
Training Loss: -7.267807960510254
Reconstruction Loss: -11.257494926452637
Iteration 6201:
Training Loss: -7.27285623550415
Reconstruction Loss: -11.261658668518066
Iteration 6301:
Training Loss: -7.277872562408447
Reconstruction Loss: -11.26584243774414
Iteration 6401:
Training Loss: -7.282875061035156
Reconstruction Loss: -11.270039558410645
Iteration 6501:
Training Loss: -7.287834644317627
Reconstruction Loss: -11.274272918701172
Iteration 6601:
Training Loss: -7.292814254760742
Reconstruction Loss: -11.278532028198242
Iteration 6701:
Training Loss: -7.297731399536133
Reconstruction Loss: -11.282800674438477
Iteration 6801:
Training Loss: -7.302628040313721
Reconstruction Loss: -11.287067413330078
Iteration 6901:
Training Loss: -7.307521820068359
Reconstruction Loss: -11.291338920593262
Iteration 7001:
Training Loss: -7.312392234802246
Reconstruction Loss: -11.295592308044434
Iteration 7101:
Training Loss: -7.317227363586426
Reconstruction Loss: -11.299843788146973
Iteration 7201:
Training Loss: -7.3220624923706055
Reconstruction Loss: -11.304082870483398
Iteration 7301:
Training Loss: -7.326876163482666
Reconstruction Loss: -11.308323860168457
Iteration 7401:
Training Loss: -7.331678867340088
Reconstruction Loss: -11.312549591064453
Iteration 7501:
Training Loss: -7.33643913269043
Reconstruction Loss: -11.316757202148438
Iteration 7601:
Training Loss: -7.341180801391602
Reconstruction Loss: -11.320951461791992
Iteration 7701:
Training Loss: -7.345901966094971
Reconstruction Loss: -11.325121879577637
Iteration 7801:
Training Loss: -7.350614547729492
Reconstruction Loss: -11.329276084899902
Iteration 7901:
Training Loss: -7.355301856994629
Reconstruction Loss: -11.33342456817627
Iteration 8001:
Training Loss: -7.359947681427002
Reconstruction Loss: -11.337541580200195
Iteration 8101:
Training Loss: -7.364591121673584
Reconstruction Loss: -11.341647148132324
Iteration 8201:
Training Loss: -7.3692240715026855
Reconstruction Loss: -11.34573745727539
Iteration 8301:
Training Loss: -7.373846530914307
Reconstruction Loss: -11.349811553955078
Iteration 8401:
Training Loss: -7.378408908843994
Reconstruction Loss: -11.353860855102539
Iteration 8501:
Training Loss: -7.382970333099365
Reconstruction Loss: -11.35789680480957
Iteration 8601:
Training Loss: -7.387521266937256
Reconstruction Loss: -11.361918449401855
Iteration 8701:
Training Loss: -7.392050743103027
Reconstruction Loss: -11.365914344787598
Iteration 8801:
Training Loss: -7.396565914154053
Reconstruction Loss: -11.369898796081543
Iteration 8901:
Training Loss: -7.401070594787598
Reconstruction Loss: -11.373861312866211
Iteration 9001:
Training Loss: -7.405543327331543
Reconstruction Loss: -11.37781810760498
Iteration 9101:
Training Loss: -7.410009384155273
Reconstruction Loss: -11.381738662719727
Iteration 9201:
Training Loss: -7.414434909820557
Reconstruction Loss: -11.38565731048584
Iteration 9301:
Training Loss: -7.418879985809326
Reconstruction Loss: -11.389558792114258
Iteration 9401:
Training Loss: -7.423274517059326
Reconstruction Loss: -11.393436431884766
Iteration 9501:
Training Loss: -7.4276556968688965
Reconstruction Loss: -11.397318840026855
Iteration 9601:
Training Loss: -7.432041645050049
Reconstruction Loss: -11.401188850402832
Iteration 9701:
Training Loss: -7.4364013671875
Reconstruction Loss: -11.405034065246582
Iteration 9801:
Training Loss: -7.440756320953369
Reconstruction Loss: -11.40887451171875
Iteration 9901:
Training Loss: -7.445055961608887
Reconstruction Loss: -11.412676811218262
Iteration 10001:
Training Loss: -7.449376583099365
Reconstruction Loss: -11.41645622253418
Iteration 10101:
Training Loss: -7.453664779663086
Reconstruction Loss: -11.420214653015137
Iteration 10201:
Training Loss: -7.457940101623535
Reconstruction Loss: -11.423968315124512
Iteration 10301:
Training Loss: -7.46220588684082
Reconstruction Loss: -11.42770767211914
Iteration 10401:
Training Loss: -7.466444492340088
Reconstruction Loss: -11.431437492370605
Iteration 10501:
Training Loss: -7.470644950866699
Reconstruction Loss: -11.435161590576172
Iteration 10601:
Training Loss: -7.474858283996582
Reconstruction Loss: -11.438859939575195
Iteration 10701:
Training Loss: -7.479064464569092
Reconstruction Loss: -11.442541122436523
Iteration 10801:
Training Loss: -7.483231067657471
Reconstruction Loss: -11.44620132446289
Iteration 10901:
Training Loss: -7.4873857498168945
Reconstruction Loss: -11.449853897094727
Iteration 11001:
Training Loss: -7.491561412811279
Reconstruction Loss: -11.453483581542969
Iteration 11101:
Training Loss: -7.495668888092041
Reconstruction Loss: -11.457098007202148
Iteration 11201:
Training Loss: -7.499802112579346
Reconstruction Loss: -11.460707664489746
Iteration 11301:
Training Loss: -7.5038981437683105
Reconstruction Loss: -11.464323043823242
Iteration 11401:
Training Loss: -7.5079851150512695
Reconstruction Loss: -11.467921257019043
Iteration 11501:
Training Loss: -7.512076377868652
Reconstruction Loss: -11.471525192260742
Iteration 11601:
Training Loss: -7.516136646270752
Reconstruction Loss: -11.475107192993164
Iteration 11701:
Training Loss: -7.520174026489258
Reconstruction Loss: -11.478670120239258
Iteration 11801:
Training Loss: -7.524210453033447
Reconstruction Loss: -11.48220443725586
Iteration 11901:
Training Loss: -7.528217792510986
Reconstruction Loss: -11.485726356506348
Iteration 12001:
Training Loss: -7.5322113037109375
Reconstruction Loss: -11.489213943481445
Iteration 12101:
Training Loss: -7.536194801330566
Reconstruction Loss: -11.492692947387695
Iteration 12201:
Training Loss: -7.540167331695557
Reconstruction Loss: -11.496147155761719
Iteration 12301:
Training Loss: -7.5441107749938965
Reconstruction Loss: -11.499593734741211
Iteration 12401:
Training Loss: -7.548037052154541
Reconstruction Loss: -11.503026008605957
Iteration 12501:
Training Loss: -7.551968097686768
Reconstruction Loss: -11.506457328796387
Iteration 12601:
Training Loss: -7.555880069732666
Reconstruction Loss: -11.50987434387207
Iteration 12701:
Training Loss: -7.559782028198242
Reconstruction Loss: -11.513269424438477
Iteration 12801:
Training Loss: -7.56366491317749
Reconstruction Loss: -11.5166597366333
Iteration 12901:
Training Loss: -7.567546367645264
Reconstruction Loss: -11.520034790039062
Iteration 13001:
Training Loss: -7.571403503417969
Reconstruction Loss: -11.523394584655762
Iteration 13101:
Training Loss: -7.575238227844238
Reconstruction Loss: -11.526744842529297
Iteration 13201:
Training Loss: -7.579102993011475
Reconstruction Loss: -11.530081748962402
Iteration 13301:
Training Loss: -7.582925319671631
Reconstruction Loss: -11.533414840698242
Iteration 13401:
Training Loss: -7.586721897125244
Reconstruction Loss: -11.536730766296387
Iteration 13501:
Training Loss: -7.590526580810547
Reconstruction Loss: -11.540038108825684
Iteration 13601:
Training Loss: -7.594281196594238
Reconstruction Loss: -11.543328285217285
Iteration 13701:
Training Loss: -7.5980634689331055
Reconstruction Loss: -11.546613693237305
Iteration 13801:
Training Loss: -7.601818561553955
Reconstruction Loss: -11.549880027770996
Iteration 13901:
Training Loss: -7.605574131011963
Reconstruction Loss: -11.55313777923584
Iteration 14001:
Training Loss: -7.609299659729004
Reconstruction Loss: -11.556391716003418
Iteration 14101:
Training Loss: -7.613027095794678
Reconstruction Loss: -11.559618949890137
Iteration 14201:
Training Loss: -7.616738319396973
Reconstruction Loss: -11.562837600708008
Iteration 14301:
Training Loss: -7.620436191558838
Reconstruction Loss: -11.566046714782715
Iteration 14401:
Training Loss: -7.624103546142578
Reconstruction Loss: -11.569254875183105
Iteration 14501:
Training Loss: -7.627767086029053
Reconstruction Loss: -11.5724515914917
Iteration 14601:
Training Loss: -7.631453990936279
Reconstruction Loss: -11.575636863708496
Iteration 14701:
Training Loss: -7.635092258453369
Reconstruction Loss: -11.578802108764648
Iteration 14801:
Training Loss: -7.638716220855713
Reconstruction Loss: -11.581961631774902
Iteration 14901:
Training Loss: -7.642341136932373
Reconstruction Loss: -11.58510971069336
