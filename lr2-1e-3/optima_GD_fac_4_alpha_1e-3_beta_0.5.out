5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.460655212402344
Reconstruction Loss: -0.560798168182373
Iteration 101:
Training Loss: 3.8518619537353516
Reconstruction Loss: -0.9212018251419067
Iteration 201:
Training Loss: 2.5268869400024414
Reconstruction Loss: -1.4212565422058105
Iteration 301:
Training Loss: 1.679396152496338
Reconstruction Loss: -1.8417255878448486
Iteration 401:
Training Loss: 1.0611543655395508
Reconstruction Loss: -2.215984582901001
Iteration 501:
Training Loss: 0.563262403011322
Reconstruction Loss: -2.547720193862915
Iteration 601:
Training Loss: 0.12937133014202118
Reconstruction Loss: -2.843201160430908
Iteration 701:
Training Loss: -0.253915935754776
Reconstruction Loss: -3.102799892425537
Iteration 801:
Training Loss: -0.5904540419578552
Reconstruction Loss: -3.3273162841796875
Iteration 901:
Training Loss: -0.8839499950408936
Reconstruction Loss: -3.519657850265503
Iteration 1001:
Training Loss: -1.139225959777832
Reconstruction Loss: -3.6841838359832764
Iteration 1101:
Training Loss: -1.3616175651550293
Reconstruction Loss: -3.8255953788757324
Iteration 1201:
Training Loss: -1.5563108921051025
Reconstruction Loss: -3.948195219039917
Iteration 1301:
Training Loss: -1.7279555797576904
Reconstruction Loss: -4.055593967437744
Iteration 1401:
Training Loss: -1.8804973363876343
Reconstruction Loss: -4.150688648223877
Iteration 1501:
Training Loss: -2.0171971321105957
Reconstruction Loss: -4.235754013061523
Iteration 1601:
Training Loss: -2.1406874656677246
Reconstruction Loss: -4.312564373016357
Iteration 1701:
Training Loss: -2.2530875205993652
Reconstruction Loss: -4.382504940032959
Iteration 1801:
Training Loss: -2.3561019897460938
Reconstruction Loss: -4.4466657638549805
Iteration 1901:
Training Loss: -2.451108932495117
Reconstruction Loss: -4.5059099197387695
Iteration 2001:
Training Loss: -2.539227247238159
Reconstruction Loss: -4.560930252075195
Iteration 2101:
Training Loss: -2.6213769912719727
Reconstruction Loss: -4.612286567687988
Iteration 2201:
Training Loss: -2.6983189582824707
Reconstruction Loss: -4.660438537597656
Iteration 2301:
Training Loss: -2.7706897258758545
Reconstruction Loss: -4.70576286315918
Iteration 2401:
Training Loss: -2.8390259742736816
Reconstruction Loss: -4.74857759475708
Iteration 2501:
Training Loss: -2.903775691986084
Reconstruction Loss: -4.78914737701416
Iteration 2601:
Training Loss: -2.965331554412842
Reconstruction Loss: -4.827698230743408
Iteration 2701:
Training Loss: -3.0240187644958496
Reconstruction Loss: -4.86442232131958
Iteration 2801:
Training Loss: -3.080124855041504
Reconstruction Loss: -4.899486064910889
Iteration 2901:
Training Loss: -3.1338980197906494
Reconstruction Loss: -4.933032989501953
Iteration 3001:
Training Loss: -3.1855521202087402
Reconstruction Loss: -4.9651875495910645
Iteration 3101:
Training Loss: -3.235273599624634
Reconstruction Loss: -4.996062278747559
Iteration 3201:
Training Loss: -3.2832305431365967
Reconstruction Loss: -5.025752067565918
Iteration 3301:
Training Loss: -3.3295669555664062
Reconstruction Loss: -5.054342746734619
Iteration 3401:
Training Loss: -3.3744072914123535
Reconstruction Loss: -5.0819091796875
Iteration 3501:
Training Loss: -3.4178717136383057
Reconstruction Loss: -5.108524322509766
Iteration 3601:
Training Loss: -3.4600560665130615
Reconstruction Loss: -5.134247303009033
Iteration 3701:
Training Loss: -3.501051664352417
Reconstruction Loss: -5.159131050109863
Iteration 3801:
Training Loss: -3.540942430496216
Reconstruction Loss: -5.183228492736816
Iteration 3901:
Training Loss: -3.5797998905181885
Reconstruction Loss: -5.20658540725708
Iteration 4001:
Training Loss: -3.617685556411743
Reconstruction Loss: -5.229242324829102
Iteration 4101:
Training Loss: -3.6546707153320312
Reconstruction Loss: -5.2512359619140625
Iteration 4201:
Training Loss: -3.690793037414551
Reconstruction Loss: -5.272599220275879
Iteration 4301:
Training Loss: -3.726111888885498
Reconstruction Loss: -5.293367862701416
Iteration 4401:
Training Loss: -3.760671615600586
Reconstruction Loss: -5.313571929931641
Iteration 4501:
Training Loss: -3.794511079788208
Reconstruction Loss: -5.3332343101501465
Iteration 4601:
Training Loss: -3.827667236328125
Reconstruction Loss: -5.352384567260742
Iteration 4701:
Training Loss: -3.8601765632629395
Reconstruction Loss: -5.371041297912598
Iteration 4801:
Training Loss: -3.89206600189209
Reconstruction Loss: -5.389231204986572
Iteration 4901:
Training Loss: -3.9233646392822266
Reconstruction Loss: -5.4069695472717285
Iteration 5001:
Training Loss: -3.9541091918945312
Reconstruction Loss: -5.424278259277344
Iteration 5101:
Training Loss: -3.984316110610962
Reconstruction Loss: -5.441176414489746
Iteration 5201:
Training Loss: -4.014007091522217
Reconstruction Loss: -5.457674980163574
Iteration 5301:
Training Loss: -4.043209552764893
Reconstruction Loss: -5.473795413970947
Iteration 5401:
Training Loss: -4.071938991546631
Reconstruction Loss: -5.48954963684082
Iteration 5501:
Training Loss: -4.100216388702393
Reconstruction Loss: -5.504954814910889
Iteration 5601:
Training Loss: -4.128063201904297
Reconstruction Loss: -5.520014762878418
Iteration 5701:
Training Loss: -4.155489444732666
Reconstruction Loss: -5.534748077392578
Iteration 5801:
Training Loss: -4.182506561279297
Reconstruction Loss: -5.549167156219482
Iteration 5901:
Training Loss: -4.209140300750732
Reconstruction Loss: -5.563281059265137
Iteration 6001:
Training Loss: -4.235398292541504
Reconstruction Loss: -5.577102184295654
Iteration 6101:
Training Loss: -4.261292457580566
Reconstruction Loss: -5.590639114379883
Iteration 6201:
Training Loss: -4.286836624145508
Reconstruction Loss: -5.6038994789123535
Iteration 6301:
Training Loss: -4.312039375305176
Reconstruction Loss: -5.616893768310547
Iteration 6401:
Training Loss: -4.336913585662842
Reconstruction Loss: -5.62963342666626
Iteration 6501:
Training Loss: -4.361471652984619
Reconstruction Loss: -5.642117977142334
Iteration 6601:
Training Loss: -4.385713577270508
Reconstruction Loss: -5.654362201690674
Iteration 6701:
Training Loss: -4.4096598625183105
Reconstruction Loss: -5.666373252868652
Iteration 6801:
Training Loss: -4.433311939239502
Reconstruction Loss: -5.678155899047852
Iteration 6901:
Training Loss: -4.45668363571167
Reconstruction Loss: -5.6897149085998535
Iteration 7001:
Training Loss: -4.479779243469238
Reconstruction Loss: -5.7010626792907715
Iteration 7101:
Training Loss: -4.502603530883789
Reconstruction Loss: -5.7122015953063965
Iteration 7201:
Training Loss: -4.52517032623291
Reconstruction Loss: -5.723138809204102
Iteration 7301:
Training Loss: -4.547484397888184
Reconstruction Loss: -5.733874320983887
Iteration 7401:
Training Loss: -4.569545745849609
Reconstruction Loss: -5.744422912597656
Iteration 7501:
Training Loss: -4.591363906860352
Reconstruction Loss: -5.754781723022461
Iteration 7601:
Training Loss: -4.612948894500732
Reconstruction Loss: -5.764963150024414
Iteration 7701:
Training Loss: -4.634303092956543
Reconstruction Loss: -5.774965286254883
Iteration 7801:
Training Loss: -4.655430316925049
Reconstruction Loss: -5.784796714782715
Iteration 7901:
Training Loss: -4.676339149475098
Reconstruction Loss: -5.794458866119385
Iteration 8001:
Training Loss: -4.6970319747924805
Reconstruction Loss: -5.803958892822266
Iteration 8101:
Training Loss: -4.717509746551514
Reconstruction Loss: -5.813298225402832
Iteration 8201:
Training Loss: -4.737785816192627
Reconstruction Loss: -5.8224873542785645
Iteration 8301:
Training Loss: -4.75786018371582
Reconstruction Loss: -5.831518650054932
Iteration 8401:
Training Loss: -4.777738571166992
Reconstruction Loss: -5.840404510498047
Iteration 8501:
Training Loss: -4.797418117523193
Reconstruction Loss: -5.849146366119385
Iteration 8601:
Training Loss: -4.8169097900390625
Reconstruction Loss: -5.857748508453369
Iteration 8701:
Training Loss: -4.83621072769165
Reconstruction Loss: -5.866212368011475
Iteration 8801:
Training Loss: -4.855340957641602
Reconstruction Loss: -5.874547958374023
Iteration 8901:
Training Loss: -4.874290466308594
Reconstruction Loss: -5.882744789123535
Iteration 9001:
Training Loss: -4.89306116104126
Reconstruction Loss: -5.890819072723389
Iteration 9101:
Training Loss: -4.911653995513916
Reconstruction Loss: -5.898763656616211
Iteration 9201:
Training Loss: -4.930086612701416
Reconstruction Loss: -5.906589031219482
Iteration 9301:
Training Loss: -4.94834566116333
Reconstruction Loss: -5.914297103881836
Iteration 9401:
Training Loss: -4.966448783874512
Reconstruction Loss: -5.921882152557373
Iteration 9501:
Training Loss: -4.984388828277588
Reconstruction Loss: -5.929357051849365
Iteration 9601:
Training Loss: -5.002173900604248
Reconstruction Loss: -5.936717987060547
Iteration 9701:
Training Loss: -5.019799709320068
Reconstruction Loss: -5.943971633911133
Iteration 9801:
Training Loss: -5.037276268005371
Reconstruction Loss: -5.9511237144470215
Iteration 9901:
Training Loss: -5.054603576660156
Reconstruction Loss: -5.958164215087891
Iteration 10001:
Training Loss: -5.071782112121582
Reconstruction Loss: -5.965106010437012
Iteration 10101:
Training Loss: -5.088815689086914
Reconstruction Loss: -5.971945285797119
Iteration 10201:
Training Loss: -5.105712413787842
Reconstruction Loss: -5.978682994842529
Iteration 10301:
Training Loss: -5.122460842132568
Reconstruction Loss: -5.98532772064209
Iteration 10401:
Training Loss: -5.139074325561523
Reconstruction Loss: -5.991881370544434
Iteration 10501:
Training Loss: -5.155552387237549
Reconstruction Loss: -5.998342037200928
Iteration 10601:
Training Loss: -5.171900272369385
Reconstruction Loss: -6.0047173500061035
Iteration 10701:
Training Loss: -5.188112258911133
Reconstruction Loss: -6.010998725891113
Iteration 10801:
Training Loss: -5.204192638397217
Reconstruction Loss: -6.017194747924805
Iteration 10901:
Training Loss: -5.220146656036377
Reconstruction Loss: -6.023301601409912
Iteration 11001:
Training Loss: -5.235977649688721
Reconstruction Loss: -6.029330253601074
Iteration 11101:
Training Loss: -5.251684188842773
Reconstruction Loss: -6.035274982452393
Iteration 11201:
Training Loss: -5.267268657684326
Reconstruction Loss: -6.0411376953125
Iteration 11301:
Training Loss: -5.282721519470215
Reconstruction Loss: -6.0469207763671875
Iteration 11401:
Training Loss: -5.29806661605835
Reconstruction Loss: -6.052628993988037
Iteration 11501:
Training Loss: -5.313291549682617
Reconstruction Loss: -6.058259010314941
Iteration 11601:
Training Loss: -5.328395843505859
Reconstruction Loss: -6.063816547393799
Iteration 11701:
Training Loss: -5.343394756317139
Reconstruction Loss: -6.069301605224609
Iteration 11801:
Training Loss: -5.358270168304443
Reconstruction Loss: -6.07472038269043
Iteration 11901:
Training Loss: -5.373046398162842
Reconstruction Loss: -6.0800676345825195
Iteration 12001:
Training Loss: -5.387704372406006
Reconstruction Loss: -6.0853400230407715
Iteration 12101:
Training Loss: -5.402256011962891
Reconstruction Loss: -6.090549945831299
Iteration 12201:
Training Loss: -5.416705131530762
Reconstruction Loss: -6.095694065093994
Iteration 12301:
Training Loss: -5.431041717529297
Reconstruction Loss: -6.100771903991699
Iteration 12401:
Training Loss: -5.445282459259033
Reconstruction Loss: -6.1057915687561035
Iteration 12501:
Training Loss: -5.459412097930908
Reconstruction Loss: -6.110743999481201
Iteration 12601:
Training Loss: -5.473439693450928
Reconstruction Loss: -6.115633964538574
Iteration 12701:
Training Loss: -5.487370491027832
Reconstruction Loss: -6.120463848114014
Iteration 12801:
Training Loss: -5.501202583312988
Reconstruction Loss: -6.125237941741943
Iteration 12901:
Training Loss: -5.5149431228637695
Reconstruction Loss: -6.129951000213623
Iteration 13001:
Training Loss: -5.528576374053955
Reconstruction Loss: -6.134603977203369
Iteration 13101:
Training Loss: -5.542116165161133
Reconstruction Loss: -6.1392035484313965
Iteration 13201:
Training Loss: -5.555569171905518
Reconstruction Loss: -6.143750190734863
Iteration 13301:
Training Loss: -5.568926811218262
Reconstruction Loss: -6.1482439041137695
Iteration 13401:
Training Loss: -5.582188606262207
Reconstruction Loss: -6.152684211730957
Iteration 13501:
Training Loss: -5.595365047454834
Reconstruction Loss: -6.1570658683776855
Iteration 13601:
Training Loss: -5.6084442138671875
Reconstruction Loss: -6.1613993644714355
Iteration 13701:
Training Loss: -5.621439456939697
Reconstruction Loss: -6.165683269500732
Iteration 13801:
Training Loss: -5.634345054626465
Reconstruction Loss: -6.169918537139893
Iteration 13901:
Training Loss: -5.64716911315918
Reconstruction Loss: -6.174102306365967
Iteration 14001:
Training Loss: -5.659900665283203
Reconstruction Loss: -6.1782355308532715
Iteration 14101:
Training Loss: -5.67255163192749
Reconstruction Loss: -6.1823201179504395
Iteration 14201:
Training Loss: -5.6851091384887695
Reconstruction Loss: -6.18635892868042
Iteration 14301:
Training Loss: -5.697599411010742
Reconstruction Loss: -6.19035530090332
Iteration 14401:
Training Loss: -5.710000038146973
Reconstruction Loss: -6.194310188293457
Iteration 14501:
Training Loss: -5.72231388092041
Reconstruction Loss: -6.198217868804932
Iteration 14601:
Training Loss: -5.734558582305908
Reconstruction Loss: -6.202078819274902
Iteration 14701:
Training Loss: -5.746714115142822
Reconstruction Loss: -6.205897808074951
Iteration 14801:
Training Loss: -5.758790493011475
Reconstruction Loss: -6.209670543670654
Iteration 14901:
Training Loss: -5.770797252655029
Reconstruction Loss: -6.213403701782227
