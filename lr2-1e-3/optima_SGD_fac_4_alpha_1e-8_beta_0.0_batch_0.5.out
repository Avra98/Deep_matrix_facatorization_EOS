5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.466174125671387
Reconstruction Loss: -0.5118736028671265
Iteration 51:
Training Loss: 5.423946380615234
Reconstruction Loss: -0.5118736028671265
Iteration 101:
Training Loss: 5.5817790031433105
Reconstruction Loss: -0.5118736028671265
Iteration 151:
Training Loss: 5.504744052886963
Reconstruction Loss: -0.5118736028671265
Iteration 201:
Training Loss: 5.611603260040283
Reconstruction Loss: -0.5118736028671265
Iteration 251:
Training Loss: 5.520518779754639
Reconstruction Loss: -0.5118734836578369
Iteration 301:
Training Loss: 5.539612293243408
Reconstruction Loss: -0.5118736028671265
Iteration 351:
Training Loss: 5.537015438079834
Reconstruction Loss: -0.5118736028671265
Iteration 401:
Training Loss: 5.427974224090576
Reconstruction Loss: -0.511873722076416
Iteration 451:
Training Loss: 5.430216312408447
Reconstruction Loss: -0.5118736028671265
Iteration 501:
Training Loss: 5.525166034698486
Reconstruction Loss: -0.511873722076416
Iteration 551:
Training Loss: 5.479380130767822
Reconstruction Loss: -0.511873722076416
Iteration 601:
Training Loss: 5.42902946472168
Reconstruction Loss: -0.511873722076416
Iteration 651:
Training Loss: 5.436967372894287
Reconstruction Loss: -0.511873722076416
Iteration 701:
Training Loss: 5.345553398132324
Reconstruction Loss: -0.511873722076416
Iteration 751:
Training Loss: 5.446116924285889
Reconstruction Loss: -0.511873722076416
Iteration 801:
Training Loss: 5.609095573425293
Reconstruction Loss: -0.5118737816810608
Iteration 851:
Training Loss: 5.499283313751221
Reconstruction Loss: -0.5118737816810608
Iteration 901:
Training Loss: 5.465485095977783
Reconstruction Loss: -0.5118737816810608
Iteration 951:
Training Loss: 5.579619407653809
Reconstruction Loss: -0.5118737816810608
Iteration 1001:
Training Loss: 5.608511924743652
Reconstruction Loss: -0.5118737816810608
Iteration 1051:
Training Loss: 5.482107162475586
Reconstruction Loss: -0.5118737816810608
Iteration 1101:
Training Loss: 5.393860816955566
Reconstruction Loss: -0.5118737816810608
Iteration 1151:
Training Loss: 5.532240390777588
Reconstruction Loss: -0.5118737816810608
Iteration 1201:
Training Loss: 5.524917125701904
Reconstruction Loss: -0.5118739008903503
Iteration 1251:
Training Loss: 5.50880241394043
Reconstruction Loss: -0.5118739604949951
Iteration 1301:
Training Loss: 5.524321556091309
Reconstruction Loss: -0.5118739604949951
Iteration 1351:
Training Loss: 5.429595947265625
Reconstruction Loss: -0.5118739604949951
Iteration 1401:
Training Loss: 5.335988521575928
Reconstruction Loss: -0.5118739604949951
Iteration 1451:
Training Loss: 5.405490398406982
Reconstruction Loss: -0.5118739604949951
Iteration 1501:
Training Loss: 5.483180046081543
Reconstruction Loss: -0.5118739604949951
Iteration 1551:
Training Loss: 5.532391548156738
Reconstruction Loss: -0.5118739604949951
Iteration 1601:
Training Loss: 5.284101486206055
Reconstruction Loss: -0.5118739604949951
Iteration 1651:
Training Loss: 5.477114200592041
Reconstruction Loss: -0.5118739604949951
Iteration 1701:
Training Loss: 5.476590633392334
Reconstruction Loss: -0.5118739604949951
Iteration 1751:
Training Loss: 5.456964492797852
Reconstruction Loss: -0.5118740797042847
Iteration 1801:
Training Loss: 5.506863594055176
Reconstruction Loss: -0.5118740797042847
Iteration 1851:
Training Loss: 5.216249465942383
Reconstruction Loss: -0.5118740797042847
Iteration 1901:
Training Loss: 5.366679668426514
Reconstruction Loss: -0.5118740797042847
Iteration 1951:
Training Loss: 5.512931823730469
Reconstruction Loss: -0.5118740797042847
Iteration 2001:
Training Loss: 5.495323657989502
Reconstruction Loss: -0.5118741989135742
Iteration 2051:
Training Loss: 5.390308380126953
Reconstruction Loss: -0.5118741989135742
Iteration 2101:
Training Loss: 5.495697498321533
Reconstruction Loss: -0.5118741989135742
Iteration 2151:
Training Loss: 5.4890456199646
Reconstruction Loss: -0.5118741989135742
Iteration 2201:
Training Loss: 5.414493560791016
Reconstruction Loss: -0.5118741989135742
Iteration 2251:
Training Loss: 5.384359359741211
Reconstruction Loss: -0.5118743181228638
Iteration 2301:
Training Loss: 5.456435680389404
Reconstruction Loss: -0.5118743181228638
Iteration 2351:
Training Loss: 5.54376745223999
Reconstruction Loss: -0.5118743181228638
Iteration 2401:
Training Loss: 5.4828009605407715
Reconstruction Loss: -0.5118743181228638
Iteration 2451:
Training Loss: 5.451371192932129
Reconstruction Loss: -0.5118743181228638
Iteration 2501:
Training Loss: 5.575433254241943
Reconstruction Loss: -0.5118744969367981
Iteration 2551:
Training Loss: 5.602570056915283
Reconstruction Loss: -0.5118744969367981
Iteration 2601:
Training Loss: 5.24466609954834
Reconstruction Loss: -0.5118744969367981
Iteration 2651:
Training Loss: 5.504671573638916
Reconstruction Loss: -0.5118744969367981
Iteration 2701:
Training Loss: 5.413808345794678
Reconstruction Loss: -0.5118744969367981
Iteration 2751:
Training Loss: 5.583651065826416
Reconstruction Loss: -0.5118744969367981
Iteration 2801:
Training Loss: 5.54557466506958
Reconstruction Loss: -0.5118745565414429
Iteration 2851:
Training Loss: 5.519340515136719
Reconstruction Loss: -0.5118745565414429
Iteration 2901:
Training Loss: 5.507569313049316
Reconstruction Loss: -0.5118745565414429
Iteration 2951:
Training Loss: 5.5919671058654785
Reconstruction Loss: -0.511874794960022
Iteration 3001:
Training Loss: 5.42865514755249
Reconstruction Loss: -0.511874794960022
Iteration 3051:
Training Loss: 5.5051422119140625
Reconstruction Loss: -0.511874794960022
Iteration 3101:
Training Loss: 5.2054338455200195
Reconstruction Loss: -0.5118749141693115
Iteration 3151:
Training Loss: 5.582601547241211
Reconstruction Loss: -0.5118749141693115
Iteration 3201:
Training Loss: 5.332780838012695
Reconstruction Loss: -0.5118749141693115
Iteration 3251:
Training Loss: 5.665040969848633
Reconstruction Loss: -0.5118749737739563
Iteration 3301:
Training Loss: 5.569675922393799
Reconstruction Loss: -0.5118749737739563
Iteration 3351:
Training Loss: 5.465324878692627
Reconstruction Loss: -0.5118750929832458
Iteration 3401:
Training Loss: 5.425285339355469
Reconstruction Loss: -0.5118751525878906
Iteration 3451:
Training Loss: 5.271722316741943
Reconstruction Loss: -0.5118751525878906
Iteration 3501:
Training Loss: 5.409181594848633
Reconstruction Loss: -0.5118752717971802
Iteration 3551:
Training Loss: 5.478722095489502
Reconstruction Loss: -0.5118753910064697
Iteration 3601:
Training Loss: 5.505507469177246
Reconstruction Loss: -0.5118753910064697
Iteration 3651:
Training Loss: 5.5157551765441895
Reconstruction Loss: -0.511875569820404
Iteration 3701:
Training Loss: 5.420662879943848
Reconstruction Loss: -0.511875569820404
Iteration 3751:
Training Loss: 5.486036777496338
Reconstruction Loss: -0.5118757486343384
Iteration 3801:
Training Loss: 5.5497331619262695
Reconstruction Loss: -0.5118758678436279
Iteration 3851:
Training Loss: 5.416378498077393
Reconstruction Loss: -0.5118758678436279
Iteration 3901:
Training Loss: 5.441825866699219
Reconstruction Loss: -0.5118759870529175
Iteration 3951:
Training Loss: 5.41806173324585
Reconstruction Loss: -0.5118761658668518
Iteration 4001:
Training Loss: 5.580602169036865
Reconstruction Loss: -0.5118762850761414
Iteration 4051:
Training Loss: 5.4374895095825195
Reconstruction Loss: -0.5118763446807861
Iteration 4101:
Training Loss: 5.493195056915283
Reconstruction Loss: -0.5118765830993652
Iteration 4151:
Training Loss: 5.651020050048828
Reconstruction Loss: -0.5118767619132996
Iteration 4201:
Training Loss: 5.349503993988037
Reconstruction Loss: -0.5118768811225891
Iteration 4251:
Training Loss: 5.416079998016357
Reconstruction Loss: -0.5118769407272339
Iteration 4301:
Training Loss: 5.5536112785339355
Reconstruction Loss: -0.5118772983551025
Iteration 4351:
Training Loss: 5.4314680099487305
Reconstruction Loss: -0.5118773579597473
Iteration 4401:
Training Loss: 5.509057998657227
Reconstruction Loss: -0.5118777751922607
Iteration 4451:
Training Loss: 5.490384578704834
Reconstruction Loss: -0.5118778944015503
Iteration 4501:
Training Loss: 5.471532344818115
Reconstruction Loss: -0.511878252029419
Iteration 4551:
Training Loss: 5.454521179199219
Reconstruction Loss: -0.5118785500526428
Iteration 4601:
Training Loss: 5.566706657409668
Reconstruction Loss: -0.5118789672851562
Iteration 4651:
Training Loss: 5.434505462646484
Reconstruction Loss: -0.5118793249130249
Iteration 4701:
Training Loss: 5.50028657913208
Reconstruction Loss: -0.5118797421455383
Iteration 4751:
Training Loss: 5.497164726257324
Reconstruction Loss: -0.5118802785873413
Iteration 4801:
Training Loss: 5.536652088165283
Reconstruction Loss: -0.5118807554244995
Iteration 4851:
Training Loss: 5.464089870452881
Reconstruction Loss: -0.5118815302848816
Iteration 4901:
Training Loss: 5.568944931030273
Reconstruction Loss: -0.5118822455406189
Iteration 4951:
Training Loss: 5.367955207824707
Reconstruction Loss: -0.5118829011917114
Iteration 5001:
Training Loss: 5.322398662567139
Reconstruction Loss: -0.5118840336799622
Iteration 5051:
Training Loss: 5.548104763031006
Reconstruction Loss: -0.5118851065635681
Iteration 5101:
Training Loss: 5.428375244140625
Reconstruction Loss: -0.511886477470398
Iteration 5151:
Training Loss: 5.436794281005859
Reconstruction Loss: -0.5118880867958069
Iteration 5201:
Training Loss: 5.556845664978027
Reconstruction Loss: -0.5118901133537292
Iteration 5251:
Training Loss: 5.431651592254639
Reconstruction Loss: -0.511892557144165
Iteration 5301:
Training Loss: 5.556477069854736
Reconstruction Loss: -0.5118956565856934
Iteration 5351:
Training Loss: 5.4634318351745605
Reconstruction Loss: -0.5118995308876038
Iteration 5401:
Training Loss: 5.5570783615112305
Reconstruction Loss: -0.5119044780731201
Iteration 5451:
Training Loss: 5.496720314025879
Reconstruction Loss: -0.511911153793335
Iteration 5501:
Training Loss: 5.403707504272461
Reconstruction Loss: -0.5119204521179199
Iteration 5551:
Training Loss: 5.512810707092285
Reconstruction Loss: -0.5119337439537048
Iteration 5601:
Training Loss: 5.342895984649658
Reconstruction Loss: -0.5119537115097046
Iteration 5651:
Training Loss: 5.430773735046387
Reconstruction Loss: -0.5119861364364624
Iteration 5701:
Training Loss: 5.444443702697754
Reconstruction Loss: -0.5120431184768677
Iteration 5751:
Training Loss: 5.414848804473877
Reconstruction Loss: -0.5121587514877319
Iteration 5801:
Training Loss: 5.42344856262207
Reconstruction Loss: -0.5124526023864746
Iteration 5851:
Training Loss: 5.351690292358398
Reconstruction Loss: -0.5136209726333618
Iteration 5901:
Training Loss: 5.534915924072266
Reconstruction Loss: -0.5339083671569824
Iteration 5951:
Training Loss: 5.112906455993652
Reconstruction Loss: -0.6186769008636475
Iteration 6001:
Training Loss: 4.881721019744873
Reconstruction Loss: -0.6089949011802673
Iteration 6051:
Training Loss: 4.948397159576416
Reconstruction Loss: -0.6113181114196777
Iteration 6101:
Training Loss: 4.968469619750977
Reconstruction Loss: -0.5976068377494812
Iteration 6151:
Training Loss: 4.995723724365234
Reconstruction Loss: -0.6130260825157166
Iteration 6201:
Training Loss: 4.920721530914307
Reconstruction Loss: -0.6046212911605835
Iteration 6251:
Training Loss: 4.973852634429932
Reconstruction Loss: -0.6053053140640259
Iteration 6301:
Training Loss: 5.032442092895508
Reconstruction Loss: -0.6037193536758423
Iteration 6351:
Training Loss: 4.910937309265137
Reconstruction Loss: -0.6069688200950623
Iteration 6401:
Training Loss: 4.900615692138672
Reconstruction Loss: -0.5892110466957092
Iteration 6451:
Training Loss: 4.9406609535217285
Reconstruction Loss: -0.604324460029602
Iteration 6501:
Training Loss: 4.926053524017334
Reconstruction Loss: -0.6069754958152771
Iteration 6551:
Training Loss: 4.766845703125
Reconstruction Loss: -0.6179825663566589
Iteration 6601:
Training Loss: 4.9158148765563965
Reconstruction Loss: -0.6023224592208862
Iteration 6651:
Training Loss: 4.940867900848389
Reconstruction Loss: -0.6089645624160767
Iteration 6701:
Training Loss: 4.993349075317383
Reconstruction Loss: -0.5882543921470642
Iteration 6751:
Training Loss: 4.754385471343994
Reconstruction Loss: -0.6020361185073853
Iteration 6801:
Training Loss: 4.948676586151123
Reconstruction Loss: -0.6056115031242371
Iteration 6851:
Training Loss: 4.954356670379639
Reconstruction Loss: -0.6096372604370117
Iteration 6901:
Training Loss: 5.056158542633057
Reconstruction Loss: -0.5974794030189514
Iteration 6951:
Training Loss: 5.000819206237793
Reconstruction Loss: -0.6017848253250122
Iteration 7001:
Training Loss: 4.890081405639648
Reconstruction Loss: -0.5989759564399719
Iteration 7051:
Training Loss: 4.937972068786621
Reconstruction Loss: -0.6163374185562134
Iteration 7101:
Training Loss: 4.953013896942139
Reconstruction Loss: -0.5956363081932068
Iteration 7151:
Training Loss: 4.662777423858643
Reconstruction Loss: -0.6013343930244446
Iteration 7201:
Training Loss: 4.957009315490723
Reconstruction Loss: -0.6190732717514038
Iteration 7251:
Training Loss: 4.800209999084473
Reconstruction Loss: -0.6176275610923767
Iteration 7301:
Training Loss: 4.931929111480713
Reconstruction Loss: -0.5992479920387268
Iteration 7351:
Training Loss: 4.807648658752441
Reconstruction Loss: -0.5829142928123474
Iteration 7401:
Training Loss: 4.924551963806152
Reconstruction Loss: -0.5999596118927002
Iteration 7451:
Training Loss: 4.915840148925781
Reconstruction Loss: -0.6134926080703735
