5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.185895919799805
Reconstruction Loss: -0.6818752288818359
Iteration 21:
Training Loss: 5.342318534851074
Reconstruction Loss: -0.6818752288818359
Iteration 41:
Training Loss: 5.564724445343018
Reconstruction Loss: -0.6818752288818359
Iteration 61:
Training Loss: 5.11692476272583
Reconstruction Loss: -0.6818752288818359
Iteration 81:
Training Loss: 5.391330718994141
Reconstruction Loss: -0.6818752288818359
Iteration 101:
Training Loss: 5.378053665161133
Reconstruction Loss: -0.6818752288818359
Iteration 121:
Training Loss: 4.9890217781066895
Reconstruction Loss: -0.6818752288818359
Iteration 141:
Training Loss: 5.280068397521973
Reconstruction Loss: -0.6818752288818359
Iteration 161:
Training Loss: 5.381687164306641
Reconstruction Loss: -0.6818752288818359
Iteration 181:
Training Loss: 5.388631343841553
Reconstruction Loss: -0.6818753480911255
Iteration 201:
Training Loss: 5.444016933441162
Reconstruction Loss: -0.6818753480911255
Iteration 221:
Training Loss: 5.067267417907715
Reconstruction Loss: -0.6818753480911255
Iteration 241:
Training Loss: 5.443204879760742
Reconstruction Loss: -0.6818753480911255
Iteration 261:
Training Loss: 5.414613723754883
Reconstruction Loss: -0.6818753480911255
Iteration 281:
Training Loss: 5.4108171463012695
Reconstruction Loss: -0.6818753480911255
Iteration 301:
Training Loss: 5.0602192878723145
Reconstruction Loss: -0.6818753480911255
Iteration 321:
Training Loss: 5.305849075317383
Reconstruction Loss: -0.6818753480911255
Iteration 341:
Training Loss: 5.239294528961182
Reconstruction Loss: -0.681875467300415
Iteration 361:
Training Loss: 5.246829986572266
Reconstruction Loss: -0.681875467300415
Iteration 381:
Training Loss: 5.377905368804932
Reconstruction Loss: -0.681875467300415
Iteration 401:
Training Loss: 5.458041667938232
Reconstruction Loss: -0.681875467300415
Iteration 421:
Training Loss: 5.302911758422852
Reconstruction Loss: -0.681875467300415
Iteration 441:
Training Loss: 5.300561904907227
Reconstruction Loss: -0.6818755865097046
Iteration 461:
Training Loss: 5.0220441818237305
Reconstruction Loss: -0.6818755865097046
Iteration 481:
Training Loss: 5.206289291381836
Reconstruction Loss: -0.6818755865097046
Iteration 501:
Training Loss: 5.5158915519714355
Reconstruction Loss: -0.6818755865097046
Iteration 521:
Training Loss: 5.5326642990112305
Reconstruction Loss: -0.6818757057189941
Iteration 541:
Training Loss: 5.682426929473877
Reconstruction Loss: -0.6818757057189941
Iteration 561:
Training Loss: 5.091198921203613
Reconstruction Loss: -0.6818757057189941
Iteration 581:
Training Loss: 5.3669657707214355
Reconstruction Loss: -0.6818757057189941
Iteration 601:
Training Loss: 5.426285266876221
Reconstruction Loss: -0.6818757057189941
Iteration 621:
Training Loss: 5.3885955810546875
Reconstruction Loss: -0.6818757057189941
Iteration 641:
Training Loss: 5.135293483734131
Reconstruction Loss: -0.6818758249282837
Iteration 661:
Training Loss: 5.474408149719238
Reconstruction Loss: -0.6818758249282837
Iteration 681:
Training Loss: 4.997573375701904
Reconstruction Loss: -0.6818759441375732
Iteration 701:
Training Loss: 5.503551959991455
Reconstruction Loss: -0.6818759441375732
Iteration 721:
Training Loss: 5.173417091369629
Reconstruction Loss: -0.6818759441375732
Iteration 741:
Training Loss: 5.3153204917907715
Reconstruction Loss: -0.6818760633468628
Iteration 761:
Training Loss: 5.729297637939453
Reconstruction Loss: -0.6818759441375732
Iteration 781:
Training Loss: 5.569668292999268
Reconstruction Loss: -0.6818759441375732
Iteration 801:
Training Loss: 5.219583511352539
Reconstruction Loss: -0.6818760633468628
Iteration 821:
Training Loss: 5.0671892166137695
Reconstruction Loss: -0.6818760633468628
Iteration 841:
Training Loss: 5.359035015106201
Reconstruction Loss: -0.6818763017654419
Iteration 861:
Training Loss: 5.442636489868164
Reconstruction Loss: -0.6818763017654419
Iteration 881:
Training Loss: 5.387209415435791
Reconstruction Loss: -0.6818763017654419
Iteration 901:
Training Loss: 5.171214580535889
Reconstruction Loss: -0.6818763017654419
Iteration 921:
Training Loss: 5.374452590942383
Reconstruction Loss: -0.6818764209747314
Iteration 941:
Training Loss: 5.292422294616699
Reconstruction Loss: -0.6818764209747314
Iteration 961:
Training Loss: 5.207366466522217
Reconstruction Loss: -0.6818764209747314
Iteration 981:
Training Loss: 5.0867462158203125
Reconstruction Loss: -0.681876540184021
Iteration 1001:
Training Loss: 5.26616907119751
Reconstruction Loss: -0.681876540184021
Iteration 1021:
Training Loss: 5.360593318939209
Reconstruction Loss: -0.6818766593933105
Iteration 1041:
Training Loss: 5.212106227874756
Reconstruction Loss: -0.6818766593933105
Iteration 1061:
Training Loss: 5.4811625480651855
Reconstruction Loss: -0.6818768978118896
Iteration 1081:
Training Loss: 5.393723487854004
Reconstruction Loss: -0.6818769574165344
Iteration 1101:
Training Loss: 5.45486307144165
Reconstruction Loss: -0.6818769574165344
Iteration 1121:
Training Loss: 5.504774570465088
Reconstruction Loss: -0.681877076625824
Iteration 1141:
Training Loss: 5.657550811767578
Reconstruction Loss: -0.681877076625824
Iteration 1161:
Training Loss: 5.228603839874268
Reconstruction Loss: -0.6818773150444031
Iteration 1181:
Training Loss: 5.1131415367126465
Reconstruction Loss: -0.6818773150444031
Iteration 1201:
Training Loss: 5.281405448913574
Reconstruction Loss: -0.6818774342536926
Iteration 1221:
Training Loss: 5.416921615600586
Reconstruction Loss: -0.6818775534629822
Iteration 1241:
Training Loss: 5.238590240478516
Reconstruction Loss: -0.6818776726722717
Iteration 1261:
Training Loss: 5.433696746826172
Reconstruction Loss: -0.6818779110908508
Iteration 1281:
Training Loss: 5.094776153564453
Reconstruction Loss: -0.6818779110908508
Iteration 1301:
Training Loss: 5.290352821350098
Reconstruction Loss: -0.6818780303001404
Iteration 1321:
Training Loss: 5.174086093902588
Reconstruction Loss: -0.681878387928009
Iteration 1341:
Training Loss: 5.238214015960693
Reconstruction Loss: -0.6818786263465881
Iteration 1361:
Training Loss: 5.170406818389893
Reconstruction Loss: -0.6818787455558777
Iteration 1381:
Training Loss: 5.16896915435791
Reconstruction Loss: -0.6818789839744568
Iteration 1401:
Training Loss: 5.584578514099121
Reconstruction Loss: -0.6818792223930359
Iteration 1421:
Training Loss: 5.495865821838379
Reconstruction Loss: -0.6818795800209045
Iteration 1441:
Training Loss: 5.371134281158447
Reconstruction Loss: -0.6818798184394836
Iteration 1461:
Training Loss: 5.342879295349121
Reconstruction Loss: -0.6818801760673523
Iteration 1481:
Training Loss: 5.4515557289123535
Reconstruction Loss: -0.681880533695221
Iteration 1501:
Training Loss: 4.941929817199707
Reconstruction Loss: -0.6818808913230896
Iteration 1521:
Training Loss: 5.1995954513549805
Reconstruction Loss: -0.6818814873695374
Iteration 1541:
Training Loss: 5.20242977142334
Reconstruction Loss: -0.6818819642066956
Iteration 1561:
Training Loss: 5.615340709686279
Reconstruction Loss: -0.681882381439209
Iteration 1581:
Training Loss: 5.295648574829102
Reconstruction Loss: -0.6818829774856567
Iteration 1601:
Training Loss: 5.122758865356445
Reconstruction Loss: -0.6818838119506836
Iteration 1621:
Training Loss: 4.970642566680908
Reconstruction Loss: -0.6818845272064209
Iteration 1641:
Training Loss: 5.274148464202881
Reconstruction Loss: -0.6818854808807373
Iteration 1661:
Training Loss: 5.233691215515137
Reconstruction Loss: -0.6818865537643433
Iteration 1681:
Training Loss: 5.246212005615234
Reconstruction Loss: -0.6818879246711731
Iteration 1701:
Training Loss: 5.642519950866699
Reconstruction Loss: -0.6818894743919373
Iteration 1721:
Training Loss: 5.4319939613342285
Reconstruction Loss: -0.6818913817405701
Iteration 1741:
Training Loss: 5.311845302581787
Reconstruction Loss: -0.6818935871124268
Iteration 1761:
Training Loss: 5.226109981536865
Reconstruction Loss: -0.6818963289260864
Iteration 1781:
Training Loss: 5.234567642211914
Reconstruction Loss: -0.6818997263908386
Iteration 1801:
Training Loss: 5.466773986816406
Reconstruction Loss: -0.6819041967391968
Iteration 1821:
Training Loss: 5.498741626739502
Reconstruction Loss: -0.6819097399711609
Iteration 1841:
Training Loss: 5.61395263671875
Reconstruction Loss: -0.6819173097610474
Iteration 1861:
Training Loss: 5.259328365325928
Reconstruction Loss: -0.6819276809692383
Iteration 1881:
Training Loss: 5.074488162994385
Reconstruction Loss: -0.6819425225257874
Iteration 1901:
Training Loss: 4.842638969421387
Reconstruction Loss: -0.6819643378257751
Iteration 1921:
Training Loss: 5.391380310058594
Reconstruction Loss: -0.6819993257522583
Iteration 1941:
Training Loss: 5.5403947830200195
Reconstruction Loss: -0.6820595860481262
Iteration 1961:
Training Loss: 5.27290678024292
Reconstruction Loss: -0.6821791529655457
Iteration 1981:
Training Loss: 5.082554340362549
Reconstruction Loss: -0.6824665069580078
Iteration 2001:
Training Loss: 5.329484939575195
Reconstruction Loss: -0.6834766268730164
Iteration 2021:
Training Loss: 5.452434062957764
Reconstruction Loss: -0.6941759586334229
Iteration 2041:
Training Loss: 5.217554569244385
Reconstruction Loss: -0.7696112394332886
Iteration 2061:
Training Loss: 4.674790382385254
Reconstruction Loss: -0.7568191885948181
Iteration 2081:
Training Loss: 4.732626914978027
Reconstruction Loss: -0.7385938167572021
Iteration 2101:
Training Loss: 4.768120288848877
Reconstruction Loss: -0.7637116312980652
Iteration 2121:
Training Loss: 4.956681251525879
Reconstruction Loss: -0.7569122314453125
Iteration 2141:
Training Loss: 4.841194152832031
Reconstruction Loss: -0.7592952251434326
Iteration 2161:
Training Loss: 4.624695777893066
Reconstruction Loss: -0.7467689514160156
Iteration 2181:
Training Loss: 5.072643756866455
Reconstruction Loss: -0.7099758386611938
Iteration 2201:
Training Loss: 4.635470390319824
Reconstruction Loss: -0.7635056376457214
Iteration 2221:
Training Loss: 4.923274517059326
Reconstruction Loss: -0.7477925419807434
Iteration 2241:
Training Loss: 4.996792793273926
Reconstruction Loss: -0.7328851819038391
Iteration 2261:
Training Loss: 4.631523132324219
Reconstruction Loss: -0.7283876538276672
Iteration 2281:
Training Loss: 4.936075687408447
Reconstruction Loss: -0.7402923107147217
Iteration 2301:
Training Loss: 4.907159328460693
Reconstruction Loss: -0.7288058400154114
Iteration 2321:
Training Loss: 4.7979865074157715
Reconstruction Loss: -0.7437625527381897
Iteration 2341:
Training Loss: 4.850703716278076
Reconstruction Loss: -0.7274894714355469
Iteration 2361:
Training Loss: 5.119710445404053
Reconstruction Loss: -0.7515157461166382
Iteration 2381:
Training Loss: 4.964025497436523
Reconstruction Loss: -0.7423996329307556
Iteration 2401:
Training Loss: 4.974917888641357
Reconstruction Loss: -0.746690571308136
Iteration 2421:
Training Loss: 5.023131847381592
Reconstruction Loss: -0.7615571022033691
Iteration 2441:
Training Loss: 4.788295269012451
Reconstruction Loss: -0.7298503518104553
Iteration 2461:
Training Loss: 4.954944610595703
Reconstruction Loss: -0.7209727764129639
Iteration 2481:
Training Loss: 5.0182294845581055
Reconstruction Loss: -0.7460424900054932
Iteration 2501:
Training Loss: 4.88320779800415
Reconstruction Loss: -0.7179296612739563
Iteration 2521:
Training Loss: 4.834408760070801
Reconstruction Loss: -0.7374319434165955
Iteration 2541:
Training Loss: 5.092462539672852
Reconstruction Loss: -0.7658464908599854
Iteration 2561:
Training Loss: 4.887279987335205
Reconstruction Loss: -0.7406914234161377
Iteration 2581:
Training Loss: 4.749700546264648
Reconstruction Loss: -0.7286736369132996
Iteration 2601:
Training Loss: 4.547197341918945
Reconstruction Loss: -0.7483705878257751
Iteration 2621:
Training Loss: 4.961116313934326
Reconstruction Loss: -0.7455928325653076
Iteration 2641:
Training Loss: 4.722039699554443
Reconstruction Loss: -0.7387654781341553
Iteration 2661:
Training Loss: 4.996933460235596
Reconstruction Loss: -0.7520675659179688
Iteration 2681:
Training Loss: 5.160515308380127
Reconstruction Loss: -0.7366692423820496
Iteration 2701:
Training Loss: 5.032093048095703
Reconstruction Loss: -0.7178248167037964
Iteration 2721:
Training Loss: 4.946030616760254
Reconstruction Loss: -0.748584508895874
Iteration 2741:
Training Loss: 4.820035934448242
Reconstruction Loss: -0.7346739172935486
Iteration 2761:
Training Loss: 4.998600006103516
Reconstruction Loss: -0.7468814849853516
Iteration 2781:
Training Loss: 4.975521087646484
Reconstruction Loss: -0.7464287877082825
Iteration 2801:
Training Loss: 4.977429389953613
Reconstruction Loss: -0.7330945730209351
Iteration 2821:
Training Loss: 4.877900123596191
Reconstruction Loss: -0.7514054775238037
Iteration 2841:
Training Loss: 4.965667724609375
Reconstruction Loss: -0.7450389266014099
Iteration 2861:
Training Loss: 4.791934490203857
Reconstruction Loss: -0.7142288088798523
Iteration 2881:
Training Loss: 4.914641380310059
Reconstruction Loss: -0.7238001823425293
Iteration 2901:
Training Loss: 4.9289703369140625
Reconstruction Loss: -0.720895528793335
Iteration 2921:
Training Loss: 4.864710807800293
Reconstruction Loss: -0.7706980109214783
Iteration 2941:
Training Loss: 5.145946025848389
Reconstruction Loss: -0.7709330320358276
Iteration 2961:
Training Loss: 4.829861640930176
Reconstruction Loss: -0.7649527788162231
Iteration 2981:
Training Loss: 4.95068359375
Reconstruction Loss: -0.7302033305168152
