5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.549275875091553
Reconstruction Loss: -0.532038152217865
Iteration 51:
Training Loss: 5.435108661651611
Reconstruction Loss: -0.5321601629257202
Iteration 101:
Training Loss: 5.540158748626709
Reconstruction Loss: -0.5323759317398071
Iteration 151:
Training Loss: 5.507638931274414
Reconstruction Loss: -0.5329403281211853
Iteration 201:
Training Loss: 5.431067943572998
Reconstruction Loss: -0.5354236960411072
Iteration 251:
Training Loss: 5.274053573608398
Reconstruction Loss: -0.5758832097053528
Iteration 301:
Training Loss: 4.966509819030762
Reconstruction Loss: -0.7406299114227295
Iteration 351:
Training Loss: 4.416934013366699
Reconstruction Loss: -0.9590434432029724
Iteration 401:
Training Loss: 3.674551486968994
Reconstruction Loss: -1.326399326324463
Iteration 451:
Training Loss: 3.124450206756592
Reconstruction Loss: -1.549516201019287
Iteration 501:
Training Loss: 3.054277181625366
Reconstruction Loss: -1.5825130939483643
Iteration 551:
Training Loss: 2.891319751739502
Reconstruction Loss: -1.5807547569274902
Iteration 601:
Training Loss: 2.991051197052002
Reconstruction Loss: -1.5945417881011963
Iteration 651:
Training Loss: 2.780970573425293
Reconstruction Loss: -1.6456184387207031
Iteration 701:
Training Loss: 2.494070291519165
Reconstruction Loss: -1.833052158355713
Iteration 751:
Training Loss: 1.851070761680603
Reconstruction Loss: -2.2839410305023193
Iteration 801:
Training Loss: 1.1693224906921387
Reconstruction Loss: -2.840545415878296
Iteration 851:
Training Loss: 0.3340966999530792
Reconstruction Loss: -3.4079954624176025
Iteration 901:
Training Loss: -0.26878026127815247
Reconstruction Loss: -3.9380483627319336
Iteration 951:
Training Loss: -0.9285086393356323
Reconstruction Loss: -4.4290032386779785
Iteration 1001:
Training Loss: -1.4082343578338623
Reconstruction Loss: -4.880673885345459
Iteration 1051:
Training Loss: -2.1090826988220215
Reconstruction Loss: -5.302591323852539
Iteration 1101:
Training Loss: -2.3095743656158447
Reconstruction Loss: -5.697461128234863
Iteration 1151:
Training Loss: -2.9284510612487793
Reconstruction Loss: -6.076735496520996
Iteration 1201:
Training Loss: -3.1917147636413574
Reconstruction Loss: -6.439152717590332
Iteration 1251:
Training Loss: -3.6415162086486816
Reconstruction Loss: -6.790651321411133
Iteration 1301:
Training Loss: -3.8886618614196777
Reconstruction Loss: -7.132569789886475
Iteration 1351:
Training Loss: -4.2220048904418945
Reconstruction Loss: -7.463131427764893
Iteration 1401:
Training Loss: -4.573022365570068
Reconstruction Loss: -7.788816928863525
Iteration 1451:
Training Loss: -4.95272159576416
Reconstruction Loss: -8.107667922973633
Iteration 1501:
Training Loss: -5.284947872161865
Reconstruction Loss: -8.419742584228516
Iteration 1551:
Training Loss: -5.614828586578369
Reconstruction Loss: -8.72612190246582
Iteration 1601:
Training Loss: -5.917239189147949
Reconstruction Loss: -9.027188301086426
Iteration 1651:
Training Loss: -6.165073871612549
Reconstruction Loss: -9.321939468383789
Iteration 1701:
Training Loss: -6.4836554527282715
Reconstruction Loss: -9.610621452331543
Iteration 1751:
Training Loss: -6.661140441894531
Reconstruction Loss: -9.892104148864746
Iteration 1801:
Training Loss: -7.026164531707764
Reconstruction Loss: -10.166707992553711
Iteration 1851:
Training Loss: -7.43344259262085
Reconstruction Loss: -10.43230152130127
Iteration 1901:
Training Loss: -7.471237659454346
Reconstruction Loss: -10.687149047851562
Iteration 1951:
Training Loss: -7.793582916259766
Reconstruction Loss: -10.932463645935059
Iteration 2001:
Training Loss: -7.954573631286621
Reconstruction Loss: -11.164057731628418
Iteration 2051:
Training Loss: -8.028952598571777
Reconstruction Loss: -11.380959510803223
Iteration 2101:
Training Loss: -8.195123672485352
Reconstruction Loss: -11.583314895629883
Iteration 2151:
Training Loss: -8.353154182434082
Reconstruction Loss: -11.767778396606445
Iteration 2201:
Training Loss: -8.49151611328125
Reconstruction Loss: -11.931775093078613
Iteration 2251:
Training Loss: -8.615864753723145
Reconstruction Loss: -12.080857276916504
Iteration 2301:
Training Loss: -8.584195137023926
Reconstruction Loss: -12.209344863891602
Iteration 2351:
Training Loss: -8.597452163696289
Reconstruction Loss: -12.31935977935791
Iteration 2401:
Training Loss: -8.669058799743652
Reconstruction Loss: -12.416016578674316
Iteration 2451:
Training Loss: -8.611307144165039
Reconstruction Loss: -12.49416446685791
Iteration 2501:
Training Loss: -8.756810188293457
Reconstruction Loss: -12.562089920043945
Iteration 2551:
Training Loss: -8.680591583251953
Reconstruction Loss: -12.6152925491333
Iteration 2601:
Training Loss: -8.808732986450195
Reconstruction Loss: -12.665197372436523
Iteration 2651:
Training Loss: -8.798639297485352
Reconstruction Loss: -12.701136589050293
Iteration 2701:
Training Loss: -8.873655319213867
Reconstruction Loss: -12.728375434875488
Iteration 2751:
Training Loss: -8.735955238342285
Reconstruction Loss: -12.756735801696777
Iteration 2801:
Training Loss: -9.030831336975098
Reconstruction Loss: -12.779078483581543
Iteration 2851:
Training Loss: -8.874812126159668
Reconstruction Loss: -12.793877601623535
Iteration 2901:
Training Loss: -8.987686157226562
Reconstruction Loss: -12.811357498168945
Iteration 2951:
Training Loss: -8.959938049316406
Reconstruction Loss: -12.82281494140625
Iteration 3001:
Training Loss: -8.834853172302246
Reconstruction Loss: -12.831499099731445
Iteration 3051:
Training Loss: -8.813016891479492
Reconstruction Loss: -12.841299057006836
Iteration 3101:
Training Loss: -8.934991836547852
Reconstruction Loss: -12.851706504821777
Iteration 3151:
Training Loss: -8.884664535522461
Reconstruction Loss: -12.85686206817627
Iteration 3201:
Training Loss: -8.817081451416016
Reconstruction Loss: -12.862771034240723
Iteration 3251:
Training Loss: -8.905938148498535
Reconstruction Loss: -12.869799613952637
Iteration 3301:
Training Loss: -8.797545433044434
Reconstruction Loss: -12.87439250946045
Iteration 3351:
Training Loss: -8.817493438720703
Reconstruction Loss: -12.87735652923584
Iteration 3401:
Training Loss: -8.810606956481934
Reconstruction Loss: -12.881115913391113
Iteration 3451:
Training Loss: -8.944775581359863
Reconstruction Loss: -12.884766578674316
Iteration 3501:
Training Loss: -9.070232391357422
Reconstruction Loss: -12.8863525390625
Iteration 3551:
Training Loss: -8.967252731323242
Reconstruction Loss: -12.89129638671875
Iteration 3601:
Training Loss: -8.891265869140625
Reconstruction Loss: -12.89487361907959
Iteration 3651:
Training Loss: -8.826390266418457
Reconstruction Loss: -12.894661903381348
Iteration 3701:
Training Loss: -8.913822174072266
Reconstruction Loss: -12.897954940795898
Iteration 3751:
Training Loss: -8.766217231750488
Reconstruction Loss: -12.902311325073242
Iteration 3801:
Training Loss: -8.858171463012695
Reconstruction Loss: -12.903806686401367
Iteration 3851:
Training Loss: -8.88056755065918
Reconstruction Loss: -12.9091157913208
Iteration 3901:
Training Loss: -8.95011043548584
Reconstruction Loss: -12.90957260131836
Iteration 3951:
Training Loss: -8.983198165893555
Reconstruction Loss: -12.91234016418457
Iteration 4001:
Training Loss: -8.840351104736328
Reconstruction Loss: -12.913177490234375
Iteration 4051:
Training Loss: -8.8864107131958
Reconstruction Loss: -12.913074493408203
Iteration 4101:
Training Loss: -8.977808952331543
Reconstruction Loss: -12.915668487548828
Iteration 4151:
Training Loss: -8.94413948059082
Reconstruction Loss: -12.918548583984375
Iteration 4201:
Training Loss: -9.093181610107422
Reconstruction Loss: -12.92211627960205
Iteration 4251:
Training Loss: -8.939874649047852
Reconstruction Loss: -12.927046775817871
Iteration 4301:
Training Loss: -8.866497993469238
Reconstruction Loss: -12.926782608032227
Iteration 4351:
Training Loss: -8.852255821228027
Reconstruction Loss: -12.92625617980957
Iteration 4401:
Training Loss: -8.81155014038086
Reconstruction Loss: -12.932562828063965
Iteration 4451:
Training Loss: -8.889459609985352
Reconstruction Loss: -12.93099594116211
Iteration 4501:
Training Loss: -8.929913520812988
Reconstruction Loss: -12.933501243591309
Iteration 4551:
Training Loss: -8.919151306152344
Reconstruction Loss: -12.936466217041016
Iteration 4601:
Training Loss: -8.916926383972168
Reconstruction Loss: -12.939846992492676
Iteration 4651:
Training Loss: -8.98861312866211
Reconstruction Loss: -12.93789005279541
Iteration 4701:
Training Loss: -8.913253784179688
Reconstruction Loss: -12.943570137023926
Iteration 4751:
Training Loss: -8.882353782653809
Reconstruction Loss: -12.94249153137207
Iteration 4801:
Training Loss: -8.86496639251709
Reconstruction Loss: -12.94705581665039
Iteration 4851:
Training Loss: -8.857298851013184
Reconstruction Loss: -12.946379661560059
Iteration 4901:
Training Loss: -8.985448837280273
Reconstruction Loss: -12.94722843170166
Iteration 4951:
Training Loss: -8.943102836608887
Reconstruction Loss: -12.949528694152832
Iteration 5001:
Training Loss: -8.970132827758789
Reconstruction Loss: -12.952178001403809
Iteration 5051:
Training Loss: -8.939908981323242
Reconstruction Loss: -12.955107688903809
Iteration 5101:
Training Loss: -9.004122734069824
Reconstruction Loss: -12.955260276794434
Iteration 5151:
Training Loss: -8.959271430969238
Reconstruction Loss: -12.95940113067627
Iteration 5201:
Training Loss: -9.014830589294434
Reconstruction Loss: -12.959671974182129
Iteration 5251:
Training Loss: -8.905378341674805
Reconstruction Loss: -12.962174415588379
Iteration 5301:
Training Loss: -8.869230270385742
Reconstruction Loss: -12.96630573272705
Iteration 5351:
Training Loss: -9.00730037689209
Reconstruction Loss: -12.964720726013184
Iteration 5401:
Training Loss: -8.975433349609375
Reconstruction Loss: -12.966682434082031
Iteration 5451:
Training Loss: -9.017044067382812
Reconstruction Loss: -12.9692964553833
Iteration 5501:
Training Loss: -8.870004653930664
Reconstruction Loss: -12.97142505645752
Iteration 5551:
Training Loss: -8.989667892456055
Reconstruction Loss: -12.972121238708496
Iteration 5601:
Training Loss: -9.054092407226562
Reconstruction Loss: -12.97437858581543
Iteration 5651:
Training Loss: -8.927446365356445
Reconstruction Loss: -12.975428581237793
Iteration 5701:
Training Loss: -9.05605697631836
Reconstruction Loss: -12.976215362548828
Iteration 5751:
Training Loss: -9.027303695678711
Reconstruction Loss: -12.980813980102539
Iteration 5801:
Training Loss: -9.05470085144043
Reconstruction Loss: -12.978792190551758
Iteration 5851:
Training Loss: -9.017416954040527
Reconstruction Loss: -12.981439590454102
Iteration 5901:
Training Loss: -8.90795612335205
Reconstruction Loss: -12.984740257263184
Iteration 5951:
Training Loss: -9.012303352355957
Reconstruction Loss: -12.985933303833008
Iteration 6001:
Training Loss: -8.928232192993164
Reconstruction Loss: -12.99006462097168
Iteration 6051:
Training Loss: -9.007466316223145
Reconstruction Loss: -12.990363121032715
Iteration 6101:
Training Loss: -8.835491180419922
Reconstruction Loss: -12.991058349609375
Iteration 6151:
Training Loss: -9.067095756530762
Reconstruction Loss: -12.99193286895752
Iteration 6201:
Training Loss: -9.002564430236816
Reconstruction Loss: -12.99711799621582
Iteration 6251:
Training Loss: -8.983075141906738
Reconstruction Loss: -12.995084762573242
Iteration 6301:
Training Loss: -9.110736846923828
Reconstruction Loss: -13.001749038696289
Iteration 6351:
Training Loss: -8.893192291259766
Reconstruction Loss: -13.00146484375
Iteration 6401:
Training Loss: -8.983148574829102
Reconstruction Loss: -13.004101753234863
Iteration 6451:
Training Loss: -8.997193336486816
Reconstruction Loss: -13.004340171813965
Iteration 6501:
Training Loss: -9.081372261047363
Reconstruction Loss: -13.008188247680664
Iteration 6551:
Training Loss: -8.968205451965332
Reconstruction Loss: -13.006083488464355
Iteration 6601:
Training Loss: -9.010252952575684
Reconstruction Loss: -13.010405540466309
Iteration 6651:
Training Loss: -9.088586807250977
Reconstruction Loss: -13.009143829345703
Iteration 6701:
Training Loss: -8.978554725646973
Reconstruction Loss: -13.014045715332031
Iteration 6751:
Training Loss: -9.152544021606445
Reconstruction Loss: -13.016341209411621
Iteration 6801:
Training Loss: -9.128673553466797
Reconstruction Loss: -13.018999099731445
Iteration 6851:
Training Loss: -9.019844055175781
Reconstruction Loss: -13.02042007446289
Iteration 6901:
Training Loss: -8.94601821899414
Reconstruction Loss: -13.018915176391602
Iteration 6951:
Training Loss: -9.05495834350586
Reconstruction Loss: -13.023771286010742
Iteration 7001:
Training Loss: -9.00837516784668
Reconstruction Loss: -13.020883560180664
Iteration 7051:
Training Loss: -9.067893028259277
Reconstruction Loss: -13.026280403137207
Iteration 7101:
Training Loss: -8.962135314941406
Reconstruction Loss: -13.027667999267578
Iteration 7151:
Training Loss: -9.01029109954834
Reconstruction Loss: -13.029346466064453
Iteration 7201:
Training Loss: -9.122852325439453
Reconstruction Loss: -13.030994415283203
Iteration 7251:
Training Loss: -9.01301383972168
Reconstruction Loss: -13.030791282653809
Iteration 7301:
Training Loss: -8.936279296875
Reconstruction Loss: -13.033870697021484
Iteration 7351:
Training Loss: -9.03622055053711
Reconstruction Loss: -13.035567283630371
Iteration 7401:
Training Loss: -8.982939720153809
Reconstruction Loss: -13.037178039550781
Iteration 7451:
Training Loss: -9.102783203125
Reconstruction Loss: -13.039873123168945
