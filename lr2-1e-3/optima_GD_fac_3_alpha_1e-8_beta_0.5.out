5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.534159183502197
Reconstruction Loss: -0.496674120426178
Iteration 101:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.496674120426178
Iteration 201:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.496674120426178
Iteration 301:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.496674120426178
Iteration 401:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.496674120426178
Iteration 501:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 601:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 701:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 801:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 901:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 1001:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 1101:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 1201:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 1301:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 1401:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 1501:
Training Loss: 5.534158706665039
Reconstruction Loss: -0.49667423963546753
Iteration 1601:
Training Loss: 5.534158229827881
Reconstruction Loss: -0.49667423963546753
Iteration 1701:
Training Loss: 5.534158229827881
Reconstruction Loss: -0.49667441844940186
Iteration 1801:
Training Loss: 5.534158229827881
Reconstruction Loss: -0.49667441844940186
Iteration 1901:
Training Loss: 5.534158229827881
Reconstruction Loss: -0.49667441844940186
Iteration 2001:
Training Loss: 5.534158229827881
Reconstruction Loss: -0.49667441844940186
Iteration 2101:
Training Loss: 5.534158229827881
Reconstruction Loss: -0.49667441844940186
Iteration 2201:
Training Loss: 5.534158229827881
Reconstruction Loss: -0.49667441844940186
Iteration 2301:
Training Loss: 5.534157752990723
Reconstruction Loss: -0.49667441844940186
Iteration 2401:
Training Loss: 5.534157752990723
Reconstruction Loss: -0.4966745376586914
Iteration 2501:
Training Loss: 5.534157752990723
Reconstruction Loss: -0.4966745376586914
Iteration 2601:
Training Loss: 5.534157752990723
Reconstruction Loss: -0.4966745972633362
Iteration 2701:
Training Loss: 5.534157752990723
Reconstruction Loss: -0.4966748058795929
Iteration 2801:
Training Loss: 5.5341572761535645
Reconstruction Loss: -0.4966748058795929
Iteration 2901:
Training Loss: 5.5341572761535645
Reconstruction Loss: -0.49667489528656006
Iteration 3001:
Training Loss: 5.5341572761535645
Reconstruction Loss: -0.49667489528656006
Iteration 3101:
Training Loss: 5.534156799316406
Reconstruction Loss: -0.4966751039028168
Iteration 3201:
Training Loss: 5.534156322479248
Reconstruction Loss: -0.4966753125190735
Iteration 3301:
Training Loss: 5.53415584564209
Reconstruction Loss: -0.49667540192604065
Iteration 3401:
Training Loss: 5.534155368804932
Reconstruction Loss: -0.49667561054229736
Iteration 3501:
Training Loss: 5.534154891967773
Reconstruction Loss: -0.49667590856552124
Iteration 3601:
Training Loss: 5.534154415130615
Reconstruction Loss: -0.4966762661933899
Iteration 3701:
Training Loss: 5.534153461456299
Reconstruction Loss: -0.4966766834259033
Iteration 3801:
Training Loss: 5.534152030944824
Reconstruction Loss: -0.4966772794723511
Iteration 3901:
Training Loss: 5.53415060043335
Reconstruction Loss: -0.49667805433273315
Iteration 4001:
Training Loss: 5.534148693084717
Reconstruction Loss: -0.49667900800704956
Iteration 4101:
Training Loss: 5.534145355224609
Reconstruction Loss: -0.49668049812316895
Iteration 4201:
Training Loss: 5.5341410636901855
Reconstruction Loss: -0.4966827630996704
Iteration 4301:
Training Loss: 5.534134387969971
Reconstruction Loss: -0.49668627977371216
Iteration 4401:
Training Loss: 5.534122943878174
Reconstruction Loss: -0.4966919422149658
Iteration 4501:
Training Loss: 5.534102916717529
Reconstruction Loss: -0.49670204520225525
Iteration 4601:
Training Loss: 5.534063339233398
Reconstruction Loss: -0.49672210216522217
Iteration 4701:
Training Loss: 5.533970832824707
Reconstruction Loss: -0.49676981568336487
Iteration 4801:
Training Loss: 5.533688545227051
Reconstruction Loss: -0.49691635370254517
Iteration 4901:
Training Loss: 5.5322651863098145
Reconstruction Loss: -0.4976639747619629
Iteration 5001:
Training Loss: 5.503848552703857
Reconstruction Loss: -0.5126193761825562
Iteration 5101:
Training Loss: 5.022770881652832
Reconstruction Loss: -0.6472097635269165
Iteration 5201:
Training Loss: 4.9975996017456055
Reconstruction Loss: -0.6470720171928406
Iteration 5301:
Training Loss: 4.990203857421875
Reconstruction Loss: -0.6420835256576538
Iteration 5401:
Training Loss: 4.984306335449219
Reconstruction Loss: -0.6405444145202637
Iteration 5501:
Training Loss: 4.943009376525879
Reconstruction Loss: -0.6606293320655823
Iteration 5601:
Training Loss: 4.224177837371826
Reconstruction Loss: -0.9295007586479187
Iteration 5701:
Training Loss: 4.151843547821045
Reconstruction Loss: -0.9193289875984192
Iteration 5801:
Training Loss: 4.135222911834717
Reconstruction Loss: -0.9059665203094482
Iteration 5901:
Training Loss: 4.131459712982178
Reconstruction Loss: -0.898665189743042
Iteration 6001:
Training Loss: 4.130486488342285
Reconstruction Loss: -0.8952746391296387
Iteration 6101:
Training Loss: 4.1301727294921875
Reconstruction Loss: -0.8937541246414185
Iteration 6201:
Training Loss: 4.130049705505371
Reconstruction Loss: -0.8930619359016418
Iteration 6301:
Training Loss: 4.129995346069336
Reconstruction Loss: -0.8927344679832458
Iteration 6401:
Training Loss: 4.129970073699951
Reconstruction Loss: -0.892572283744812
Iteration 6501:
Training Loss: 4.12995719909668
Reconstruction Loss: -0.8924877047538757
Iteration 6601:
Training Loss: 4.129951000213623
Reconstruction Loss: -0.8924415111541748
Iteration 6701:
Training Loss: 4.129947662353516
Reconstruction Loss: -0.8924149870872498
Iteration 6801:
Training Loss: 4.129945755004883
Reconstruction Loss: -0.892399251461029
Iteration 6901:
Training Loss: 4.129944801330566
Reconstruction Loss: -0.8923895955085754
Iteration 7001:
Training Loss: 4.12994384765625
Reconstruction Loss: -0.8923836946487427
Iteration 7101:
Training Loss: 4.129942893981934
Reconstruction Loss: -0.8923799991607666
Iteration 7201:
Training Loss: 4.129941940307617
Reconstruction Loss: -0.8923777341842651
Iteration 7301:
Training Loss: 4.129941463470459
Reconstruction Loss: -0.8923763036727905
Iteration 7401:
Training Loss: 4.129940509796143
Reconstruction Loss: -0.8923757076263428
Iteration 7501:
Training Loss: 4.129939556121826
Reconstruction Loss: -0.8923754692077637
Iteration 7601:
Training Loss: 4.129938125610352
Reconstruction Loss: -0.8923754692077637
Iteration 7701:
Training Loss: 4.129937171936035
Reconstruction Loss: -0.8923760652542114
Iteration 7801:
Training Loss: 4.1299357414245605
Reconstruction Loss: -0.8923767805099487
Iteration 7901:
Training Loss: 4.129933834075928
Reconstruction Loss: -0.8923776149749756
Iteration 8001:
Training Loss: 4.129932403564453
Reconstruction Loss: -0.892378568649292
Iteration 8101:
Training Loss: 4.129930019378662
Reconstruction Loss: -0.8923797607421875
Iteration 8201:
Training Loss: 4.129927635192871
Reconstruction Loss: -0.8923813104629517
Iteration 8301:
Training Loss: 4.129924774169922
Reconstruction Loss: -0.8923829793930054
Iteration 8401:
Training Loss: 4.1299214363098145
Reconstruction Loss: -0.8923850059509277
Iteration 8501:
Training Loss: 4.129917621612549
Reconstruction Loss: -0.8923875689506531
Iteration 8601:
Training Loss: 4.129912853240967
Reconstruction Loss: -0.8923905491828918
Iteration 8701:
Training Loss: 4.129907131195068
Reconstruction Loss: -0.892393946647644
Iteration 8801:
Training Loss: 4.1299004554748535
Reconstruction Loss: -0.8923982381820679
Iteration 8901:
Training Loss: 4.129892349243164
Reconstruction Loss: -0.8924034237861633
Iteration 9001:
Training Loss: 4.129881858825684
Reconstruction Loss: -0.8924097418785095
Iteration 9101:
Training Loss: 4.129868984222412
Reconstruction Loss: -0.8924179673194885
Iteration 9201:
Training Loss: 4.129852294921875
Reconstruction Loss: -0.892427921295166
Iteration 9301:
Training Loss: 4.129830360412598
Reconstruction Loss: -0.8924411535263062
Iteration 9401:
Training Loss: 4.1298017501831055
Reconstruction Loss: -0.892458438873291
Iteration 9501:
Training Loss: 4.129762172698975
Reconstruction Loss: -0.8924821019172668
Iteration 9601:
Training Loss: 4.129706382751465
Reconstruction Loss: -0.8925150036811829
Iteration 9701:
Training Loss: 4.12962532043457
Reconstruction Loss: -0.8925622701644897
Iteration 9801:
Training Loss: 4.129502296447754
Reconstruction Loss: -0.8926337361335754
Iteration 9901:
Training Loss: 4.129303455352783
Reconstruction Loss: -0.8927473425865173
Iteration 10001:
Training Loss: 4.128960609436035
Reconstruction Loss: -0.8929409384727478
Iteration 10101:
Training Loss: 4.12830924987793
Reconstruction Loss: -0.8933029174804688
Iteration 10201:
Training Loss: 4.126898288726807
Reconstruction Loss: -0.8940724730491638
Iteration 10301:
Training Loss: 4.123182773590088
Reconstruction Loss: -0.8960537314414978
Iteration 10401:
Training Loss: 4.1097259521484375
Reconstruction Loss: -0.902999997138977
Iteration 10501:
Training Loss: 4.025749683380127
Reconstruction Loss: -0.9436722993850708
Iteration 10601:
Training Loss: 3.6189184188842773
Reconstruction Loss: -1.106515884399414
Iteration 10701:
Training Loss: 3.4869985580444336
Reconstruction Loss: -1.1360974311828613
Iteration 10801:
Training Loss: 3.45810604095459
Reconstruction Loss: -1.147687554359436
Iteration 10901:
Training Loss: 3.4477133750915527
Reconstruction Loss: -1.1586415767669678
Iteration 11001:
Training Loss: 3.4422295093536377
Reconstruction Loss: -1.1676068305969238
Iteration 11101:
Training Loss: 3.43831205368042
Reconstruction Loss: -1.174704909324646
Iteration 11201:
Training Loss: 3.4345335960388184
Reconstruction Loss: -1.1807159185409546
Iteration 11301:
Training Loss: 3.4295108318328857
Reconstruction Loss: -1.1866865158081055
Iteration 11401:
Training Loss: 3.4202020168304443
Reconstruction Loss: -1.1945394277572632
Iteration 11501:
Training Loss: 3.395893096923828
Reconstruction Loss: -1.2101545333862305
Iteration 11601:
Training Loss: 3.3056812286376953
Reconstruction Loss: -1.2596216201782227
Iteration 11701:
Training Loss: 3.008969306945801
Reconstruction Loss: -1.4214389324188232
Iteration 11801:
Training Loss: 2.805589199066162
Reconstruction Loss: -1.573787808418274
Iteration 11901:
Training Loss: 2.7265467643737793
Reconstruction Loss: -1.649137020111084
Iteration 12001:
Training Loss: 2.683211326599121
Reconstruction Loss: -1.6916937828063965
Iteration 12101:
Training Loss: 2.6568241119384766
Reconstruction Loss: -1.7152125835418701
Iteration 12201:
Training Loss: 2.6390912532806396
Reconstruction Loss: -1.727311611175537
Iteration 12301:
Training Loss: 2.6262199878692627
Reconstruction Loss: -1.7327278852462769
Iteration 12401:
Training Loss: 2.616499900817871
Reconstruction Loss: -1.734415054321289
Iteration 12501:
Training Loss: 2.609057903289795
Reconstruction Loss: -1.7342088222503662
Iteration 12601:
Training Loss: 2.603344678878784
Reconstruction Loss: -1.7332093715667725
Iteration 12701:
Training Loss: 2.5989603996276855
Reconstruction Loss: -1.7320315837860107
Iteration 12801:
Training Loss: 2.5955982208251953
Reconstruction Loss: -1.7309865951538086
Iteration 12901:
Training Loss: 2.5930185317993164
Reconstruction Loss: -1.730208158493042
Iteration 13001:
Training Loss: 2.5910356044769287
Reconstruction Loss: -1.7297319173812866
Iteration 13101:
Training Loss: 2.589506149291992
Reconstruction Loss: -1.7295446395874023
Iteration 13201:
Training Loss: 2.588320016860962
Reconstruction Loss: -1.7296111583709717
Iteration 13301:
Training Loss: 2.5873942375183105
Reconstruction Loss: -1.7298884391784668
Iteration 13401:
Training Loss: 2.586665391921997
Reconstruction Loss: -1.7303341627120972
Iteration 13501:
Training Loss: 2.5860869884490967
Reconstruction Loss: -1.730908989906311
Iteration 13601:
Training Loss: 2.585623264312744
Reconstruction Loss: -1.7315782308578491
Iteration 13701:
Training Loss: 2.585247755050659
Reconstruction Loss: -1.732313632965088
Iteration 13801:
Training Loss: 2.5849409103393555
Reconstruction Loss: -1.7330913543701172
Iteration 13901:
Training Loss: 2.5846879482269287
Reconstruction Loss: -1.7338924407958984
Iteration 14001:
Training Loss: 2.584477186203003
Reconstruction Loss: -1.7347016334533691
Iteration 14101:
Training Loss: 2.5842998027801514
Reconstruction Loss: -1.7355077266693115
Iteration 14201:
Training Loss: 2.5841495990753174
Reconstruction Loss: -1.7363018989562988
Iteration 14301:
Training Loss: 2.58402156829834
Reconstruction Loss: -1.7370774745941162
Iteration 14401:
Training Loss: 2.583911418914795
Reconstruction Loss: -1.7378299236297607
Iteration 14501:
Training Loss: 2.583815336227417
Reconstruction Loss: -1.7385565042495728
Iteration 14601:
Training Loss: 2.5837316513061523
Reconstruction Loss: -1.7392547130584717
Iteration 14701:
Training Loss: 2.583657741546631
Reconstruction Loss: -1.7399245500564575
Iteration 14801:
Training Loss: 2.583592176437378
Reconstruction Loss: -1.7405648231506348
Iteration 14901:
Training Loss: 2.58353328704834
Reconstruction Loss: -1.741175889968872
