5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.473177433013916
Reconstruction Loss: -0.5069140195846558
Iteration 101:
Training Loss: 5.4638895988464355
Reconstruction Loss: -0.5112434029579163
Iteration 201:
Training Loss: 4.953381061553955
Reconstruction Loss: -0.6624163389205933
Iteration 301:
Training Loss: 4.506094455718994
Reconstruction Loss: -0.8744525909423828
Iteration 401:
Training Loss: 4.301530361175537
Reconstruction Loss: -0.9757469892501831
Iteration 501:
Training Loss: 3.8478798866271973
Reconstruction Loss: -1.1288963556289673
Iteration 601:
Training Loss: 3.770090341567993
Reconstruction Loss: -1.0808143615722656
Iteration 701:
Training Loss: 3.672466516494751
Reconstruction Loss: -1.0895065069198608
Iteration 801:
Training Loss: 3.2955422401428223
Reconstruction Loss: -1.2762304544448853
Iteration 901:
Training Loss: 2.708254337310791
Reconstruction Loss: -1.5320980548858643
Iteration 1001:
Training Loss: 1.4426089525222778
Reconstruction Loss: -2.3135085105895996
Iteration 1101:
Training Loss: 0.42744213342666626
Reconstruction Loss: -3.1250085830688477
Iteration 1201:
Training Loss: -0.40959131717681885
Reconstruction Loss: -3.904388904571533
Iteration 1301:
Training Loss: -1.2162033319473267
Reconstruction Loss: -4.692540168762207
Iteration 1401:
Training Loss: -2.0258326530456543
Reconstruction Loss: -5.490971565246582
Iteration 1501:
Training Loss: -2.8349051475524902
Reconstruction Loss: -6.288408279418945
Iteration 1601:
Training Loss: -3.6257121562957764
Reconstruction Loss: -7.06982946395874
Iteration 1701:
Training Loss: -4.368936538696289
Reconstruction Loss: -7.816844463348389
Iteration 1801:
Training Loss: -5.0222578048706055
Reconstruction Loss: -8.505462646484375
Iteration 1901:
Training Loss: -5.539093017578125
Reconstruction Loss: -9.105728149414062
Iteration 2001:
Training Loss: -5.8955278396606445
Reconstruction Loss: -9.588863372802734
Iteration 2101:
Training Loss: -6.110253810882568
Reconstruction Loss: -9.942056655883789
Iteration 2201:
Training Loss: -6.228028774261475
Reconstruction Loss: -10.177663803100586
Iteration 2301:
Training Loss: -6.290524005889893
Reconstruction Loss: -10.325045585632324
Iteration 2401:
Training Loss: -6.324635982513428
Reconstruction Loss: -10.414801597595215
Iteration 2501:
Training Loss: -6.344881534576416
Reconstruction Loss: -10.4699068069458
Iteration 2601:
Training Loss: -6.358471393585205
Reconstruction Loss: -10.504934310913086
Iteration 2701:
Training Loss: -6.36887788772583
Reconstruction Loss: -10.528379440307617
Iteration 2801:
Training Loss: -6.377739906311035
Reconstruction Loss: -10.545088768005371
Iteration 2901:
Training Loss: -6.385843753814697
Reconstruction Loss: -10.557769775390625
Iteration 3001:
Training Loss: -6.3935465812683105
Reconstruction Loss: -10.567991256713867
Iteration 3101:
Training Loss: -6.40103006362915
Reconstruction Loss: -10.576702117919922
Iteration 3201:
Training Loss: -6.408403396606445
Reconstruction Loss: -10.5844144821167
Iteration 3301:
Training Loss: -6.415665149688721
Reconstruction Loss: -10.591460227966309
Iteration 3401:
Training Loss: -6.422858715057373
Reconstruction Loss: -10.598099708557129
Iteration 3501:
Training Loss: -6.430002212524414
Reconstruction Loss: -10.60439682006836
Iteration 3601:
Training Loss: -6.437107563018799
Reconstruction Loss: -10.610474586486816
Iteration 3701:
Training Loss: -6.444149494171143
Reconstruction Loss: -10.616388320922852
Iteration 3801:
Training Loss: -6.451130390167236
Reconstruction Loss: -10.62218189239502
Iteration 3901:
Training Loss: -6.458090782165527
Reconstruction Loss: -10.62787914276123
Iteration 4001:
Training Loss: -6.464998245239258
Reconstruction Loss: -10.63351821899414
Iteration 4101:
Training Loss: -6.471858024597168
Reconstruction Loss: -10.639094352722168
Iteration 4201:
Training Loss: -6.478661060333252
Reconstruction Loss: -10.644621849060059
Iteration 4301:
Training Loss: -6.485439777374268
Reconstruction Loss: -10.65008544921875
Iteration 4401:
Training Loss: -6.492168426513672
Reconstruction Loss: -10.655500411987305
Iteration 4501:
Training Loss: -6.498842716217041
Reconstruction Loss: -10.660887718200684
Iteration 4601:
Training Loss: -6.5054826736450195
Reconstruction Loss: -10.66623306274414
Iteration 4701:
Training Loss: -6.512086868286133
Reconstruction Loss: -10.671548843383789
Iteration 4801:
Training Loss: -6.518638610839844
Reconstruction Loss: -10.676804542541504
Iteration 4901:
Training Loss: -6.525145530700684
Reconstruction Loss: -10.682034492492676
Iteration 5001:
Training Loss: -6.531636714935303
Reconstruction Loss: -10.687231063842773
Iteration 5101:
Training Loss: -6.5380706787109375
Reconstruction Loss: -10.692375183105469
Iteration 5201:
Training Loss: -6.544475555419922
Reconstruction Loss: -10.69748592376709
Iteration 5301:
Training Loss: -6.550829887390137
Reconstruction Loss: -10.702569007873535
Iteration 5401:
Training Loss: -6.557168006896973
Reconstruction Loss: -10.707632064819336
Iteration 5501:
Training Loss: -6.5634589195251465
Reconstruction Loss: -10.71265697479248
Iteration 5601:
Training Loss: -6.569702625274658
Reconstruction Loss: -10.717634201049805
Iteration 5701:
Training Loss: -6.5759172439575195
Reconstruction Loss: -10.722570419311523
Iteration 5801:
Training Loss: -6.582087993621826
Reconstruction Loss: -10.727482795715332
Iteration 5901:
Training Loss: -6.588228702545166
Reconstruction Loss: -10.732358932495117
Iteration 6001:
Training Loss: -6.594329833984375
Reconstruction Loss: -10.737204551696777
Iteration 6101:
Training Loss: -6.600413799285889
Reconstruction Loss: -10.742026329040527
Iteration 6201:
Training Loss: -6.606457710266113
Reconstruction Loss: -10.746822357177734
Iteration 6301:
Training Loss: -6.612448692321777
Reconstruction Loss: -10.751593589782715
Iteration 6401:
Training Loss: -6.6184210777282715
Reconstruction Loss: -10.75633716583252
Iteration 6501:
Training Loss: -6.6243577003479
Reconstruction Loss: -10.761051177978516
Iteration 6601:
Training Loss: -6.630270004272461
Reconstruction Loss: -10.765727043151855
Iteration 6701:
Training Loss: -6.636138916015625
Reconstruction Loss: -10.770364761352539
Iteration 6801:
Training Loss: -6.6419806480407715
Reconstruction Loss: -10.774971008300781
Iteration 6901:
Training Loss: -6.647792816162109
Reconstruction Loss: -10.779553413391113
Iteration 7001:
Training Loss: -6.653579235076904
Reconstruction Loss: -10.784119606018066
Iteration 7101:
Training Loss: -6.659333229064941
Reconstruction Loss: -10.788653373718262
Iteration 7201:
Training Loss: -6.665040969848633
Reconstruction Loss: -10.793184280395508
Iteration 7301:
Training Loss: -6.6707329750061035
Reconstruction Loss: -10.797673225402832
Iteration 7401:
Training Loss: -6.6763834953308105
Reconstruction Loss: -10.802127838134766
Iteration 7501:
Training Loss: -6.682016372680664
Reconstruction Loss: -10.806571960449219
Iteration 7601:
Training Loss: -6.687614917755127
Reconstruction Loss: -10.810992240905762
Iteration 7701:
Training Loss: -6.693177223205566
Reconstruction Loss: -10.815398216247559
Iteration 7801:
Training Loss: -6.698705673217773
Reconstruction Loss: -10.819770812988281
Iteration 7901:
Training Loss: -6.704224109649658
Reconstruction Loss: -10.824097633361816
Iteration 8001:
Training Loss: -6.70970344543457
Reconstruction Loss: -10.82840633392334
Iteration 8101:
Training Loss: -6.715168476104736
Reconstruction Loss: -10.832686424255371
Iteration 8201:
Training Loss: -6.7205810546875
Reconstruction Loss: -10.836952209472656
Iteration 8301:
Training Loss: -6.725979328155518
Reconstruction Loss: -10.84119701385498
Iteration 8401:
Training Loss: -6.73136043548584
Reconstruction Loss: -10.845418930053711
Iteration 8501:
Training Loss: -6.736720085144043
Reconstruction Loss: -10.84962272644043
Iteration 8601:
Training Loss: -6.742044925689697
Reconstruction Loss: -10.853812217712402
Iteration 8701:
Training Loss: -6.747318267822266
Reconstruction Loss: -10.857975959777832
Iteration 8801:
Training Loss: -6.752584934234619
Reconstruction Loss: -10.862104415893555
Iteration 8901:
Training Loss: -6.757837772369385
Reconstruction Loss: -10.866218566894531
Iteration 9001:
Training Loss: -6.763055324554443
Reconstruction Loss: -10.870317459106445
Iteration 9101:
Training Loss: -6.7682414054870605
Reconstruction Loss: -10.874388694763184
Iteration 9201:
Training Loss: -6.773403644561768
Reconstruction Loss: -10.878437042236328
Iteration 9301:
Training Loss: -6.77855920791626
Reconstruction Loss: -10.882448196411133
Iteration 9401:
Training Loss: -6.783661842346191
Reconstruction Loss: -10.886431694030762
Iteration 9501:
Training Loss: -6.7887749671936035
Reconstruction Loss: -10.890393257141113
Iteration 9601:
Training Loss: -6.793839454650879
Reconstruction Loss: -10.894335746765137
Iteration 9701:
Training Loss: -6.798879623413086
Reconstruction Loss: -10.898274421691895
Iteration 9801:
Training Loss: -6.803925514221191
Reconstruction Loss: -10.90218448638916
Iteration 9901:
Training Loss: -6.808925628662109
Reconstruction Loss: -10.906072616577148
Iteration 10001:
Training Loss: -6.81390380859375
Reconstruction Loss: -10.909952163696289
Iteration 10101:
Training Loss: -6.818853855133057
Reconstruction Loss: -10.913810729980469
Iteration 10201:
Training Loss: -6.823803901672363
Reconstruction Loss: -10.917647361755371
Iteration 10301:
Training Loss: -6.828702449798584
Reconstruction Loss: -10.921472549438477
Iteration 10401:
Training Loss: -6.833598613739014
Reconstruction Loss: -10.925284385681152
Iteration 10501:
Training Loss: -6.838460922241211
Reconstruction Loss: -10.929071426391602
Iteration 10601:
Training Loss: -6.843307971954346
Reconstruction Loss: -10.932845115661621
Iteration 10701:
Training Loss: -6.848134994506836
Reconstruction Loss: -10.936596870422363
Iteration 10801:
Training Loss: -6.852932453155518
Reconstruction Loss: -10.940322875976562
Iteration 10901:
Training Loss: -6.8577094078063965
Reconstruction Loss: -10.944031715393066
Iteration 11001:
Training Loss: -6.8624725341796875
Reconstruction Loss: -10.947709083557129
Iteration 11101:
Training Loss: -6.867217540740967
Reconstruction Loss: -10.951367378234863
Iteration 11201:
Training Loss: -6.8719401359558105
Reconstruction Loss: -10.9550199508667
Iteration 11301:
Training Loss: -6.876648426055908
Reconstruction Loss: -10.958669662475586
Iteration 11401:
Training Loss: -6.881326675415039
Reconstruction Loss: -10.962308883666992
Iteration 11501:
Training Loss: -6.885978698730469
Reconstruction Loss: -10.965932846069336
Iteration 11601:
Training Loss: -6.890629291534424
Reconstruction Loss: -10.969533920288086
Iteration 11701:
Training Loss: -6.895245552062988
Reconstruction Loss: -10.97311019897461
Iteration 11801:
Training Loss: -6.899837970733643
Reconstruction Loss: -10.976667404174805
Iteration 11901:
Training Loss: -6.904418468475342
Reconstruction Loss: -10.98022174835205
Iteration 12001:
Training Loss: -6.908974647521973
Reconstruction Loss: -10.983747482299805
Iteration 12101:
Training Loss: -6.913511276245117
Reconstruction Loss: -10.987258911132812
Iteration 12201:
Training Loss: -6.918032646179199
Reconstruction Loss: -10.99075698852539
Iteration 12301:
Training Loss: -6.922540664672852
Reconstruction Loss: -10.994241714477539
Iteration 12401:
Training Loss: -6.927019119262695
Reconstruction Loss: -10.997711181640625
Iteration 12501:
Training Loss: -6.931502819061279
Reconstruction Loss: -11.001158714294434
Iteration 12601:
Training Loss: -6.935946941375732
Reconstruction Loss: -11.004581451416016
Iteration 12701:
Training Loss: -6.940367698669434
Reconstruction Loss: -11.007997512817383
Iteration 12801:
Training Loss: -6.944789409637451
Reconstruction Loss: -11.011394500732422
Iteration 12901:
Training Loss: -6.949178218841553
Reconstruction Loss: -11.014765739440918
Iteration 13001:
Training Loss: -6.953563213348389
Reconstruction Loss: -11.018129348754883
Iteration 13101:
Training Loss: -6.957921028137207
Reconstruction Loss: -11.02147102355957
Iteration 13201:
Training Loss: -6.962265968322754
Reconstruction Loss: -11.02481746673584
Iteration 13301:
Training Loss: -6.966587543487549
Reconstruction Loss: -11.028143882751465
Iteration 13401:
Training Loss: -6.9709086418151855
Reconstruction Loss: -11.031468391418457
Iteration 13501:
Training Loss: -6.9752020835876465
Reconstruction Loss: -11.034775733947754
Iteration 13601:
Training Loss: -6.979475021362305
Reconstruction Loss: -11.038061141967773
Iteration 13701:
Training Loss: -6.983736991882324
Reconstruction Loss: -11.041337966918945
Iteration 13801:
Training Loss: -6.987988471984863
Reconstruction Loss: -11.044609069824219
Iteration 13901:
Training Loss: -6.992218017578125
Reconstruction Loss: -11.04784870147705
Iteration 14001:
Training Loss: -6.99643611907959
Reconstruction Loss: -11.051094055175781
Iteration 14101:
Training Loss: -7.000624179840088
Reconstruction Loss: -11.054313659667969
Iteration 14201:
Training Loss: -7.004813194274902
Reconstruction Loss: -11.057527542114258
Iteration 14301:
Training Loss: -7.008987903594971
Reconstruction Loss: -11.060717582702637
Iteration 14401:
Training Loss: -7.013119220733643
Reconstruction Loss: -11.063908576965332
Iteration 14501:
Training Loss: -7.017261981964111
Reconstruction Loss: -11.067086219787598
Iteration 14601:
Training Loss: -7.02140474319458
Reconstruction Loss: -11.070245742797852
Iteration 14701:
Training Loss: -7.025505065917969
Reconstruction Loss: -11.073405265808105
Iteration 14801:
Training Loss: -7.02958869934082
Reconstruction Loss: -11.076549530029297
Iteration 14901:
Training Loss: -7.033660411834717
Reconstruction Loss: -11.079682350158691
Iteration 15001:
Training Loss: -7.0377116203308105
Reconstruction Loss: -11.082795143127441
Iteration 15101:
Training Loss: -7.041744709014893
Reconstruction Loss: -11.085887908935547
Iteration 15201:
Training Loss: -7.045790672302246
Reconstruction Loss: -11.088969230651855
Iteration 15301:
Training Loss: -7.049796104431152
Reconstruction Loss: -11.092041969299316
Iteration 15401:
Training Loss: -7.053782939910889
Reconstruction Loss: -11.09510326385498
Iteration 15501:
Training Loss: -7.057782173156738
Reconstruction Loss: -11.098152160644531
Iteration 15601:
Training Loss: -7.06175422668457
Reconstruction Loss: -11.101187705993652
Iteration 15701:
Training Loss: -7.0656938552856445
Reconstruction Loss: -11.10421085357666
Iteration 15801:
Training Loss: -7.06964111328125
Reconstruction Loss: -11.107216835021973
Iteration 15901:
Training Loss: -7.073552131652832
Reconstruction Loss: -11.110199928283691
Iteration 16001:
Training Loss: -7.0774712562561035
Reconstruction Loss: -11.113189697265625
Iteration 16101:
Training Loss: -7.0813703536987305
Reconstruction Loss: -11.116157531738281
Iteration 16201:
Training Loss: -7.085261344909668
Reconstruction Loss: -11.119122505187988
Iteration 16301:
Training Loss: -7.089131832122803
Reconstruction Loss: -11.122087478637695
Iteration 16401:
Training Loss: -7.092983245849609
Reconstruction Loss: -11.125020980834961
Iteration 16501:
Training Loss: -7.096824645996094
Reconstruction Loss: -11.12796688079834
Iteration 16601:
Training Loss: -7.100656509399414
Reconstruction Loss: -11.130904197692871
Iteration 16701:
Training Loss: -7.104495048522949
Reconstruction Loss: -11.133838653564453
Iteration 16801:
Training Loss: -7.108285427093506
Reconstruction Loss: -11.136754989624023
Iteration 16901:
Training Loss: -7.112085342407227
Reconstruction Loss: -11.13966178894043
Iteration 17001:
Training Loss: -7.115877628326416
Reconstruction Loss: -11.142563819885254
Iteration 17101:
Training Loss: -7.119647026062012
Reconstruction Loss: -11.145451545715332
Iteration 17201:
Training Loss: -7.123388767242432
Reconstruction Loss: -11.148322105407715
Iteration 17301:
Training Loss: -7.1271538734436035
Reconstruction Loss: -11.151180267333984
Iteration 17401:
Training Loss: -7.1308817863464355
Reconstruction Loss: -11.154026985168457
Iteration 17501:
Training Loss: -7.134598731994629
Reconstruction Loss: -11.156853675842285
Iteration 17601:
Training Loss: -7.13830041885376
Reconstruction Loss: -11.159663200378418
Iteration 17701:
Training Loss: -7.141986846923828
Reconstruction Loss: -11.1624755859375
Iteration 17801:
Training Loss: -7.145668983459473
Reconstruction Loss: -11.165266990661621
Iteration 17901:
Training Loss: -7.149346351623535
Reconstruction Loss: -11.16804027557373
Iteration 18001:
Training Loss: -7.153008937835693
Reconstruction Loss: -11.170791625976562
Iteration 18101:
Training Loss: -7.156655788421631
Reconstruction Loss: -11.173552513122559
Iteration 18201:
Training Loss: -7.160296440124512
Reconstruction Loss: -11.176285743713379
Iteration 18301:
Training Loss: -7.16392183303833
Reconstruction Loss: -11.1790189743042
Iteration 18401:
Training Loss: -7.167523384094238
Reconstruction Loss: -11.181747436523438
Iteration 18501:
Training Loss: -7.17111873626709
Reconstruction Loss: -11.184471130371094
Iteration 18601:
Training Loss: -7.174716472625732
Reconstruction Loss: -11.187193870544434
Iteration 18701:
Training Loss: -7.1782965660095215
Reconstruction Loss: -11.189898490905762
Iteration 18801:
Training Loss: -7.181865692138672
Reconstruction Loss: -11.192608833312988
Iteration 18901:
Training Loss: -7.185407638549805
Reconstruction Loss: -11.195298194885254
Iteration 19001:
Training Loss: -7.188967227935791
Reconstruction Loss: -11.198007583618164
Iteration 19101:
Training Loss: -7.192511081695557
Reconstruction Loss: -11.200688362121582
Iteration 19201:
Training Loss: -7.196035385131836
Reconstruction Loss: -11.20336627960205
Iteration 19301:
Training Loss: -7.199539661407471
Reconstruction Loss: -11.20604133605957
Iteration 19401:
Training Loss: -7.203056812286377
Reconstruction Loss: -11.208693504333496
Iteration 19501:
Training Loss: -7.206534385681152
Reconstruction Loss: -11.211341857910156
Iteration 19601:
Training Loss: -7.210012912750244
Reconstruction Loss: -11.213979721069336
Iteration 19701:
Training Loss: -7.213481903076172
Reconstruction Loss: -11.216608047485352
Iteration 19801:
Training Loss: -7.216945171356201
Reconstruction Loss: -11.219229698181152
Iteration 19901:
Training Loss: -7.220369338989258
Reconstruction Loss: -11.221837043762207
Iteration 20001:
Training Loss: -7.223805904388428
Reconstruction Loss: -11.224438667297363
Iteration 20101:
Training Loss: -7.227225303649902
Reconstruction Loss: -11.22702693939209
Iteration 20201:
Training Loss: -7.230643272399902
Reconstruction Loss: -11.229616165161133
Iteration 20301:
Training Loss: -7.23405647277832
Reconstruction Loss: -11.232195854187012
Iteration 20401:
Training Loss: -7.237435817718506
Reconstruction Loss: -11.234764099121094
Iteration 20501:
Training Loss: -7.240830898284912
Reconstruction Loss: -11.237330436706543
Iteration 20601:
Training Loss: -7.2442169189453125
Reconstruction Loss: -11.239886283874512
Iteration 20701:
Training Loss: -7.247579097747803
Reconstruction Loss: -11.242438316345215
Iteration 20801:
Training Loss: -7.250940799713135
Reconstruction Loss: -11.24497127532959
Iteration 20901:
Training Loss: -7.254278182983398
Reconstruction Loss: -11.2475004196167
Iteration 21001:
Training Loss: -7.25760555267334
Reconstruction Loss: -11.250015258789062
Iteration 21101:
Training Loss: -7.260953903198242
Reconstruction Loss: -11.25252914428711
Iteration 21201:
Training Loss: -7.264256477355957
Reconstruction Loss: -11.255024909973145
Iteration 21301:
Training Loss: -7.267548084259033
Reconstruction Loss: -11.257499694824219
Iteration 21401:
Training Loss: -7.2708539962768555
Reconstruction Loss: -11.25998592376709
Iteration 21501:
Training Loss: -7.274124622344971
Reconstruction Loss: -11.262457847595215
Iteration 21601:
Training Loss: -7.27739953994751
Reconstruction Loss: -11.26491641998291
Iteration 21701:
Training Loss: -7.280671119689941
Reconstruction Loss: -11.267379760742188
Iteration 21801:
Training Loss: -7.2839131355285645
Reconstruction Loss: -11.269824981689453
Iteration 21901:
Training Loss: -7.287137508392334
Reconstruction Loss: -11.272268295288086
Iteration 22001:
Training Loss: -7.2903900146484375
Reconstruction Loss: -11.274703979492188
Iteration 22101:
Training Loss: -7.293614387512207
Reconstruction Loss: -11.277128219604492
Iteration 22201:
Training Loss: -7.296840190887451
Reconstruction Loss: -11.27955436706543
Iteration 22301:
Training Loss: -7.300043106079102
Reconstruction Loss: -11.281970024108887
Iteration 22401:
Training Loss: -7.303230285644531
Reconstruction Loss: -11.284387588500977
Iteration 22501:
Training Loss: -7.3064117431640625
Reconstruction Loss: -11.286783218383789
Iteration 22601:
Training Loss: -7.309592247009277
Reconstruction Loss: -11.289165496826172
Iteration 22701:
Training Loss: -7.3127851486206055
Reconstruction Loss: -11.291544914245605
Iteration 22801:
Training Loss: -7.315943241119385
Reconstruction Loss: -11.29391860961914
Iteration 22901:
Training Loss: -7.319087982177734
Reconstruction Loss: -11.296290397644043
Iteration 23001:
Training Loss: -7.322224140167236
Reconstruction Loss: -11.298662185668945
Iteration 23101:
Training Loss: -7.32536506652832
Reconstruction Loss: -11.301026344299316
Iteration 23201:
Training Loss: -7.3284783363342285
Reconstruction Loss: -11.303391456604004
Iteration 23301:
Training Loss: -7.331609725952148
Reconstruction Loss: -11.305743217468262
Iteration 23401:
Training Loss: -7.33473014831543
Reconstruction Loss: -11.308090209960938
Iteration 23501:
Training Loss: -7.337837219238281
Reconstruction Loss: -11.310436248779297
Iteration 23601:
Training Loss: -7.340935707092285
Reconstruction Loss: -11.312780380249023
Iteration 23701:
Training Loss: -7.3440070152282715
Reconstruction Loss: -11.315107345581055
Iteration 23801:
Training Loss: -7.347095966339111
Reconstruction Loss: -11.317426681518555
Iteration 23901:
Training Loss: -7.35018253326416
Reconstruction Loss: -11.319744110107422
Iteration 24001:
Training Loss: -7.353252410888672
Reconstruction Loss: -11.322050094604492
Iteration 24101:
Training Loss: -7.356308460235596
Reconstruction Loss: -11.324344635009766
Iteration 24201:
Training Loss: -7.3593549728393555
Reconstruction Loss: -11.326623916625977
Iteration 24301:
Training Loss: -7.362377643585205
Reconstruction Loss: -11.328898429870605
Iteration 24401:
Training Loss: -7.365411758422852
Reconstruction Loss: -11.331169128417969
Iteration 24501:
Training Loss: -7.36845064163208
Reconstruction Loss: -11.333430290222168
Iteration 24601:
Training Loss: -7.371453762054443
Reconstruction Loss: -11.335689544677734
Iteration 24701:
Training Loss: -7.374451637268066
Reconstruction Loss: -11.337942123413086
Iteration 24801:
Training Loss: -7.37746524810791
Reconstruction Loss: -11.340182304382324
Iteration 24901:
Training Loss: -7.380466938018799
Reconstruction Loss: -11.342403411865234
Iteration 25001:
Training Loss: -7.383430480957031
Reconstruction Loss: -11.344629287719727
Iteration 25101:
Training Loss: -7.386422157287598
Reconstruction Loss: -11.346841812133789
Iteration 25201:
Training Loss: -7.389373779296875
Reconstruction Loss: -11.34906005859375
Iteration 25301:
Training Loss: -7.392343521118164
Reconstruction Loss: -11.351272583007812
Iteration 25401:
Training Loss: -7.395307540893555
Reconstruction Loss: -11.353469848632812
Iteration 25501:
Training Loss: -7.398238658905029
Reconstruction Loss: -11.35566520690918
Iteration 25601:
Training Loss: -7.401195526123047
Reconstruction Loss: -11.357872009277344
Iteration 25701:
Training Loss: -7.404119968414307
Reconstruction Loss: -11.360055923461914
Iteration 25801:
Training Loss: -7.407039165496826
Reconstruction Loss: -11.362245559692383
Iteration 25901:
Training Loss: -7.409956932067871
Reconstruction Loss: -11.364426612854004
Iteration 26001:
Training Loss: -7.412872314453125
Reconstruction Loss: -11.366594314575195
Iteration 26101:
Training Loss: -7.415773868560791
Reconstruction Loss: -11.368768692016602
Iteration 26201:
Training Loss: -7.418656349182129
Reconstruction Loss: -11.370944023132324
Iteration 26301:
Training Loss: -7.421516418457031
Reconstruction Loss: -11.373103141784668
Iteration 26401:
Training Loss: -7.4244256019592285
Reconstruction Loss: -11.375248908996582
Iteration 26501:
Training Loss: -7.42730188369751
Reconstruction Loss: -11.377388954162598
Iteration 26601:
Training Loss: -7.430140972137451
Reconstruction Loss: -11.379523277282715
Iteration 26701:
Training Loss: -7.43300724029541
Reconstruction Loss: -11.381659507751465
Iteration 26801:
Training Loss: -7.43584680557251
Reconstruction Loss: -11.383781433105469
Iteration 26901:
Training Loss: -7.438675403594971
Reconstruction Loss: -11.385902404785156
Iteration 27001:
Training Loss: -7.441525459289551
Reconstruction Loss: -11.38801383972168
Iteration 27101:
Training Loss: -7.444336891174316
Reconstruction Loss: -11.390121459960938
Iteration 27201:
Training Loss: -7.447145462036133
Reconstruction Loss: -11.392223358154297
Iteration 27301:
Training Loss: -7.449990749359131
Reconstruction Loss: -11.394318580627441
Iteration 27401:
Training Loss: -7.452789783477783
Reconstruction Loss: -11.396409034729004
Iteration 27501:
Training Loss: -7.455589294433594
Reconstruction Loss: -11.398502349853516
Iteration 27601:
Training Loss: -7.4583916664123535
Reconstruction Loss: -11.400577545166016
Iteration 27701:
Training Loss: -7.461170673370361
Reconstruction Loss: -11.402652740478516
Iteration 27801:
Training Loss: -7.4639573097229
Reconstruction Loss: -11.4047212600708
Iteration 27901:
Training Loss: -7.466753959655762
Reconstruction Loss: -11.406791687011719
Iteration 28001:
Training Loss: -7.4695210456848145
Reconstruction Loss: -11.408852577209473
Iteration 28101:
Training Loss: -7.472263336181641
Reconstruction Loss: -11.410903930664062
Iteration 28201:
Training Loss: -7.475033283233643
Reconstruction Loss: -11.4129638671875
Iteration 28301:
Training Loss: -7.477778434753418
Reconstruction Loss: -11.415006637573242
Iteration 28401:
Training Loss: -7.480532646179199
Reconstruction Loss: -11.417052268981934
Iteration 28501:
Training Loss: -7.483245372772217
Reconstruction Loss: -11.41909122467041
Iteration 28601:
Training Loss: -7.486002445220947
Reconstruction Loss: -11.42112922668457
Iteration 28701:
Training Loss: -7.488724231719971
Reconstruction Loss: -11.423158645629883
Iteration 28801:
Training Loss: -7.49142599105835
Reconstruction Loss: -11.425180435180664
Iteration 28901:
Training Loss: -7.494146347045898
Reconstruction Loss: -11.427203178405762
Iteration 29001:
Training Loss: -7.496852397918701
Reconstruction Loss: -11.42921257019043
Iteration 29101:
Training Loss: -7.4995527267456055
Reconstruction Loss: -11.431218147277832
Iteration 29201:
Training Loss: -7.502233028411865
Reconstruction Loss: -11.433225631713867
Iteration 29301:
Training Loss: -7.504940986633301
Reconstruction Loss: -11.435230255126953
Iteration 29401:
Training Loss: -7.507624626159668
Reconstruction Loss: -11.437232971191406
Iteration 29501:
Training Loss: -7.510276794433594
Reconstruction Loss: -11.439226150512695
Iteration 29601:
Training Loss: -7.512951850891113
Reconstruction Loss: -11.441202163696289
Iteration 29701:
Training Loss: -7.515603542327881
Reconstruction Loss: -11.443185806274414
Iteration 29801:
Training Loss: -7.51827335357666
Reconstruction Loss: -11.445161819458008
Iteration 29901:
Training Loss: -7.520910739898682
Reconstruction Loss: -11.447124481201172
Iteration 30001:
Training Loss: -7.523562431335449
Reconstruction Loss: -11.4490966796875
Iteration 30101:
Training Loss: -7.526200771331787
Reconstruction Loss: -11.451050758361816
Iteration 30201:
Training Loss: -7.528841495513916
Reconstruction Loss: -11.453003883361816
Iteration 30301:
Training Loss: -7.531456470489502
Reconstruction Loss: -11.454948425292969
Iteration 30401:
Training Loss: -7.534092903137207
Reconstruction Loss: -11.45689582824707
Iteration 30501:
Training Loss: -7.536701202392578
Reconstruction Loss: -11.458829879760742
Iteration 30601:
Training Loss: -7.539307117462158
Reconstruction Loss: -11.460773468017578
Iteration 30701:
Training Loss: -7.541906356811523
Reconstruction Loss: -11.462698936462402
Iteration 30801:
Training Loss: -7.544501781463623
Reconstruction Loss: -11.464628219604492
Iteration 30901:
Training Loss: -7.547111988067627
Reconstruction Loss: -11.466553688049316
Iteration 31001:
Training Loss: -7.549697399139404
Reconstruction Loss: -11.468475341796875
Iteration 31101:
Training Loss: -7.552247047424316
Reconstruction Loss: -11.470394134521484
Iteration 31201:
Training Loss: -7.554838180541992
Reconstruction Loss: -11.472305297851562
Iteration 31301:
Training Loss: -7.55740213394165
Reconstruction Loss: -11.47421646118164
Iteration 31401:
Training Loss: -7.559957504272461
Reconstruction Loss: -11.476118087768555
Iteration 31501:
Training Loss: -7.562525749206543
Reconstruction Loss: -11.478009223937988
Iteration 31601:
Training Loss: -7.565051555633545
Reconstruction Loss: -11.479903221130371
Iteration 31701:
Training Loss: -7.567615985870361
Reconstruction Loss: -11.481788635253906
Iteration 31801:
Training Loss: -7.570162773132324
Reconstruction Loss: -11.48367691040039
Iteration 31901:
Training Loss: -7.5726752281188965
Reconstruction Loss: -11.485548973083496
Iteration 32001:
Training Loss: -7.5752129554748535
Reconstruction Loss: -11.487433433532715
Iteration 32101:
Training Loss: -7.577728271484375
Reconstruction Loss: -11.489304542541504
Iteration 32201:
Training Loss: -7.580255508422852
Reconstruction Loss: -11.491168022155762
Iteration 32301:
Training Loss: -7.5827717781066895
Reconstruction Loss: -11.493033409118652
Iteration 32401:
Training Loss: -7.585265159606934
Reconstruction Loss: -11.494884490966797
Iteration 32501:
Training Loss: -7.587761878967285
Reconstruction Loss: -11.49673843383789
Iteration 32601:
Training Loss: -7.5902557373046875
Reconstruction Loss: -11.498578071594238
Iteration 32701:
Training Loss: -7.592758655548096
Reconstruction Loss: -11.500422477722168
Iteration 32801:
Training Loss: -7.595260143280029
Reconstruction Loss: -11.502252578735352
Iteration 32901:
Training Loss: -7.59772253036499
Reconstruction Loss: -11.504085540771484
Iteration 33001:
Training Loss: -7.60019063949585
Reconstruction Loss: -11.50590705871582
Iteration 33101:
Training Loss: -7.602664470672607
Reconstruction Loss: -11.50772476196289
Iteration 33201:
Training Loss: -7.605146408081055
Reconstruction Loss: -11.509550094604492
Iteration 33301:
Training Loss: -7.607588291168213
Reconstruction Loss: -11.511354446411133
Iteration 33401:
Training Loss: -7.610042572021484
Reconstruction Loss: -11.513163566589355
Iteration 33501:
Training Loss: -7.612489700317383
Reconstruction Loss: -11.514971733093262
Iteration 33601:
Training Loss: -7.614949703216553
Reconstruction Loss: -11.516772270202637
Iteration 33701:
Training Loss: -7.617371559143066
Reconstruction Loss: -11.518568992614746
Iteration 33801:
Training Loss: -7.619810581207275
Reconstruction Loss: -11.520378112792969
Iteration 33901:
Training Loss: -7.622256278991699
Reconstruction Loss: -11.522168159484863
Iteration 34001:
Training Loss: -7.6246843338012695
Reconstruction Loss: -11.52396011352539
Iteration 34101:
Training Loss: -7.627102375030518
Reconstruction Loss: -11.525753021240234
Iteration 34201:
Training Loss: -7.62952184677124
Reconstruction Loss: -11.527532577514648
Iteration 34301:
Training Loss: -7.631930351257324
Reconstruction Loss: -11.529317855834961
Iteration 34401:
Training Loss: -7.634327411651611
Reconstruction Loss: -11.53109073638916
Iteration 34501:
Training Loss: -7.636738300323486
Reconstruction Loss: -11.532857894897461
Iteration 34601:
Training Loss: -7.639113903045654
Reconstruction Loss: -11.534619331359863
Iteration 34701:
Training Loss: -7.641489028930664
Reconstruction Loss: -11.536370277404785
Iteration 34801:
Training Loss: -7.6438775062561035
Reconstruction Loss: -11.538130760192871
Iteration 34901:
Training Loss: -7.646252155303955
Reconstruction Loss: -11.53988265991211
Iteration 35001:
Training Loss: -7.648627758026123
Reconstruction Loss: -11.541627883911133
Iteration 35101:
Training Loss: -7.650994777679443
Reconstruction Loss: -11.543378829956055
Iteration 35201:
Training Loss: -7.653375625610352
Reconstruction Loss: -11.545114517211914
Iteration 35301:
Training Loss: -7.655704498291016
Reconstruction Loss: -11.546847343444824
Iteration 35401:
Training Loss: -7.658063888549805
Reconstruction Loss: -11.548574447631836
Iteration 35501:
Training Loss: -7.6604156494140625
Reconstruction Loss: -11.550312042236328
Iteration 35601:
Training Loss: -7.662749290466309
Reconstruction Loss: -11.552024841308594
Iteration 35701:
Training Loss: -7.665085315704346
Reconstruction Loss: -11.553750991821289
Iteration 35801:
Training Loss: -7.667443752288818
Reconstruction Loss: -11.555473327636719
Iteration 35901:
Training Loss: -7.6697821617126465
Reconstruction Loss: -11.55718994140625
Iteration 36001:
Training Loss: -7.672099590301514
Reconstruction Loss: -11.558895111083984
Iteration 36101:
Training Loss: -7.674424648284912
Reconstruction Loss: -11.5606050491333
Iteration 36201:
Training Loss: -7.676723957061768
Reconstruction Loss: -11.562307357788086
Iteration 36301:
Training Loss: -7.679050922393799
Reconstruction Loss: -11.564013481140137
Iteration 36401:
Training Loss: -7.6813740730285645
Reconstruction Loss: -11.565716743469238
Iteration 36501:
Training Loss: -7.683676719665527
Reconstruction Loss: -11.56742000579834
Iteration 36601:
Training Loss: -7.685980319976807
Reconstruction Loss: -11.569114685058594
Iteration 36701:
Training Loss: -7.6882500648498535
Reconstruction Loss: -11.570808410644531
Iteration 36801:
Training Loss: -7.690558433532715
Reconstruction Loss: -11.572500228881836
Iteration 36901:
Training Loss: -7.692840099334717
Reconstruction Loss: -11.574190139770508
Iteration 37001:
Training Loss: -7.695131301879883
Reconstruction Loss: -11.575870513916016
Iteration 37101:
Training Loss: -7.697399616241455
Reconstruction Loss: -11.577557563781738
Iteration 37201:
Training Loss: -7.699678897857666
Reconstruction Loss: -11.579232215881348
Iteration 37301:
Training Loss: -7.701951503753662
Reconstruction Loss: -11.580902099609375
Iteration 37401:
Training Loss: -7.7042341232299805
Reconstruction Loss: -11.582572937011719
Iteration 37501:
Training Loss: -7.7064738273620605
Reconstruction Loss: -11.584254264831543
Iteration 37601:
Training Loss: -7.708743572235107
Reconstruction Loss: -11.58591365814209
Iteration 37701:
Training Loss: -7.710987091064453
Reconstruction Loss: -11.58757209777832
Iteration 37801:
Training Loss: -7.713256359100342
Reconstruction Loss: -11.589238166809082
Iteration 37901:
Training Loss: -7.715506553649902
Reconstruction Loss: -11.590901374816895
Iteration 38001:
Training Loss: -7.71773624420166
Reconstruction Loss: -11.592551231384277
Iteration 38101:
Training Loss: -7.719969272613525
Reconstruction Loss: -11.594204902648926
Iteration 38201:
Training Loss: -7.722221374511719
Reconstruction Loss: -11.595857620239258
Iteration 38301:
Training Loss: -7.724445343017578
Reconstruction Loss: -11.597502708435059
Iteration 38401:
Training Loss: -7.726677894592285
Reconstruction Loss: -11.599145889282227
Iteration 38501:
Training Loss: -7.728902339935303
Reconstruction Loss: -11.600787162780762
Iteration 38601:
Training Loss: -7.731100559234619
Reconstruction Loss: -11.602421760559082
Iteration 38701:
Training Loss: -7.733336925506592
Reconstruction Loss: -11.604050636291504
Iteration 38801:
Training Loss: -7.7355499267578125
Reconstruction Loss: -11.605674743652344
Iteration 38901:
Training Loss: -7.737748622894287
Reconstruction Loss: -11.6072998046875
Iteration 39001:
Training Loss: -7.739950180053711
Reconstruction Loss: -11.608924865722656
Iteration 39101:
Training Loss: -7.742137432098389
Reconstruction Loss: -11.610542297363281
Iteration 39201:
Training Loss: -7.744337558746338
Reconstruction Loss: -11.612143516540527
Iteration 39301:
Training Loss: -7.7465081214904785
Reconstruction Loss: -11.613751411437988
Iteration 39401:
Training Loss: -7.748697280883789
Reconstruction Loss: -11.615348815917969
Iteration 39501:
Training Loss: -7.750892162322998
Reconstruction Loss: -11.616941452026367
Iteration 39601:
Training Loss: -7.753043174743652
Reconstruction Loss: -11.61853313446045
Iteration 39701:
Training Loss: -7.755218029022217
Reconstruction Loss: -11.620118141174316
Iteration 39801:
Training Loss: -7.757369518280029
Reconstruction Loss: -11.621705055236816
Iteration 39901:
Training Loss: -7.759564399719238
Reconstruction Loss: -11.623290061950684
Iteration 40001:
Training Loss: -7.761714935302734
Reconstruction Loss: -11.624868392944336
Iteration 40101:
Training Loss: -7.763855457305908
Reconstruction Loss: -11.626442909240723
Iteration 40201:
Training Loss: -7.7660231590271
Reconstruction Loss: -11.62802505493164
Iteration 40301:
Training Loss: -7.7681660652160645
Reconstruction Loss: -11.629607200622559
Iteration 40401:
Training Loss: -7.770315170288086
Reconstruction Loss: -11.631168365478516
Iteration 40501:
Training Loss: -7.772468566894531
Reconstruction Loss: -11.632741928100586
Iteration 40601:
Training Loss: -7.774592399597168
Reconstruction Loss: -11.634305000305176
Iteration 40701:
Training Loss: -7.7767157554626465
Reconstruction Loss: -11.635866165161133
Iteration 40801:
Training Loss: -7.7788615226745605
Reconstruction Loss: -11.637438774108887
Iteration 40901:
Training Loss: -7.780966758728027
Reconstruction Loss: -11.638991355895996
Iteration 41001:
Training Loss: -7.783101558685303
Reconstruction Loss: -11.640548706054688
Iteration 41101:
Training Loss: -7.785228252410889
Reconstruction Loss: -11.642101287841797
Iteration 41201:
Training Loss: -7.7873454093933105
Reconstruction Loss: -11.643648147583008
Iteration 41301:
Training Loss: -7.789422988891602
Reconstruction Loss: -11.645197868347168
Iteration 41401:
Training Loss: -7.791532516479492
Reconstruction Loss: -11.646745681762695
Iteration 41501:
Training Loss: -7.793636322021484
Reconstruction Loss: -11.648283958435059
Iteration 41601:
Training Loss: -7.795755863189697
Reconstruction Loss: -11.649837493896484
Iteration 41701:
Training Loss: -7.797824859619141
Reconstruction Loss: -11.65137767791748
Iteration 41801:
Training Loss: -7.799941062927246
Reconstruction Loss: -11.65291690826416
Iteration 41901:
Training Loss: -7.802023410797119
Reconstruction Loss: -11.65445327758789
Iteration 42001:
Training Loss: -7.804125785827637
Reconstruction Loss: -11.65598201751709
Iteration 42101:
Training Loss: -7.806199073791504
Reconstruction Loss: -11.657512664794922
Iteration 42201:
Training Loss: -7.808291912078857
Reconstruction Loss: -11.659041404724121
Iteration 42301:
Training Loss: -7.81035041809082
Reconstruction Loss: -11.660566329956055
Iteration 42401:
Training Loss: -7.812435150146484
Reconstruction Loss: -11.66208267211914
Iteration 42501:
Training Loss: -7.814511299133301
Reconstruction Loss: -11.663599967956543
Iteration 42601:
Training Loss: -7.816572666168213
Reconstruction Loss: -11.665116310119629
Iteration 42701:
Training Loss: -7.818621635437012
Reconstruction Loss: -11.666625022888184
Iteration 42801:
Training Loss: -7.820679187774658
Reconstruction Loss: -11.668134689331055
Iteration 42901:
Training Loss: -7.822717189788818
Reconstruction Loss: -11.669629096984863
Iteration 43001:
Training Loss: -7.824797630310059
Reconstruction Loss: -11.67113208770752
Iteration 43101:
Training Loss: -7.8268303871154785
Reconstruction Loss: -11.672625541687012
Iteration 43201:
Training Loss: -7.828896999359131
Reconstruction Loss: -11.674127578735352
Iteration 43301:
Training Loss: -7.830920219421387
Reconstruction Loss: -11.675609588623047
Iteration 43401:
Training Loss: -7.832955360412598
Reconstruction Loss: -11.677101135253906
Iteration 43501:
Training Loss: -7.834966659545898
Reconstruction Loss: -11.6785888671875
Iteration 43601:
Training Loss: -7.8370041847229
Reconstruction Loss: -11.680068016052246
Iteration 43701:
Training Loss: -7.839041233062744
Reconstruction Loss: -11.681547164916992
Iteration 43801:
Training Loss: -7.841057777404785
Reconstruction Loss: -11.683024406433105
Iteration 43901:
Training Loss: -7.843067646026611
Reconstruction Loss: -11.68449592590332
Iteration 44001:
Training Loss: -7.845092296600342
Reconstruction Loss: -11.68596076965332
Iteration 44101:
Training Loss: -7.847084999084473
Reconstruction Loss: -11.687430381774902
Iteration 44201:
Training Loss: -7.849090576171875
Reconstruction Loss: -11.688901901245117
Iteration 44301:
Training Loss: -7.851099014282227
Reconstruction Loss: -11.690361022949219
Iteration 44401:
Training Loss: -7.853117942810059
Reconstruction Loss: -11.69182014465332
Iteration 44501:
Training Loss: -7.855113983154297
Reconstruction Loss: -11.693266868591309
Iteration 44601:
Training Loss: -7.857071876525879
Reconstruction Loss: -11.694717407226562
Iteration 44701:
Training Loss: -7.859068393707275
Reconstruction Loss: -11.696161270141602
Iteration 44801:
Training Loss: -7.861063003540039
Reconstruction Loss: -11.697593688964844
Iteration 44901:
Training Loss: -7.863058090209961
Reconstruction Loss: -11.69903564453125
Iteration 45001:
Training Loss: -7.865023136138916
Reconstruction Loss: -11.700478553771973
Iteration 45101:
Training Loss: -7.867005348205566
Reconstruction Loss: -11.701913833618164
Iteration 45201:
Training Loss: -7.868985176086426
Reconstruction Loss: -11.703363418579102
Iteration 45301:
Training Loss: -7.870966911315918
Reconstruction Loss: -11.704805374145508
Iteration 45401:
Training Loss: -7.872921943664551
Reconstruction Loss: -11.706236839294434
Iteration 45501:
Training Loss: -7.874873161315918
Reconstruction Loss: -11.707669258117676
Iteration 45601:
Training Loss: -7.8768463134765625
Reconstruction Loss: -11.70909309387207
Iteration 45701:
Training Loss: -7.878791332244873
Reconstruction Loss: -11.710515022277832
Iteration 45801:
Training Loss: -7.880753040313721
Reconstruction Loss: -11.711945533752441
Iteration 45901:
Training Loss: -7.882709503173828
Reconstruction Loss: -11.713377952575684
Iteration 46001:
Training Loss: -7.884666442871094
Reconstruction Loss: -11.714797973632812
Iteration 46101:
Training Loss: -7.88660192489624
Reconstruction Loss: -11.71623420715332
Iteration 46201:
Training Loss: -7.888546466827393
Reconstruction Loss: -11.717650413513184
Iteration 46301:
Training Loss: -7.890491008758545
Reconstruction Loss: -11.719059944152832
Iteration 46401:
Training Loss: -7.892416477203369
Reconstruction Loss: -11.720486640930176
Iteration 46501:
Training Loss: -7.894368648529053
Reconstruction Loss: -11.721898078918457
Iteration 46601:
Training Loss: -7.896289825439453
Reconstruction Loss: -11.723313331604004
Iteration 46701:
Training Loss: -7.8982415199279785
Reconstruction Loss: -11.724733352661133
Iteration 46801:
Training Loss: -7.900157451629639
Reconstruction Loss: -11.726141929626465
Iteration 46901:
Training Loss: -7.902101993560791
Reconstruction Loss: -11.727550506591797
Iteration 47001:
Training Loss: -7.904016017913818
Reconstruction Loss: -11.728952407836914
Iteration 47101:
Training Loss: -7.905930995941162
Reconstruction Loss: -11.730354309082031
Iteration 47201:
Training Loss: -7.907851696014404
Reconstruction Loss: -11.73175048828125
Iteration 47301:
Training Loss: -7.909776210784912
Reconstruction Loss: -11.733158111572266
Iteration 47401:
Training Loss: -7.9116740226745605
Reconstruction Loss: -11.734541893005371
Iteration 47501:
Training Loss: -7.913597106933594
Reconstruction Loss: -11.735943794250488
Iteration 47601:
Training Loss: -7.915506362915039
Reconstruction Loss: -11.737324714660645
Iteration 47701:
Training Loss: -7.917407512664795
Reconstruction Loss: -11.73871898651123
Iteration 47801:
Training Loss: -7.91929292678833
Reconstruction Loss: -11.740106582641602
Iteration 47901:
Training Loss: -7.921225547790527
Reconstruction Loss: -11.741487503051758
Iteration 48001:
Training Loss: -7.92310094833374
Reconstruction Loss: -11.742868423461914
Iteration 48101:
Training Loss: -7.9250006675720215
Reconstruction Loss: -11.744235038757324
Iteration 48201:
Training Loss: -7.926882266998291
Reconstruction Loss: -11.745610237121582
Iteration 48301:
Training Loss: -7.928743839263916
Reconstruction Loss: -11.746979713439941
Iteration 48401:
Training Loss: -7.930653095245361
Reconstruction Loss: -11.748342514038086
Iteration 48501:
Training Loss: -7.932510852813721
Reconstruction Loss: -11.749698638916016
Iteration 48601:
Training Loss: -7.9343743324279785
Reconstruction Loss: -11.751066207885742
Iteration 48701:
Training Loss: -7.936257362365723
Reconstruction Loss: -11.752427101135254
Iteration 48801:
Training Loss: -7.938117027282715
Reconstruction Loss: -11.753783226013184
Iteration 48901:
Training Loss: -7.93997859954834
Reconstruction Loss: -11.75514030456543
Iteration 49001:
Training Loss: -7.9418439865112305
Reconstruction Loss: -11.756495475769043
Iteration 49101:
Training Loss: -7.943710803985596
Reconstruction Loss: -11.757844924926758
Iteration 49201:
Training Loss: -7.945568084716797
Reconstruction Loss: -11.759208679199219
Iteration 49301:
Training Loss: -7.947415351867676
Reconstruction Loss: -11.760549545288086
Iteration 49401:
Training Loss: -7.949274063110352
Reconstruction Loss: -11.761902809143066
Iteration 49501:
Training Loss: -7.951138019561768
Reconstruction Loss: -11.763245582580566
Iteration 49601:
Training Loss: -7.952968120574951
Reconstruction Loss: -11.764588356018066
Iteration 49701:
Training Loss: -7.954800605773926
Reconstruction Loss: -11.765929222106934
Iteration 49801:
Training Loss: -7.956642150878906
Reconstruction Loss: -11.767269134521484
Iteration 49901:
Training Loss: -7.958489894866943
Reconstruction Loss: -11.768598556518555
