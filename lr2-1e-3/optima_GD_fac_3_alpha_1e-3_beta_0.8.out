5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.621350288391113
Reconstruction Loss: -0.49771571159362793
Iteration 101:
Training Loss: 4.563833236694336
Reconstruction Loss: -0.8746362924575806
Iteration 201:
Training Loss: 3.4557979106903076
Reconstruction Loss: -1.2817085981369019
Iteration 301:
Training Loss: 2.622453451156616
Reconstruction Loss: -1.6306204795837402
Iteration 401:
Training Loss: 1.9178606271743774
Reconstruction Loss: -1.948751449584961
Iteration 501:
Training Loss: 1.2632251977920532
Reconstruction Loss: -2.288275718688965
Iteration 601:
Training Loss: 0.6580039262771606
Reconstruction Loss: -2.6233572959899902
Iteration 701:
Training Loss: 0.1709980070590973
Reconstruction Loss: -2.9107284545898438
Iteration 801:
Training Loss: -0.2092243731021881
Reconstruction Loss: -3.149144172668457
Iteration 901:
Training Loss: -0.5174829959869385
Reconstruction Loss: -3.349909543991089
Iteration 1001:
Training Loss: -0.77610182762146
Reconstruction Loss: -3.522231101989746
Iteration 1101:
Training Loss: -0.9980416297912598
Reconstruction Loss: -3.672384262084961
Iteration 1201:
Training Loss: -1.1919554471969604
Reconstruction Loss: -3.804763078689575
Iteration 1301:
Training Loss: -1.364083170890808
Reconstruction Loss: -3.922577381134033
Iteration 1401:
Training Loss: -1.518986701965332
Reconstruction Loss: -4.028248310089111
Iteration 1501:
Training Loss: -1.6599839925765991
Reconstruction Loss: -4.12365198135376
Iteration 1601:
Training Loss: -1.78950834274292
Reconstruction Loss: -4.210274696350098
Iteration 1701:
Training Loss: -1.909361481666565
Reconstruction Loss: -4.289317607879639
Iteration 1801:
Training Loss: -2.020909070968628
Reconstruction Loss: -4.361767292022705
Iteration 1901:
Training Loss: -2.1252095699310303
Reconstruction Loss: -4.428445339202881
Iteration 2001:
Training Loss: -2.223101854324341
Reconstruction Loss: -4.490041255950928
Iteration 2101:
Training Loss: -2.3152692317962646
Reconstruction Loss: -4.547142028808594
Iteration 2201:
Training Loss: -2.4022743701934814
Reconstruction Loss: -4.600247383117676
Iteration 2301:
Training Loss: -2.484598159790039
Reconstruction Loss: -4.6497883796691895
Iteration 2401:
Training Loss: -2.5626444816589355
Reconstruction Loss: -4.696136474609375
Iteration 2501:
Training Loss: -2.636772394180298
Reconstruction Loss: -4.7396135330200195
Iteration 2601:
Training Loss: -2.7072901725769043
Reconstruction Loss: -4.780500888824463
Iteration 2701:
Training Loss: -2.7744734287261963
Reconstruction Loss: -4.819045066833496
Iteration 2801:
Training Loss: -2.838569402694702
Reconstruction Loss: -4.855462074279785
Iteration 2901:
Training Loss: -2.899797201156616
Reconstruction Loss: -4.889942169189453
Iteration 3001:
Training Loss: -2.958355665206909
Reconstruction Loss: -4.922653675079346
Iteration 3101:
Training Loss: -3.014423370361328
Reconstruction Loss: -4.953746795654297
Iteration 3201:
Training Loss: -3.0681731700897217
Reconstruction Loss: -4.9833550453186035
Iteration 3301:
Training Loss: -3.119749069213867
Reconstruction Loss: -5.011597633361816
Iteration 3401:
Training Loss: -3.1692910194396973
Reconstruction Loss: -5.03857946395874
Iteration 3501:
Training Loss: -3.216925859451294
Reconstruction Loss: -5.064397811889648
Iteration 3601:
Training Loss: -3.262773036956787
Reconstruction Loss: -5.089138984680176
Iteration 3701:
Training Loss: -3.306943416595459
Reconstruction Loss: -5.112878799438477
Iteration 3801:
Training Loss: -3.349531412124634
Reconstruction Loss: -5.1356892585754395
Iteration 3901:
Training Loss: -3.3906354904174805
Reconstruction Loss: -5.157634735107422
Iteration 4001:
Training Loss: -3.430341958999634
Reconstruction Loss: -5.178771495819092
Iteration 4101:
Training Loss: -3.4687283039093018
Reconstruction Loss: -5.199154376983643
Iteration 4201:
Training Loss: -3.5058703422546387
Reconstruction Loss: -5.218829154968262
Iteration 4301:
Training Loss: -3.5418384075164795
Reconstruction Loss: -5.23784065246582
Iteration 4401:
Training Loss: -3.57669997215271
Reconstruction Loss: -5.25623083114624
Iteration 4501:
Training Loss: -3.610513925552368
Reconstruction Loss: -5.274036407470703
Iteration 4601:
Training Loss: -3.643336534500122
Reconstruction Loss: -5.291291236877441
Iteration 4701:
Training Loss: -3.675217628479004
Reconstruction Loss: -5.308025360107422
Iteration 4801:
Training Loss: -3.70621395111084
Reconstruction Loss: -5.324269771575928
Iteration 4901:
Training Loss: -3.7363672256469727
Reconstruction Loss: -5.340049743652344
Iteration 5001:
Training Loss: -3.7657182216644287
Reconstruction Loss: -5.35538911819458
Iteration 5101:
Training Loss: -3.794313430786133
Reconstruction Loss: -5.370312690734863
Iteration 5201:
Training Loss: -3.822186231613159
Reconstruction Loss: -5.384842395782471
Iteration 5301:
Training Loss: -3.849374294281006
Reconstruction Loss: -5.398996353149414
Iteration 5401:
Training Loss: -3.8759090900421143
Reconstruction Loss: -5.412797927856445
Iteration 5501:
Training Loss: -3.901826858520508
Reconstruction Loss: -5.4262542724609375
Iteration 5601:
Training Loss: -3.9271490573883057
Reconstruction Loss: -5.439389705657959
Iteration 5701:
Training Loss: -3.951910972595215
Reconstruction Loss: -5.452215194702148
Iteration 5801:
Training Loss: -3.9761393070220947
Reconstruction Loss: -5.464745998382568
Iteration 5901:
Training Loss: -3.9998414516448975
Reconstruction Loss: -5.476993560791016
Iteration 6001:
Training Loss: -4.023059368133545
Reconstruction Loss: -5.48897647857666
Iteration 6101:
Training Loss: -4.045809745788574
Reconstruction Loss: -5.500695705413818
Iteration 6201:
Training Loss: -4.068110942840576
Reconstruction Loss: -5.512171745300293
Iteration 6301:
Training Loss: -4.0899786949157715
Reconstruction Loss: -5.523410797119141
Iteration 6401:
Training Loss: -4.11143684387207
Reconstruction Loss: -5.534424781799316
Iteration 6501:
Training Loss: -4.132501125335693
Reconstruction Loss: -5.545218467712402
Iteration 6601:
Training Loss: -4.153187274932861
Reconstruction Loss: -5.555805206298828
Iteration 6701:
Training Loss: -4.1735053062438965
Reconstruction Loss: -5.566186904907227
Iteration 6801:
Training Loss: -4.193477630615234
Reconstruction Loss: -5.5763750076293945
Iteration 6901:
Training Loss: -4.213112831115723
Reconstruction Loss: -5.58637809753418
Iteration 7001:
Training Loss: -4.232425689697266
Reconstruction Loss: -5.596199035644531
Iteration 7101:
Training Loss: -4.251428604125977
Reconstruction Loss: -5.605849742889404
Iteration 7201:
Training Loss: -4.270133018493652
Reconstruction Loss: -5.615332126617432
Iteration 7301:
Training Loss: -4.288549423217773
Reconstruction Loss: -5.624655246734619
Iteration 7401:
Training Loss: -4.306684970855713
Reconstruction Loss: -5.633816242218018
Iteration 7501:
Training Loss: -4.324552536010742
Reconstruction Loss: -5.642828941345215
Iteration 7601:
Training Loss: -4.342161178588867
Reconstruction Loss: -5.651694297790527
Iteration 7701:
Training Loss: -4.359520435333252
Reconstruction Loss: -5.6604180335998535
Iteration 7801:
Training Loss: -4.376633167266846
Reconstruction Loss: -5.66900634765625
Iteration 7901:
Training Loss: -4.393517017364502
Reconstruction Loss: -5.677462100982666
Iteration 8001:
Training Loss: -4.410175323486328
Reconstruction Loss: -5.685791969299316
Iteration 8101:
Training Loss: -4.426610946655273
Reconstruction Loss: -5.693994522094727
Iteration 8201:
Training Loss: -4.442838668823242
Reconstruction Loss: -5.702078342437744
Iteration 8301:
Training Loss: -4.458857536315918
Reconstruction Loss: -5.71004056930542
Iteration 8401:
Training Loss: -4.474677562713623
Reconstruction Loss: -5.717888832092285
Iteration 8501:
Training Loss: -4.490304946899414
Reconstruction Loss: -5.725628852844238
Iteration 8601:
Training Loss: -4.505743503570557
Reconstruction Loss: -5.7332634925842285
Iteration 8701:
Training Loss: -4.521000385284424
Reconstruction Loss: -5.740789413452148
Iteration 8801:
Training Loss: -4.53608512878418
Reconstruction Loss: -5.748211860656738
Iteration 8901:
Training Loss: -4.550990581512451
Reconstruction Loss: -5.755537033081055
Iteration 9001:
Training Loss: -4.565728187561035
Reconstruction Loss: -5.762763977050781
Iteration 9101:
Training Loss: -4.580310344696045
Reconstruction Loss: -5.769900321960449
Iteration 9201:
Training Loss: -4.594728469848633
Reconstruction Loss: -5.776944160461426
Iteration 9301:
Training Loss: -4.608994960784912
Reconstruction Loss: -5.783902645111084
Iteration 9401:
Training Loss: -4.623108386993408
Reconstruction Loss: -5.790767192840576
Iteration 9501:
Training Loss: -4.637077331542969
Reconstruction Loss: -5.797548770904541
Iteration 9601:
Training Loss: -4.650906562805176
Reconstruction Loss: -5.804245948791504
Iteration 9701:
Training Loss: -4.664593696594238
Reconstruction Loss: -5.8108649253845215
Iteration 9801:
Training Loss: -4.678146839141846
Reconstruction Loss: -5.817403793334961
Iteration 9901:
Training Loss: -4.691568851470947
Reconstruction Loss: -5.8238630294799805
Iteration 10001:
Training Loss: -4.704856872558594
Reconstruction Loss: -5.83024787902832
Iteration 10101:
Training Loss: -4.718023300170898
Reconstruction Loss: -5.836558818817139
Iteration 10201:
Training Loss: -4.7310590744018555
Reconstruction Loss: -5.842797756195068
Iteration 10301:
Training Loss: -4.743983745574951
Reconstruction Loss: -5.8489670753479
Iteration 10401:
Training Loss: -4.756789684295654
Reconstruction Loss: -5.855067253112793
Iteration 10501:
Training Loss: -4.769476413726807
Reconstruction Loss: -5.861102104187012
Iteration 10601:
Training Loss: -4.782053470611572
Reconstruction Loss: -5.867073059082031
Iteration 10701:
Training Loss: -4.794522285461426
Reconstruction Loss: -5.872974395751953
Iteration 10801:
Training Loss: -4.806880950927734
Reconstruction Loss: -5.878811359405518
Iteration 10901:
Training Loss: -4.819128513336182
Reconstruction Loss: -5.884583473205566
Iteration 11001:
Training Loss: -4.8312764167785645
Reconstruction Loss: -5.8902974128723145
Iteration 11101:
Training Loss: -4.843321800231934
Reconstruction Loss: -5.895955562591553
Iteration 11201:
Training Loss: -4.855274200439453
Reconstruction Loss: -5.901554107666016
Iteration 11301:
Training Loss: -4.867124080657959
Reconstruction Loss: -5.907094955444336
Iteration 11401:
Training Loss: -4.878880977630615
Reconstruction Loss: -5.912581443786621
Iteration 11501:
Training Loss: -4.8905439376831055
Reconstruction Loss: -5.9180097579956055
Iteration 11601:
Training Loss: -4.902112007141113
Reconstruction Loss: -5.923379421234131
Iteration 11701:
Training Loss: -4.913588523864746
Reconstruction Loss: -5.92869758605957
Iteration 11801:
Training Loss: -4.924987316131592
Reconstruction Loss: -5.933964729309082
Iteration 11901:
Training Loss: -4.936290264129639
Reconstruction Loss: -5.939183235168457
Iteration 12001:
Training Loss: -4.947517395019531
Reconstruction Loss: -5.9443559646606445
Iteration 12101:
Training Loss: -4.958651542663574
Reconstruction Loss: -5.949478626251221
Iteration 12201:
Training Loss: -4.969705581665039
Reconstruction Loss: -5.954549789428711
Iteration 12301:
Training Loss: -4.980681419372559
Reconstruction Loss: -5.959569454193115
Iteration 12401:
Training Loss: -4.991580009460449
Reconstruction Loss: -5.964544773101807
Iteration 12501:
Training Loss: -5.002392768859863
Reconstruction Loss: -5.969472885131836
Iteration 12601:
Training Loss: -5.013136863708496
Reconstruction Loss: -5.9743547439575195
Iteration 12701:
Training Loss: -5.023808002471924
Reconstruction Loss: -5.979191780090332
Iteration 12801:
Training Loss: -5.034399032592773
Reconstruction Loss: -5.983987808227539
Iteration 12901:
Training Loss: -5.044919013977051
Reconstruction Loss: -5.988741397857666
Iteration 13001:
Training Loss: -5.055367946624756
Reconstruction Loss: -5.993453502655029
Iteration 13101:
Training Loss: -5.065744876861572
Reconstruction Loss: -5.998123645782471
Iteration 13201:
Training Loss: -5.0760579109191895
Reconstruction Loss: -6.002756118774414
Iteration 13301:
Training Loss: -5.086293697357178
Reconstruction Loss: -6.0073442459106445
Iteration 13401:
Training Loss: -5.096471786499023
Reconstruction Loss: -6.011891841888428
Iteration 13501:
Training Loss: -5.106575012207031
Reconstruction Loss: -6.016400337219238
Iteration 13601:
Training Loss: -5.11661958694458
Reconstruction Loss: -6.020869255065918
Iteration 13701:
Training Loss: -5.126593112945557
Reconstruction Loss: -6.025304794311523
Iteration 13801:
Training Loss: -5.136510848999023
Reconstruction Loss: -6.029703140258789
Iteration 13901:
Training Loss: -5.146361827850342
Reconstruction Loss: -6.034058570861816
Iteration 14001:
Training Loss: -5.15615177154541
Reconstruction Loss: -6.038381099700928
Iteration 14101:
Training Loss: -5.165884017944336
Reconstruction Loss: -6.04266357421875
Iteration 14201:
Training Loss: -5.17555046081543
Reconstruction Loss: -6.0469136238098145
Iteration 14301:
Training Loss: -5.185159206390381
Reconstruction Loss: -6.051133155822754
Iteration 14401:
Training Loss: -5.194711208343506
Reconstruction Loss: -6.05531644821167
Iteration 14501:
Training Loss: -5.204207420349121
Reconstruction Loss: -6.059464931488037
Iteration 14601:
Training Loss: -5.213656902313232
Reconstruction Loss: -6.063584327697754
Iteration 14701:
Training Loss: -5.2230353355407715
Reconstruction Loss: -6.067667484283447
Iteration 14801:
Training Loss: -5.232357025146484
Reconstruction Loss: -6.071713447570801
Iteration 14901:
Training Loss: -5.241628170013428
Reconstruction Loss: -6.0757293701171875
