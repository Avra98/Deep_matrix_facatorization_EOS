5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.537535190582275
Reconstruction Loss: -0.2755371630191803
Iteration 51:
Training Loss: 2.6748740673065186
Reconstruction Loss: -1.2203469276428223
Iteration 101:
Training Loss: 1.5717456340789795
Reconstruction Loss: -1.6846942901611328
Iteration 151:
Training Loss: 0.8114306926727295
Reconstruction Loss: -2.0738794803619385
Iteration 201:
Training Loss: 0.11858464777469635
Reconstruction Loss: -2.3622467517852783
Iteration 251:
Training Loss: -0.31201010942459106
Reconstruction Loss: -2.589738607406616
Iteration 301:
Training Loss: -0.7496642470359802
Reconstruction Loss: -2.77292537689209
Iteration 351:
Training Loss: -0.9194946885108948
Reconstruction Loss: -2.9209530353546143
Iteration 401:
Training Loss: -1.1049867868423462
Reconstruction Loss: -3.0505266189575195
Iteration 451:
Training Loss: -1.2531944513320923
Reconstruction Loss: -3.161134719848633
Iteration 501:
Training Loss: -1.544205904006958
Reconstruction Loss: -3.256629228591919
Iteration 551:
Training Loss: -1.7335546016693115
Reconstruction Loss: -3.3442599773406982
Iteration 601:
Training Loss: -1.8717633485794067
Reconstruction Loss: -3.4181323051452637
Iteration 651:
Training Loss: -2.040015935897827
Reconstruction Loss: -3.488215208053589
Iteration 701:
Training Loss: -2.153568744659424
Reconstruction Loss: -3.5491957664489746
Iteration 751:
Training Loss: -2.2186689376831055
Reconstruction Loss: -3.6037418842315674
Iteration 801:
Training Loss: -2.3617966175079346
Reconstruction Loss: -3.652277708053589
Iteration 851:
Training Loss: -2.5657389163970947
Reconstruction Loss: -3.698451519012451
Iteration 901:
Training Loss: -2.5729684829711914
Reconstruction Loss: -3.7387349605560303
Iteration 951:
Training Loss: -2.7429347038269043
Reconstruction Loss: -3.77809739112854
Iteration 1001:
Training Loss: -2.856548309326172
Reconstruction Loss: -3.8113231658935547
Iteration 1051:
Training Loss: -2.9245266914367676
Reconstruction Loss: -3.8437588214874268
Iteration 1101:
Training Loss: -2.984395980834961
Reconstruction Loss: -3.873167037963867
Iteration 1151:
Training Loss: -2.990715503692627
Reconstruction Loss: -3.9007697105407715
Iteration 1201:
Training Loss: -3.1808013916015625
Reconstruction Loss: -3.926353693008423
Iteration 1251:
Training Loss: -3.284628391265869
Reconstruction Loss: -3.949824571609497
Iteration 1301:
Training Loss: -3.151369094848633
Reconstruction Loss: -3.972384214401245
Iteration 1351:
Training Loss: -3.422184705734253
Reconstruction Loss: -3.99379301071167
Iteration 1401:
Training Loss: -3.4557414054870605
Reconstruction Loss: -4.0128173828125
Iteration 1451:
Training Loss: -3.433555841445923
Reconstruction Loss: -4.033061981201172
Iteration 1501:
Training Loss: -3.454558849334717
Reconstruction Loss: -4.049136638641357
Iteration 1551:
Training Loss: -3.509092330932617
Reconstruction Loss: -4.0647382736206055
Iteration 1601:
Training Loss: -3.5424156188964844
Reconstruction Loss: -4.081818580627441
Iteration 1651:
Training Loss: -3.577146530151367
Reconstruction Loss: -4.096376419067383
Iteration 1701:
Training Loss: -3.6742608547210693
Reconstruction Loss: -4.110711574554443
Iteration 1751:
Training Loss: -3.8025929927825928
Reconstruction Loss: -4.124688625335693
Iteration 1801:
Training Loss: -3.9485018253326416
Reconstruction Loss: -4.138236045837402
Iteration 1851:
Training Loss: -3.8449807167053223
Reconstruction Loss: -4.1497344970703125
Iteration 1901:
Training Loss: -3.95169997215271
Reconstruction Loss: -4.162024974822998
Iteration 1951:
Training Loss: -3.8186521530151367
Reconstruction Loss: -4.173062324523926
Iteration 2001:
Training Loss: -3.9257774353027344
Reconstruction Loss: -4.183469295501709
Iteration 2051:
Training Loss: -4.07029914855957
Reconstruction Loss: -4.193244457244873
Iteration 2101:
Training Loss: -3.932478666305542
Reconstruction Loss: -4.2035675048828125
Iteration 2151:
Training Loss: -4.014375686645508
Reconstruction Loss: -4.213011741638184
Iteration 2201:
Training Loss: -4.1108551025390625
Reconstruction Loss: -4.2238264083862305
Iteration 2251:
Training Loss: -4.145415782928467
Reconstruction Loss: -4.2315778732299805
Iteration 2301:
Training Loss: -4.250903606414795
Reconstruction Loss: -4.2400431632995605
Iteration 2351:
Training Loss: -4.162425994873047
Reconstruction Loss: -4.24853515625
Iteration 2401:
Training Loss: -4.22253942489624
Reconstruction Loss: -4.255619525909424
Iteration 2451:
Training Loss: -4.290019989013672
Reconstruction Loss: -4.263272285461426
Iteration 2501:
Training Loss: -4.344449043273926
Reconstruction Loss: -4.271383762359619
Iteration 2551:
Training Loss: -4.3632121086120605
Reconstruction Loss: -4.278803825378418
Iteration 2601:
Training Loss: -4.433816432952881
Reconstruction Loss: -4.285635948181152
Iteration 2651:
Training Loss: -4.463491439819336
Reconstruction Loss: -4.2920122146606445
Iteration 2701:
Training Loss: -4.513321876525879
Reconstruction Loss: -4.298093795776367
Iteration 2751:
Training Loss: -4.607551097869873
Reconstruction Loss: -4.305482864379883
Iteration 2801:
Training Loss: -4.615817546844482
Reconstruction Loss: -4.310497283935547
Iteration 2851:
Training Loss: -4.54254674911499
Reconstruction Loss: -4.317752361297607
Iteration 2901:
Training Loss: -4.5693182945251465
Reconstruction Loss: -4.322976112365723
Iteration 2951:
Training Loss: -4.67996072769165
Reconstruction Loss: -4.328765392303467
Iteration 3001:
Training Loss: -4.791822910308838
Reconstruction Loss: -4.334346294403076
Iteration 3051:
Training Loss: -4.742640018463135
Reconstruction Loss: -4.338669300079346
Iteration 3101:
Training Loss: -4.695116996765137
Reconstruction Loss: -4.344562530517578
Iteration 3151:
Training Loss: -4.712565898895264
Reconstruction Loss: -4.348857402801514
Iteration 3201:
Training Loss: -4.793349742889404
Reconstruction Loss: -4.354304790496826
Iteration 3251:
Training Loss: -4.8029866218566895
Reconstruction Loss: -4.358523368835449
Iteration 3301:
Training Loss: -4.894533634185791
Reconstruction Loss: -4.363741397857666
Iteration 3351:
Training Loss: -4.855236053466797
Reconstruction Loss: -4.367913722991943
Iteration 3401:
Training Loss: -4.878735542297363
Reconstruction Loss: -4.3726091384887695
Iteration 3451:
Training Loss: -4.976223468780518
Reconstruction Loss: -4.3763275146484375
Iteration 3501:
Training Loss: -4.870160102844238
Reconstruction Loss: -4.380276679992676
Iteration 3551:
Training Loss: -5.0914177894592285
Reconstruction Loss: -4.384936332702637
Iteration 3601:
Training Loss: -5.1173553466796875
Reconstruction Loss: -4.3887128829956055
Iteration 3651:
Training Loss: -4.931878089904785
Reconstruction Loss: -4.3913726806640625
Iteration 3701:
Training Loss: -5.019220352172852
Reconstruction Loss: -4.3945746421813965
Iteration 3751:
Training Loss: -5.075332164764404
Reconstruction Loss: -4.4004998207092285
Iteration 3801:
Training Loss: -5.027710437774658
Reconstruction Loss: -4.402250289916992
Iteration 3851:
Training Loss: -5.069019317626953
Reconstruction Loss: -4.406507968902588
Iteration 3901:
Training Loss: -5.113524913787842
Reconstruction Loss: -4.4094085693359375
Iteration 3951:
Training Loss: -5.273994445800781
Reconstruction Loss: -4.413450241088867
Iteration 4001:
Training Loss: -5.309732913970947
Reconstruction Loss: -4.416494846343994
Iteration 4051:
Training Loss: -5.220820426940918
Reconstruction Loss: -4.419642448425293
Iteration 4101:
Training Loss: -5.169419288635254
Reconstruction Loss: -4.422649383544922
Iteration 4151:
Training Loss: -5.253357887268066
Reconstruction Loss: -4.42590856552124
Iteration 4201:
Training Loss: -5.238339424133301
Reconstruction Loss: -4.428922653198242
Iteration 4251:
Training Loss: -5.409446716308594
Reconstruction Loss: -4.4317426681518555
Iteration 4301:
Training Loss: -5.416574001312256
Reconstruction Loss: -4.43400764465332
Iteration 4351:
Training Loss: -5.3539042472839355
Reconstruction Loss: -4.437401294708252
Iteration 4401:
Training Loss: -5.31011962890625
Reconstruction Loss: -4.4395647048950195
Iteration 4451:
Training Loss: -5.474292755126953
Reconstruction Loss: -4.442488670349121
Iteration 4501:
Training Loss: -5.486285209655762
Reconstruction Loss: -4.4451680183410645
Iteration 4551:
Training Loss: -5.507290840148926
Reconstruction Loss: -4.448144912719727
Iteration 4601:
Training Loss: -5.469267845153809
Reconstruction Loss: -4.449647903442383
Iteration 4651:
Training Loss: -5.455291271209717
Reconstruction Loss: -4.452166557312012
Iteration 4701:
Training Loss: -5.487792015075684
Reconstruction Loss: -4.455119609832764
Iteration 4751:
Training Loss: -5.478185653686523
Reconstruction Loss: -4.457566261291504
Iteration 4801:
Training Loss: -5.534809589385986
Reconstruction Loss: -4.459653854370117
Iteration 4851:
Training Loss: -5.58292818069458
Reconstruction Loss: -4.461633682250977
Iteration 4901:
Training Loss: -5.649249076843262
Reconstruction Loss: -4.464293003082275
Iteration 4951:
Training Loss: -5.831290245056152
Reconstruction Loss: -4.46655797958374
Iteration 5001:
Training Loss: -5.717983245849609
Reconstruction Loss: -4.46811580657959
Iteration 5051:
Training Loss: -5.697323322296143
Reconstruction Loss: -4.470432281494141
Iteration 5101:
Training Loss: -5.677943229675293
Reconstruction Loss: -4.472639083862305
Iteration 5151:
Training Loss: -5.696834564208984
Reconstruction Loss: -4.474939346313477
Iteration 5201:
Training Loss: -5.677903652191162
Reconstruction Loss: -4.476449966430664
Iteration 5251:
Training Loss: -5.687911510467529
Reconstruction Loss: -4.4788818359375
Iteration 5301:
Training Loss: -5.641152381896973
Reconstruction Loss: -4.480427265167236
Iteration 5351:
Training Loss: -5.696220874786377
Reconstruction Loss: -4.48309850692749
Iteration 5401:
Training Loss: -5.866772174835205
Reconstruction Loss: -4.484673500061035
Iteration 5451:
Training Loss: -5.827540397644043
Reconstruction Loss: -4.48618221282959
Iteration 5501:
Training Loss: -5.800914287567139
Reconstruction Loss: -4.4872212409973145
Iteration 5551:
Training Loss: -5.995646953582764
Reconstruction Loss: -4.489816188812256
Iteration 5601:
Training Loss: -5.795045375823975
Reconstruction Loss: -4.491280555725098
Iteration 5651:
Training Loss: -5.851085662841797
Reconstruction Loss: -4.492486953735352
Iteration 5701:
Training Loss: -5.972496032714844
Reconstruction Loss: -4.495120048522949
Iteration 5751:
Training Loss: -6.001959323883057
Reconstruction Loss: -4.4961957931518555
Iteration 5801:
Training Loss: -5.965231418609619
Reconstruction Loss: -4.498251914978027
Iteration 5851:
Training Loss: -6.067846298217773
Reconstruction Loss: -4.4999918937683105
Iteration 5901:
Training Loss: -5.932352066040039
Reconstruction Loss: -4.501588344573975
Iteration 5951:
Training Loss: -6.144969940185547
Reconstruction Loss: -4.502753257751465
Iteration 6001:
Training Loss: -6.157967567443848
Reconstruction Loss: -4.504289150238037
Iteration 6051:
Training Loss: -5.986597537994385
Reconstruction Loss: -4.505270481109619
Iteration 6101:
Training Loss: -6.152706146240234
Reconstruction Loss: -4.507416248321533
Iteration 6151:
Training Loss: -6.10335111618042
Reconstruction Loss: -4.508371353149414
Iteration 6201:
Training Loss: -6.156959056854248
Reconstruction Loss: -4.50996208190918
Iteration 6251:
Training Loss: -6.088441371917725
Reconstruction Loss: -4.511241912841797
Iteration 6301:
Training Loss: -6.191226959228516
Reconstruction Loss: -4.512545585632324
Iteration 6351:
Training Loss: -6.184095859527588
Reconstruction Loss: -4.5142035484313965
Iteration 6401:
Training Loss: -6.19149112701416
Reconstruction Loss: -4.515536308288574
Iteration 6451:
Training Loss: -6.225165367126465
Reconstruction Loss: -4.5164570808410645
Iteration 6501:
Training Loss: -6.149689197540283
Reconstruction Loss: -4.518094062805176
Iteration 6551:
Training Loss: -6.3037824630737305
Reconstruction Loss: -4.519148349761963
Iteration 6601:
Training Loss: -6.235917568206787
Reconstruction Loss: -4.520644664764404
Iteration 6651:
Training Loss: -6.280254364013672
Reconstruction Loss: -4.52203893661499
Iteration 6701:
Training Loss: -6.31589937210083
Reconstruction Loss: -4.5232930183410645
Iteration 6751:
Training Loss: -6.308295249938965
Reconstruction Loss: -4.5241804122924805
Iteration 6801:
Training Loss: -6.405601978302002
Reconstruction Loss: -4.525660037994385
Iteration 6851:
Training Loss: -6.32496452331543
Reconstruction Loss: -4.526082515716553
Iteration 6901:
Training Loss: -6.495214462280273
Reconstruction Loss: -4.527878761291504
Iteration 6951:
Training Loss: -6.255101203918457
Reconstruction Loss: -4.528888702392578
Iteration 7001:
Training Loss: -6.535375595092773
Reconstruction Loss: -4.529656410217285
Iteration 7051:
Training Loss: -6.510092258453369
Reconstruction Loss: -4.531091690063477
Iteration 7101:
Training Loss: -6.373331069946289
Reconstruction Loss: -4.532204627990723
Iteration 7151:
Training Loss: -6.353776454925537
Reconstruction Loss: -4.532831192016602
Iteration 7201:
Training Loss: -6.439070701599121
Reconstruction Loss: -4.53418493270874
Iteration 7251:
Training Loss: -6.2895941734313965
Reconstruction Loss: -4.535232067108154
Iteration 7301:
Training Loss: -6.59666633605957
Reconstruction Loss: -4.5364155769348145
Iteration 7351:
Training Loss: -6.412238121032715
Reconstruction Loss: -4.537379264831543
Iteration 7401:
Training Loss: -6.565049648284912
Reconstruction Loss: -4.5383782386779785
Iteration 7451:
Training Loss: -6.453439235687256
Reconstruction Loss: -4.539378643035889
Iteration 7501:
Training Loss: -6.640122890472412
Reconstruction Loss: -4.540005207061768
Iteration 7551:
Training Loss: -6.595215797424316
Reconstruction Loss: -4.54093074798584
Iteration 7601:
Training Loss: -6.572052955627441
Reconstruction Loss: -4.5420122146606445
Iteration 7651:
Training Loss: -6.685642719268799
Reconstruction Loss: -4.542633533477783
Iteration 7701:
Training Loss: -6.692885875701904
Reconstruction Loss: -4.543781280517578
Iteration 7751:
Training Loss: -6.570680618286133
Reconstruction Loss: -4.5449934005737305
Iteration 7801:
Training Loss: -6.798767566680908
Reconstruction Loss: -4.545780658721924
Iteration 7851:
Training Loss: -6.753182888031006
Reconstruction Loss: -4.546455383300781
Iteration 7901:
Training Loss: -6.669226169586182
Reconstruction Loss: -4.547183036804199
Iteration 7951:
Training Loss: -6.802481174468994
Reconstruction Loss: -4.548184871673584
Iteration 8001:
Training Loss: -6.898518085479736
Reconstruction Loss: -4.549113750457764
Iteration 8051:
Training Loss: -6.848388671875
Reconstruction Loss: -4.549808025360107
Iteration 8101:
Training Loss: -6.71599817276001
Reconstruction Loss: -4.550835132598877
Iteration 8151:
Training Loss: -6.851676940917969
Reconstruction Loss: -4.551191329956055
Iteration 8201:
Training Loss: -6.7672119140625
Reconstruction Loss: -4.552579402923584
Iteration 8251:
Training Loss: -6.811734199523926
Reconstruction Loss: -4.5535688400268555
Iteration 8301:
Training Loss: -6.839085578918457
Reconstruction Loss: -4.553462505340576
Iteration 8351:
Training Loss: -6.810809135437012
Reconstruction Loss: -4.55420446395874
Iteration 8401:
Training Loss: -6.939857482910156
Reconstruction Loss: -4.55544376373291
Iteration 8451:
Training Loss: -6.9273176193237305
Reconstruction Loss: -4.55588436126709
Iteration 8501:
Training Loss: -6.906473636627197
Reconstruction Loss: -4.556847095489502
Iteration 8551:
Training Loss: -6.968201160430908
Reconstruction Loss: -4.5575151443481445
Iteration 8601:
Training Loss: -6.990401268005371
Reconstruction Loss: -4.5583343505859375
Iteration 8651:
Training Loss: -7.215795040130615
Reconstruction Loss: -4.559004783630371
Iteration 8701:
Training Loss: -7.140069484710693
Reconstruction Loss: -4.559464454650879
Iteration 8751:
Training Loss: -7.0415191650390625
Reconstruction Loss: -4.560450077056885
Iteration 8801:
Training Loss: -7.008995056152344
Reconstruction Loss: -4.561127662658691
Iteration 8851:
Training Loss: -7.049469470977783
Reconstruction Loss: -4.561854839324951
Iteration 8901:
Training Loss: -7.197543621063232
Reconstruction Loss: -4.562572956085205
Iteration 8951:
Training Loss: -6.958575248718262
Reconstruction Loss: -4.562738418579102
Iteration 9001:
Training Loss: -7.016903400421143
Reconstruction Loss: -4.563535213470459
Iteration 9051:
Training Loss: -7.1354827880859375
Reconstruction Loss: -4.5642595291137695
Iteration 9101:
Training Loss: -7.065898895263672
Reconstruction Loss: -4.564988136291504
Iteration 9151:
Training Loss: -7.091744422912598
Reconstruction Loss: -4.565219879150391
Iteration 9201:
Training Loss: -7.177486419677734
Reconstruction Loss: -4.566226005554199
Iteration 9251:
Training Loss: -7.238652229309082
Reconstruction Loss: -4.5666937828063965
Iteration 9301:
Training Loss: -7.276211738586426
Reconstruction Loss: -4.567275047302246
Iteration 9351:
Training Loss: -7.324343681335449
Reconstruction Loss: -4.5678839683532715
Iteration 9401:
Training Loss: -7.134627342224121
Reconstruction Loss: -4.568493366241455
Iteration 9451:
Training Loss: -7.229520320892334
Reconstruction Loss: -4.569187641143799
Iteration 9501:
Training Loss: -7.2003493309021
Reconstruction Loss: -4.569457530975342
Iteration 9551:
Training Loss: -7.230104446411133
Reconstruction Loss: -4.570009231567383
Iteration 9601:
Training Loss: -7.227313995361328
Reconstruction Loss: -4.570735931396484
Iteration 9651:
Training Loss: -7.276259422302246
Reconstruction Loss: -4.57131814956665
Iteration 9701:
Training Loss: -7.196713924407959
Reconstruction Loss: -4.571682929992676
Iteration 9751:
Training Loss: -7.27930212020874
Reconstruction Loss: -4.572314262390137
Iteration 9801:
Training Loss: -7.3870720863342285
Reconstruction Loss: -4.572847366333008
Iteration 9851:
Training Loss: -7.419705390930176
Reconstruction Loss: -4.5735931396484375
Iteration 9901:
Training Loss: -7.313113212585449
Reconstruction Loss: -4.5738420486450195
Iteration 9951:
Training Loss: -7.362095355987549
Reconstruction Loss: -4.574367046356201
Iteration 10001:
Training Loss: -7.5356550216674805
Reconstruction Loss: -4.574925422668457
Iteration 10051:
Training Loss: -7.39314079284668
Reconstruction Loss: -4.575626373291016
Iteration 10101:
Training Loss: -7.520828723907471
Reconstruction Loss: -4.575766563415527
Iteration 10151:
Training Loss: -7.323083877563477
Reconstruction Loss: -4.576555252075195
Iteration 10201:
Training Loss: -7.462963581085205
Reconstruction Loss: -4.576748847961426
Iteration 10251:
Training Loss: -7.474507808685303
Reconstruction Loss: -4.577373027801514
Iteration 10301:
Training Loss: -7.406989097595215
Reconstruction Loss: -4.577907085418701
Iteration 10351:
Training Loss: -7.403365135192871
Reconstruction Loss: -4.578175067901611
Iteration 10401:
Training Loss: -7.373154640197754
Reconstruction Loss: -4.578790664672852
Iteration 10451:
Training Loss: -7.485603332519531
Reconstruction Loss: -4.579203128814697
Iteration 10501:
Training Loss: -7.539912223815918
Reconstruction Loss: -4.579596519470215
Iteration 10551:
Training Loss: -7.737953186035156
Reconstruction Loss: -4.58022928237915
Iteration 10601:
Training Loss: -7.619156360626221
Reconstruction Loss: -4.5804924964904785
Iteration 10651:
Training Loss: -7.748687267303467
Reconstruction Loss: -4.580953121185303
Iteration 10701:
Training Loss: -7.632722854614258
Reconstruction Loss: -4.581435680389404
Iteration 10751:
Training Loss: -7.72885799407959
Reconstruction Loss: -4.581899166107178
Iteration 10801:
Training Loss: -7.73650598526001
Reconstruction Loss: -4.582264423370361
Iteration 10851:
Training Loss: -7.64093542098999
Reconstruction Loss: -4.582757949829102
Iteration 10901:
Training Loss: -7.6158952713012695
Reconstruction Loss: -4.583003520965576
Iteration 10951:
Training Loss: -7.824611663818359
Reconstruction Loss: -4.5838422775268555
Iteration 11001:
Training Loss: -7.793842792510986
Reconstruction Loss: -4.583961009979248
Iteration 11051:
Training Loss: -7.778885364532471
Reconstruction Loss: -4.584541320800781
Iteration 11101:
Training Loss: -7.560202598571777
Reconstruction Loss: -4.584774971008301
Iteration 11151:
Training Loss: -7.727898120880127
Reconstruction Loss: -4.585180759429932
Iteration 11201:
Training Loss: -7.770254135131836
Reconstruction Loss: -4.585751056671143
Iteration 11251:
Training Loss: -7.889996528625488
Reconstruction Loss: -4.585844039916992
Iteration 11301:
Training Loss: -7.808281421661377
Reconstruction Loss: -4.5862226486206055
Iteration 11351:
Training Loss: -7.727467060089111
Reconstruction Loss: -4.586611747741699
Iteration 11401:
Training Loss: -7.7185821533203125
Reconstruction Loss: -4.586890697479248
Iteration 11451:
Training Loss: -7.906161785125732
Reconstruction Loss: -4.587461948394775
Iteration 11501:
Training Loss: -7.988859176635742
Reconstruction Loss: -4.58786678314209
Iteration 11551:
Training Loss: -7.819109916687012
Reconstruction Loss: -4.587893486022949
Iteration 11601:
Training Loss: -8.011207580566406
Reconstruction Loss: -4.58848762512207
Iteration 11651:
Training Loss: -7.982603073120117
Reconstruction Loss: -4.588686466217041
Iteration 11701:
Training Loss: -7.8518290519714355
Reconstruction Loss: -4.588881015777588
Iteration 11751:
Training Loss: -7.854663848876953
Reconstruction Loss: -4.589445114135742
Iteration 11801:
Training Loss: -7.89448356628418
Reconstruction Loss: -4.5897016525268555
Iteration 11851:
Training Loss: -7.983760356903076
Reconstruction Loss: -4.590274333953857
Iteration 11901:
Training Loss: -8.122659683227539
Reconstruction Loss: -4.590388774871826
Iteration 11951:
Training Loss: -7.870174884796143
Reconstruction Loss: -4.590859413146973
Iteration 12001:
Training Loss: -8.054916381835938
Reconstruction Loss: -4.5910420417785645
Iteration 12051:
Training Loss: -8.077444076538086
Reconstruction Loss: -4.591378688812256
Iteration 12101:
Training Loss: -8.170913696289062
Reconstruction Loss: -4.591853141784668
Iteration 12151:
Training Loss: -8.130906105041504
Reconstruction Loss: -4.5922369956970215
Iteration 12201:
Training Loss: -7.986166954040527
Reconstruction Loss: -4.592478275299072
Iteration 12251:
Training Loss: -8.167255401611328
Reconstruction Loss: -4.592701435089111
Iteration 12301:
Training Loss: -8.034876823425293
Reconstruction Loss: -4.592956066131592
Iteration 12351:
Training Loss: -8.145125389099121
Reconstruction Loss: -4.593348979949951
Iteration 12401:
Training Loss: -8.05315113067627
Reconstruction Loss: -4.593589782714844
Iteration 12451:
Training Loss: -8.043464660644531
Reconstruction Loss: -4.593921661376953
Iteration 12501:
Training Loss: -8.210453987121582
Reconstruction Loss: -4.5940680503845215
Iteration 12551:
Training Loss: -8.19303035736084
Reconstruction Loss: -4.5944108963012695
Iteration 12601:
Training Loss: -8.158595085144043
Reconstruction Loss: -4.594709873199463
Iteration 12651:
Training Loss: -8.29759693145752
Reconstruction Loss: -4.594939231872559
Iteration 12701:
Training Loss: -8.225318908691406
Reconstruction Loss: -4.595343589782715
Iteration 12751:
Training Loss: -8.135932922363281
Reconstruction Loss: -4.595436096191406
Iteration 12801:
Training Loss: -8.159221649169922
Reconstruction Loss: -4.595996856689453
Iteration 12851:
Training Loss: -8.409310340881348
Reconstruction Loss: -4.596106052398682
Iteration 12901:
Training Loss: -8.165595054626465
Reconstruction Loss: -4.59630823135376
Iteration 12951:
Training Loss: -8.20571517944336
Reconstruction Loss: -4.596629619598389
Iteration 13001:
Training Loss: -8.449501037597656
Reconstruction Loss: -4.596895217895508
Iteration 13051:
Training Loss: -8.370577812194824
Reconstruction Loss: -4.597141265869141
Iteration 13101:
Training Loss: -8.229570388793945
Reconstruction Loss: -4.597290515899658
Iteration 13151:
Training Loss: -8.329195022583008
Reconstruction Loss: -4.597645282745361
Iteration 13201:
Training Loss: -8.290827751159668
Reconstruction Loss: -4.597920894622803
Iteration 13251:
Training Loss: -8.406511306762695
Reconstruction Loss: -4.598178863525391
Iteration 13301:
Training Loss: -8.431848526000977
Reconstruction Loss: -4.598481178283691
Iteration 13351:
Training Loss: -8.367483139038086
Reconstruction Loss: -4.598763465881348
Iteration 13401:
Training Loss: -8.366837501525879
Reconstruction Loss: -4.598855018615723
Iteration 13451:
Training Loss: -8.34044361114502
Reconstruction Loss: -4.598978042602539
Iteration 13501:
Training Loss: -8.548088073730469
Reconstruction Loss: -4.599431037902832
Iteration 13551:
Training Loss: -8.525854110717773
Reconstruction Loss: -4.599704742431641
Iteration 13601:
Training Loss: -8.344358444213867
Reconstruction Loss: -4.599748134613037
Iteration 13651:
Training Loss: -8.533868789672852
Reconstruction Loss: -4.600129127502441
Iteration 13701:
Training Loss: -8.468485832214355
Reconstruction Loss: -4.600451946258545
Iteration 13751:
Training Loss: -8.417292594909668
Reconstruction Loss: -4.600551128387451
Iteration 13801:
Training Loss: -8.426522254943848
Reconstruction Loss: -4.600848197937012
Iteration 13851:
Training Loss: -8.433001518249512
Reconstruction Loss: -4.6008381843566895
Iteration 13901:
Training Loss: -8.506914138793945
Reconstruction Loss: -4.601156234741211
Iteration 13951:
Training Loss: -8.481554985046387
Reconstruction Loss: -4.601325988769531
Iteration 14001:
Training Loss: -8.561596870422363
Reconstruction Loss: -4.601460933685303
Iteration 14051:
Training Loss: -8.559650421142578
Reconstruction Loss: -4.6019110679626465
Iteration 14101:
Training Loss: -8.726181030273438
Reconstruction Loss: -4.602046012878418
Iteration 14151:
Training Loss: -8.538445472717285
Reconstruction Loss: -4.60214900970459
Iteration 14201:
Training Loss: -8.630266189575195
Reconstruction Loss: -4.602586269378662
Iteration 14251:
Training Loss: -8.711431503295898
Reconstruction Loss: -4.602687835693359
Iteration 14301:
Training Loss: -8.635712623596191
Reconstruction Loss: -4.602696418762207
Iteration 14351:
Training Loss: -8.620622634887695
Reconstruction Loss: -4.602950096130371
Iteration 14401:
Training Loss: -8.544912338256836
Reconstruction Loss: -4.603348731994629
Iteration 14451:
Training Loss: -8.639269828796387
Reconstruction Loss: -4.603525161743164
Iteration 14501:
Training Loss: -8.781960487365723
Reconstruction Loss: -4.6036272048950195
Iteration 14551:
Training Loss: -8.646002769470215
Reconstruction Loss: -4.60399055480957
Iteration 14601:
Training Loss: -8.683000564575195
Reconstruction Loss: -4.604071140289307
Iteration 14651:
Training Loss: -8.65160846710205
Reconstruction Loss: -4.604181289672852
Iteration 14701:
Training Loss: -8.9343900680542
Reconstruction Loss: -4.604427814483643
Iteration 14751:
Training Loss: -8.652796745300293
Reconstruction Loss: -4.60469388961792
Iteration 14801:
Training Loss: -8.89459228515625
Reconstruction Loss: -4.604740142822266
Iteration 14851:
Training Loss: -8.840825080871582
Reconstruction Loss: -4.604965686798096
Iteration 14901:
Training Loss: -8.782212257385254
Reconstruction Loss: -4.6052069664001465
Iteration 14951:
Training Loss: -8.77589225769043
Reconstruction Loss: -4.605341911315918
Iteration 15001:
Training Loss: -8.833477020263672
Reconstruction Loss: -4.605517387390137
Iteration 15051:
Training Loss: -8.878619194030762
Reconstruction Loss: -4.605628490447998
Iteration 15101:
Training Loss: -8.785362243652344
Reconstruction Loss: -4.605851173400879
Iteration 15151:
Training Loss: -8.795868873596191
Reconstruction Loss: -4.60596227645874
Iteration 15201:
Training Loss: -8.785867691040039
Reconstruction Loss: -4.606196403503418
Iteration 15251:
Training Loss: -8.954690933227539
Reconstruction Loss: -4.606273174285889
Iteration 15301:
Training Loss: -8.877664566040039
Reconstruction Loss: -4.606478214263916
Iteration 15351:
Training Loss: -8.869833946228027
Reconstruction Loss: -4.606769561767578
Iteration 15401:
Training Loss: -9.079875946044922
Reconstruction Loss: -4.606968879699707
Iteration 15451:
Training Loss: -8.86910343170166
Reconstruction Loss: -4.607052326202393
Iteration 15501:
Training Loss: -9.062469482421875
Reconstruction Loss: -4.607215404510498
Iteration 15551:
Training Loss: -9.013343811035156
Reconstruction Loss: -4.60725736618042
Iteration 15601:
Training Loss: -9.05482292175293
Reconstruction Loss: -4.607623100280762
Iteration 15651:
Training Loss: -8.973458290100098
Reconstruction Loss: -4.607605934143066
Iteration 15701:
Training Loss: -8.924860954284668
Reconstruction Loss: -4.607717514038086
Iteration 15751:
Training Loss: -9.130291938781738
Reconstruction Loss: -4.6078901290893555
Iteration 15801:
Training Loss: -9.089653968811035
Reconstruction Loss: -4.608224391937256
Iteration 15851:
Training Loss: -9.034323692321777
Reconstruction Loss: -4.608366012573242
Iteration 15901:
Training Loss: -9.005253791809082
Reconstruction Loss: -4.608475208282471
Iteration 15951:
Training Loss: -9.280744552612305
Reconstruction Loss: -4.608598709106445
Iteration 16001:
Training Loss: -9.13063907623291
Reconstruction Loss: -4.608740329742432
Iteration 16051:
Training Loss: -9.080548286437988
Reconstruction Loss: -4.608808517456055
Iteration 16101:
Training Loss: -9.161361694335938
Reconstruction Loss: -4.609078884124756
Iteration 16151:
Training Loss: -9.270341873168945
Reconstruction Loss: -4.609274387359619
Iteration 16201:
Training Loss: -9.139106750488281
Reconstruction Loss: -4.609300136566162
Iteration 16251:
Training Loss: -9.0923490524292
Reconstruction Loss: -4.609528541564941
Iteration 16301:
Training Loss: -9.253984451293945
Reconstruction Loss: -4.609581470489502
Iteration 16351:
Training Loss: -9.192824363708496
Reconstruction Loss: -4.6096577644348145
Iteration 16401:
Training Loss: -9.15629768371582
Reconstruction Loss: -4.6098222732543945
Iteration 16451:
Training Loss: -9.242589950561523
Reconstruction Loss: -4.610057353973389
Iteration 16501:
Training Loss: -9.395890235900879
Reconstruction Loss: -4.610147476196289
Iteration 16551:
Training Loss: -9.365591049194336
Reconstruction Loss: -4.610323905944824
Iteration 16601:
Training Loss: -9.252917289733887
Reconstruction Loss: -4.6105170249938965
Iteration 16651:
Training Loss: -9.263492584228516
Reconstruction Loss: -4.610623836517334
Iteration 16701:
Training Loss: -9.210297584533691
Reconstruction Loss: -4.610635757446289
Iteration 16751:
Training Loss: -9.365189552307129
Reconstruction Loss: -4.610779285430908
Iteration 16801:
Training Loss: -9.456900596618652
Reconstruction Loss: -4.610990047454834
Iteration 16851:
Training Loss: -9.22658634185791
Reconstruction Loss: -4.611016750335693
Iteration 16901:
Training Loss: -9.33090591430664
Reconstruction Loss: -4.611134052276611
Iteration 16951:
Training Loss: -9.391480445861816
Reconstruction Loss: -4.611342430114746
Iteration 17001:
Training Loss: -9.38414478302002
Reconstruction Loss: -4.611480712890625
Iteration 17051:
Training Loss: -9.401796340942383
Reconstruction Loss: -4.611573696136475
Iteration 17101:
Training Loss: -9.330162048339844
Reconstruction Loss: -4.611600875854492
Iteration 17151:
Training Loss: -9.492855072021484
Reconstruction Loss: -4.611752510070801
Iteration 17201:
Training Loss: -9.560237884521484
Reconstruction Loss: -4.611900806427002
Iteration 17251:
Training Loss: -9.50770092010498
Reconstruction Loss: -4.6119818687438965
Iteration 17301:
Training Loss: -9.494194030761719
Reconstruction Loss: -4.612198829650879
Iteration 17351:
Training Loss: -9.424302101135254
Reconstruction Loss: -4.612266540527344
Iteration 17401:
Training Loss: -9.563811302185059
Reconstruction Loss: -4.612338066101074
Iteration 17451:
Training Loss: -9.48619556427002
Reconstruction Loss: -4.612553596496582
Iteration 17501:
Training Loss: -9.587825775146484
Reconstruction Loss: -4.6125969886779785
Iteration 17551:
Training Loss: -9.41452407836914
Reconstruction Loss: -4.612759113311768
Iteration 17601:
Training Loss: -9.497930526733398
Reconstruction Loss: -4.612865924835205
Iteration 17651:
Training Loss: -9.57880687713623
Reconstruction Loss: -4.612827301025391
Iteration 17701:
Training Loss: -9.614495277404785
Reconstruction Loss: -4.6131110191345215
Iteration 17751:
Training Loss: -9.502060890197754
Reconstruction Loss: -4.613122940063477
Iteration 17801:
Training Loss: -9.626977920532227
Reconstruction Loss: -4.613264083862305
Iteration 17851:
Training Loss: -9.609147071838379
Reconstruction Loss: -4.613507270812988
Iteration 17901:
Training Loss: -9.618232727050781
Reconstruction Loss: -4.613577365875244
Iteration 17951:
Training Loss: -9.571294784545898
Reconstruction Loss: -4.613560199737549
Iteration 18001:
Training Loss: -9.71549129486084
Reconstruction Loss: -4.613688945770264
Iteration 18051:
Training Loss: -9.698858261108398
Reconstruction Loss: -4.613804340362549
Iteration 18101:
Training Loss: -9.686149597167969
Reconstruction Loss: -4.613942623138428
Iteration 18151:
Training Loss: -9.655332565307617
Reconstruction Loss: -4.614018440246582
Iteration 18201:
Training Loss: -9.6853666305542
Reconstruction Loss: -4.614140510559082
Iteration 18251:
Training Loss: -9.857091903686523
Reconstruction Loss: -4.614130020141602
Iteration 18301:
Training Loss: -9.766749382019043
Reconstruction Loss: -4.614358425140381
Iteration 18351:
Training Loss: -9.611286163330078
Reconstruction Loss: -4.61434268951416
Iteration 18401:
Training Loss: -9.665221214294434
Reconstruction Loss: -4.614484786987305
Iteration 18451:
Training Loss: -9.832531929016113
Reconstruction Loss: -4.614619731903076
Iteration 18501:
Training Loss: -9.803847312927246
Reconstruction Loss: -4.614683628082275
Iteration 18551:
Training Loss: -9.772210121154785
Reconstruction Loss: -4.61478853225708
Iteration 18601:
Training Loss: -9.80814266204834
Reconstruction Loss: -4.614892482757568
Iteration 18651:
Training Loss: -9.774711608886719
Reconstruction Loss: -4.614963531494141
Iteration 18701:
Training Loss: -9.924851417541504
Reconstruction Loss: -4.615055084228516
Iteration 18751:
Training Loss: -9.753498077392578
Reconstruction Loss: -4.615076065063477
Iteration 18801:
Training Loss: -9.793188095092773
Reconstruction Loss: -4.615225315093994
Iteration 18851:
Training Loss: -9.788199424743652
Reconstruction Loss: -4.615296840667725
Iteration 18901:
Training Loss: -9.793126106262207
Reconstruction Loss: -4.615406513214111
Iteration 18951:
Training Loss: -9.89173412322998
Reconstruction Loss: -4.615506172180176
Iteration 19001:
Training Loss: -9.878209114074707
Reconstruction Loss: -4.615527629852295
Iteration 19051:
Training Loss: -9.87409496307373
Reconstruction Loss: -4.615715980529785
Iteration 19101:
Training Loss: -9.776505470275879
Reconstruction Loss: -4.61578893661499
Iteration 19151:
Training Loss: -9.886455535888672
Reconstruction Loss: -4.615857124328613
Iteration 19201:
Training Loss: -9.949954986572266
Reconstruction Loss: -4.616032123565674
Iteration 19251:
Training Loss: -10.027315139770508
Reconstruction Loss: -4.616085052490234
Iteration 19301:
Training Loss: -9.997014045715332
Reconstruction Loss: -4.616093158721924
Iteration 19351:
Training Loss: -10.03959846496582
Reconstruction Loss: -4.616158962249756
Iteration 19401:
Training Loss: -9.969078063964844
Reconstruction Loss: -4.616231441497803
Iteration 19451:
Training Loss: -10.043700218200684
Reconstruction Loss: -4.6164069175720215
Iteration 19501:
Training Loss: -10.014851570129395
Reconstruction Loss: -4.6164445877075195
Iteration 19551:
Training Loss: -10.028995513916016
Reconstruction Loss: -4.616492748260498
Iteration 19601:
Training Loss: -10.106239318847656
Reconstruction Loss: -4.616607189178467
Iteration 19651:
Training Loss: -10.07898235321045
Reconstruction Loss: -4.6166863441467285
Iteration 19701:
Training Loss: -9.962418556213379
Reconstruction Loss: -4.616811275482178
Iteration 19751:
Training Loss: -10.148270606994629
Reconstruction Loss: -4.6168341636657715
Iteration 19801:
Training Loss: -10.021858215332031
Reconstruction Loss: -4.6169257164001465
Iteration 19851:
Training Loss: -10.071893692016602
Reconstruction Loss: -4.617040157318115
Iteration 19901:
Training Loss: -9.97944164276123
Reconstruction Loss: -4.617096424102783
Iteration 19951:
Training Loss: -10.221288681030273
Reconstruction Loss: -4.6171464920043945
Iteration 20001:
Training Loss: -10.104934692382812
Reconstruction Loss: -4.6172709465026855
Iteration 20051:
Training Loss: -10.174948692321777
Reconstruction Loss: -4.6172614097595215
Iteration 20101:
Training Loss: -10.130880355834961
Reconstruction Loss: -4.61735200881958
Iteration 20151:
Training Loss: -10.061443328857422
Reconstruction Loss: -4.617434024810791
Iteration 20201:
Training Loss: -10.13028335571289
Reconstruction Loss: -4.617555618286133
Iteration 20251:
Training Loss: -10.11191177368164
Reconstruction Loss: -4.617631912231445
Iteration 20301:
Training Loss: -10.074943542480469
Reconstruction Loss: -4.617684841156006
Iteration 20351:
Training Loss: -10.248787879943848
Reconstruction Loss: -4.617677211761475
Iteration 20401:
Training Loss: -10.274856567382812
Reconstruction Loss: -4.617794036865234
Iteration 20451:
Training Loss: -10.150029182434082
Reconstruction Loss: -4.617908954620361
Iteration 20501:
Training Loss: -10.308309555053711
Reconstruction Loss: -4.617928981781006
Iteration 20551:
Training Loss: -10.195033073425293
Reconstruction Loss: -4.618018627166748
Iteration 20601:
Training Loss: -10.29174518585205
Reconstruction Loss: -4.618096828460693
Iteration 20651:
Training Loss: -10.385320663452148
Reconstruction Loss: -4.618126392364502
Iteration 20701:
Training Loss: -10.286389350891113
Reconstruction Loss: -4.618222236633301
Iteration 20751:
Training Loss: -10.311694145202637
Reconstruction Loss: -4.618325710296631
Iteration 20801:
Training Loss: -10.34943675994873
Reconstruction Loss: -4.618338108062744
Iteration 20851:
Training Loss: -10.288695335388184
Reconstruction Loss: -4.618389129638672
Iteration 20901:
Training Loss: -10.358104705810547
Reconstruction Loss: -4.618465900421143
Iteration 20951:
Training Loss: -10.336163520812988
Reconstruction Loss: -4.618598461151123
Iteration 21001:
Training Loss: -10.371630668640137
Reconstruction Loss: -4.618646621704102
Iteration 21051:
Training Loss: -10.406128883361816
Reconstruction Loss: -4.618635654449463
Iteration 21101:
Training Loss: -10.262029647827148
Reconstruction Loss: -4.618754863739014
Iteration 21151:
Training Loss: -10.322196960449219
Reconstruction Loss: -4.6187591552734375
Iteration 21201:
Training Loss: -10.368861198425293
Reconstruction Loss: -4.618833541870117
Iteration 21251:
Training Loss: -10.375198364257812
Reconstruction Loss: -4.618934154510498
Iteration 21301:
Training Loss: -10.316727638244629
Reconstruction Loss: -4.618960380554199
Iteration 21351:
Training Loss: -10.37502670288086
Reconstruction Loss: -4.619017124176025
Iteration 21401:
Training Loss: -10.424324035644531
Reconstruction Loss: -4.6191301345825195
Iteration 21451:
Training Loss: -10.407563209533691
Reconstruction Loss: -4.619142532348633
Iteration 21501:
Training Loss: -10.379121780395508
Reconstruction Loss: -4.6192169189453125
Iteration 21551:
Training Loss: -10.571287155151367
Reconstruction Loss: -4.61932897567749
Iteration 21601:
Training Loss: -10.397123336791992
Reconstruction Loss: -4.61931037902832
Iteration 21651:
Training Loss: -10.50524616241455
Reconstruction Loss: -4.619444847106934
Iteration 21701:
Training Loss: -10.605709075927734
Reconstruction Loss: -4.619521141052246
Iteration 21751:
Training Loss: -10.555800437927246
Reconstruction Loss: -4.6195502281188965
Iteration 21801:
Training Loss: -10.411982536315918
Reconstruction Loss: -4.6196794509887695
Iteration 21851:
Training Loss: -10.510303497314453
Reconstruction Loss: -4.619632720947266
Iteration 21901:
Training Loss: -10.5509614944458
Reconstruction Loss: -4.619668960571289
Iteration 21951:
Training Loss: -10.615711212158203
Reconstruction Loss: -4.619763374328613
Iteration 22001:
Training Loss: -10.666116714477539
Reconstruction Loss: -4.61983060836792
Iteration 22051:
Training Loss: -10.677597045898438
Reconstruction Loss: -4.619844436645508
Iteration 22101:
Training Loss: -10.52623176574707
Reconstruction Loss: -4.619913101196289
Iteration 22151:
Training Loss: -10.561553001403809
Reconstruction Loss: -4.61997127532959
Iteration 22201:
Training Loss: -10.506782531738281
Reconstruction Loss: -4.620077610015869
Iteration 22251:
Training Loss: -10.584206581115723
Reconstruction Loss: -4.62009334564209
Iteration 22301:
Training Loss: -10.699006080627441
Reconstruction Loss: -4.620128154754639
Iteration 22351:
Training Loss: -10.609275817871094
Reconstruction Loss: -4.62017297744751
Iteration 22401:
Training Loss: -10.590157508850098
Reconstruction Loss: -4.620212554931641
Iteration 22451:
Training Loss: -10.689230918884277
Reconstruction Loss: -4.62035608291626
Iteration 22501:
Training Loss: -10.822366714477539
Reconstruction Loss: -4.620395183563232
Iteration 22551:
Training Loss: -10.6361665725708
Reconstruction Loss: -4.620419502258301
Iteration 22601:
Training Loss: -10.731792449951172
Reconstruction Loss: -4.620482444763184
Iteration 22651:
Training Loss: -10.671192169189453
Reconstruction Loss: -4.620483875274658
Iteration 22701:
Training Loss: -10.668950080871582
Reconstruction Loss: -4.620541095733643
Iteration 22751:
Training Loss: -10.760196685791016
Reconstruction Loss: -4.620636463165283
Iteration 22801:
Training Loss: -10.83969497680664
Reconstruction Loss: -4.620654582977295
Iteration 22851:
Training Loss: -10.735187530517578
Reconstruction Loss: -4.620751857757568
Iteration 22901:
Training Loss: -10.820704460144043
Reconstruction Loss: -4.620734214782715
Iteration 22951:
Training Loss: -10.72919750213623
Reconstruction Loss: -4.620780944824219
Iteration 23001:
Training Loss: -10.941728591918945
Reconstruction Loss: -4.6208343505859375
Iteration 23051:
Training Loss: -10.797124862670898
Reconstruction Loss: -4.620963096618652
Iteration 23101:
Training Loss: -10.877424240112305
Reconstruction Loss: -4.620937347412109
Iteration 23151:
Training Loss: -10.996535301208496
Reconstruction Loss: -4.620936393737793
Iteration 23201:
Training Loss: -10.897163391113281
Reconstruction Loss: -4.6210455894470215
Iteration 23251:
Training Loss: -10.763617515563965
Reconstruction Loss: -4.621102809906006
Iteration 23301:
Training Loss: -10.875323295593262
Reconstruction Loss: -4.621091365814209
Iteration 23351:
Training Loss: -10.970925331115723
Reconstruction Loss: -4.621167182922363
Iteration 23401:
Training Loss: -10.994955062866211
Reconstruction Loss: -4.621246337890625
Iteration 23451:
Training Loss: -10.90042495727539
Reconstruction Loss: -4.621211051940918
Iteration 23501:
Training Loss: -11.015559196472168
Reconstruction Loss: -4.621286392211914
Iteration 23551:
Training Loss: -11.007689476013184
Reconstruction Loss: -4.621379852294922
Iteration 23601:
Training Loss: -10.836852073669434
Reconstruction Loss: -4.621404647827148
Iteration 23651:
Training Loss: -10.884913444519043
Reconstruction Loss: -4.6214518547058105
Iteration 23701:
Training Loss: -11.007343292236328
Reconstruction Loss: -4.62150764465332
Iteration 23751:
Training Loss: -11.034643173217773
Reconstruction Loss: -4.621469020843506
Iteration 23801:
Training Loss: -11.006291389465332
Reconstruction Loss: -4.621583938598633
Iteration 23851:
Training Loss: -10.959548950195312
Reconstruction Loss: -4.621686935424805
Iteration 23901:
Training Loss: -11.069994926452637
Reconstruction Loss: -4.621645450592041
Iteration 23951:
Training Loss: -10.994974136352539
Reconstruction Loss: -4.621702194213867
Iteration 24001:
Training Loss: -11.019074440002441
Reconstruction Loss: -4.62172269821167
Iteration 24051:
Training Loss: -11.122934341430664
Reconstruction Loss: -4.62177038192749
Iteration 24101:
Training Loss: -10.961552619934082
Reconstruction Loss: -4.621840476989746
Iteration 24151:
Training Loss: -11.072107315063477
Reconstruction Loss: -4.621860980987549
Iteration 24201:
Training Loss: -11.15045166015625
Reconstruction Loss: -4.621898651123047
Iteration 24251:
Training Loss: -10.940323829650879
Reconstruction Loss: -4.621969223022461
Iteration 24301:
Training Loss: -10.95626163482666
Reconstruction Loss: -4.621933460235596
Iteration 24351:
Training Loss: -11.170308113098145
Reconstruction Loss: -4.622029781341553
Iteration 24401:
Training Loss: -11.136591911315918
Reconstruction Loss: -4.62207555770874
Iteration 24451:
Training Loss: -11.090872764587402
Reconstruction Loss: -4.622132301330566
Iteration 24501:
Training Loss: -11.08945083618164
Reconstruction Loss: -4.622124195098877
Iteration 24551:
Training Loss: -11.284488677978516
Reconstruction Loss: -4.622173309326172
Iteration 24601:
Training Loss: -11.18802261352539
Reconstruction Loss: -4.622182369232178
Iteration 24651:
Training Loss: -11.17410945892334
Reconstruction Loss: -4.622219085693359
Iteration 24701:
Training Loss: -11.132096290588379
Reconstruction Loss: -4.622243881225586
Iteration 24751:
Training Loss: -11.1664400100708
Reconstruction Loss: -4.622340202331543
Iteration 24801:
Training Loss: -11.02920150756836
Reconstruction Loss: -4.622386455535889
Iteration 24851:
Training Loss: -11.192151069641113
Reconstruction Loss: -4.622414588928223
Iteration 24901:
Training Loss: -11.213156700134277
Reconstruction Loss: -4.6224589347839355
Iteration 24951:
Training Loss: -11.217850685119629
Reconstruction Loss: -4.622464179992676
