5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.637156963348389
Reconstruction Loss: -0.46770983934402466
Iteration 11:
Training Loss: 5.394623756408691
Reconstruction Loss: -0.46770983934402466
Iteration 21:
Training Loss: 5.4977264404296875
Reconstruction Loss: -0.46770983934402466
Iteration 31:
Training Loss: 5.0889573097229
Reconstruction Loss: -0.46770983934402466
Iteration 41:
Training Loss: 5.435770034790039
Reconstruction Loss: -0.46770983934402466
Iteration 51:
Training Loss: 5.202569484710693
Reconstruction Loss: -0.46770989894866943
Iteration 61:
Training Loss: 5.3569183349609375
Reconstruction Loss: -0.46770989894866943
Iteration 71:
Training Loss: 5.5561628341674805
Reconstruction Loss: -0.46770989894866943
Iteration 81:
Training Loss: 5.241336345672607
Reconstruction Loss: -0.46770989894866943
Iteration 91:
Training Loss: 5.484822750091553
Reconstruction Loss: -0.46770989894866943
Iteration 101:
Training Loss: 5.420700550079346
Reconstruction Loss: -0.46770989894866943
Iteration 111:
Training Loss: 5.565515995025635
Reconstruction Loss: -0.46770989894866943
Iteration 121:
Training Loss: 5.796713829040527
Reconstruction Loss: -0.46770989894866943
Iteration 131:
Training Loss: 5.41636323928833
Reconstruction Loss: -0.46770989894866943
Iteration 141:
Training Loss: 5.505927085876465
Reconstruction Loss: -0.467710018157959
Iteration 151:
Training Loss: 5.16013240814209
Reconstruction Loss: -0.467710018157959
Iteration 161:
Training Loss: 5.475587368011475
Reconstruction Loss: -0.467710018157959
Iteration 171:
Training Loss: 5.118922233581543
Reconstruction Loss: -0.467710018157959
Iteration 181:
Training Loss: 5.484634876251221
Reconstruction Loss: -0.467710018157959
Iteration 191:
Training Loss: 5.609277248382568
Reconstruction Loss: -0.46771010756492615
Iteration 201:
Training Loss: 5.4302659034729
Reconstruction Loss: -0.46771010756492615
Iteration 211:
Training Loss: 5.429492473602295
Reconstruction Loss: -0.46771010756492615
Iteration 221:
Training Loss: 5.686572551727295
Reconstruction Loss: -0.46771010756492615
Iteration 231:
Training Loss: 5.046633243560791
Reconstruction Loss: -0.46771010756492615
Iteration 241:
Training Loss: 5.642154693603516
Reconstruction Loss: -0.46771010756492615
Iteration 251:
Training Loss: 5.424023151397705
Reconstruction Loss: -0.46771010756492615
Iteration 261:
Training Loss: 5.953871726989746
Reconstruction Loss: -0.46771010756492615
Iteration 271:
Training Loss: 5.459815502166748
Reconstruction Loss: -0.46771010756492615
Iteration 281:
Training Loss: 5.2611002922058105
Reconstruction Loss: -0.46771010756492615
Iteration 291:
Training Loss: 5.51051664352417
Reconstruction Loss: -0.4677101969718933
Iteration 301:
Training Loss: 5.090630531311035
Reconstruction Loss: -0.4677101969718933
Iteration 311:
Training Loss: 5.403332710266113
Reconstruction Loss: -0.4677101969718933
Iteration 321:
Training Loss: 5.34014368057251
Reconstruction Loss: -0.4677101969718933
Iteration 331:
Training Loss: 5.994866847991943
Reconstruction Loss: -0.4677102863788605
Iteration 341:
Training Loss: 5.363754749298096
Reconstruction Loss: -0.4677102863788605
Iteration 351:
Training Loss: 5.592713832855225
Reconstruction Loss: -0.4677102863788605
Iteration 361:
Training Loss: 5.389308452606201
Reconstruction Loss: -0.4677102863788605
Iteration 371:
Training Loss: 5.563331127166748
Reconstruction Loss: -0.46771037578582764
Iteration 381:
Training Loss: 5.213353157043457
Reconstruction Loss: -0.4677104949951172
Iteration 391:
Training Loss: 5.088017463684082
Reconstruction Loss: -0.4677104949951172
Iteration 401:
Training Loss: 5.693497657775879
Reconstruction Loss: -0.4677104949951172
Iteration 411:
Training Loss: 5.390978813171387
Reconstruction Loss: -0.4677104949951172
Iteration 421:
Training Loss: 4.978143692016602
Reconstruction Loss: -0.4677104949951172
Iteration 431:
Training Loss: 5.376335620880127
Reconstruction Loss: -0.4677104949951172
Iteration 441:
Training Loss: 5.532705783843994
Reconstruction Loss: -0.4677104949951172
Iteration 451:
Training Loss: 4.957871913909912
Reconstruction Loss: -0.4677104949951172
Iteration 461:
Training Loss: 5.518861293792725
Reconstruction Loss: -0.46771058440208435
Iteration 471:
Training Loss: 5.189067840576172
Reconstruction Loss: -0.46771058440208435
Iteration 481:
Training Loss: 4.747089385986328
Reconstruction Loss: -0.4677107632160187
Iteration 491:
Training Loss: 5.398116111755371
Reconstruction Loss: -0.4677107632160187
Iteration 501:
Training Loss: 5.241618633270264
Reconstruction Loss: -0.4677107632160187
Iteration 511:
Training Loss: 5.530625343322754
Reconstruction Loss: -0.46771085262298584
Iteration 521:
Training Loss: 5.434187412261963
Reconstruction Loss: -0.4677109718322754
Iteration 531:
Training Loss: 5.741121292114258
Reconstruction Loss: -0.4677109718322754
Iteration 541:
Training Loss: 5.695088863372803
Reconstruction Loss: -0.4677109718322754
Iteration 551:
Training Loss: 5.689418792724609
Reconstruction Loss: -0.4677109718322754
Iteration 561:
Training Loss: 5.779697895050049
Reconstruction Loss: -0.46771106123924255
Iteration 571:
Training Loss: 5.717042922973633
Reconstruction Loss: -0.46771106123924255
Iteration 581:
Training Loss: 5.47327184677124
Reconstruction Loss: -0.46771106123924255
Iteration 591:
Training Loss: 5.222089767456055
Reconstruction Loss: -0.4677112400531769
Iteration 601:
Training Loss: 5.540556907653809
Reconstruction Loss: -0.4677112400531769
Iteration 611:
Training Loss: 5.246417045593262
Reconstruction Loss: -0.46771132946014404
Iteration 621:
Training Loss: 5.233211040496826
Reconstruction Loss: -0.46771132946014404
Iteration 631:
Training Loss: 5.5904669761657715
Reconstruction Loss: -0.4677114486694336
Iteration 641:
Training Loss: 5.875201225280762
Reconstruction Loss: -0.4677114486694336
Iteration 651:
Training Loss: 5.481356620788574
Reconstruction Loss: -0.46771150827407837
Iteration 661:
Training Loss: 5.624512195587158
Reconstruction Loss: -0.4677117168903351
Iteration 671:
Training Loss: 5.584771156311035
Reconstruction Loss: -0.4677117168903351
Iteration 681:
Training Loss: 5.240981578826904
Reconstruction Loss: -0.46771180629730225
Iteration 691:
Training Loss: 5.536398410797119
Reconstruction Loss: -0.4677119255065918
Iteration 701:
Training Loss: 5.4669718742370605
Reconstruction Loss: -0.4677119851112366
Iteration 711:
Training Loss: 5.794222831726074
Reconstruction Loss: -0.4677121043205261
Iteration 721:
Training Loss: 5.871478080749512
Reconstruction Loss: -0.46771228313446045
Iteration 731:
Training Loss: 5.533748149871826
Reconstruction Loss: -0.4677124619483948
Iteration 741:
Training Loss: 5.366883277893066
Reconstruction Loss: -0.4677125811576843
Iteration 751:
Training Loss: 5.667720794677734
Reconstruction Loss: -0.46771275997161865
Iteration 761:
Training Loss: 5.675770282745361
Reconstruction Loss: -0.467712938785553
Iteration 771:
Training Loss: 5.542136192321777
Reconstruction Loss: -0.46771305799484253
Iteration 781:
Training Loss: 5.563941478729248
Reconstruction Loss: -0.46771323680877686
Iteration 791:
Training Loss: 5.194789409637451
Reconstruction Loss: -0.4677136242389679
Iteration 801:
Training Loss: 5.949304103851318
Reconstruction Loss: -0.4677138924598694
Iteration 811:
Training Loss: 5.734941005706787
Reconstruction Loss: -0.4677141010761261
Iteration 821:
Training Loss: 5.4598283767700195
Reconstruction Loss: -0.46771448850631714
Iteration 831:
Training Loss: 5.319243907928467
Reconstruction Loss: -0.46771496534347534
Iteration 841:
Training Loss: 5.257101535797119
Reconstruction Loss: -0.467715322971344
Iteration 851:
Training Loss: 5.547412872314453
Reconstruction Loss: -0.4677157998085022
Iteration 861:
Training Loss: 5.447162628173828
Reconstruction Loss: -0.4677162766456604
Iteration 871:
Training Loss: 4.87033224105835
Reconstruction Loss: -0.4677169620990753
Iteration 881:
Training Loss: 5.322454929351807
Reconstruction Loss: -0.467717707157135
Iteration 891:
Training Loss: 5.407734394073486
Reconstruction Loss: -0.46771857142448425
Iteration 901:
Training Loss: 5.592353820800781
Reconstruction Loss: -0.4677194356918335
Iteration 911:
Training Loss: 5.666280269622803
Reconstruction Loss: -0.46772056818008423
Iteration 921:
Training Loss: 5.219033718109131
Reconstruction Loss: -0.46772199869155884
Iteration 931:
Training Loss: 5.248124122619629
Reconstruction Loss: -0.4677237868309021
Iteration 941:
Training Loss: 5.566166877746582
Reconstruction Loss: -0.4677256941795349
Iteration 951:
Training Loss: 5.685089588165283
Reconstruction Loss: -0.4677283763885498
Iteration 961:
Training Loss: 5.281516075134277
Reconstruction Loss: -0.4677315056324005
Iteration 971:
Training Loss: 5.6698079109191895
Reconstruction Loss: -0.46773579716682434
Iteration 981:
Training Loss: 5.473005771636963
Reconstruction Loss: -0.4677414000034332
Iteration 991:
Training Loss: 5.735612392425537
Reconstruction Loss: -0.4677489995956421
Iteration 1001:
Training Loss: 5.306741714477539
Reconstruction Loss: -0.46775996685028076
Iteration 1011:
Training Loss: 5.090032577514648
Reconstruction Loss: -0.4677758514881134
Iteration 1021:
Training Loss: 5.358858585357666
Reconstruction Loss: -0.46780145168304443
Iteration 1031:
Training Loss: 5.632153034210205
Reconstruction Loss: -0.4678443670272827
Iteration 1041:
Training Loss: 5.554533004760742
Reconstruction Loss: -0.46792781352996826
Iteration 1051:
Training Loss: 5.793851375579834
Reconstruction Loss: -0.4681188464164734
Iteration 1061:
Training Loss: 6.003064155578613
Reconstruction Loss: -0.4687400460243225
Iteration 1071:
Training Loss: 5.14924955368042
Reconstruction Loss: -0.4737233817577362
Iteration 1081:
Training Loss: 4.800300598144531
Reconstruction Loss: -0.6109764575958252
Iteration 1091:
Training Loss: 4.952116012573242
Reconstruction Loss: -0.5964050889015198
Iteration 1101:
Training Loss: 5.248119831085205
Reconstruction Loss: -0.57051020860672
Iteration 1111:
Training Loss: 4.962369441986084
Reconstruction Loss: -0.593501627445221
Iteration 1121:
Training Loss: 4.798602104187012
Reconstruction Loss: -0.5777698755264282
Iteration 1131:
Training Loss: 5.152517318725586
Reconstruction Loss: -0.580244243144989
Iteration 1141:
Training Loss: 5.205349922180176
Reconstruction Loss: -0.5819296836853027
Iteration 1151:
Training Loss: 5.2786760330200195
Reconstruction Loss: -0.5876602530479431
Iteration 1161:
Training Loss: 5.3549580574035645
Reconstruction Loss: -0.5899057388305664
Iteration 1171:
Training Loss: 5.467792510986328
Reconstruction Loss: -0.5947597026824951
Iteration 1181:
Training Loss: 5.098165035247803
Reconstruction Loss: -0.5871303081512451
Iteration 1191:
Training Loss: 5.111904621124268
Reconstruction Loss: -0.5679794549942017
Iteration 1201:
Training Loss: 5.121622562408447
Reconstruction Loss: -0.5200653076171875
Iteration 1211:
Training Loss: 4.704590797424316
Reconstruction Loss: -0.576968789100647
Iteration 1221:
Training Loss: 4.6528096199035645
Reconstruction Loss: -0.5816631317138672
Iteration 1231:
Training Loss: 5.102001667022705
Reconstruction Loss: -0.5813249349594116
Iteration 1241:
Training Loss: 5.397616863250732
Reconstruction Loss: -0.582866370677948
Iteration 1251:
Training Loss: 4.801456451416016
Reconstruction Loss: -0.5780187845230103
Iteration 1261:
Training Loss: 5.443747520446777
Reconstruction Loss: -0.5996710062026978
Iteration 1271:
Training Loss: 5.466958045959473
Reconstruction Loss: -0.5399848818778992
Iteration 1281:
Training Loss: 4.528864860534668
Reconstruction Loss: -0.5905411839485168
Iteration 1291:
Training Loss: 4.749580383300781
Reconstruction Loss: -0.5526296496391296
Iteration 1301:
Training Loss: 5.246864318847656
Reconstruction Loss: -0.5731616616249084
Iteration 1311:
Training Loss: 4.901148796081543
Reconstruction Loss: -0.607241690158844
Iteration 1321:
Training Loss: 5.062874794006348
Reconstruction Loss: -0.5339107513427734
Iteration 1331:
Training Loss: 4.770420551300049
Reconstruction Loss: -0.5885559320449829
Iteration 1341:
Training Loss: 5.011601448059082
Reconstruction Loss: -0.589859127998352
Iteration 1351:
Training Loss: 5.00586462020874
Reconstruction Loss: -0.5393528938293457
Iteration 1361:
Training Loss: 5.170504093170166
Reconstruction Loss: -0.5724552273750305
Iteration 1371:
Training Loss: 5.0954108238220215
Reconstruction Loss: -0.5796459913253784
Iteration 1381:
Training Loss: 5.068932056427002
Reconstruction Loss: -0.5340688228607178
Iteration 1391:
Training Loss: 4.336619853973389
Reconstruction Loss: -0.5382798910140991
Iteration 1401:
Training Loss: 4.7418437004089355
Reconstruction Loss: -0.5559224486351013
Iteration 1411:
Training Loss: 5.0705246925354
Reconstruction Loss: -0.5811846852302551
Iteration 1421:
Training Loss: 4.807490348815918
Reconstruction Loss: -0.5777059197425842
Iteration 1431:
Training Loss: 4.877689361572266
Reconstruction Loss: -0.6001207232475281
Iteration 1441:
Training Loss: 5.124488353729248
Reconstruction Loss: -0.594136655330658
Iteration 1451:
Training Loss: 5.109675884246826
Reconstruction Loss: -0.556848406791687
Iteration 1461:
Training Loss: 5.034999370574951
Reconstruction Loss: -0.5835526585578918
Iteration 1471:
Training Loss: 5.487838268280029
Reconstruction Loss: -0.5997357368469238
Iteration 1481:
Training Loss: 5.191421985626221
Reconstruction Loss: -0.5545413494110107
Iteration 1491:
Training Loss: 4.976556301116943
Reconstruction Loss: -0.5782498717308044
