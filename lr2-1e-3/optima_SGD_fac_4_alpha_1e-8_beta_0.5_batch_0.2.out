5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.725833415985107
Reconstruction Loss: -0.5364506244659424
Iteration 21:
Training Loss: 5.625805377960205
Reconstruction Loss: -0.5364506244659424
Iteration 41:
Training Loss: 5.7488508224487305
Reconstruction Loss: -0.5364507436752319
Iteration 61:
Training Loss: 5.622446060180664
Reconstruction Loss: -0.5364507436752319
Iteration 81:
Training Loss: 5.455008506774902
Reconstruction Loss: -0.5364507436752319
Iteration 101:
Training Loss: 5.35200834274292
Reconstruction Loss: -0.5364507436752319
Iteration 121:
Training Loss: 5.672144412994385
Reconstruction Loss: -0.5364507436752319
Iteration 141:
Training Loss: 5.656011581420898
Reconstruction Loss: -0.5364507436752319
Iteration 161:
Training Loss: 5.46721076965332
Reconstruction Loss: -0.5364507436752319
Iteration 181:
Training Loss: 5.701802730560303
Reconstruction Loss: -0.5364507436752319
Iteration 201:
Training Loss: 5.59145975112915
Reconstruction Loss: -0.5364507436752319
Iteration 221:
Training Loss: 5.61005163192749
Reconstruction Loss: -0.5364507436752319
Iteration 241:
Training Loss: 5.401949882507324
Reconstruction Loss: -0.5364509224891663
Iteration 261:
Training Loss: 5.262451648712158
Reconstruction Loss: -0.5364509224891663
Iteration 281:
Training Loss: 5.594401836395264
Reconstruction Loss: -0.5364509224891663
Iteration 301:
Training Loss: 5.526071071624756
Reconstruction Loss: -0.5364509224891663
Iteration 321:
Training Loss: 5.411437511444092
Reconstruction Loss: -0.5364509224891663
Iteration 341:
Training Loss: 5.676126956939697
Reconstruction Loss: -0.5364509224891663
Iteration 361:
Training Loss: 5.350178241729736
Reconstruction Loss: -0.5364509224891663
Iteration 381:
Training Loss: 5.653619766235352
Reconstruction Loss: -0.5364509224891663
Iteration 401:
Training Loss: 5.662275791168213
Reconstruction Loss: -0.5364509224891663
Iteration 421:
Training Loss: 5.597938060760498
Reconstruction Loss: -0.5364509224891663
Iteration 441:
Training Loss: 5.546258449554443
Reconstruction Loss: -0.5364509224891663
Iteration 461:
Training Loss: 5.57375431060791
Reconstruction Loss: -0.5364509224891663
Iteration 481:
Training Loss: 5.387817859649658
Reconstruction Loss: -0.5364509224891663
Iteration 501:
Training Loss: 5.671396255493164
Reconstruction Loss: -0.5364509224891663
Iteration 521:
Training Loss: 5.183933734893799
Reconstruction Loss: -0.536450982093811
Iteration 541:
Training Loss: 5.844984531402588
Reconstruction Loss: -0.536450982093811
Iteration 561:
Training Loss: 5.340834617614746
Reconstruction Loss: -0.536450982093811
Iteration 581:
Training Loss: 5.694453716278076
Reconstruction Loss: -0.536450982093811
Iteration 601:
Training Loss: 5.637929916381836
Reconstruction Loss: -0.5364511013031006
Iteration 621:
Training Loss: 5.710556983947754
Reconstruction Loss: -0.5364511013031006
Iteration 641:
Training Loss: 5.64310359954834
Reconstruction Loss: -0.5364511013031006
Iteration 661:
Training Loss: 5.484510898590088
Reconstruction Loss: -0.5364511013031006
Iteration 681:
Training Loss: 5.6190009117126465
Reconstruction Loss: -0.5364511013031006
Iteration 701:
Training Loss: 5.435711860656738
Reconstruction Loss: -0.5364511013031006
Iteration 721:
Training Loss: 5.3846917152404785
Reconstruction Loss: -0.5364512205123901
Iteration 741:
Training Loss: 5.558680057525635
Reconstruction Loss: -0.5364512205123901
Iteration 761:
Training Loss: 5.823652267456055
Reconstruction Loss: -0.5364512205123901
Iteration 781:
Training Loss: 5.446751117706299
Reconstruction Loss: -0.5364512205123901
Iteration 801:
Training Loss: 5.474048137664795
Reconstruction Loss: -0.5364513397216797
Iteration 821:
Training Loss: 5.505270004272461
Reconstruction Loss: -0.5364513397216797
Iteration 841:
Training Loss: 5.434642314910889
Reconstruction Loss: -0.5364514589309692
Iteration 861:
Training Loss: 5.396639347076416
Reconstruction Loss: -0.5364514589309692
Iteration 881:
Training Loss: 5.516849517822266
Reconstruction Loss: -0.5364514589309692
Iteration 901:
Training Loss: 5.520675182342529
Reconstruction Loss: -0.5364514589309692
Iteration 921:
Training Loss: 5.594783306121826
Reconstruction Loss: -0.5364514589309692
Iteration 941:
Training Loss: 5.514322757720947
Reconstruction Loss: -0.536451518535614
Iteration 961:
Training Loss: 5.343777656555176
Reconstruction Loss: -0.5364514589309692
Iteration 981:
Training Loss: 5.312329292297363
Reconstruction Loss: -0.536451518535614
Iteration 1001:
Training Loss: 5.671810626983643
Reconstruction Loss: -0.5364516377449036
Iteration 1021:
Training Loss: 5.315139293670654
Reconstruction Loss: -0.5364516377449036
Iteration 1041:
Training Loss: 5.422669410705566
Reconstruction Loss: -0.5364516377449036
Iteration 1061:
Training Loss: 5.511298179626465
Reconstruction Loss: -0.5364516973495483
Iteration 1081:
Training Loss: 5.29299783706665
Reconstruction Loss: -0.5364516973495483
Iteration 1101:
Training Loss: 5.419548511505127
Reconstruction Loss: -0.5364518165588379
Iteration 1121:
Training Loss: 5.5395121574401855
Reconstruction Loss: -0.5364518165588379
Iteration 1141:
Training Loss: 5.586719989776611
Reconstruction Loss: -0.5364518165588379
Iteration 1161:
Training Loss: 5.487015724182129
Reconstruction Loss: -0.5364519357681274
Iteration 1181:
Training Loss: 5.047796726226807
Reconstruction Loss: -0.5364519357681274
Iteration 1201:
Training Loss: 5.368254661560059
Reconstruction Loss: -0.536452054977417
Iteration 1221:
Training Loss: 5.458202838897705
Reconstruction Loss: -0.5364521741867065
Iteration 1241:
Training Loss: 5.624345302581787
Reconstruction Loss: -0.5364521741867065
Iteration 1261:
Training Loss: 5.613694667816162
Reconstruction Loss: -0.5364521741867065
Iteration 1281:
Training Loss: 5.352210998535156
Reconstruction Loss: -0.5364522337913513
Iteration 1301:
Training Loss: 5.476201057434082
Reconstruction Loss: -0.5364523530006409
Iteration 1321:
Training Loss: 5.615960121154785
Reconstruction Loss: -0.5364524126052856
Iteration 1341:
Training Loss: 5.716947555541992
Reconstruction Loss: -0.5364524126052856
Iteration 1361:
Training Loss: 5.484611511230469
Reconstruction Loss: -0.5364525318145752
Iteration 1381:
Training Loss: 5.398675918579102
Reconstruction Loss: -0.5364525318145752
Iteration 1401:
Training Loss: 5.540046691894531
Reconstruction Loss: -0.5364527702331543
Iteration 1421:
Training Loss: 5.723879337310791
Reconstruction Loss: -0.5364527702331543
Iteration 1441:
Training Loss: 5.416189193725586
Reconstruction Loss: -0.5364529490470886
Iteration 1461:
Training Loss: 5.43890380859375
Reconstruction Loss: -0.5364530682563782
Iteration 1481:
Training Loss: 5.49610710144043
Reconstruction Loss: -0.5364530682563782
Iteration 1501:
Training Loss: 5.715729236602783
Reconstruction Loss: -0.5364532470703125
Iteration 1521:
Training Loss: 5.36729621887207
Reconstruction Loss: -0.536453366279602
Iteration 1541:
Training Loss: 5.358775615692139
Reconstruction Loss: -0.5364534854888916
Iteration 1561:
Training Loss: 5.224483013153076
Reconstruction Loss: -0.5364535450935364
Iteration 1581:
Training Loss: 5.599606990814209
Reconstruction Loss: -0.5364537835121155
Iteration 1601:
Training Loss: 5.397022247314453
Reconstruction Loss: -0.5364539623260498
Iteration 1621:
Training Loss: 5.78298807144165
Reconstruction Loss: -0.5364540815353394
Iteration 1641:
Training Loss: 5.574356555938721
Reconstruction Loss: -0.5364543795585632
Iteration 1661:
Training Loss: 5.314334392547607
Reconstruction Loss: -0.5364544987678528
Iteration 1681:
Training Loss: 5.3794474601745605
Reconstruction Loss: -0.5364547967910767
Iteration 1701:
Training Loss: 5.395578861236572
Reconstruction Loss: -0.5364549160003662
Iteration 1721:
Training Loss: 5.474634647369385
Reconstruction Loss: -0.5364552736282349
Iteration 1741:
Training Loss: 5.401823043823242
Reconstruction Loss: -0.5364556312561035
Iteration 1761:
Training Loss: 5.270305156707764
Reconstruction Loss: -0.5364559888839722
Iteration 1781:
Training Loss: 5.424422740936279
Reconstruction Loss: -0.5364562273025513
Iteration 1801:
Training Loss: 5.361908435821533
Reconstruction Loss: -0.5364567041397095
Iteration 1821:
Training Loss: 5.7272114753723145
Reconstruction Loss: -0.5364572405815125
Iteration 1841:
Training Loss: 5.322332859039307
Reconstruction Loss: -0.5364577770233154
Iteration 1861:
Training Loss: 5.576532363891602
Reconstruction Loss: -0.5364584922790527
Iteration 1881:
Training Loss: 5.852797985076904
Reconstruction Loss: -0.5364590883255005
Iteration 1901:
Training Loss: 5.532154083251953
Reconstruction Loss: -0.5364598035812378
Iteration 1921:
Training Loss: 5.689092636108398
Reconstruction Loss: -0.5364608764648438
Iteration 1941:
Training Loss: 5.35260534286499
Reconstruction Loss: -0.5364619493484497
Iteration 1961:
Training Loss: 5.5730180740356445
Reconstruction Loss: -0.5364632606506348
Iteration 1981:
Training Loss: 5.714040279388428
Reconstruction Loss: -0.5364648699760437
Iteration 2001:
Training Loss: 5.544313430786133
Reconstruction Loss: -0.5364668369293213
Iteration 2021:
Training Loss: 5.505476951599121
Reconstruction Loss: -0.536469042301178
Iteration 2041:
Training Loss: 5.417685031890869
Reconstruction Loss: -0.5364719033241272
Iteration 2061:
Training Loss: 5.604000091552734
Reconstruction Loss: -0.5364757776260376
Iteration 2081:
Training Loss: 5.207794189453125
Reconstruction Loss: -0.5364807844161987
Iteration 2101:
Training Loss: 5.426374435424805
Reconstruction Loss: -0.5364874005317688
Iteration 2121:
Training Loss: 5.345456600189209
Reconstruction Loss: -0.5364968776702881
Iteration 2141:
Training Loss: 5.381772041320801
Reconstruction Loss: -0.5365105271339417
Iteration 2161:
Training Loss: 5.439377307891846
Reconstruction Loss: -0.5365315675735474
Iteration 2181:
Training Loss: 5.492159366607666
Reconstruction Loss: -0.5365666151046753
Iteration 2201:
Training Loss: 5.572493076324463
Reconstruction Loss: -0.5366313457489014
Iteration 2221:
Training Loss: 5.550812244415283
Reconstruction Loss: -0.5367738604545593
Iteration 2241:
Training Loss: 5.317951679229736
Reconstruction Loss: -0.5371880531311035
Iteration 2261:
Training Loss: 5.198794364929199
Reconstruction Loss: -0.5395514965057373
Iteration 2281:
Training Loss: 5.067985534667969
Reconstruction Loss: -0.6879711747169495
Iteration 2301:
Training Loss: 5.109490394592285
Reconstruction Loss: -0.6393761038780212
Iteration 2321:
Training Loss: 4.926754951477051
Reconstruction Loss: -0.6524873971939087
Iteration 2341:
Training Loss: 4.813663959503174
Reconstruction Loss: -0.668505072593689
Iteration 2361:
Training Loss: 5.042147636413574
Reconstruction Loss: -0.6839995384216309
Iteration 2381:
Training Loss: 4.825469493865967
Reconstruction Loss: -0.6738881468772888
Iteration 2401:
Training Loss: 4.6191582679748535
Reconstruction Loss: -0.6738730669021606
Iteration 2421:
Training Loss: 4.869655609130859
Reconstruction Loss: -0.6670715808868408
Iteration 2441:
Training Loss: 5.041596412658691
Reconstruction Loss: -0.6659302115440369
Iteration 2461:
Training Loss: 4.973109722137451
Reconstruction Loss: -0.6932328343391418
Iteration 2481:
Training Loss: 4.820534706115723
Reconstruction Loss: -0.6823187470436096
Iteration 2501:
Training Loss: 4.905869483947754
Reconstruction Loss: -0.6714963316917419
Iteration 2521:
Training Loss: 4.6520915031433105
Reconstruction Loss: -0.6757786870002747
Iteration 2541:
Training Loss: 4.782637119293213
Reconstruction Loss: -0.659965455532074
Iteration 2561:
Training Loss: 4.980041027069092
Reconstruction Loss: -0.6418887376785278
Iteration 2581:
Training Loss: 5.082614898681641
Reconstruction Loss: -0.6644361019134521
Iteration 2601:
Training Loss: 4.8646321296691895
Reconstruction Loss: -0.6646369695663452
Iteration 2621:
Training Loss: 4.547662734985352
Reconstruction Loss: -0.6678565144538879
Iteration 2641:
Training Loss: 5.118733882904053
Reconstruction Loss: -0.638789176940918
Iteration 2661:
Training Loss: 4.832817077636719
Reconstruction Loss: -0.6585692167282104
Iteration 2681:
Training Loss: 4.918259620666504
Reconstruction Loss: -0.6477829813957214
Iteration 2701:
Training Loss: 5.069616317749023
Reconstruction Loss: -0.6635450124740601
Iteration 2721:
Training Loss: 4.791654586791992
Reconstruction Loss: -0.6672976016998291
Iteration 2741:
Training Loss: 4.829033374786377
Reconstruction Loss: -0.6652913093566895
Iteration 2761:
Training Loss: 4.667505741119385
Reconstruction Loss: -0.6642526388168335
Iteration 2781:
Training Loss: 4.811153888702393
Reconstruction Loss: -0.6751770377159119
Iteration 2801:
Training Loss: 4.777381896972656
Reconstruction Loss: -0.6783472299575806
Iteration 2821:
Training Loss: 4.661162376403809
Reconstruction Loss: -0.6565478444099426
Iteration 2841:
Training Loss: 4.7663140296936035
Reconstruction Loss: -0.6654836535453796
Iteration 2861:
Training Loss: 5.070338726043701
Reconstruction Loss: -0.6769458055496216
Iteration 2881:
Training Loss: 5.128870964050293
Reconstruction Loss: -0.652981162071228
Iteration 2901:
Training Loss: 4.8429999351501465
Reconstruction Loss: -0.639406144618988
Iteration 2921:
Training Loss: 4.814502716064453
Reconstruction Loss: -0.7045183777809143
Iteration 2941:
Training Loss: 4.864489555358887
Reconstruction Loss: -0.6624585390090942
Iteration 2961:
Training Loss: 4.939327239990234
Reconstruction Loss: -0.6758512258529663
Iteration 2981:
Training Loss: 5.126391410827637
Reconstruction Loss: -0.6815964579582214
