5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.727092266082764
Reconstruction Loss: -0.3968835473060608
Iteration 21:
Training Loss: 6.057650566101074
Reconstruction Loss: -0.3970593214035034
Iteration 41:
Training Loss: 5.808745861053467
Reconstruction Loss: -0.3974468410015106
Iteration 61:
Training Loss: 5.6668782234191895
Reconstruction Loss: -0.3996591567993164
Iteration 81:
Training Loss: 5.344273090362549
Reconstruction Loss: -0.6036348342895508
Iteration 101:
Training Loss: 5.076138019561768
Reconstruction Loss: -0.5428043603897095
Iteration 121:
Training Loss: 4.316700458526611
Reconstruction Loss: -0.9247865676879883
Iteration 141:
Training Loss: 3.6406311988830566
Reconstruction Loss: -1.0452423095703125
Iteration 161:
Training Loss: 4.034815311431885
Reconstruction Loss: -1.04976224899292
Iteration 181:
Training Loss: 3.490481376647949
Reconstruction Loss: -1.0582122802734375
Iteration 201:
Training Loss: 3.570483684539795
Reconstruction Loss: -1.1915949583053589
Iteration 221:
Training Loss: 2.453643560409546
Reconstruction Loss: -1.7816517353057861
Iteration 241:
Training Loss: 1.4301598072052002
Reconstruction Loss: -2.603609561920166
Iteration 261:
Training Loss: 0.7676288485527039
Reconstruction Loss: -3.3899266719818115
Iteration 281:
Training Loss: -0.048527590930461884
Reconstruction Loss: -4.136373996734619
Iteration 301:
Training Loss: -0.78305983543396
Reconstruction Loss: -4.81601095199585
Iteration 321:
Training Loss: -1.702124834060669
Reconstruction Loss: -5.458796501159668
Iteration 341:
Training Loss: -2.2715415954589844
Reconstruction Loss: -6.059161186218262
Iteration 361:
Training Loss: -2.870201587677002
Reconstruction Loss: -6.627901077270508
Iteration 381:
Training Loss: -3.497056722640991
Reconstruction Loss: -7.1770124435424805
Iteration 401:
Training Loss: -3.8761589527130127
Reconstruction Loss: -7.702115058898926
Iteration 421:
Training Loss: -4.703583240509033
Reconstruction Loss: -8.217450141906738
Iteration 441:
Training Loss: -5.225870132446289
Reconstruction Loss: -8.715085983276367
Iteration 461:
Training Loss: -5.559756278991699
Reconstruction Loss: -9.199199676513672
Iteration 481:
Training Loss: -6.319607257843018
Reconstruction Loss: -9.676396369934082
Iteration 501:
Training Loss: -6.631065845489502
Reconstruction Loss: -10.137406349182129
Iteration 521:
Training Loss: -6.826280117034912
Reconstruction Loss: -10.583573341369629
Iteration 541:
Training Loss: -7.462881088256836
Reconstruction Loss: -11.019922256469727
Iteration 561:
Training Loss: -7.9994587898254395
Reconstruction Loss: -11.43872356414795
Iteration 581:
Training Loss: -8.05510425567627
Reconstruction Loss: -11.840008735656738
Iteration 601:
Training Loss: -8.759981155395508
Reconstruction Loss: -12.226494789123535
Iteration 621:
Training Loss: -8.931778907775879
Reconstruction Loss: -12.581357955932617
Iteration 641:
Training Loss: -9.234410285949707
Reconstruction Loss: -12.910604476928711
Iteration 661:
Training Loss: -9.42910385131836
Reconstruction Loss: -13.20714282989502
Iteration 681:
Training Loss: -9.606014251708984
Reconstruction Loss: -13.461997985839844
Iteration 701:
Training Loss: -9.612282752990723
Reconstruction Loss: -13.675519943237305
Iteration 721:
Training Loss: -9.912726402282715
Reconstruction Loss: -13.847996711730957
Iteration 741:
Training Loss: -9.462553977966309
Reconstruction Loss: -13.979215621948242
Iteration 761:
Training Loss: -9.979979515075684
Reconstruction Loss: -14.07874870300293
Iteration 781:
Training Loss: -10.05306625366211
Reconstruction Loss: -14.154631614685059
Iteration 801:
Training Loss: -9.984148025512695
Reconstruction Loss: -14.20981216430664
Iteration 821:
Training Loss: -9.83633804321289
Reconstruction Loss: -14.239688873291016
Iteration 841:
Training Loss: -9.959518432617188
Reconstruction Loss: -14.263521194458008
Iteration 861:
Training Loss: -10.186963081359863
Reconstruction Loss: -14.284994125366211
Iteration 881:
Training Loss: -10.27932357788086
Reconstruction Loss: -14.29464054107666
Iteration 901:
Training Loss: -10.023150444030762
Reconstruction Loss: -14.302901268005371
Iteration 921:
Training Loss: -10.143165588378906
Reconstruction Loss: -14.310321807861328
Iteration 941:
Training Loss: -10.056829452514648
Reconstruction Loss: -14.312759399414062
Iteration 961:
Training Loss: -10.320181846618652
Reconstruction Loss: -14.312023162841797
Iteration 981:
Training Loss: -9.874282836914062
Reconstruction Loss: -14.305301666259766
Iteration 1001:
Training Loss: -9.8004150390625
Reconstruction Loss: -14.310111045837402
Iteration 1021:
Training Loss: -10.061384201049805
Reconstruction Loss: -14.316967964172363
Iteration 1041:
Training Loss: -9.965063095092773
Reconstruction Loss: -14.310343742370605
Iteration 1061:
Training Loss: -9.737485885620117
Reconstruction Loss: -14.311223983764648
Iteration 1081:
Training Loss: -9.554401397705078
Reconstruction Loss: -14.321294784545898
Iteration 1101:
Training Loss: -9.769172668457031
Reconstruction Loss: -14.314299583435059
Iteration 1121:
Training Loss: -10.230295181274414
Reconstruction Loss: -14.31517505645752
Iteration 1141:
Training Loss: -10.06213665008545
Reconstruction Loss: -14.305804252624512
Iteration 1161:
Training Loss: -9.973134994506836
Reconstruction Loss: -14.313008308410645
Iteration 1181:
Training Loss: -10.241314888000488
Reconstruction Loss: -14.309269905090332
Iteration 1201:
Training Loss: -10.266996383666992
Reconstruction Loss: -14.311534881591797
Iteration 1221:
Training Loss: -10.216297149658203
Reconstruction Loss: -14.313505172729492
Iteration 1241:
Training Loss: -9.638291358947754
Reconstruction Loss: -14.312714576721191
Iteration 1261:
Training Loss: -10.204919815063477
Reconstruction Loss: -14.31000804901123
Iteration 1281:
Training Loss: -10.10965347290039
Reconstruction Loss: -14.322014808654785
Iteration 1301:
Training Loss: -9.867555618286133
Reconstruction Loss: -14.308910369873047
Iteration 1321:
Training Loss: -10.176682472229004
Reconstruction Loss: -14.319631576538086
Iteration 1341:
Training Loss: -10.232647895812988
Reconstruction Loss: -14.317934036254883
Iteration 1361:
Training Loss: -10.168340682983398
Reconstruction Loss: -14.316695213317871
Iteration 1381:
Training Loss: -9.893887519836426
Reconstruction Loss: -14.323165893554688
Iteration 1401:
Training Loss: -9.707442283630371
Reconstruction Loss: -14.32017707824707
Iteration 1421:
Training Loss: -10.419463157653809
Reconstruction Loss: -14.32082462310791
Iteration 1441:
Training Loss: -10.222883224487305
Reconstruction Loss: -14.318757057189941
Iteration 1461:
Training Loss: -10.160013198852539
Reconstruction Loss: -14.32532024383545
Iteration 1481:
Training Loss: -10.126344680786133
Reconstruction Loss: -14.322897911071777
Iteration 1501:
Training Loss: -9.994490623474121
Reconstruction Loss: -14.322664260864258
Iteration 1521:
Training Loss: -10.165217399597168
Reconstruction Loss: -14.322456359863281
Iteration 1541:
Training Loss: -10.095951080322266
Reconstruction Loss: -14.32375717163086
Iteration 1561:
Training Loss: -10.025298118591309
Reconstruction Loss: -14.320610046386719
Iteration 1581:
Training Loss: -9.702579498291016
Reconstruction Loss: -14.326812744140625
Iteration 1601:
Training Loss: -10.278425216674805
Reconstruction Loss: -14.323515892028809
Iteration 1621:
Training Loss: -9.792637825012207
Reconstruction Loss: -14.324061393737793
Iteration 1641:
Training Loss: -9.76956558227539
Reconstruction Loss: -14.321328163146973
Iteration 1661:
Training Loss: -9.976828575134277
Reconstruction Loss: -14.334878921508789
Iteration 1681:
Training Loss: -10.19321060180664
Reconstruction Loss: -14.32731819152832
Iteration 1701:
Training Loss: -10.468677520751953
Reconstruction Loss: -14.328670501708984
Iteration 1721:
Training Loss: -10.07400131225586
Reconstruction Loss: -14.32590103149414
Iteration 1741:
Training Loss: -10.221684455871582
Reconstruction Loss: -14.326931953430176
Iteration 1761:
Training Loss: -10.031664848327637
Reconstruction Loss: -14.334943771362305
Iteration 1781:
Training Loss: -10.429133415222168
Reconstruction Loss: -14.329858779907227
Iteration 1801:
Training Loss: -10.22785472869873
Reconstruction Loss: -14.328604698181152
Iteration 1821:
Training Loss: -10.284148216247559
Reconstruction Loss: -14.327364921569824
Iteration 1841:
Training Loss: -9.945772171020508
Reconstruction Loss: -14.33829116821289
Iteration 1861:
Training Loss: -10.222644805908203
Reconstruction Loss: -14.329791069030762
Iteration 1881:
Training Loss: -9.980687141418457
Reconstruction Loss: -14.336247444152832
Iteration 1901:
Training Loss: -9.918041229248047
Reconstruction Loss: -14.332210540771484
Iteration 1921:
Training Loss: -10.368654251098633
Reconstruction Loss: -14.336114883422852
Iteration 1941:
Training Loss: -9.867255210876465
Reconstruction Loss: -14.330646514892578
Iteration 1961:
Training Loss: -10.187295913696289
Reconstruction Loss: -14.335373878479004
Iteration 1981:
Training Loss: -10.067580223083496
Reconstruction Loss: -14.334620475769043
Iteration 2001:
Training Loss: -10.183771133422852
Reconstruction Loss: -14.330513954162598
Iteration 2021:
Training Loss: -10.272183418273926
Reconstruction Loss: -14.340646743774414
Iteration 2041:
Training Loss: -10.20614242553711
Reconstruction Loss: -14.34102725982666
Iteration 2061:
Training Loss: -10.086875915527344
Reconstruction Loss: -14.338077545166016
Iteration 2081:
Training Loss: -9.720772743225098
Reconstruction Loss: -14.342158317565918
Iteration 2101:
Training Loss: -10.152425765991211
Reconstruction Loss: -14.339515686035156
Iteration 2121:
Training Loss: -9.968459129333496
Reconstruction Loss: -14.34105396270752
Iteration 2141:
Training Loss: -9.839803695678711
Reconstruction Loss: -14.345209121704102
Iteration 2161:
Training Loss: -9.989778518676758
Reconstruction Loss: -14.344107627868652
Iteration 2181:
Training Loss: -10.049931526184082
Reconstruction Loss: -14.339132308959961
Iteration 2201:
Training Loss: -9.973420143127441
Reconstruction Loss: -14.341448783874512
Iteration 2221:
Training Loss: -9.909882545471191
Reconstruction Loss: -14.336612701416016
Iteration 2241:
Training Loss: -9.925423622131348
Reconstruction Loss: -14.34536361694336
Iteration 2261:
Training Loss: -9.98813533782959
Reconstruction Loss: -14.349827766418457
Iteration 2281:
Training Loss: -10.174027442932129
Reconstruction Loss: -14.346257209777832
Iteration 2301:
Training Loss: -10.358439445495605
Reconstruction Loss: -14.335639953613281
Iteration 2321:
Training Loss: -9.946232795715332
Reconstruction Loss: -14.350796699523926
Iteration 2341:
Training Loss: -10.167037010192871
Reconstruction Loss: -14.348331451416016
Iteration 2361:
Training Loss: -9.829843521118164
Reconstruction Loss: -14.355293273925781
Iteration 2381:
Training Loss: -10.402103424072266
Reconstruction Loss: -14.347576141357422
Iteration 2401:
Training Loss: -10.307297706604004
Reconstruction Loss: -14.349515914916992
Iteration 2421:
Training Loss: -9.938932418823242
Reconstruction Loss: -14.356688499450684
Iteration 2441:
Training Loss: -10.26514720916748
Reconstruction Loss: -14.350519180297852
Iteration 2461:
Training Loss: -9.918477058410645
Reconstruction Loss: -14.347902297973633
Iteration 2481:
Training Loss: -10.21458625793457
Reconstruction Loss: -14.347872734069824
Iteration 2501:
Training Loss: -10.28135871887207
Reconstruction Loss: -14.35544490814209
Iteration 2521:
Training Loss: -10.244606018066406
Reconstruction Loss: -14.352153778076172
Iteration 2541:
Training Loss: -10.074792861938477
Reconstruction Loss: -14.353972434997559
Iteration 2561:
Training Loss: -10.14665699005127
Reconstruction Loss: -14.357698440551758
Iteration 2581:
Training Loss: -10.338659286499023
Reconstruction Loss: -14.354279518127441
Iteration 2601:
Training Loss: -9.999958992004395
Reconstruction Loss: -14.35799503326416
Iteration 2621:
Training Loss: -10.178075790405273
Reconstruction Loss: -14.357964515686035
Iteration 2641:
Training Loss: -10.0216064453125
Reconstruction Loss: -14.35210132598877
Iteration 2661:
Training Loss: -10.150318145751953
Reconstruction Loss: -14.356367111206055
Iteration 2681:
Training Loss: -10.121293067932129
Reconstruction Loss: -14.35422134399414
Iteration 2701:
Training Loss: -10.462836265563965
Reconstruction Loss: -14.357620239257812
Iteration 2721:
Training Loss: -9.655304908752441
Reconstruction Loss: -14.36433219909668
Iteration 2741:
Training Loss: -10.229248046875
Reconstruction Loss: -14.364591598510742
Iteration 2761:
Training Loss: -9.908515930175781
Reconstruction Loss: -14.360960006713867
Iteration 2781:
Training Loss: -10.262310981750488
Reconstruction Loss: -14.354938507080078
Iteration 2801:
Training Loss: -10.365281105041504
Reconstruction Loss: -14.357375144958496
Iteration 2821:
Training Loss: -10.265275001525879
Reconstruction Loss: -14.360824584960938
Iteration 2841:
Training Loss: -10.400530815124512
Reconstruction Loss: -14.359763145446777
Iteration 2861:
Training Loss: -10.160430908203125
Reconstruction Loss: -14.361224174499512
Iteration 2881:
Training Loss: -10.307140350341797
Reconstruction Loss: -14.356680870056152
Iteration 2901:
Training Loss: -9.960128784179688
Reconstruction Loss: -14.363983154296875
Iteration 2921:
Training Loss: -10.082420349121094
Reconstruction Loss: -14.361693382263184
Iteration 2941:
Training Loss: -9.89676284790039
Reconstruction Loss: -14.366876602172852
Iteration 2961:
Training Loss: -9.936616897583008
Reconstruction Loss: -14.362255096435547
Iteration 2981:
Training Loss: -10.271461486816406
Reconstruction Loss: -14.369749069213867
