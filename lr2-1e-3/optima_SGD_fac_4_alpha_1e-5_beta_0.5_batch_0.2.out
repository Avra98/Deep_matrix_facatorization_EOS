5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.4382710456848145
Reconstruction Loss: -0.45127952098846436
Iteration 21:
Training Loss: 5.300322532653809
Reconstruction Loss: -0.4516902565956116
Iteration 41:
Training Loss: 5.674835681915283
Reconstruction Loss: -0.45242995023727417
Iteration 61:
Training Loss: 5.6554155349731445
Reconstruction Loss: -0.45507118105888367
Iteration 81:
Training Loss: 5.184353828430176
Reconstruction Loss: -0.5355308055877686
Iteration 101:
Training Loss: 5.112699508666992
Reconstruction Loss: -0.632474422454834
Iteration 121:
Training Loss: 5.096851825714111
Reconstruction Loss: -0.7631598114967346
Iteration 141:
Training Loss: 4.155427932739258
Reconstruction Loss: -0.9894372820854187
Iteration 161:
Training Loss: 3.854078531265259
Reconstruction Loss: -0.9907305240631104
Iteration 181:
Training Loss: 3.683973550796509
Reconstruction Loss: -1.111525058746338
Iteration 201:
Training Loss: 3.2358741760253906
Reconstruction Loss: -1.4596459865570068
Iteration 221:
Training Loss: 3.517585515975952
Reconstruction Loss: -1.5365784168243408
Iteration 241:
Training Loss: 3.355168104171753
Reconstruction Loss: -1.5388197898864746
Iteration 261:
Training Loss: 3.0689213275909424
Reconstruction Loss: -1.502425193786621
Iteration 281:
Training Loss: 2.831798553466797
Reconstruction Loss: -1.4803950786590576
Iteration 301:
Training Loss: 3.132169723510742
Reconstruction Loss: -1.465429425239563
Iteration 321:
Training Loss: 3.0033202171325684
Reconstruction Loss: -1.4592394828796387
Iteration 341:
Training Loss: 3.056037187576294
Reconstruction Loss: -1.4551327228546143
Iteration 361:
Training Loss: 2.7526819705963135
Reconstruction Loss: -1.4541102647781372
Iteration 381:
Training Loss: 3.0212841033935547
Reconstruction Loss: -1.443048357963562
Iteration 401:
Training Loss: 2.7169759273529053
Reconstruction Loss: -1.4471834897994995
Iteration 421:
Training Loss: 2.8664445877075195
Reconstruction Loss: -1.4668662548065186
Iteration 441:
Training Loss: 2.477741241455078
Reconstruction Loss: -1.5469582080841064
Iteration 461:
Training Loss: 2.2444779872894287
Reconstruction Loss: -1.8748180866241455
Iteration 481:
Training Loss: 1.5772680044174194
Reconstruction Loss: -2.4492363929748535
Iteration 501:
Training Loss: 0.6755297780036926
Reconstruction Loss: -3.1138148307800293
Iteration 521:
Training Loss: 0.23416957259178162
Reconstruction Loss: -3.8219308853149414
Iteration 541:
Training Loss: -1.0299391746520996
Reconstruction Loss: -4.539813041687012
Iteration 561:
Training Loss: -1.382061243057251
Reconstruction Loss: -5.225406646728516
Iteration 581:
Training Loss: -2.4311275482177734
Reconstruction Loss: -5.879077911376953
Iteration 601:
Training Loss: -2.641467571258545
Reconstruction Loss: -6.482070446014404
Iteration 621:
Training Loss: -3.6139488220214844
Reconstruction Loss: -7.034377098083496
Iteration 641:
Training Loss: -4.0066938400268555
Reconstruction Loss: -7.52513313293457
Iteration 661:
Training Loss: -4.2815423011779785
Reconstruction Loss: -7.939810752868652
Iteration 681:
Training Loss: -4.692417144775391
Reconstruction Loss: -8.2813720703125
Iteration 701:
Training Loss: -4.932051658630371
Reconstruction Loss: -8.550618171691895
Iteration 721:
Training Loss: -4.782487392425537
Reconstruction Loss: -8.749746322631836
Iteration 741:
Training Loss: -4.793008804321289
Reconstruction Loss: -8.901826858520508
Iteration 761:
Training Loss: -5.378964900970459
Reconstruction Loss: -9.004964828491211
Iteration 781:
Training Loss: -4.919491291046143
Reconstruction Loss: -9.079643249511719
Iteration 801:
Training Loss: -5.1841511726379395
Reconstruction Loss: -9.135905265808105
Iteration 821:
Training Loss: -5.384061336517334
Reconstruction Loss: -9.178552627563477
Iteration 841:
Training Loss: -5.070850372314453
Reconstruction Loss: -9.21529769897461
Iteration 861:
Training Loss: -5.2663116455078125
Reconstruction Loss: -9.247485160827637
Iteration 881:
Training Loss: -5.320913791656494
Reconstruction Loss: -9.270736694335938
Iteration 901:
Training Loss: -5.350647449493408
Reconstruction Loss: -9.294221878051758
Iteration 921:
Training Loss: -5.347382068634033
Reconstruction Loss: -9.315695762634277
Iteration 941:
Training Loss: -5.239505290985107
Reconstruction Loss: -9.327624320983887
Iteration 961:
Training Loss: -5.508786201477051
Reconstruction Loss: -9.34736156463623
Iteration 981:
Training Loss: -5.616103649139404
Reconstruction Loss: -9.361725807189941
Iteration 1001:
Training Loss: -5.253665924072266
Reconstruction Loss: -9.383198738098145
Iteration 1021:
Training Loss: -5.403944969177246
Reconstruction Loss: -9.396108627319336
Iteration 1041:
Training Loss: -5.283514022827148
Reconstruction Loss: -9.414403915405273
Iteration 1061:
Training Loss: -5.525381088256836
Reconstruction Loss: -9.432180404663086
Iteration 1081:
Training Loss: -5.333667755126953
Reconstruction Loss: -9.443154335021973
Iteration 1101:
Training Loss: -5.276041507720947
Reconstruction Loss: -9.456637382507324
Iteration 1121:
Training Loss: -5.474703311920166
Reconstruction Loss: -9.480932235717773
Iteration 1141:
Training Loss: -5.530237674713135
Reconstruction Loss: -9.489623069763184
Iteration 1161:
Training Loss: -5.297510623931885
Reconstruction Loss: -9.510063171386719
Iteration 1181:
Training Loss: -5.667413711547852
Reconstruction Loss: -9.519411087036133
Iteration 1201:
Training Loss: -5.793977737426758
Reconstruction Loss: -9.53868293762207
Iteration 1221:
Training Loss: -5.618866443634033
Reconstruction Loss: -9.553131103515625
Iteration 1241:
Training Loss: -5.61593770980835
Reconstruction Loss: -9.570809364318848
Iteration 1261:
Training Loss: -5.702630519866943
Reconstruction Loss: -9.58205795288086
Iteration 1281:
Training Loss: -5.722177982330322
Reconstruction Loss: -9.589794158935547
Iteration 1301:
Training Loss: -5.647676467895508
Reconstruction Loss: -9.605066299438477
Iteration 1321:
Training Loss: -5.580400466918945
Reconstruction Loss: -9.615087509155273
Iteration 1341:
Training Loss: -5.8772077560424805
Reconstruction Loss: -9.62786865234375
Iteration 1361:
Training Loss: -5.851282119750977
Reconstruction Loss: -9.648245811462402
Iteration 1381:
Training Loss: -5.858098983764648
Reconstruction Loss: -9.654440879821777
Iteration 1401:
Training Loss: -5.4854655265808105
Reconstruction Loss: -9.664508819580078
Iteration 1421:
Training Loss: -5.96643590927124
Reconstruction Loss: -9.679267883300781
Iteration 1441:
Training Loss: -5.777970790863037
Reconstruction Loss: -9.684520721435547
Iteration 1461:
Training Loss: -5.558014392852783
Reconstruction Loss: -9.695638656616211
Iteration 1481:
Training Loss: -5.707444667816162
Reconstruction Loss: -9.712751388549805
Iteration 1501:
Training Loss: -5.765268802642822
Reconstruction Loss: -9.734432220458984
Iteration 1521:
Training Loss: -5.771103382110596
Reconstruction Loss: -9.734391212463379
Iteration 1541:
Training Loss: -5.542665004730225
Reconstruction Loss: -9.752392768859863
Iteration 1561:
Training Loss: -6.382001876831055
Reconstruction Loss: -9.760184288024902
Iteration 1581:
Training Loss: -5.619158744812012
Reconstruction Loss: -9.773810386657715
Iteration 1601:
Training Loss: -5.767066955566406
Reconstruction Loss: -9.784255981445312
Iteration 1621:
Training Loss: -5.8774237632751465
Reconstruction Loss: -9.799042701721191
Iteration 1641:
Training Loss: -6.203795433044434
Reconstruction Loss: -9.805813789367676
Iteration 1661:
Training Loss: -5.636526107788086
Reconstruction Loss: -9.814294815063477
Iteration 1681:
Training Loss: -6.093102931976318
Reconstruction Loss: -9.824844360351562
Iteration 1701:
Training Loss: -5.981572151184082
Reconstruction Loss: -9.833414077758789
Iteration 1721:
Training Loss: -5.924262523651123
Reconstruction Loss: -9.852400779724121
Iteration 1741:
Training Loss: -5.760933876037598
Reconstruction Loss: -9.854790687561035
Iteration 1761:
Training Loss: -5.901983261108398
Reconstruction Loss: -9.869528770446777
Iteration 1781:
Training Loss: -6.242226600646973
Reconstruction Loss: -9.884332656860352
Iteration 1801:
Training Loss: -6.108526229858398
Reconstruction Loss: -9.891287803649902
Iteration 1821:
Training Loss: -5.947352886199951
Reconstruction Loss: -9.90353775024414
Iteration 1841:
Training Loss: -5.777333736419678
Reconstruction Loss: -9.915565490722656
Iteration 1861:
Training Loss: -6.191622734069824
Reconstruction Loss: -9.918431282043457
Iteration 1881:
Training Loss: -6.013955116271973
Reconstruction Loss: -9.930352210998535
Iteration 1901:
Training Loss: -6.191433906555176
Reconstruction Loss: -9.944686889648438
Iteration 1921:
Training Loss: -5.978123664855957
Reconstruction Loss: -9.946990966796875
Iteration 1941:
Training Loss: -6.191692352294922
Reconstruction Loss: -9.953156471252441
Iteration 1961:
Training Loss: -6.348527908325195
Reconstruction Loss: -9.972102165222168
Iteration 1981:
Training Loss: -6.074792385101318
Reconstruction Loss: -9.976532936096191
Iteration 2001:
Training Loss: -5.941314220428467
Reconstruction Loss: -9.990303039550781
Iteration 2021:
Training Loss: -6.251428604125977
Reconstruction Loss: -9.994980812072754
Iteration 2041:
Training Loss: -6.0642805099487305
Reconstruction Loss: -10.001770973205566
Iteration 2061:
Training Loss: -6.296901226043701
Reconstruction Loss: -10.005821228027344
Iteration 2081:
Training Loss: -5.982809066772461
Reconstruction Loss: -10.022934913635254
Iteration 2101:
Training Loss: -6.166688919067383
Reconstruction Loss: -10.022980690002441
Iteration 2121:
Training Loss: -6.507145404815674
Reconstruction Loss: -10.042057991027832
Iteration 2141:
Training Loss: -5.954409599304199
Reconstruction Loss: -10.059168815612793
Iteration 2161:
Training Loss: -5.893813133239746
Reconstruction Loss: -10.056436538696289
Iteration 2181:
Training Loss: -6.049070358276367
Reconstruction Loss: -10.064334869384766
Iteration 2201:
Training Loss: -6.107280731201172
Reconstruction Loss: -10.069737434387207
Iteration 2221:
Training Loss: -6.476476192474365
Reconstruction Loss: -10.079081535339355
Iteration 2241:
Training Loss: -6.107656955718994
Reconstruction Loss: -10.096223831176758
Iteration 2261:
Training Loss: -6.248946666717529
Reconstruction Loss: -10.092828750610352
Iteration 2281:
Training Loss: -6.074851989746094
Reconstruction Loss: -10.106369018554688
Iteration 2301:
Training Loss: -6.105650424957275
Reconstruction Loss: -10.115801811218262
Iteration 2321:
Training Loss: -6.156364440917969
Reconstruction Loss: -10.11599063873291
Iteration 2341:
Training Loss: -6.196561336517334
Reconstruction Loss: -10.124006271362305
Iteration 2361:
Training Loss: -6.256883144378662
Reconstruction Loss: -10.14181137084961
Iteration 2381:
Training Loss: -6.3105363845825195
Reconstruction Loss: -10.145746231079102
Iteration 2401:
Training Loss: -6.124616622924805
Reconstruction Loss: -10.150379180908203
Iteration 2421:
Training Loss: -6.203629970550537
Reconstruction Loss: -10.161276817321777
Iteration 2441:
Training Loss: -6.360738277435303
Reconstruction Loss: -10.167410850524902
Iteration 2461:
Training Loss: -6.31168270111084
Reconstruction Loss: -10.173104286193848
Iteration 2481:
Training Loss: -6.381234169006348
Reconstruction Loss: -10.17748737335205
Iteration 2501:
Training Loss: -6.177320957183838
Reconstruction Loss: -10.190385818481445
Iteration 2521:
Training Loss: -6.295674800872803
Reconstruction Loss: -10.19666576385498
Iteration 2541:
Training Loss: -6.284642219543457
Reconstruction Loss: -10.206488609313965
Iteration 2561:
Training Loss: -6.346962928771973
Reconstruction Loss: -10.208144187927246
Iteration 2581:
Training Loss: -6.356601715087891
Reconstruction Loss: -10.225274085998535
Iteration 2601:
Training Loss: -6.264830589294434
Reconstruction Loss: -10.228472709655762
Iteration 2621:
Training Loss: -6.169816017150879
Reconstruction Loss: -10.220020294189453
Iteration 2641:
Training Loss: -6.382297515869141
Reconstruction Loss: -10.243471145629883
Iteration 2661:
Training Loss: -6.479669570922852
Reconstruction Loss: -10.252663612365723
Iteration 2681:
Training Loss: -6.459811210632324
Reconstruction Loss: -10.25941276550293
Iteration 2701:
Training Loss: -6.244671821594238
Reconstruction Loss: -10.255514144897461
Iteration 2721:
Training Loss: -6.30282735824585
Reconstruction Loss: -10.26963996887207
Iteration 2741:
Training Loss: -6.680457592010498
Reconstruction Loss: -10.270962715148926
Iteration 2761:
Training Loss: -6.280433654785156
Reconstruction Loss: -10.272957801818848
Iteration 2781:
Training Loss: -6.5013427734375
Reconstruction Loss: -10.285387992858887
Iteration 2801:
Training Loss: -6.413367748260498
Reconstruction Loss: -10.287731170654297
Iteration 2821:
Training Loss: -6.550828456878662
Reconstruction Loss: -10.303991317749023
Iteration 2841:
Training Loss: -6.545207500457764
Reconstruction Loss: -10.306331634521484
Iteration 2861:
Training Loss: -6.462055206298828
Reconstruction Loss: -10.313896179199219
Iteration 2881:
Training Loss: -6.6563401222229
Reconstruction Loss: -10.317354202270508
Iteration 2901:
Training Loss: -6.757969856262207
Reconstruction Loss: -10.32113265991211
Iteration 2921:
Training Loss: -6.558712005615234
Reconstruction Loss: -10.333022117614746
Iteration 2941:
Training Loss: -6.714371681213379
Reconstruction Loss: -10.333361625671387
Iteration 2961:
Training Loss: -6.541313648223877
Reconstruction Loss: -10.337370872497559
Iteration 2981:
Training Loss: -6.39452600479126
Reconstruction Loss: -10.34931755065918
