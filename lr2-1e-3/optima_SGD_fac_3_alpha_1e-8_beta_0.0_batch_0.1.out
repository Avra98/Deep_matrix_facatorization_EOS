5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.399395942687988
Reconstruction Loss: -0.3342910408973694
Iteration 11:
Training Loss: 5.7253828048706055
Reconstruction Loss: -0.3342910408973694
Iteration 21:
Training Loss: 5.621761322021484
Reconstruction Loss: -0.3342910408973694
Iteration 31:
Training Loss: 5.547708511352539
Reconstruction Loss: -0.3342910408973694
Iteration 41:
Training Loss: 5.695035934448242
Reconstruction Loss: -0.33429113030433655
Iteration 51:
Training Loss: 5.450259208679199
Reconstruction Loss: -0.33429113030433655
Iteration 61:
Training Loss: 5.269796371459961
Reconstruction Loss: -0.33429113030433655
Iteration 71:
Training Loss: 5.370513916015625
Reconstruction Loss: -0.33429113030433655
Iteration 81:
Training Loss: 5.375248432159424
Reconstruction Loss: -0.33429113030433655
Iteration 91:
Training Loss: 5.759018421173096
Reconstruction Loss: -0.33429113030433655
Iteration 101:
Training Loss: 5.457847595214844
Reconstruction Loss: -0.33429113030433655
Iteration 111:
Training Loss: 5.166374206542969
Reconstruction Loss: -0.33429113030433655
Iteration 121:
Training Loss: 5.449877738952637
Reconstruction Loss: -0.33429136872291565
Iteration 131:
Training Loss: 5.749383926391602
Reconstruction Loss: -0.33429136872291565
Iteration 141:
Training Loss: 5.526413440704346
Reconstruction Loss: -0.33429136872291565
Iteration 151:
Training Loss: 5.833748817443848
Reconstruction Loss: -0.33429136872291565
Iteration 161:
Training Loss: 5.659910678863525
Reconstruction Loss: -0.3342914581298828
Iteration 171:
Training Loss: 5.693563938140869
Reconstruction Loss: -0.3342914581298828
Iteration 181:
Training Loss: 5.780468940734863
Reconstruction Loss: -0.3342914581298828
Iteration 191:
Training Loss: 5.560488224029541
Reconstruction Loss: -0.3342914581298828
Iteration 201:
Training Loss: 5.355166912078857
Reconstruction Loss: -0.3342914581298828
Iteration 211:
Training Loss: 5.141964912414551
Reconstruction Loss: -0.33429154753685
Iteration 221:
Training Loss: 6.032875061035156
Reconstruction Loss: -0.3342917263507843
Iteration 231:
Training Loss: 5.615970611572266
Reconstruction Loss: -0.3342917859554291
Iteration 241:
Training Loss: 5.649583339691162
Reconstruction Loss: -0.3342917859554291
Iteration 251:
Training Loss: 5.981688499450684
Reconstruction Loss: -0.3342919647693634
Iteration 261:
Training Loss: 5.742612361907959
Reconstruction Loss: -0.33429211378097534
Iteration 271:
Training Loss: 5.618704795837402
Reconstruction Loss: -0.3342922031879425
Iteration 281:
Training Loss: 5.672703266143799
Reconstruction Loss: -0.334292471408844
Iteration 291:
Training Loss: 5.49395751953125
Reconstruction Loss: -0.33429262042045593
Iteration 301:
Training Loss: 6.002525329589844
Reconstruction Loss: -0.3342928886413574
Iteration 311:
Training Loss: 5.373292922973633
Reconstruction Loss: -0.33429330587387085
Iteration 321:
Training Loss: 5.569870471954346
Reconstruction Loss: -0.3342938721179962
Iteration 331:
Training Loss: 5.423242568969727
Reconstruction Loss: -0.3342946171760559
Iteration 341:
Training Loss: 5.539336681365967
Reconstruction Loss: -0.3342958092689514
Iteration 351:
Training Loss: 5.2535834312438965
Reconstruction Loss: -0.33429720997810364
Iteration 361:
Training Loss: 5.228551864624023
Reconstruction Loss: -0.3342995345592499
Iteration 371:
Training Loss: 5.253042221069336
Reconstruction Loss: -0.33430320024490356
Iteration 381:
Training Loss: 5.491755485534668
Reconstruction Loss: -0.3343096077442169
Iteration 391:
Training Loss: 5.37578821182251
Reconstruction Loss: -0.3343217670917511
Iteration 401:
Training Loss: 5.155500411987305
Reconstruction Loss: -0.334347665309906
Iteration 411:
Training Loss: 5.93373441696167
Reconstruction Loss: -0.33441394567489624
Iteration 421:
Training Loss: 5.644347190856934
Reconstruction Loss: -0.33465340733528137
Iteration 431:
Training Loss: 5.788107395172119
Reconstruction Loss: -0.3362778127193451
Iteration 441:
Training Loss: 5.248653888702393
Reconstruction Loss: -0.4164385199546814
Iteration 451:
Training Loss: 4.984699249267578
Reconstruction Loss: -0.4194140136241913
Iteration 461:
Training Loss: 5.459103107452393
Reconstruction Loss: -0.37661659717559814
Iteration 471:
Training Loss: 5.324308395385742
Reconstruction Loss: -0.40316835045814514
Iteration 481:
Training Loss: 5.195601463317871
Reconstruction Loss: -0.4242349863052368
Iteration 491:
Training Loss: 4.665640354156494
Reconstruction Loss: -0.390886515378952
Iteration 501:
Training Loss: 4.904440402984619
Reconstruction Loss: -0.3814435601234436
Iteration 511:
Training Loss: 4.613190650939941
Reconstruction Loss: -0.3793306350708008
Iteration 521:
Training Loss: 5.341813087463379
Reconstruction Loss: -0.43628546595573425
Iteration 531:
Training Loss: 5.138202667236328
Reconstruction Loss: -0.4011564552783966
Iteration 541:
Training Loss: 4.662137031555176
Reconstruction Loss: -0.3665087819099426
Iteration 551:
Training Loss: 5.042342185974121
Reconstruction Loss: -0.39135175943374634
Iteration 561:
Training Loss: 4.910789966583252
Reconstruction Loss: -0.3885580003261566
Iteration 571:
Training Loss: 4.784094333648682
Reconstruction Loss: -0.4912871718406677
Iteration 581:
Training Loss: 4.389980316162109
Reconstruction Loss: -0.6499963998794556
Iteration 591:
Training Loss: 4.7340803146362305
Reconstruction Loss: -0.6749593019485474
Iteration 601:
Training Loss: 4.586019515991211
Reconstruction Loss: -0.6751591563224792
Iteration 611:
Training Loss: 4.43502140045166
Reconstruction Loss: -0.689216136932373
Iteration 621:
Training Loss: 4.986130714416504
Reconstruction Loss: -0.6751208305358887
Iteration 631:
Training Loss: 4.18622350692749
Reconstruction Loss: -0.6779206395149231
Iteration 641:
Training Loss: 4.419828414916992
Reconstruction Loss: -0.6576861143112183
Iteration 651:
Training Loss: 4.500654220581055
Reconstruction Loss: -0.6932292580604553
Iteration 661:
Training Loss: 4.536037921905518
Reconstruction Loss: -0.6842490434646606
Iteration 671:
Training Loss: 4.689554214477539
Reconstruction Loss: -0.6447441577911377
Iteration 681:
Training Loss: 4.385730266571045
Reconstruction Loss: -0.6662560105323792
Iteration 691:
Training Loss: 4.507027626037598
Reconstruction Loss: -0.6835245490074158
Iteration 701:
Training Loss: 4.406050205230713
Reconstruction Loss: -0.6947357058525085
Iteration 711:
Training Loss: 4.393991947174072
Reconstruction Loss: -0.6813843846321106
Iteration 721:
Training Loss: 5.015097618103027
Reconstruction Loss: -0.6944818496704102
Iteration 731:
Training Loss: 4.57822322845459
Reconstruction Loss: -0.669903576374054
Iteration 741:
Training Loss: 4.804784774780273
Reconstruction Loss: -0.6779364943504333
Iteration 751:
Training Loss: 4.332818984985352
Reconstruction Loss: -0.6804311275482178
Iteration 761:
Training Loss: 4.825297832489014
Reconstruction Loss: -0.6914274096488953
Iteration 771:
Training Loss: 4.3061957359313965
Reconstruction Loss: -0.6908979415893555
Iteration 781:
Training Loss: 4.8446807861328125
Reconstruction Loss: -0.6807278990745544
Iteration 791:
Training Loss: 4.718239784240723
Reconstruction Loss: -0.682389497756958
Iteration 801:
Training Loss: 4.817500591278076
Reconstruction Loss: -0.6953245401382446
Iteration 811:
Training Loss: 4.814485549926758
Reconstruction Loss: -0.6516106724739075
Iteration 821:
Training Loss: 4.705974578857422
Reconstruction Loss: -0.6615293622016907
Iteration 831:
Training Loss: 4.430749893188477
Reconstruction Loss: -0.6891566514968872
Iteration 841:
Training Loss: 4.4543561935424805
Reconstruction Loss: -0.6706874966621399
Iteration 851:
Training Loss: 4.817501544952393
Reconstruction Loss: -0.6758818030357361
Iteration 861:
Training Loss: 4.098297595977783
Reconstruction Loss: -0.6856511831283569
Iteration 871:
Training Loss: 4.437289237976074
Reconstruction Loss: -0.6587101817131042
Iteration 881:
Training Loss: 4.375556945800781
Reconstruction Loss: -0.6877639889717102
Iteration 891:
Training Loss: 4.5838165283203125
Reconstruction Loss: -0.673464834690094
Iteration 901:
Training Loss: 4.392376899719238
Reconstruction Loss: -0.6782825589179993
Iteration 911:
Training Loss: 4.255781173706055
Reconstruction Loss: -0.6865354180335999
Iteration 921:
Training Loss: 4.143157482147217
Reconstruction Loss: -0.6876615881919861
Iteration 931:
Training Loss: 4.533311367034912
Reconstruction Loss: -0.7073559761047363
Iteration 941:
Training Loss: 4.410436630249023
Reconstruction Loss: -0.6983018517494202
Iteration 951:
Training Loss: 4.552850246429443
Reconstruction Loss: -0.6808821558952332
Iteration 961:
Training Loss: 4.660852432250977
Reconstruction Loss: -0.6502172946929932
Iteration 971:
Training Loss: 4.455358028411865
Reconstruction Loss: -0.6893152594566345
Iteration 981:
Training Loss: 3.996314287185669
Reconstruction Loss: -0.6808033585548401
Iteration 991:
Training Loss: 4.338061332702637
Reconstruction Loss: -0.7802000641822815
Iteration 1001:
Training Loss: 3.9592037200927734
Reconstruction Loss: -0.9303156733512878
Iteration 1011:
Training Loss: 4.058650970458984
Reconstruction Loss: -0.8761048316955566
Iteration 1021:
Training Loss: 3.756833553314209
Reconstruction Loss: -0.8587195873260498
Iteration 1031:
Training Loss: 3.991544008255005
Reconstruction Loss: -0.8459678292274475
Iteration 1041:
Training Loss: 4.27261209487915
Reconstruction Loss: -0.8335344195365906
Iteration 1051:
Training Loss: 4.019547939300537
Reconstruction Loss: -0.8582786917686462
Iteration 1061:
Training Loss: 3.336832046508789
Reconstruction Loss: -0.8514791131019592
Iteration 1071:
Training Loss: 4.036049842834473
Reconstruction Loss: -0.8555791974067688
Iteration 1081:
Training Loss: 4.296939849853516
Reconstruction Loss: -0.840800404548645
Iteration 1091:
Training Loss: 3.3211519718170166
Reconstruction Loss: -0.8608161807060242
Iteration 1101:
Training Loss: 4.1024298667907715
Reconstruction Loss: -0.8374855518341064
Iteration 1111:
Training Loss: 3.7069413661956787
Reconstruction Loss: -0.8722498416900635
Iteration 1121:
Training Loss: 3.905670404434204
Reconstruction Loss: -0.8242805004119873
Iteration 1131:
Training Loss: 3.829263687133789
Reconstruction Loss: -0.8343011140823364
Iteration 1141:
Training Loss: 3.963188409805298
Reconstruction Loss: -0.8406794667243958
Iteration 1151:
Training Loss: 3.587684392929077
Reconstruction Loss: -0.8385192155838013
Iteration 1161:
Training Loss: 4.07277250289917
Reconstruction Loss: -0.8171871900558472
Iteration 1171:
Training Loss: 4.087845325469971
Reconstruction Loss: -0.8482162952423096
Iteration 1181:
Training Loss: 4.075039386749268
Reconstruction Loss: -0.8286521434783936
Iteration 1191:
Training Loss: 3.9337103366851807
Reconstruction Loss: -0.8680050373077393
Iteration 1201:
Training Loss: 4.253373146057129
Reconstruction Loss: -0.8687621355056763
Iteration 1211:
Training Loss: 3.7911877632141113
Reconstruction Loss: -1.0191636085510254
Iteration 1221:
Training Loss: 3.2921369075775146
Reconstruction Loss: -1.2269465923309326
Iteration 1231:
Training Loss: 2.980107307434082
Reconstruction Loss: -1.3321561813354492
Iteration 1241:
Training Loss: 3.1736817359924316
Reconstruction Loss: -1.4239301681518555
Iteration 1251:
Training Loss: 3.0697052478790283
Reconstruction Loss: -1.480846881866455
Iteration 1261:
Training Loss: 3.0995891094207764
Reconstruction Loss: -1.5422682762145996
Iteration 1271:
Training Loss: 2.7964909076690674
Reconstruction Loss: -1.5858075618743896
Iteration 1281:
Training Loss: 3.180990219116211
Reconstruction Loss: -1.631054401397705
Iteration 1291:
Training Loss: 2.696885585784912
Reconstruction Loss: -1.6357307434082031
Iteration 1301:
Training Loss: 2.9553797245025635
Reconstruction Loss: -1.6591713428497314
Iteration 1311:
Training Loss: 3.257650375366211
Reconstruction Loss: -1.6564960479736328
Iteration 1321:
Training Loss: 3.1692051887512207
Reconstruction Loss: -1.6623605489730835
Iteration 1331:
Training Loss: 3.0383682250976562
Reconstruction Loss: -1.6440167427062988
Iteration 1341:
Training Loss: 3.0188093185424805
Reconstruction Loss: -1.649484395980835
Iteration 1351:
Training Loss: 2.926415205001831
Reconstruction Loss: -1.6550421714782715
Iteration 1361:
Training Loss: 3.0531134605407715
Reconstruction Loss: -1.6608697175979614
Iteration 1371:
Training Loss: 2.979609966278076
Reconstruction Loss: -1.6468945741653442
Iteration 1381:
Training Loss: 3.361741065979004
Reconstruction Loss: -1.6479755640029907
Iteration 1391:
Training Loss: 3.3089590072631836
Reconstruction Loss: -1.6514832973480225
Iteration 1401:
Training Loss: 3.213351011276245
Reconstruction Loss: -1.649349570274353
Iteration 1411:
Training Loss: 2.694350481033325
Reconstruction Loss: -1.6608617305755615
Iteration 1421:
Training Loss: 3.1367247104644775
Reconstruction Loss: -1.6591711044311523
Iteration 1431:
Training Loss: 3.2583680152893066
Reconstruction Loss: -1.6507506370544434
Iteration 1441:
Training Loss: 3.420182466506958
Reconstruction Loss: -1.6418087482452393
Iteration 1451:
Training Loss: 3.2298598289489746
Reconstruction Loss: -1.6412047147750854
Iteration 1461:
Training Loss: 3.007808208465576
Reconstruction Loss: -1.645825743675232
Iteration 1471:
Training Loss: 2.9573726654052734
Reconstruction Loss: -1.6350138187408447
Iteration 1481:
Training Loss: 2.7372963428497314
Reconstruction Loss: -1.6454130411148071
Iteration 1491:
Training Loss: 3.283440589904785
Reconstruction Loss: -1.6422207355499268
