5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.568121910095215
Reconstruction Loss: -0.42914479970932007
Iteration 11:
Training Loss: 5.639947414398193
Reconstruction Loss: -0.42953038215637207
Iteration 21:
Training Loss: 5.505487442016602
Reconstruction Loss: -0.43019983172416687
Iteration 31:
Training Loss: 5.125212669372559
Reconstruction Loss: -0.43233609199523926
Iteration 41:
Training Loss: 5.535575866699219
Reconstruction Loss: -0.462233304977417
Iteration 51:
Training Loss: 4.617308139801025
Reconstruction Loss: -0.7166126370429993
Iteration 61:
Training Loss: 4.14329195022583
Reconstruction Loss: -0.9883097410202026
Iteration 71:
Training Loss: 4.3039727210998535
Reconstruction Loss: -1.1286442279815674
Iteration 81:
Training Loss: 4.2555623054504395
Reconstruction Loss: -1.0946401357650757
Iteration 91:
Training Loss: 3.921851873397827
Reconstruction Loss: -1.1190818548202515
Iteration 101:
Training Loss: 3.6732969284057617
Reconstruction Loss: -1.117289662361145
Iteration 111:
Training Loss: 4.003864288330078
Reconstruction Loss: -1.159287452697754
Iteration 121:
Training Loss: 3.7065963745117188
Reconstruction Loss: -1.283667802810669
Iteration 131:
Training Loss: 3.417858362197876
Reconstruction Loss: -1.407365322113037
Iteration 141:
Training Loss: 3.4668421745300293
Reconstruction Loss: -1.485731840133667
Iteration 151:
Training Loss: 3.489658832550049
Reconstruction Loss: -1.503075361251831
Iteration 161:
Training Loss: 2.81619930267334
Reconstruction Loss: -1.5969034433364868
Iteration 171:
Training Loss: 1.6101051568984985
Reconstruction Loss: -2.1237294673919678
Iteration 181:
Training Loss: 1.4279967546463013
Reconstruction Loss: -2.9183692932128906
Iteration 191:
Training Loss: 0.41968947649002075
Reconstruction Loss: -3.6371970176696777
Iteration 201:
Training Loss: -0.5403783321380615
Reconstruction Loss: -4.327922821044922
Iteration 211:
Training Loss: -1.606671929359436
Reconstruction Loss: -4.975027561187744
Iteration 221:
Training Loss: -1.9475892782211304
Reconstruction Loss: -5.6149821281433105
Iteration 231:
Training Loss: -2.12734317779541
Reconstruction Loss: -6.218807220458984
Iteration 241:
Training Loss: -2.7318532466888428
Reconstruction Loss: -6.812272071838379
Iteration 251:
Training Loss: -3.250284433364868
Reconstruction Loss: -7.365793228149414
Iteration 261:
Training Loss: -4.434141159057617
Reconstruction Loss: -7.8863396644592285
Iteration 271:
Training Loss: -4.4410505294799805
Reconstruction Loss: -8.354636192321777
Iteration 281:
Training Loss: -5.310853481292725
Reconstruction Loss: -8.783663749694824
Iteration 291:
Training Loss: -5.467160701751709
Reconstruction Loss: -9.167386054992676
Iteration 301:
Training Loss: -5.831451416015625
Reconstruction Loss: -9.503565788269043
Iteration 311:
Training Loss: -6.007420063018799
Reconstruction Loss: -9.774064064025879
Iteration 321:
Training Loss: -6.04451847076416
Reconstruction Loss: -10.004749298095703
Iteration 331:
Training Loss: -6.0819783210754395
Reconstruction Loss: -10.19117546081543
Iteration 341:
Training Loss: -5.932559013366699
Reconstruction Loss: -10.337271690368652
Iteration 351:
Training Loss: -6.775424957275391
Reconstruction Loss: -10.461012840270996
Iteration 361:
Training Loss: -6.497127532958984
Reconstruction Loss: -10.548982620239258
Iteration 371:
Training Loss: -6.912091255187988
Reconstruction Loss: -10.636222839355469
Iteration 381:
Training Loss: -6.461574554443359
Reconstruction Loss: -10.695019721984863
Iteration 391:
Training Loss: -6.933578968048096
Reconstruction Loss: -10.74429702758789
Iteration 401:
Training Loss: -6.773630142211914
Reconstruction Loss: -10.787154197692871
Iteration 411:
Training Loss: -6.236559867858887
Reconstruction Loss: -10.803454399108887
Iteration 421:
Training Loss: -6.861952304840088
Reconstruction Loss: -10.833226203918457
Iteration 431:
Training Loss: -6.603093147277832
Reconstruction Loss: -10.855567932128906
Iteration 441:
Training Loss: -6.647710800170898
Reconstruction Loss: -10.878913879394531
Iteration 451:
Training Loss: -6.35461950302124
Reconstruction Loss: -10.888063430786133
Iteration 461:
Training Loss: -6.334131717681885
Reconstruction Loss: -10.92585277557373
Iteration 471:
Training Loss: -7.125776290893555
Reconstruction Loss: -10.921917915344238
Iteration 481:
Training Loss: -6.652141571044922
Reconstruction Loss: -10.94703197479248
Iteration 491:
Training Loss: -6.611401557922363
Reconstruction Loss: -10.937867164611816
Iteration 501:
Training Loss: -6.691770553588867
Reconstruction Loss: -10.953444480895996
Iteration 511:
Training Loss: -7.156446933746338
Reconstruction Loss: -10.967434883117676
Iteration 521:
Training Loss: -6.719925880432129
Reconstruction Loss: -10.968283653259277
Iteration 531:
Training Loss: -6.686297416687012
Reconstruction Loss: -10.984050750732422
Iteration 541:
Training Loss: -7.20131778717041
Reconstruction Loss: -10.975810050964355
Iteration 551:
Training Loss: -6.504095554351807
Reconstruction Loss: -10.980655670166016
Iteration 561:
Training Loss: -6.382963180541992
Reconstruction Loss: -10.992531776428223
Iteration 571:
Training Loss: -6.469616413116455
Reconstruction Loss: -11.003358840942383
Iteration 581:
Training Loss: -6.728618144989014
Reconstruction Loss: -11.023914337158203
Iteration 591:
Training Loss: -7.081457614898682
Reconstruction Loss: -11.014326095581055
Iteration 601:
Training Loss: -6.536442279815674
Reconstruction Loss: -11.012721061706543
Iteration 611:
Training Loss: -6.924355506896973
Reconstruction Loss: -11.016043663024902
Iteration 621:
Training Loss: -6.318267345428467
Reconstruction Loss: -11.00881576538086
Iteration 631:
Training Loss: -6.554476737976074
Reconstruction Loss: -11.026660919189453
Iteration 641:
Training Loss: -6.849612712860107
Reconstruction Loss: -11.034855842590332
Iteration 651:
Training Loss: -7.204300403594971
Reconstruction Loss: -11.042645454406738
Iteration 661:
Training Loss: -6.842331886291504
Reconstruction Loss: -11.0595064163208
Iteration 671:
Training Loss: -7.511720657348633
Reconstruction Loss: -11.05429744720459
Iteration 681:
Training Loss: -6.655172824859619
Reconstruction Loss: -11.059536933898926
Iteration 691:
Training Loss: -6.637585163116455
Reconstruction Loss: -11.065326690673828
Iteration 701:
Training Loss: -6.571995258331299
Reconstruction Loss: -11.07588005065918
Iteration 711:
Training Loss: -6.904118537902832
Reconstruction Loss: -11.083622932434082
Iteration 721:
Training Loss: -6.997341156005859
Reconstruction Loss: -11.08194637298584
Iteration 731:
Training Loss: -6.974131107330322
Reconstruction Loss: -11.089752197265625
Iteration 741:
Training Loss: -6.888555526733398
Reconstruction Loss: -11.093647003173828
Iteration 751:
Training Loss: -7.088428497314453
Reconstruction Loss: -11.09063720703125
Iteration 761:
Training Loss: -6.743393898010254
Reconstruction Loss: -11.091241836547852
Iteration 771:
Training Loss: -6.8966875076293945
Reconstruction Loss: -11.09961986541748
Iteration 781:
Training Loss: -7.336309909820557
Reconstruction Loss: -11.098666191101074
Iteration 791:
Training Loss: -6.464461326599121
Reconstruction Loss: -11.118986129760742
Iteration 801:
Training Loss: -6.706057548522949
Reconstruction Loss: -11.12870979309082
Iteration 811:
Training Loss: -6.908326625823975
Reconstruction Loss: -11.12098217010498
Iteration 821:
Training Loss: -6.803935527801514
Reconstruction Loss: -11.12106704711914
Iteration 831:
Training Loss: -7.145263195037842
Reconstruction Loss: -11.129022598266602
Iteration 841:
Training Loss: -6.980844020843506
Reconstruction Loss: -11.12555980682373
Iteration 851:
Training Loss: -7.0168633460998535
Reconstruction Loss: -11.137579917907715
Iteration 861:
Training Loss: -6.3315043449401855
Reconstruction Loss: -11.159762382507324
Iteration 871:
Training Loss: -6.422008037567139
Reconstruction Loss: -11.139321327209473
Iteration 881:
Training Loss: -7.1441521644592285
Reconstruction Loss: -11.161919593811035
Iteration 891:
Training Loss: -6.958255290985107
Reconstruction Loss: -11.165057182312012
Iteration 901:
Training Loss: -6.747502326965332
Reconstruction Loss: -11.167336463928223
Iteration 911:
Training Loss: -6.940501689910889
Reconstruction Loss: -11.163579940795898
Iteration 921:
Training Loss: -6.625340938568115
Reconstruction Loss: -11.187203407287598
Iteration 931:
Training Loss: -6.729400157928467
Reconstruction Loss: -11.168357849121094
Iteration 941:
Training Loss: -6.728689670562744
Reconstruction Loss: -11.178275108337402
Iteration 951:
Training Loss: -7.1232781410217285
Reconstruction Loss: -11.186257362365723
Iteration 961:
Training Loss: -7.406820297241211
Reconstruction Loss: -11.201676368713379
Iteration 971:
Training Loss: -6.718335151672363
Reconstruction Loss: -11.196198463439941
Iteration 981:
Training Loss: -6.995926856994629
Reconstruction Loss: -11.203194618225098
Iteration 991:
Training Loss: -6.729277610778809
Reconstruction Loss: -11.208163261413574
Iteration 1001:
Training Loss: -6.730669975280762
Reconstruction Loss: -11.222085952758789
Iteration 1011:
Training Loss: -6.8125457763671875
Reconstruction Loss: -11.238042831420898
Iteration 1021:
Training Loss: -6.892367362976074
Reconstruction Loss: -11.22900390625
Iteration 1031:
Training Loss: -6.951837539672852
Reconstruction Loss: -11.227377891540527
Iteration 1041:
Training Loss: -6.468317985534668
Reconstruction Loss: -11.23381519317627
Iteration 1051:
Training Loss: -7.234219551086426
Reconstruction Loss: -11.245491981506348
Iteration 1061:
Training Loss: -6.87414026260376
Reconstruction Loss: -11.237703323364258
Iteration 1071:
Training Loss: -7.27770471572876
Reconstruction Loss: -11.244084358215332
Iteration 1081:
Training Loss: -7.203007698059082
Reconstruction Loss: -11.248347282409668
Iteration 1091:
Training Loss: -6.97853946685791
Reconstruction Loss: -11.267284393310547
Iteration 1101:
Training Loss: -7.033759117126465
Reconstruction Loss: -11.275810241699219
Iteration 1111:
Training Loss: -7.119418621063232
Reconstruction Loss: -11.26854133605957
Iteration 1121:
Training Loss: -6.445115089416504
Reconstruction Loss: -11.260735511779785
Iteration 1131:
Training Loss: -7.585174083709717
Reconstruction Loss: -11.277828216552734
Iteration 1141:
Training Loss: -6.944948196411133
Reconstruction Loss: -11.280060768127441
Iteration 1151:
Training Loss: -7.039553642272949
Reconstruction Loss: -11.270866394042969
Iteration 1161:
Training Loss: -6.751707553863525
Reconstruction Loss: -11.283487319946289
Iteration 1171:
Training Loss: -6.848981857299805
Reconstruction Loss: -11.289800643920898
Iteration 1181:
Training Loss: -7.396300792694092
Reconstruction Loss: -11.286661148071289
Iteration 1191:
Training Loss: -6.810351848602295
Reconstruction Loss: -11.306382179260254
Iteration 1201:
Training Loss: -7.083630084991455
Reconstruction Loss: -11.309768676757812
Iteration 1211:
Training Loss: -7.383687973022461
Reconstruction Loss: -11.309793472290039
Iteration 1221:
Training Loss: -7.372910022735596
Reconstruction Loss: -11.309473991394043
Iteration 1231:
Training Loss: -6.825835227966309
Reconstruction Loss: -11.310157775878906
Iteration 1241:
Training Loss: -7.040604591369629
Reconstruction Loss: -11.30968952178955
Iteration 1251:
Training Loss: -6.7736968994140625
Reconstruction Loss: -11.308911323547363
Iteration 1261:
Training Loss: -7.065403938293457
Reconstruction Loss: -11.328093528747559
Iteration 1271:
Training Loss: -6.728106498718262
Reconstruction Loss: -11.316831588745117
Iteration 1281:
Training Loss: -7.078286647796631
Reconstruction Loss: -11.329832077026367
Iteration 1291:
Training Loss: -7.398642063140869
Reconstruction Loss: -11.348536491394043
Iteration 1301:
Training Loss: -7.228880882263184
Reconstruction Loss: -11.344654083251953
Iteration 1311:
Training Loss: -6.8692708015441895
Reconstruction Loss: -11.350906372070312
Iteration 1321:
Training Loss: -6.955387115478516
Reconstruction Loss: -11.347002983093262
Iteration 1331:
Training Loss: -6.800774574279785
Reconstruction Loss: -11.343024253845215
Iteration 1341:
Training Loss: -7.15369176864624
Reconstruction Loss: -11.355022430419922
Iteration 1351:
Training Loss: -6.777194499969482
Reconstruction Loss: -11.36133861541748
Iteration 1361:
Training Loss: -6.933287143707275
Reconstruction Loss: -11.358916282653809
Iteration 1371:
Training Loss: -7.639162063598633
Reconstruction Loss: -11.36681842803955
Iteration 1381:
Training Loss: -6.687010288238525
Reconstruction Loss: -11.373320579528809
Iteration 1391:
Training Loss: -7.209737777709961
Reconstruction Loss: -11.378694534301758
Iteration 1401:
Training Loss: -7.049690246582031
Reconstruction Loss: -11.383822441101074
Iteration 1411:
Training Loss: -7.094404220581055
Reconstruction Loss: -11.388258934020996
Iteration 1421:
Training Loss: -7.492887020111084
Reconstruction Loss: -11.391528129577637
Iteration 1431:
Training Loss: -7.496712684631348
Reconstruction Loss: -11.39248275756836
Iteration 1441:
Training Loss: -7.458981513977051
Reconstruction Loss: -11.390793800354004
Iteration 1451:
Training Loss: -7.081124782562256
Reconstruction Loss: -11.412220001220703
Iteration 1461:
Training Loss: -6.723579406738281
Reconstruction Loss: -11.397990226745605
Iteration 1471:
Training Loss: -7.020882606506348
Reconstruction Loss: -11.40920352935791
Iteration 1481:
Training Loss: -7.072309494018555
Reconstruction Loss: -11.412399291992188
Iteration 1491:
Training Loss: -7.025440216064453
Reconstruction Loss: -11.41466236114502
