5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.810824394226074
Reconstruction Loss: -0.3733793795108795
Iteration 11:
Training Loss: 5.405091285705566
Reconstruction Loss: -0.3733793795108795
Iteration 21:
Training Loss: 6.0978779792785645
Reconstruction Loss: -0.3733793795108795
Iteration 31:
Training Loss: 5.886767387390137
Reconstruction Loss: -0.3733793795108795
Iteration 41:
Training Loss: 5.65999698638916
Reconstruction Loss: -0.3733793795108795
Iteration 51:
Training Loss: 5.586787223815918
Reconstruction Loss: -0.3733793795108795
Iteration 61:
Training Loss: 5.668875694274902
Reconstruction Loss: -0.3733793795108795
Iteration 71:
Training Loss: 5.217580318450928
Reconstruction Loss: -0.37337952852249146
Iteration 81:
Training Loss: 5.708475589752197
Reconstruction Loss: -0.37337952852249146
Iteration 91:
Training Loss: 5.5471062660217285
Reconstruction Loss: -0.373379647731781
Iteration 101:
Training Loss: 5.0636820793151855
Reconstruction Loss: -0.373379647731781
Iteration 111:
Training Loss: 5.637812614440918
Reconstruction Loss: -0.373379647731781
Iteration 121:
Training Loss: 5.618457794189453
Reconstruction Loss: -0.373379647731781
Iteration 131:
Training Loss: 5.583622455596924
Reconstruction Loss: -0.373379647731781
Iteration 141:
Training Loss: 5.581271648406982
Reconstruction Loss: -0.373379647731781
Iteration 151:
Training Loss: 5.501834869384766
Reconstruction Loss: -0.373379647731781
Iteration 161:
Training Loss: 5.2986650466918945
Reconstruction Loss: -0.373379647731781
Iteration 171:
Training Loss: 5.780378818511963
Reconstruction Loss: -0.373379647731781
Iteration 181:
Training Loss: 5.739526271820068
Reconstruction Loss: -0.373379647731781
Iteration 191:
Training Loss: 6.096398830413818
Reconstruction Loss: -0.3733797073364258
Iteration 201:
Training Loss: 5.522894382476807
Reconstruction Loss: -0.3733797073364258
Iteration 211:
Training Loss: 6.025333881378174
Reconstruction Loss: -0.3733797073364258
Iteration 221:
Training Loss: 6.12365198135376
Reconstruction Loss: -0.3733797073364258
Iteration 231:
Training Loss: 5.230678558349609
Reconstruction Loss: -0.3733797073364258
Iteration 241:
Training Loss: 5.5023112297058105
Reconstruction Loss: -0.3733797073364258
Iteration 251:
Training Loss: 4.661108016967773
Reconstruction Loss: -0.3733797073364258
Iteration 261:
Training Loss: 5.630814075469971
Reconstruction Loss: -0.37337979674339294
Iteration 271:
Training Loss: 5.648231506347656
Reconstruction Loss: -0.37337979674339294
Iteration 281:
Training Loss: 5.527896881103516
Reconstruction Loss: -0.37337979674339294
Iteration 291:
Training Loss: 6.115118980407715
Reconstruction Loss: -0.37337997555732727
Iteration 301:
Training Loss: 5.809856414794922
Reconstruction Loss: -0.37337997555732727
Iteration 311:
Training Loss: 5.7859578132629395
Reconstruction Loss: -0.37337997555732727
Iteration 321:
Training Loss: 5.723262310028076
Reconstruction Loss: -0.37337997555732727
Iteration 331:
Training Loss: 5.480191230773926
Reconstruction Loss: -0.37337997555732727
Iteration 341:
Training Loss: 5.744749069213867
Reconstruction Loss: -0.37337997555732727
Iteration 351:
Training Loss: 5.726712226867676
Reconstruction Loss: -0.37337997555732727
Iteration 361:
Training Loss: 5.454474925994873
Reconstruction Loss: -0.37338006496429443
Iteration 371:
Training Loss: 5.042870044708252
Reconstruction Loss: -0.37338006496429443
Iteration 381:
Training Loss: 5.789583683013916
Reconstruction Loss: -0.3733801543712616
Iteration 391:
Training Loss: 5.605997562408447
Reconstruction Loss: -0.3733801543712616
Iteration 401:
Training Loss: 5.879879951477051
Reconstruction Loss: -0.3733801543712616
Iteration 411:
Training Loss: 5.545706748962402
Reconstruction Loss: -0.3733801543712616
Iteration 421:
Training Loss: 5.79774808883667
Reconstruction Loss: -0.3733801543712616
Iteration 431:
Training Loss: 5.636075496673584
Reconstruction Loss: -0.3733801543712616
Iteration 441:
Training Loss: 5.447086334228516
Reconstruction Loss: -0.3733801543712616
Iteration 451:
Training Loss: 5.876246929168701
Reconstruction Loss: -0.3733804225921631
Iteration 461:
Training Loss: 5.484560012817383
Reconstruction Loss: -0.3733804225921631
Iteration 471:
Training Loss: 5.469427585601807
Reconstruction Loss: -0.37338048219680786
Iteration 481:
Training Loss: 5.677302360534668
Reconstruction Loss: -0.37338048219680786
Iteration 491:
Training Loss: 5.700107097625732
Reconstruction Loss: -0.37338048219680786
Iteration 501:
Training Loss: 5.597762584686279
Reconstruction Loss: -0.37338048219680786
Iteration 511:
Training Loss: 5.535761833190918
Reconstruction Loss: -0.3733806014060974
Iteration 521:
Training Loss: 5.358400821685791
Reconstruction Loss: -0.3733806014060974
Iteration 531:
Training Loss: 5.516926288604736
Reconstruction Loss: -0.37338075041770935
Iteration 541:
Training Loss: 5.898471832275391
Reconstruction Loss: -0.37338075041770935
Iteration 551:
Training Loss: 5.440153121948242
Reconstruction Loss: -0.3733808398246765
Iteration 561:
Training Loss: 5.620822429656982
Reconstruction Loss: -0.3733808398246765
Iteration 571:
Training Loss: 5.2866106033325195
Reconstruction Loss: -0.3733809292316437
Iteration 581:
Training Loss: 5.096923828125
Reconstruction Loss: -0.37338101863861084
Iteration 591:
Training Loss: 6.048315048217773
Reconstruction Loss: -0.37338101863861084
Iteration 601:
Training Loss: 5.369386196136475
Reconstruction Loss: -0.37338119745254517
Iteration 611:
Training Loss: 5.4529128074646
Reconstruction Loss: -0.37338119745254517
Iteration 621:
Training Loss: 5.668638229370117
Reconstruction Loss: -0.3733813762664795
Iteration 631:
Training Loss: 5.389667987823486
Reconstruction Loss: -0.3733813762664795
Iteration 641:
Training Loss: 5.450398921966553
Reconstruction Loss: -0.37338152527809143
Iteration 651:
Training Loss: 5.831151485443115
Reconstruction Loss: -0.3733816146850586
Iteration 661:
Training Loss: 5.264528751373291
Reconstruction Loss: -0.37338170409202576
Iteration 671:
Training Loss: 5.711750030517578
Reconstruction Loss: -0.3733817934989929
Iteration 681:
Training Loss: 5.814639091491699
Reconstruction Loss: -0.3733820617198944
Iteration 691:
Training Loss: 5.220481872558594
Reconstruction Loss: -0.3733821511268616
Iteration 701:
Training Loss: 5.7943267822265625
Reconstruction Loss: -0.3733821511268616
Iteration 711:
Training Loss: 5.685345649719238
Reconstruction Loss: -0.3733823895454407
Iteration 721:
Training Loss: 5.593321323394775
Reconstruction Loss: -0.37338247895240784
Iteration 731:
Training Loss: 5.268981456756592
Reconstruction Loss: -0.3733828365802765
Iteration 741:
Training Loss: 5.545247554779053
Reconstruction Loss: -0.373383104801178
Iteration 751:
Training Loss: 5.602497577667236
Reconstruction Loss: -0.37338316440582275
Iteration 761:
Training Loss: 5.510513782501221
Reconstruction Loss: -0.3733835220336914
Iteration 771:
Training Loss: 5.742514610290527
Reconstruction Loss: -0.37338370084762573
Iteration 781:
Training Loss: 5.295854568481445
Reconstruction Loss: -0.3733840584754944
Iteration 791:
Training Loss: 5.66676664352417
Reconstruction Loss: -0.37338438630104065
Iteration 801:
Training Loss: 5.954792022705078
Reconstruction Loss: -0.3733847439289093
Iteration 811:
Training Loss: 5.754082202911377
Reconstruction Loss: -0.37338516116142273
Iteration 821:
Training Loss: 5.400971412658691
Reconstruction Loss: -0.3733856976032257
Iteration 831:
Training Loss: 5.705770492553711
Reconstruction Loss: -0.37338629364967346
Iteration 841:
Training Loss: 5.958845615386963
Reconstruction Loss: -0.37338680028915405
Iteration 851:
Training Loss: 6.127269268035889
Reconstruction Loss: -0.3733876943588257
Iteration 861:
Training Loss: 5.5762939453125
Reconstruction Loss: -0.3733886480331421
Iteration 871:
Training Loss: 5.521729469299316
Reconstruction Loss: -0.37338966131210327
Iteration 881:
Training Loss: 5.624086856842041
Reconstruction Loss: -0.37339070439338684
Iteration 891:
Training Loss: 5.336296081542969
Reconstruction Loss: -0.3733922839164734
Iteration 901:
Training Loss: 5.281472206115723
Reconstruction Loss: -0.3733941912651062
Iteration 911:
Training Loss: 5.634707450866699
Reconstruction Loss: -0.3733963370323181
Iteration 921:
Training Loss: 5.813124179840088
Reconstruction Loss: -0.37339910864830017
Iteration 931:
Training Loss: 5.178077220916748
Reconstruction Loss: -0.3734029233455658
Iteration 941:
Training Loss: 5.269191265106201
Reconstruction Loss: -0.373407781124115
Iteration 951:
Training Loss: 5.231255054473877
Reconstruction Loss: -0.3734145164489746
Iteration 961:
Training Loss: 5.833683490753174
Reconstruction Loss: -0.37342405319213867
Iteration 971:
Training Loss: 5.316630840301514
Reconstruction Loss: -0.3734385073184967
Iteration 981:
Training Loss: 5.5886101722717285
Reconstruction Loss: -0.37346163392066956
Iteration 991:
Training Loss: 5.3042731285095215
Reconstruction Loss: -0.37350231409072876
Iteration 1001:
Training Loss: 5.175439834594727
Reconstruction Loss: -0.37358468770980835
Iteration 1011:
Training Loss: 5.820498466491699
Reconstruction Loss: -0.37378934025764465
Iteration 1021:
Training Loss: 5.293613433837891
Reconstruction Loss: -0.3745964467525482
Iteration 1031:
Training Loss: 5.887794494628906
Reconstruction Loss: -0.38769179582595825
Iteration 1041:
Training Loss: 5.646615028381348
Reconstruction Loss: -0.5559611320495605
Iteration 1051:
Training Loss: 4.938914775848389
Reconstruction Loss: -0.5387156009674072
Iteration 1061:
Training Loss: 5.453958511352539
Reconstruction Loss: -0.5366159677505493
Iteration 1071:
Training Loss: 5.044277191162109
Reconstruction Loss: -0.5541281700134277
Iteration 1081:
Training Loss: 5.766312122344971
Reconstruction Loss: -0.4790297746658325
Iteration 1091:
Training Loss: 5.073128700256348
Reconstruction Loss: -0.5139869451522827
Iteration 1101:
Training Loss: 4.581552028656006
Reconstruction Loss: -0.4937779903411865
Iteration 1111:
Training Loss: 5.091108798980713
Reconstruction Loss: -0.4665073752403259
Iteration 1121:
Training Loss: 5.265526294708252
Reconstruction Loss: -0.5284628868103027
Iteration 1131:
Training Loss: 5.416833400726318
Reconstruction Loss: -0.542228102684021
Iteration 1141:
Training Loss: 5.200908184051514
Reconstruction Loss: -0.5261446237564087
Iteration 1151:
Training Loss: 5.495682716369629
Reconstruction Loss: -0.542134702205658
Iteration 1161:
Training Loss: 5.345150470733643
Reconstruction Loss: -0.518186092376709
Iteration 1171:
Training Loss: 5.031448841094971
Reconstruction Loss: -0.5290387272834778
Iteration 1181:
Training Loss: 5.037285804748535
Reconstruction Loss: -0.47035759687423706
Iteration 1191:
Training Loss: 5.239400386810303
Reconstruction Loss: -0.538501501083374
Iteration 1201:
Training Loss: 5.325808525085449
Reconstruction Loss: -0.506421685218811
Iteration 1211:
Training Loss: 5.3124680519104
Reconstruction Loss: -0.48393991589546204
Iteration 1221:
Training Loss: 5.602599143981934
Reconstruction Loss: -0.5225777626037598
Iteration 1231:
Training Loss: 5.267172813415527
Reconstruction Loss: -0.5598264932632446
Iteration 1241:
Training Loss: 5.608795166015625
Reconstruction Loss: -0.49225443601608276
Iteration 1251:
Training Loss: 5.133143424987793
Reconstruction Loss: -0.5462328195571899
Iteration 1261:
Training Loss: 5.33378267288208
Reconstruction Loss: -0.4983777701854706
Iteration 1271:
Training Loss: 5.071348667144775
Reconstruction Loss: -0.5120201110839844
Iteration 1281:
Training Loss: 5.368857383728027
Reconstruction Loss: -0.5548412799835205
Iteration 1291:
Training Loss: 5.387266159057617
Reconstruction Loss: -0.4868450164794922
Iteration 1301:
Training Loss: 5.066347599029541
Reconstruction Loss: -0.5083121061325073
Iteration 1311:
Training Loss: 5.539344310760498
Reconstruction Loss: -0.5598686933517456
Iteration 1321:
Training Loss: 5.4837422370910645
Reconstruction Loss: -0.4543112814426422
Iteration 1331:
Training Loss: 4.961603164672852
Reconstruction Loss: -0.5532019138336182
Iteration 1341:
Training Loss: 5.28659725189209
Reconstruction Loss: -0.5587483644485474
Iteration 1351:
Training Loss: 5.316792964935303
Reconstruction Loss: -0.426379919052124
Iteration 1361:
Training Loss: 5.40423583984375
Reconstruction Loss: -0.5632688999176025
Iteration 1371:
Training Loss: 4.89056921005249
Reconstruction Loss: -0.5288220643997192
Iteration 1381:
Training Loss: 4.7945356369018555
Reconstruction Loss: -0.5352159738540649
Iteration 1391:
Training Loss: 5.2578606605529785
Reconstruction Loss: -0.5155950784683228
Iteration 1401:
Training Loss: 5.281460285186768
Reconstruction Loss: -0.5450088381767273
Iteration 1411:
Training Loss: 5.168396949768066
Reconstruction Loss: -0.5224224925041199
Iteration 1421:
Training Loss: 5.259822845458984
Reconstruction Loss: -0.5311052799224854
Iteration 1431:
Training Loss: 4.796896934509277
Reconstruction Loss: -0.5554525852203369
Iteration 1441:
Training Loss: 5.272960186004639
Reconstruction Loss: -0.5334007143974304
Iteration 1451:
Training Loss: 5.176865577697754
Reconstruction Loss: -0.5550045967102051
Iteration 1461:
Training Loss: 5.157844543457031
Reconstruction Loss: -0.5044949054718018
Iteration 1471:
Training Loss: 5.191300868988037
Reconstruction Loss: -0.5232231020927429
Iteration 1481:
Training Loss: 5.445227146148682
Reconstruction Loss: -0.516765832901001
Iteration 1491:
Training Loss: 5.233146667480469
Reconstruction Loss: -0.541128933429718
