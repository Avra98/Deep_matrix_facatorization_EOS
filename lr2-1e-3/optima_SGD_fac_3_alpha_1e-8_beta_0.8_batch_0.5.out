5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.6662373542785645
Reconstruction Loss: -0.3424018621444702
Iteration 51:
Training Loss: 5.6075544357299805
Reconstruction Loss: -0.3424019515514374
Iteration 101:
Training Loss: 5.686835765838623
Reconstruction Loss: -0.3424019515514374
Iteration 151:
Training Loss: 5.706573963165283
Reconstruction Loss: -0.3424019515514374
Iteration 201:
Training Loss: 5.661618709564209
Reconstruction Loss: -0.3424019515514374
Iteration 251:
Training Loss: 5.635473728179932
Reconstruction Loss: -0.3424019515514374
Iteration 301:
Training Loss: 5.663613319396973
Reconstruction Loss: -0.3424019515514374
Iteration 351:
Training Loss: 5.567811012268066
Reconstruction Loss: -0.3424019515514374
Iteration 401:
Training Loss: 5.757561683654785
Reconstruction Loss: -0.3424019515514374
Iteration 451:
Training Loss: 5.7008819580078125
Reconstruction Loss: -0.3424019515514374
Iteration 501:
Training Loss: 5.652990818023682
Reconstruction Loss: -0.3424019515514374
Iteration 551:
Training Loss: 5.614962100982666
Reconstruction Loss: -0.3424019515514374
Iteration 601:
Training Loss: 5.671782493591309
Reconstruction Loss: -0.3424019515514374
Iteration 651:
Training Loss: 5.651459693908691
Reconstruction Loss: -0.3424021005630493
Iteration 701:
Training Loss: 5.708188533782959
Reconstruction Loss: -0.3424021005630493
Iteration 751:
Training Loss: 5.608720779418945
Reconstruction Loss: -0.3424021005630493
Iteration 801:
Training Loss: 5.672458648681641
Reconstruction Loss: -0.3424021005630493
Iteration 851:
Training Loss: 5.606611728668213
Reconstruction Loss: -0.3424021005630493
Iteration 901:
Training Loss: 5.7603960037231445
Reconstruction Loss: -0.3424021005630493
Iteration 951:
Training Loss: 5.678358554840088
Reconstruction Loss: -0.3424021005630493
Iteration 1001:
Training Loss: 5.646564483642578
Reconstruction Loss: -0.3424021899700165
Iteration 1051:
Training Loss: 5.572214126586914
Reconstruction Loss: -0.3424021899700165
Iteration 1101:
Training Loss: 5.5896172523498535
Reconstruction Loss: -0.3424021899700165
Iteration 1151:
Training Loss: 5.623617172241211
Reconstruction Loss: -0.34240227937698364
Iteration 1201:
Training Loss: 5.796205520629883
Reconstruction Loss: -0.34240227937698364
Iteration 1251:
Training Loss: 5.568965911865234
Reconstruction Loss: -0.34240227937698364
Iteration 1301:
Training Loss: 5.626763820648193
Reconstruction Loss: -0.34240227937698364
Iteration 1351:
Training Loss: 5.717507362365723
Reconstruction Loss: -0.34240245819091797
Iteration 1401:
Training Loss: 5.611100673675537
Reconstruction Loss: -0.34240251779556274
Iteration 1451:
Training Loss: 5.633141040802002
Reconstruction Loss: -0.34240251779556274
Iteration 1501:
Training Loss: 5.452207565307617
Reconstruction Loss: -0.34240251779556274
Iteration 1551:
Training Loss: 5.612770080566406
Reconstruction Loss: -0.3424026072025299
Iteration 1601:
Training Loss: 5.598271369934082
Reconstruction Loss: -0.3424026072025299
Iteration 1651:
Training Loss: 5.632643699645996
Reconstruction Loss: -0.34240269660949707
Iteration 1701:
Training Loss: 5.577432155609131
Reconstruction Loss: -0.3424028754234314
Iteration 1751:
Training Loss: 5.624484539031982
Reconstruction Loss: -0.34240296483039856
Iteration 1801:
Training Loss: 5.729728698730469
Reconstruction Loss: -0.34240320324897766
Iteration 1851:
Training Loss: 5.7623515129089355
Reconstruction Loss: -0.3424032926559448
Iteration 1901:
Training Loss: 5.669880390167236
Reconstruction Loss: -0.3424036204814911
Iteration 1951:
Training Loss: 5.6998162269592285
Reconstruction Loss: -0.34240394830703735
Iteration 2001:
Training Loss: 5.740099906921387
Reconstruction Loss: -0.34240421652793884
Iteration 2051:
Training Loss: 5.631401062011719
Reconstruction Loss: -0.34240463376045227
Iteration 2101:
Training Loss: 5.587225914001465
Reconstruction Loss: -0.3424052894115448
Iteration 2151:
Training Loss: 5.479318141937256
Reconstruction Loss: -0.34240639209747314
Iteration 2201:
Training Loss: 5.720688819885254
Reconstruction Loss: -0.3424076437950134
Iteration 2251:
Training Loss: 5.681114673614502
Reconstruction Loss: -0.3424094021320343
Iteration 2301:
Training Loss: 5.66533899307251
Reconstruction Loss: -0.34241244196891785
Iteration 2351:
Training Loss: 5.704078674316406
Reconstruction Loss: -0.3424172103404999
Iteration 2401:
Training Loss: 5.660758018493652
Reconstruction Loss: -0.3424259424209595
Iteration 2451:
Training Loss: 5.587367057800293
Reconstruction Loss: -0.34244322776794434
Iteration 2501:
Training Loss: 5.754889011383057
Reconstruction Loss: -0.3424840271472931
Iteration 2551:
Training Loss: 5.705520153045654
Reconstruction Loss: -0.3426085412502289
Iteration 2601:
Training Loss: 5.728178977966309
Reconstruction Loss: -0.3432202935218811
Iteration 2651:
Training Loss: 5.576876163482666
Reconstruction Loss: -0.3541623055934906
Iteration 2701:
Training Loss: 4.8346943855285645
Reconstruction Loss: -0.5512505769729614
Iteration 2751:
Training Loss: 5.002035617828369
Reconstruction Loss: -0.5205391645431519
Iteration 2801:
Training Loss: 5.074514865875244
Reconstruction Loss: -0.517671525478363
Iteration 2851:
Training Loss: 5.015979766845703
Reconstruction Loss: -0.5149651765823364
Iteration 2901:
Training Loss: 4.928393840789795
Reconstruction Loss: -0.5130412578582764
Iteration 2951:
Training Loss: 5.057268142700195
Reconstruction Loss: -0.5159252285957336
Iteration 3001:
Training Loss: 4.94256067276001
Reconstruction Loss: -0.5179522037506104
Iteration 3051:
Training Loss: 5.040878772735596
Reconstruction Loss: -0.5218273997306824
Iteration 3101:
Training Loss: 4.8618998527526855
Reconstruction Loss: -0.5129013061523438
Iteration 3151:
Training Loss: 4.993980884552002
Reconstruction Loss: -0.5189117789268494
Iteration 3201:
Training Loss: 5.025094985961914
Reconstruction Loss: -0.5271855592727661
Iteration 3251:
Training Loss: 4.851407051086426
Reconstruction Loss: -0.523292064666748
Iteration 3301:
Training Loss: 4.962669849395752
Reconstruction Loss: -0.526372492313385
Iteration 3351:
Training Loss: 4.379555702209473
Reconstruction Loss: -0.7420495748519897
Iteration 3401:
Training Loss: 4.38394021987915
Reconstruction Loss: -0.7488011121749878
Iteration 3451:
Training Loss: 4.343442916870117
Reconstruction Loss: -0.7440106868743896
Iteration 3501:
Training Loss: 4.225588798522949
Reconstruction Loss: -0.7216112017631531
Iteration 3551:
Training Loss: 4.453912258148193
Reconstruction Loss: -0.724232017993927
Iteration 3601:
Training Loss: 4.431341648101807
Reconstruction Loss: -0.7263283133506775
Iteration 3651:
Training Loss: 4.373709678649902
Reconstruction Loss: -0.7239004373550415
Iteration 3701:
Training Loss: 4.28552770614624
Reconstruction Loss: -0.7271291017532349
Iteration 3751:
Training Loss: 4.300448417663574
Reconstruction Loss: -0.7228100299835205
Iteration 3801:
Training Loss: 4.3186421394348145
Reconstruction Loss: -0.7229589223861694
Iteration 3851:
Training Loss: 4.39180850982666
Reconstruction Loss: -0.7197809219360352
Iteration 3901:
Training Loss: 4.224343299865723
Reconstruction Loss: -0.722381055355072
Iteration 3951:
Training Loss: 4.382278919219971
Reconstruction Loss: -0.7223031520843506
Iteration 4001:
Training Loss: 4.347503185272217
Reconstruction Loss: -0.7236204743385315
Iteration 4051:
Training Loss: 4.367586135864258
Reconstruction Loss: -0.7239906191825867
Iteration 4101:
Training Loss: 4.320977210998535
Reconstruction Loss: -0.7298310995101929
Iteration 4151:
Training Loss: 4.279407024383545
Reconstruction Loss: -0.7328042387962341
Iteration 4201:
Training Loss: 4.454387664794922
Reconstruction Loss: -0.7256888151168823
Iteration 4251:
Training Loss: 4.3464226722717285
Reconstruction Loss: -0.7264324426651001
Iteration 4301:
Training Loss: 4.309120178222656
Reconstruction Loss: -0.7315475344657898
Iteration 4351:
Training Loss: 4.384366512298584
Reconstruction Loss: -0.7465648651123047
Iteration 4401:
Training Loss: 4.072356224060059
Reconstruction Loss: -0.8891019821166992
Iteration 4451:
Training Loss: 3.4639663696289062
Reconstruction Loss: -1.0226166248321533
Iteration 4501:
Training Loss: 3.6021406650543213
Reconstruction Loss: -1.0165385007858276
Iteration 4551:
Training Loss: 3.6535210609436035
Reconstruction Loss: -1.0110725164413452
Iteration 4601:
Training Loss: 3.6870579719543457
Reconstruction Loss: -1.0070912837982178
Iteration 4651:
Training Loss: 3.6452276706695557
Reconstruction Loss: -1.0040733814239502
Iteration 4701:
Training Loss: 3.687328815460205
Reconstruction Loss: -0.9999024868011475
Iteration 4751:
Training Loss: 3.582015037536621
Reconstruction Loss: -1.0000548362731934
Iteration 4801:
Training Loss: 3.51649808883667
Reconstruction Loss: -0.9968113899230957
Iteration 4851:
Training Loss: 3.4897446632385254
Reconstruction Loss: -0.9931581616401672
Iteration 4901:
Training Loss: 3.571220874786377
Reconstruction Loss: -0.9934071898460388
Iteration 4951:
Training Loss: 3.492875576019287
Reconstruction Loss: -0.9910973310470581
Iteration 5001:
Training Loss: 3.652547836303711
Reconstruction Loss: -0.9854797720909119
Iteration 5051:
Training Loss: 3.6711177825927734
Reconstruction Loss: -0.985744833946228
Iteration 5101:
Training Loss: 3.711280584335327
Reconstruction Loss: -0.9854881763458252
Iteration 5151:
Training Loss: 3.4970550537109375
Reconstruction Loss: -0.9836201667785645
Iteration 5201:
Training Loss: 3.543790102005005
Reconstruction Loss: -0.9860279560089111
Iteration 5251:
Training Loss: 3.4587771892547607
Reconstruction Loss: -0.9787623286247253
Iteration 5301:
Training Loss: 3.562732696533203
Reconstruction Loss: -0.9855162501335144
Iteration 5351:
Training Loss: 3.5496485233306885
Reconstruction Loss: -0.9885874390602112
Iteration 5401:
Training Loss: 3.5589730739593506
Reconstruction Loss: -0.9857140779495239
Iteration 5451:
Training Loss: 3.5031399726867676
Reconstruction Loss: -0.9864281415939331
Iteration 5501:
Training Loss: 3.6367273330688477
Reconstruction Loss: -0.9813844561576843
Iteration 5551:
Training Loss: 3.4975056648254395
Reconstruction Loss: -0.9817082285881042
Iteration 5601:
Training Loss: 3.710592269897461
Reconstruction Loss: -0.985553503036499
Iteration 5651:
Training Loss: 3.550469160079956
Reconstruction Loss: -0.9790409803390503
Iteration 5701:
Training Loss: 3.489318370819092
Reconstruction Loss: -0.9804794788360596
Iteration 5751:
Training Loss: 3.571087121963501
Reconstruction Loss: -0.9809774160385132
Iteration 5801:
Training Loss: 3.5904526710510254
Reconstruction Loss: -0.9825814962387085
Iteration 5851:
Training Loss: 3.3651113510131836
Reconstruction Loss: -0.9821889996528625
Iteration 5901:
Training Loss: 3.5271799564361572
Reconstruction Loss: -0.9830708503723145
Iteration 5951:
Training Loss: 3.5574755668640137
Reconstruction Loss: -0.9794190526008606
Iteration 6001:
Training Loss: 3.6245474815368652
Reconstruction Loss: -0.98646080493927
Iteration 6051:
Training Loss: 3.547804832458496
Reconstruction Loss: -0.9802691340446472
Iteration 6101:
Training Loss: 3.599280834197998
Reconstruction Loss: -0.9832203388214111
Iteration 6151:
Training Loss: 3.5879619121551514
Reconstruction Loss: -0.9822732210159302
Iteration 6201:
Training Loss: 3.4940366744995117
Reconstruction Loss: -0.9839471578598022
Iteration 6251:
Training Loss: 3.4276516437530518
Reconstruction Loss: -0.981937050819397
Iteration 6301:
Training Loss: 3.7370285987854004
Reconstruction Loss: -0.9822250604629517
Iteration 6351:
Training Loss: 3.593773365020752
Reconstruction Loss: -0.9873912334442139
Iteration 6401:
Training Loss: 3.4630956649780273
Reconstruction Loss: -0.9875595569610596
Iteration 6451:
Training Loss: 3.460904121398926
Reconstruction Loss: -0.9880670309066772
Iteration 6501:
Training Loss: 3.3649346828460693
Reconstruction Loss: -0.9830982685089111
Iteration 6551:
Training Loss: 3.567577838897705
Reconstruction Loss: -0.9883060455322266
Iteration 6601:
Training Loss: 3.5122106075286865
Reconstruction Loss: -0.9914532899856567
Iteration 6651:
Training Loss: 3.544420003890991
Reconstruction Loss: -0.997437059879303
Iteration 6701:
Training Loss: 3.4757049083709717
Reconstruction Loss: -1.0317792892456055
Iteration 6751:
Training Loss: 3.135883092880249
Reconstruction Loss: -1.1562421321868896
Iteration 6801:
Training Loss: 2.9459621906280518
Reconstruction Loss: -1.3575739860534668
Iteration 6851:
Training Loss: 3.0621657371520996
Reconstruction Loss: -1.475616216659546
Iteration 6901:
Training Loss: 2.9035704135894775
Reconstruction Loss: -1.5511868000030518
Iteration 6951:
Training Loss: 2.9006235599517822
Reconstruction Loss: -1.6076456308364868
Iteration 7001:
Training Loss: 2.8250088691711426
Reconstruction Loss: -1.6486763954162598
Iteration 7051:
Training Loss: 2.8885018825531006
Reconstruction Loss: -1.6825202703475952
Iteration 7101:
Training Loss: 2.8906731605529785
Reconstruction Loss: -1.7080495357513428
Iteration 7151:
Training Loss: 2.9815332889556885
Reconstruction Loss: -1.7229748964309692
Iteration 7201:
Training Loss: 2.794840097427368
Reconstruction Loss: -1.7396907806396484
Iteration 7251:
Training Loss: 2.8764166831970215
Reconstruction Loss: -1.7500778436660767
Iteration 7301:
Training Loss: 2.695645332336426
Reconstruction Loss: -1.7528038024902344
Iteration 7351:
Training Loss: 2.7045624256134033
Reconstruction Loss: -1.7530546188354492
Iteration 7401:
Training Loss: 2.9330894947052
Reconstruction Loss: -1.747288465499878
Iteration 7451:
Training Loss: 2.8950705528259277
Reconstruction Loss: -1.7460871934890747
