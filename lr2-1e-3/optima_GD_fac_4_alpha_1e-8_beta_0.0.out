5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.4275399446487427
Iteration 101:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.42754003405570984
Iteration 201:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.42754003405570984
Iteration 301:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.4275399446487427
Iteration 401:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.4275399446487427
Iteration 501:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.4275399446487427
Iteration 601:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.4275399446487427
Iteration 701:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.42754003405570984
Iteration 801:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.42754003405570984
Iteration 901:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.42754003405570984
Iteration 1001:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.42754003405570984
Iteration 1101:
Training Loss: 5.504491329193115
Reconstruction Loss: -0.42754003405570984
Iteration 1201:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754003405570984
Iteration 1301:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754003405570984
Iteration 1401:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754003405570984
Iteration 1501:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754003405570984
Iteration 1601:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754003405570984
Iteration 1701:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754003405570984
Iteration 1801:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 1901:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 2001:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 2101:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 2201:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 2301:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 2401:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 2501:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754030227661133
Iteration 2601:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.427540123462677
Iteration 2701:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754030227661133
Iteration 2801:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754030227661133
Iteration 2901:
Training Loss: 5.504490852355957
Reconstruction Loss: -0.42754030227661133
Iteration 3001:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.42754030227661133
Iteration 3101:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3201:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3301:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3401:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3501:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3601:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3701:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3801:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 3901:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 4001:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 4101:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.4275403916835785
Iteration 4201:
Training Loss: 5.504490375518799
Reconstruction Loss: -0.42754048109054565
Iteration 4301:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.4275403916835785
Iteration 4401:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754048109054565
Iteration 4501:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.4275403916835785
Iteration 4601:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754048109054565
Iteration 4701:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.4275403916835785
Iteration 4801:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754048109054565
Iteration 4901:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754048109054565
Iteration 5001:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754048109054565
Iteration 5101:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754048109054565
Iteration 5201:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754068970680237
Iteration 5301:
Training Loss: 5.504489898681641
Reconstruction Loss: -0.42754068970680237
Iteration 5401:
Training Loss: 5.504489421844482
Reconstruction Loss: -0.42754068970680237
Iteration 5501:
Training Loss: 5.504489421844482
Reconstruction Loss: -0.42754068970680237
Iteration 5601:
Training Loss: 5.504489421844482
Reconstruction Loss: -0.42754068970680237
Iteration 5701:
Training Loss: 5.504489421844482
Reconstruction Loss: -0.42754068970680237
Iteration 5801:
Training Loss: 5.504489421844482
Reconstruction Loss: -0.42754077911376953
Iteration 5901:
Training Loss: 5.504489421844482
Reconstruction Loss: -0.42754077911376953
Iteration 6001:
Training Loss: 5.504489421844482
Reconstruction Loss: -0.42754077911376953
Iteration 6101:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.42754077911376953
Iteration 6201:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.42754077911376953
Iteration 6301:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.42754077911376953
Iteration 6401:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.4275408685207367
Iteration 6501:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.4275408685207367
Iteration 6601:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.4275408685207367
Iteration 6701:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.4275408685207367
Iteration 6801:
Training Loss: 5.504488945007324
Reconstruction Loss: -0.4275408685207367
Iteration 6901:
Training Loss: 5.504488468170166
Reconstruction Loss: -0.4275408685207367
Iteration 7001:
Training Loss: 5.504488468170166
Reconstruction Loss: -0.4275411367416382
Iteration 7101:
Training Loss: 5.504488468170166
Reconstruction Loss: -0.42754122614860535
Iteration 7201:
Training Loss: 5.504488468170166
Reconstruction Loss: -0.42754122614860535
Iteration 7301:
Training Loss: 5.504487991333008
Reconstruction Loss: -0.42754122614860535
Iteration 7401:
Training Loss: 5.504487991333008
Reconstruction Loss: -0.4275413155555725
Iteration 7501:
Training Loss: 5.504487991333008
Reconstruction Loss: -0.4275413155555725
Iteration 7601:
Training Loss: 5.504487991333008
Reconstruction Loss: -0.4275413155555725
Iteration 7701:
Training Loss: 5.504487991333008
Reconstruction Loss: -0.4275413155555725
Iteration 7801:
Training Loss: 5.504487991333008
Reconstruction Loss: -0.4275413155555725
Iteration 7901:
Training Loss: 5.50448751449585
Reconstruction Loss: -0.42754149436950684
Iteration 8001:
Training Loss: 5.50448751449585
Reconstruction Loss: -0.427541583776474
Iteration 8101:
Training Loss: 5.50448751449585
Reconstruction Loss: -0.427541583776474
Iteration 8201:
Training Loss: 5.50448751449585
Reconstruction Loss: -0.427541583776474
Iteration 8301:
Training Loss: 5.504487037658691
Reconstruction Loss: -0.427541583776474
Iteration 8401:
Training Loss: 5.504487037658691
Reconstruction Loss: -0.42754167318344116
Iteration 8501:
Training Loss: 5.504487037658691
Reconstruction Loss: -0.4275417923927307
Iteration 8601:
Training Loss: 5.504486560821533
Reconstruction Loss: -0.4275417923927307
Iteration 8701:
Training Loss: 5.504486560821533
Reconstruction Loss: -0.4275417923927307
Iteration 8801:
Training Loss: 5.504486560821533
Reconstruction Loss: -0.4275420606136322
Iteration 8901:
Training Loss: 5.504486083984375
Reconstruction Loss: -0.4275420606136322
Iteration 9001:
Training Loss: 5.504486083984375
Reconstruction Loss: -0.42754215002059937
Iteration 9101:
Training Loss: 5.504485607147217
Reconstruction Loss: -0.42754223942756653
Iteration 9201:
Training Loss: 5.504485607147217
Reconstruction Loss: -0.42754223942756653
Iteration 9301:
Training Loss: 5.504485607147217
Reconstruction Loss: -0.427542507648468
Iteration 9401:
Training Loss: 5.504485130310059
Reconstruction Loss: -0.427542507648468
Iteration 9501:
Training Loss: 5.504485130310059
Reconstruction Loss: -0.4275425970554352
Iteration 9601:
Training Loss: 5.5044846534729
Reconstruction Loss: -0.4275427758693695
Iteration 9701:
Training Loss: 5.504484176635742
Reconstruction Loss: -0.42754286527633667
Iteration 9801:
Training Loss: 5.504484176635742
Reconstruction Loss: -0.42754286527633667
Iteration 9901:
Training Loss: 5.504483699798584
Reconstruction Loss: -0.427543044090271
Iteration 10001:
Training Loss: 5.504483699798584
Reconstruction Loss: -0.4275432527065277
Iteration 10101:
Training Loss: 5.504483222961426
Reconstruction Loss: -0.4275433421134949
Iteration 10201:
Training Loss: 5.504482746124268
Reconstruction Loss: -0.42754361033439636
Iteration 10301:
Training Loss: 5.504482269287109
Reconstruction Loss: -0.4275437891483307
Iteration 10401:
Training Loss: 5.504481792449951
Reconstruction Loss: -0.42754387855529785
Iteration 10501:
Training Loss: 5.504481315612793
Reconstruction Loss: -0.42754387855529785
Iteration 10601:
Training Loss: 5.504481315612793
Reconstruction Loss: -0.42754414677619934
Iteration 10701:
Training Loss: 5.504480361938477
Reconstruction Loss: -0.42754441499710083
Iteration 10801:
Training Loss: 5.504479885101318
Reconstruction Loss: -0.4275447130203247
Iteration 10901:
Training Loss: 5.50447940826416
Reconstruction Loss: -0.42754489183425903
Iteration 11001:
Training Loss: 5.504478931427002
Reconstruction Loss: -0.4275451600551605
Iteration 11101:
Training Loss: 5.5044779777526855
Reconstruction Loss: -0.427545428276062
Iteration 11201:
Training Loss: 5.504477024078369
Reconstruction Loss: -0.42754578590393066
Iteration 11301:
Training Loss: 5.504476547241211
Reconstruction Loss: -0.4275461733341217
Iteration 11401:
Training Loss: 5.5044755935668945
Reconstruction Loss: -0.42754653096199036
Iteration 11501:
Training Loss: 5.50447416305542
Reconstruction Loss: -0.42754679918289185
Iteration 11601:
Training Loss: 5.5044732093811035
Reconstruction Loss: -0.4275473654270172
Iteration 11701:
Training Loss: 5.504471778869629
Reconstruction Loss: -0.4275479018688202
Iteration 11801:
Training Loss: 5.504470348358154
Reconstruction Loss: -0.4275486171245575
Iteration 11901:
Training Loss: 5.50446891784668
Reconstruction Loss: -0.42754918336868286
Iteration 12001:
Training Loss: 5.504467010498047
Reconstruction Loss: -0.42754989862442017
Iteration 12101:
Training Loss: 5.504464626312256
Reconstruction Loss: -0.427550733089447
Iteration 12201:
Training Loss: 5.504462242126465
Reconstruction Loss: -0.4275517463684082
Iteration 12301:
Training Loss: 5.504459381103516
Reconstruction Loss: -0.42755284905433655
Iteration 12401:
Training Loss: 5.504456520080566
Reconstruction Loss: -0.4275541305541992
Iteration 12501:
Training Loss: 5.504452228546143
Reconstruction Loss: -0.42755556106567383
Iteration 12601:
Training Loss: 5.504447937011719
Reconstruction Loss: -0.42755749821662903
Iteration 12701:
Training Loss: 5.50444221496582
Reconstruction Loss: -0.4275597929954529
Iteration 12801:
Training Loss: 5.504435062408447
Reconstruction Loss: -0.42756253480911255
Iteration 12901:
Training Loss: 5.504426002502441
Reconstruction Loss: -0.42756617069244385
Iteration 13001:
Training Loss: 5.5044145584106445
Reconstruction Loss: -0.4275705814361572
Iteration 13101:
Training Loss: 5.504399299621582
Reconstruction Loss: -0.42757678031921387
Iteration 13201:
Training Loss: 5.504378318786621
Reconstruction Loss: -0.42758509516716003
Iteration 13301:
Training Loss: 5.5043487548828125
Reconstruction Loss: -0.42759716510772705
Iteration 13401:
Training Loss: 5.504303455352783
Reconstruction Loss: -0.4276156425476074
Iteration 13501:
Training Loss: 5.5042290687561035
Reconstruction Loss: -0.42764607071876526
Iteration 13601:
Training Loss: 5.504095554351807
Reconstruction Loss: -0.42770200967788696
Iteration 13701:
Training Loss: 5.503812313079834
Reconstruction Loss: -0.4278225898742676
Iteration 13801:
Training Loss: 5.503025054931641
Reconstruction Loss: -0.42816680669784546
Iteration 13901:
Training Loss: 5.499058246612549
Reconstruction Loss: -0.4299691915512085
Iteration 14001:
Training Loss: 5.264761447906494
Reconstruction Loss: -0.5249943733215332
Iteration 14101:
Training Loss: 5.02787446975708
Reconstruction Loss: -0.6054916977882385
Iteration 14201:
Training Loss: 5.002360820770264
Reconstruction Loss: -0.6101782917976379
Iteration 14301:
Training Loss: 4.987279415130615
Reconstruction Loss: -0.5836132168769836
Iteration 14401:
Training Loss: 4.979428291320801
Reconstruction Loss: -0.5567145347595215
Iteration 14501:
Training Loss: 4.976712703704834
Reconstruction Loss: -0.5430070161819458
Iteration 14601:
Training Loss: 4.9757513999938965
Reconstruction Loss: -0.5365521907806396
Iteration 14701:
Training Loss: 4.975348949432373
Reconstruction Loss: -0.5330451726913452
Iteration 14801:
Training Loss: 4.97515344619751
Reconstruction Loss: -0.5308682918548584
Iteration 14901:
Training Loss: 4.975047588348389
Reconstruction Loss: -0.5293930768966675
