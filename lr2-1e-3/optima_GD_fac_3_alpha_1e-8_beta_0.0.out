5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.72279167175293
Reconstruction Loss: -0.3404857814311981
Iteration 101:
Training Loss: 5.72279167175293
Reconstruction Loss: -0.3404857814311981
Iteration 201:
Training Loss: 5.72279167175293
Reconstruction Loss: -0.3404857814311981
Iteration 301:
Training Loss: 5.72279167175293
Reconstruction Loss: -0.3404857814311981
Iteration 401:
Training Loss: 5.72279167175293
Reconstruction Loss: -0.3404858708381653
Iteration 501:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404858708381653
Iteration 601:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404858708381653
Iteration 701:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404858708381653
Iteration 801:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404858708381653
Iteration 901:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404858708381653
Iteration 1001:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404858708381653
Iteration 1101:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404860198497772
Iteration 1201:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404860198497772
Iteration 1301:
Training Loss: 5.7227911949157715
Reconstruction Loss: -0.3404860198497772
Iteration 1401:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.3404860198497772
Iteration 1501:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.3404861092567444
Iteration 1601:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.3404861092567444
Iteration 1701:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.3404861092567444
Iteration 1801:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.3404861092567444
Iteration 1901:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.34048619866371155
Iteration 2001:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.34048619866371155
Iteration 2101:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.3404863476753235
Iteration 2201:
Training Loss: 5.722790718078613
Reconstruction Loss: -0.3404863476753235
Iteration 2301:
Training Loss: 5.722790241241455
Reconstruction Loss: -0.34048643708229065
Iteration 2401:
Training Loss: 5.722790241241455
Reconstruction Loss: -0.34048643708229065
Iteration 2501:
Training Loss: 5.722790241241455
Reconstruction Loss: -0.34048643708229065
Iteration 2601:
Training Loss: 5.722789764404297
Reconstruction Loss: -0.340486615896225
Iteration 2701:
Training Loss: 5.722789764404297
Reconstruction Loss: -0.3404867649078369
Iteration 2801:
Training Loss: 5.722789287567139
Reconstruction Loss: -0.34048694372177124
Iteration 2901:
Training Loss: 5.722789287567139
Reconstruction Loss: -0.34048718214035034
Iteration 3001:
Training Loss: 5.7227888107299805
Reconstruction Loss: -0.34048736095428467
Iteration 3101:
Training Loss: 5.722788333892822
Reconstruction Loss: -0.340487539768219
Iteration 3201:
Training Loss: 5.722787857055664
Reconstruction Loss: -0.3404879570007324
Iteration 3301:
Training Loss: 5.722786903381348
Reconstruction Loss: -0.3404885232448578
Iteration 3401:
Training Loss: 5.722785949707031
Reconstruction Loss: -0.34048911929130554
Iteration 3501:
Training Loss: 5.722784042358398
Reconstruction Loss: -0.34049004316329956
Iteration 3601:
Training Loss: 5.722782135009766
Reconstruction Loss: -0.340491384267807
Iteration 3701:
Training Loss: 5.722779273986816
Reconstruction Loss: -0.3404933214187622
Iteration 3801:
Training Loss: 5.722773551940918
Reconstruction Loss: -0.3404964208602905
Iteration 3901:
Training Loss: 5.72276496887207
Reconstruction Loss: -0.3405015170574188
Iteration 4001:
Training Loss: 5.72274923324585
Reconstruction Loss: -0.34051090478897095
Iteration 4101:
Training Loss: 5.722715854644775
Reconstruction Loss: -0.34053075313568115
Iteration 4201:
Training Loss: 5.722630023956299
Reconstruction Loss: -0.34058111906051636
Iteration 4301:
Training Loss: 5.722320079803467
Reconstruction Loss: -0.3407595157623291
Iteration 4401:
Training Loss: 5.720073223114014
Reconstruction Loss: -0.3420274555683136
Iteration 4501:
Training Loss: 5.500668048858643
Reconstruction Loss: -0.4478064179420471
Iteration 4601:
Training Loss: 5.171580791473389
Reconstruction Loss: -0.46105068922042847
Iteration 4701:
Training Loss: 5.148604869842529
Reconstruction Loss: -0.42448410391807556
Iteration 4801:
Training Loss: 5.143825531005859
Reconstruction Loss: -0.41384533047676086
Iteration 4901:
Training Loss: 5.143242359161377
Reconstruction Loss: -0.41311073303222656
Iteration 5001:
Training Loss: 5.143030643463135
Reconstruction Loss: -0.4136415123939514
Iteration 5101:
Training Loss: 5.1425981521606445
Reconstruction Loss: -0.414220929145813
Iteration 5201:
Training Loss: 5.140942573547363
Reconstruction Loss: -0.4154393672943115
Iteration 5301:
Training Loss: 5.127152442932129
Reconstruction Loss: -0.4241049885749817
Iteration 5401:
Training Loss: 4.749114990234375
Reconstruction Loss: -0.6426718235015869
Iteration 5501:
Training Loss: 4.620389938354492
Reconstruction Loss: -0.6876248121261597
Iteration 5601:
Training Loss: 4.595946788787842
Reconstruction Loss: -0.668687105178833
Iteration 5701:
Training Loss: 4.588416576385498
Reconstruction Loss: -0.6610403656959534
Iteration 5801:
Training Loss: 4.583630561828613
Reconstruction Loss: -0.6556025743484497
Iteration 5901:
Training Loss: 4.579857349395752
Reconstruction Loss: -0.6501023769378662
Iteration 6001:
Training Loss: 4.576711177825928
Reconstruction Loss: -0.6444969177246094
Iteration 6101:
Training Loss: 4.574113368988037
Reconstruction Loss: -0.6389600038528442
Iteration 6201:
Training Loss: 4.572058200836182
Reconstruction Loss: -0.6336284279823303
Iteration 6301:
Training Loss: 4.570523262023926
Reconstruction Loss: -0.6286130547523499
Iteration 6401:
Training Loss: 4.569444179534912
Reconstruction Loss: -0.6240203976631165
Iteration 6501:
Training Loss: 4.568724155426025
Reconstruction Loss: -0.6199369430541992
Iteration 6601:
Training Loss: 4.568261623382568
Reconstruction Loss: -0.6164063215255737
Iteration 6701:
Training Loss: 4.567971706390381
Reconstruction Loss: -0.6134235858917236
Iteration 6801:
Training Loss: 4.56779146194458
Reconstruction Loss: -0.6109476685523987
Iteration 6901:
Training Loss: 4.567679405212402
Reconstruction Loss: -0.6089192628860474
Iteration 7001:
Training Loss: 4.567608833312988
Reconstruction Loss: -0.6072741746902466
Iteration 7101:
Training Loss: 4.567563056945801
Reconstruction Loss: -0.6059511303901672
Iteration 7201:
Training Loss: 4.567531108856201
Reconstruction Loss: -0.6048941016197205
Iteration 7301:
Training Loss: 4.567506313323975
Reconstruction Loss: -0.6040555834770203
Iteration 7401:
Training Loss: 4.567483901977539
Reconstruction Loss: -0.6033952236175537
Iteration 7501:
Training Loss: 4.567458629608154
Reconstruction Loss: -0.6028802990913391
Iteration 7601:
Training Loss: 4.56742525100708
Reconstruction Loss: -0.6024851202964783
Iteration 7701:
Training Loss: 4.567375183105469
Reconstruction Loss: -0.6021917462348938
Iteration 7801:
Training Loss: 4.567291259765625
Reconstruction Loss: -0.6019930243492126
Iteration 7901:
Training Loss: 4.567135334014893
Reconstruction Loss: -0.6018986701965332
Iteration 8001:
Training Loss: 4.566807270050049
Reconstruction Loss: -0.6019627451896667
Iteration 8101:
Training Loss: 4.56597900390625
Reconstruction Loss: -0.602385401725769
Iteration 8201:
Training Loss: 4.563182830810547
Reconstruction Loss: -0.6040753126144409
Iteration 8301:
Training Loss: 4.546687126159668
Reconstruction Loss: -0.6140508651733398
Iteration 8401:
Training Loss: 4.26081657409668
Reconstruction Loss: -0.7780429124832153
Iteration 8501:
Training Loss: 3.9265100955963135
Reconstruction Loss: -1.0025883913040161
Iteration 8601:
Training Loss: 3.8927807807922363
Reconstruction Loss: -1.0029031038284302
Iteration 8701:
Training Loss: 3.8830342292785645
Reconstruction Loss: -0.9988807439804077
Iteration 8801:
Training Loss: 3.8776562213897705
Reconstruction Loss: -0.9982818365097046
Iteration 8901:
Training Loss: 3.873394727706909
Reconstruction Loss: -0.9980002045631409
Iteration 9001:
Training Loss: 3.869494676589966
Reconstruction Loss: -0.99622642993927
Iteration 9101:
Training Loss: 3.865830659866333
Reconstruction Loss: -0.9923228025436401
Iteration 9201:
Training Loss: 3.8624916076660156
Reconstruction Loss: -0.9863874912261963
Iteration 9301:
Training Loss: 3.8596057891845703
Reconstruction Loss: -0.9789849519729614
Iteration 9401:
Training Loss: 3.857243776321411
Reconstruction Loss: -0.9708825349807739
Iteration 9501:
Training Loss: 3.8553900718688965
Reconstruction Loss: -0.9627918004989624
Iteration 9601:
Training Loss: 3.8539679050445557
Reconstruction Loss: -0.9552055597305298
Iteration 9701:
Training Loss: 3.8528807163238525
Reconstruction Loss: -0.9483662843704224
Iteration 9801:
Training Loss: 3.852041482925415
Reconstruction Loss: -0.9423269033432007
Iteration 9901:
Training Loss: 3.85138201713562
Reconstruction Loss: -0.9370304346084595
Iteration 10001:
Training Loss: 3.85085391998291
Reconstruction Loss: -0.9323763847351074
Iteration 10101:
Training Loss: 3.8504221439361572
Reconstruction Loss: -0.9282582998275757
Iteration 10201:
Training Loss: 3.8500633239746094
Reconstruction Loss: -0.9245816469192505
Iteration 10301:
Training Loss: 3.8497605323791504
Reconstruction Loss: -0.9212685227394104
Iteration 10401:
Training Loss: 3.849501132965088
Reconstruction Loss: -0.918257474899292
Iteration 10501:
Training Loss: 3.8492767810821533
Reconstruction Loss: -0.9155006408691406
Iteration 10601:
Training Loss: 3.8490800857543945
Reconstruction Loss: -0.9129611253738403
Iteration 10701:
Training Loss: 3.848905324935913
Reconstruction Loss: -0.9106098413467407
Iteration 10801:
Training Loss: 3.848748207092285
Reconstruction Loss: -0.9084241390228271
Iteration 10901:
Training Loss: 3.848604679107666
Reconstruction Loss: -0.9063858985900879
Iteration 11001:
Training Loss: 3.848471164703369
Reconstruction Loss: -0.9044802784919739
Iteration 11101:
Training Loss: 3.848343849182129
Reconstruction Loss: -0.9026962518692017
Iteration 11201:
Training Loss: 3.848217487335205
Reconstruction Loss: -0.9010241031646729
Iteration 11301:
Training Loss: 3.8480873107910156
Reconstruction Loss: -0.8994575142860413
Iteration 11401:
Training Loss: 3.8479442596435547
Reconstruction Loss: -0.8979917764663696
Iteration 11501:
Training Loss: 3.8477752208709717
Reconstruction Loss: -0.8966267704963684
Iteration 11601:
Training Loss: 3.847557544708252
Reconstruction Loss: -0.8953673839569092
Iteration 11701:
Training Loss: 3.847247362136841
Reconstruction Loss: -0.8942296504974365
Iteration 11801:
Training Loss: 3.846754550933838
Reconstruction Loss: -0.8932534456253052
Iteration 11901:
Training Loss: 3.8458707332611084
Reconstruction Loss: -0.8925372362136841
Iteration 12001:
Training Loss: 3.8440351486206055
Reconstruction Loss: -0.892350435256958
Iteration 12101:
Training Loss: 3.839402437210083
Reconstruction Loss: -0.8935765027999878
Iteration 12201:
Training Loss: 3.823718786239624
Reconstruction Loss: -0.9000893831253052
Iteration 12301:
Training Loss: 3.73957896232605
Reconstruction Loss: -0.9380215406417847
Iteration 12401:
Training Loss: 3.3612053394317627
Reconstruction Loss: -1.13239324092865
Iteration 12501:
Training Loss: 3.1486737728118896
Reconstruction Loss: -1.330558180809021
Iteration 12601:
Training Loss: 3.0561132431030273
Reconstruction Loss: -1.4458216428756714
Iteration 12701:
Training Loss: 3.0163867473602295
Reconstruction Loss: -1.5012494325637817
Iteration 12801:
Training Loss: 2.9998326301574707
Reconstruction Loss: -1.5251684188842773
Iteration 12901:
Training Loss: 2.9925570487976074
Reconstruction Loss: -1.535116195678711
Iteration 13001:
Training Loss: 2.9889473915100098
Reconstruction Loss: -1.5393013954162598
Iteration 13101:
Training Loss: 2.986823081970215
Reconstruction Loss: -1.541233777999878
Iteration 13201:
Training Loss: 2.985316514968872
Reconstruction Loss: -1.5423513650894165
Iteration 13301:
Training Loss: 2.984060525894165
Reconstruction Loss: -1.5432052612304688
Iteration 13401:
Training Loss: 2.9828877449035645
Reconstruction Loss: -1.543971061706543
Iteration 13501:
Training Loss: 2.981715202331543
Reconstruction Loss: -1.544666051864624
Iteration 13601:
Training Loss: 2.980496406555176
Reconstruction Loss: -1.5452417135238647
Iteration 13701:
Training Loss: 2.979203701019287
Reconstruction Loss: -1.5456244945526123
Iteration 13801:
Training Loss: 2.977818489074707
Reconstruction Loss: -1.5457332134246826
Iteration 13901:
Training Loss: 2.976330280303955
Reconstruction Loss: -1.5454870462417603
Iteration 14001:
Training Loss: 2.974730968475342
Reconstruction Loss: -1.5448098182678223
Iteration 14101:
Training Loss: 2.9730186462402344
Reconstruction Loss: -1.5436333417892456
Iteration 14201:
Training Loss: 2.9711952209472656
Reconstruction Loss: -1.5419015884399414
Iteration 14301:
Training Loss: 2.969268321990967
Reconstruction Loss: -1.5395725965499878
Iteration 14401:
Training Loss: 2.9672505855560303
Reconstruction Loss: -1.5366239547729492
Iteration 14501:
Training Loss: 2.9651596546173096
Reconstruction Loss: -1.5330562591552734
Iteration 14601:
Training Loss: 2.9630179405212402
Reconstruction Loss: -1.528896689414978
Iteration 14701:
Training Loss: 2.9608514308929443
Reconstruction Loss: -1.5242003202438354
Iteration 14801:
Training Loss: 2.9586868286132812
Reconstruction Loss: -1.5190500020980835
Iteration 14901:
Training Loss: 2.9565482139587402
Reconstruction Loss: -1.5135538578033447
