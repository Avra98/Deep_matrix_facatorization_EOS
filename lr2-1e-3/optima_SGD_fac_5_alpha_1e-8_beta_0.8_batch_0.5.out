5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.446907997131348
Reconstruction Loss: -0.5627557635307312
Iteration 51:
Training Loss: 5.525177001953125
Reconstruction Loss: -0.5627557635307312
Iteration 101:
Training Loss: 5.3683648109436035
Reconstruction Loss: -0.562755823135376
Iteration 151:
Training Loss: 5.500406265258789
Reconstruction Loss: -0.562755823135376
Iteration 201:
Training Loss: 5.389371395111084
Reconstruction Loss: -0.5627559423446655
Iteration 251:
Training Loss: 5.221271514892578
Reconstruction Loss: -0.5627560615539551
Iteration 301:
Training Loss: 5.451977729797363
Reconstruction Loss: -0.5627560615539551
Iteration 351:
Training Loss: 5.421665191650391
Reconstruction Loss: -0.5627561807632446
Iteration 401:
Training Loss: 5.3170647621154785
Reconstruction Loss: -0.5627562999725342
Iteration 451:
Training Loss: 5.394532680511475
Reconstruction Loss: -0.5627562999725342
Iteration 501:
Training Loss: 5.355093002319336
Reconstruction Loss: -0.5627562999725342
Iteration 551:
Training Loss: 5.372204303741455
Reconstruction Loss: -0.5627564787864685
Iteration 601:
Training Loss: 5.440924167633057
Reconstruction Loss: -0.5627564787864685
Iteration 651:
Training Loss: 5.463892459869385
Reconstruction Loss: -0.5627565979957581
Iteration 701:
Training Loss: 5.482004642486572
Reconstruction Loss: -0.5627565979957581
Iteration 751:
Training Loss: 5.397405624389648
Reconstruction Loss: -0.5627567172050476
Iteration 801:
Training Loss: 5.36003303527832
Reconstruction Loss: -0.5627567768096924
Iteration 851:
Training Loss: 5.351441383361816
Reconstruction Loss: -0.5627570152282715
Iteration 901:
Training Loss: 5.315535545349121
Reconstruction Loss: -0.5627570152282715
Iteration 951:
Training Loss: 5.631014347076416
Reconstruction Loss: -0.5627570152282715
Iteration 1001:
Training Loss: 5.497130870819092
Reconstruction Loss: -0.5627572536468506
Iteration 1051:
Training Loss: 5.36855411529541
Reconstruction Loss: -0.5627572536468506
Iteration 1101:
Training Loss: 5.528308868408203
Reconstruction Loss: -0.5627572536468506
Iteration 1151:
Training Loss: 5.568139553070068
Reconstruction Loss: -0.5627573132514954
Iteration 1201:
Training Loss: 5.419118881225586
Reconstruction Loss: -0.5627574324607849
Iteration 1251:
Training Loss: 5.448209762573242
Reconstruction Loss: -0.5627574324607849
Iteration 1301:
Training Loss: 5.466335773468018
Reconstruction Loss: -0.5627576112747192
Iteration 1351:
Training Loss: 5.311099529266357
Reconstruction Loss: -0.5627577304840088
Iteration 1401:
Training Loss: 5.483543395996094
Reconstruction Loss: -0.5627577304840088
Iteration 1451:
Training Loss: 5.443492889404297
Reconstruction Loss: -0.5627578496932983
Iteration 1501:
Training Loss: 5.378678798675537
Reconstruction Loss: -0.5627579689025879
Iteration 1551:
Training Loss: 5.444354057312012
Reconstruction Loss: -0.5627580881118774
Iteration 1601:
Training Loss: 5.356553554534912
Reconstruction Loss: -0.5627581477165222
Iteration 1651:
Training Loss: 5.484882831573486
Reconstruction Loss: -0.5627581477165222
Iteration 1701:
Training Loss: 5.387399196624756
Reconstruction Loss: -0.5627582669258118
Iteration 1751:
Training Loss: 5.440764904022217
Reconstruction Loss: -0.5627584457397461
Iteration 1801:
Training Loss: 5.350384712219238
Reconstruction Loss: -0.5627584457397461
Iteration 1851:
Training Loss: 5.519166469573975
Reconstruction Loss: -0.5627586841583252
Iteration 1901:
Training Loss: 5.521729469299316
Reconstruction Loss: -0.5627588033676147
Iteration 1951:
Training Loss: 5.4517011642456055
Reconstruction Loss: -0.5627589225769043
Iteration 2001:
Training Loss: 5.309518337249756
Reconstruction Loss: -0.5627589225769043
Iteration 2051:
Training Loss: 5.506978511810303
Reconstruction Loss: -0.5627591013908386
Iteration 2101:
Training Loss: 5.422743797302246
Reconstruction Loss: -0.5627592206001282
Iteration 2151:
Training Loss: 5.297133922576904
Reconstruction Loss: -0.562759280204773
Iteration 2201:
Training Loss: 5.423361301422119
Reconstruction Loss: -0.562759518623352
Iteration 2251:
Training Loss: 5.44912052154541
Reconstruction Loss: -0.5627596378326416
Iteration 2301:
Training Loss: 5.223954200744629
Reconstruction Loss: -0.5627597570419312
Iteration 2351:
Training Loss: 5.4779863357543945
Reconstruction Loss: -0.5627599358558655
Iteration 2401:
Training Loss: 5.354470729827881
Reconstruction Loss: -0.562760055065155
Iteration 2451:
Training Loss: 5.491943359375
Reconstruction Loss: -0.5627602338790894
Iteration 2501:
Training Loss: 5.451881408691406
Reconstruction Loss: -0.5627603530883789
Iteration 2551:
Training Loss: 5.398880958557129
Reconstruction Loss: -0.562760591506958
Iteration 2601:
Training Loss: 5.463942050933838
Reconstruction Loss: -0.5627607703208923
Iteration 2651:
Training Loss: 5.296245098114014
Reconstruction Loss: -0.5627608895301819
Iteration 2701:
Training Loss: 5.467287540435791
Reconstruction Loss: -0.5627610683441162
Iteration 2751:
Training Loss: 5.538765907287598
Reconstruction Loss: -0.5627610683441162
Iteration 2801:
Training Loss: 5.426063537597656
Reconstruction Loss: -0.5627615451812744
Iteration 2851:
Training Loss: 5.419132709503174
Reconstruction Loss: -0.5627617239952087
Iteration 2901:
Training Loss: 5.312228202819824
Reconstruction Loss: -0.5627620220184326
Iteration 2951:
Training Loss: 5.600195407867432
Reconstruction Loss: -0.5627622604370117
Iteration 3001:
Training Loss: 5.409717559814453
Reconstruction Loss: -0.562762439250946
Iteration 3051:
Training Loss: 5.480623245239258
Reconstruction Loss: -0.5627627372741699
Iteration 3101:
Training Loss: 5.16492223739624
Reconstruction Loss: -0.562762975692749
Iteration 3151:
Training Loss: 5.3616485595703125
Reconstruction Loss: -0.5627633929252625
Iteration 3201:
Training Loss: 5.454180717468262
Reconstruction Loss: -0.5627635717391968
Iteration 3251:
Training Loss: 5.404109001159668
Reconstruction Loss: -0.5627641081809998
Iteration 3301:
Training Loss: 5.3306756019592285
Reconstruction Loss: -0.5627644062042236
Iteration 3351:
Training Loss: 5.523804187774658
Reconstruction Loss: -0.5627648830413818
Iteration 3401:
Training Loss: 5.412809371948242
Reconstruction Loss: -0.5627652406692505
Iteration 3451:
Training Loss: 5.4326324462890625
Reconstruction Loss: -0.5627658367156982
Iteration 3501:
Training Loss: 5.454784393310547
Reconstruction Loss: -0.5627663135528564
Iteration 3551:
Training Loss: 5.366092205047607
Reconstruction Loss: -0.5627668499946594
Iteration 3601:
Training Loss: 5.312452793121338
Reconstruction Loss: -0.5627673864364624
Iteration 3651:
Training Loss: 5.402977466583252
Reconstruction Loss: -0.5627681016921997
Iteration 3701:
Training Loss: 5.354735374450684
Reconstruction Loss: -0.562768816947937
Iteration 3751:
Training Loss: 5.387588977813721
Reconstruction Loss: -0.5627695322036743
Iteration 3801:
Training Loss: 5.358340263366699
Reconstruction Loss: -0.5627703666687012
Iteration 3851:
Training Loss: 5.4992899894714355
Reconstruction Loss: -0.5627713203430176
Iteration 3901:
Training Loss: 5.518395900726318
Reconstruction Loss: -0.5627725124359131
Iteration 3951:
Training Loss: 5.406900405883789
Reconstruction Loss: -0.5627736449241638
Iteration 4001:
Training Loss: 5.472785949707031
Reconstruction Loss: -0.5627751350402832
Iteration 4051:
Training Loss: 5.459758758544922
Reconstruction Loss: -0.5627766847610474
Iteration 4101:
Training Loss: 5.347588539123535
Reconstruction Loss: -0.5627785921096802
Iteration 4151:
Training Loss: 5.513580799102783
Reconstruction Loss: -0.5627806782722473
Iteration 4201:
Training Loss: 5.530141830444336
Reconstruction Loss: -0.5627830624580383
Iteration 4251:
Training Loss: 5.408498764038086
Reconstruction Loss: -0.5627861022949219
Iteration 4301:
Training Loss: 5.437087059020996
Reconstruction Loss: -0.5627895593643188
Iteration 4351:
Training Loss: 5.313365936279297
Reconstruction Loss: -0.5627940893173218
Iteration 4401:
Training Loss: 5.473159313201904
Reconstruction Loss: -0.5627992749214172
Iteration 4451:
Training Loss: 5.140464782714844
Reconstruction Loss: -0.5628063082695007
Iteration 4501:
Training Loss: 5.447805881500244
Reconstruction Loss: -0.5628153085708618
Iteration 4551:
Training Loss: 5.324491500854492
Reconstruction Loss: -0.5628273487091064
Iteration 4601:
Training Loss: 5.365116596221924
Reconstruction Loss: -0.5628442764282227
Iteration 4651:
Training Loss: 5.4432501792907715
Reconstruction Loss: -0.5628694295883179
Iteration 4701:
Training Loss: 5.413472652435303
Reconstruction Loss: -0.5629088282585144
Iteration 4751:
Training Loss: 5.556582927703857
Reconstruction Loss: -0.5629774332046509
Iteration 4801:
Training Loss: 5.335869789123535
Reconstruction Loss: -0.5631166100502014
Iteration 4851:
Training Loss: 5.590799331665039
Reconstruction Loss: -0.5634788274765015
Iteration 4901:
Training Loss: 5.446248531341553
Reconstruction Loss: -0.5652002096176147
Iteration 4951:
Training Loss: 5.002429008483887
Reconstruction Loss: -0.7435148358345032
Iteration 5001:
Training Loss: 4.811172008514404
Reconstruction Loss: -0.76972895860672
Iteration 5051:
Training Loss: 4.791776180267334
Reconstruction Loss: -0.7542687654495239
Iteration 5101:
Training Loss: 4.864604473114014
Reconstruction Loss: -0.7405462861061096
Iteration 5151:
Training Loss: 4.760763168334961
Reconstruction Loss: -0.742354154586792
Iteration 5201:
Training Loss: 4.763662815093994
Reconstruction Loss: -0.7418754696846008
Iteration 5251:
Training Loss: 4.729888916015625
Reconstruction Loss: -0.7467319965362549
Iteration 5301:
Training Loss: 4.914923191070557
Reconstruction Loss: -0.7624526023864746
Iteration 5351:
Training Loss: 4.749086856842041
Reconstruction Loss: -0.7411636710166931
Iteration 5401:
Training Loss: 4.805455207824707
Reconstruction Loss: -0.7500610947608948
Iteration 5451:
Training Loss: 4.791451454162598
Reconstruction Loss: -0.7564253807067871
Iteration 5501:
Training Loss: 4.769797325134277
Reconstruction Loss: -0.7744340300559998
Iteration 5551:
Training Loss: 4.7898383140563965
Reconstruction Loss: -0.7714201211929321
Iteration 5601:
Training Loss: 4.740962505340576
Reconstruction Loss: -0.7662961483001709
Iteration 5651:
Training Loss: 4.8403778076171875
Reconstruction Loss: -0.7679378390312195
Iteration 5701:
Training Loss: 4.694551944732666
Reconstruction Loss: -0.7634190917015076
Iteration 5751:
Training Loss: 4.792794227600098
Reconstruction Loss: -0.7637370824813843
Iteration 5801:
Training Loss: 4.793399333953857
Reconstruction Loss: -0.7602904438972473
Iteration 5851:
Training Loss: 4.781264781951904
Reconstruction Loss: -0.733964204788208
Iteration 5901:
Training Loss: 4.705619812011719
Reconstruction Loss: -0.7518737316131592
Iteration 5951:
Training Loss: 4.779967784881592
Reconstruction Loss: -0.7603956460952759
Iteration 6001:
Training Loss: 4.915954113006592
Reconstruction Loss: -0.778745710849762
Iteration 6051:
Training Loss: 4.829282760620117
Reconstruction Loss: -0.7681032419204712
Iteration 6101:
Training Loss: 4.749490261077881
Reconstruction Loss: -0.7633178234100342
Iteration 6151:
Training Loss: 4.71632719039917
Reconstruction Loss: -0.7624468207359314
Iteration 6201:
Training Loss: 4.721010684967041
Reconstruction Loss: -0.7667157649993896
Iteration 6251:
Training Loss: 4.843563556671143
Reconstruction Loss: -0.7620909810066223
Iteration 6301:
Training Loss: 4.828498363494873
Reconstruction Loss: -0.770693302154541
Iteration 6351:
Training Loss: 4.872137069702148
Reconstruction Loss: -0.7627037763595581
Iteration 6401:
Training Loss: 4.812212944030762
Reconstruction Loss: -0.7786312699317932
Iteration 6451:
Training Loss: 4.877871036529541
Reconstruction Loss: -0.743245005607605
Iteration 6501:
Training Loss: 4.912103176116943
Reconstruction Loss: -0.761241614818573
Iteration 6551:
Training Loss: 4.860677719116211
Reconstruction Loss: -0.7682351469993591
Iteration 6601:
Training Loss: 4.794599533081055
Reconstruction Loss: -0.7705239057540894
Iteration 6651:
Training Loss: 4.916365146636963
Reconstruction Loss: -0.7593921422958374
Iteration 6701:
Training Loss: 4.836593151092529
Reconstruction Loss: -0.7546241283416748
Iteration 6751:
Training Loss: 4.865859508514404
Reconstruction Loss: -0.763883650302887
Iteration 6801:
Training Loss: 4.792244911193848
Reconstruction Loss: -0.7606573700904846
Iteration 6851:
Training Loss: 4.824080944061279
Reconstruction Loss: -0.7658335566520691
Iteration 6901:
Training Loss: 4.833670139312744
Reconstruction Loss: -0.7691131830215454
Iteration 6951:
Training Loss: 4.8528618812561035
Reconstruction Loss: -0.7824046015739441
Iteration 7001:
Training Loss: 4.877463340759277
Reconstruction Loss: -0.7594351768493652
Iteration 7051:
Training Loss: 4.527892589569092
Reconstruction Loss: -0.9239296913146973
Iteration 7101:
Training Loss: 4.058670520782471
Reconstruction Loss: -1.0081112384796143
Iteration 7151:
Training Loss: 4.184505462646484
Reconstruction Loss: -0.9979318380355835
Iteration 7201:
Training Loss: 4.028438568115234
Reconstruction Loss: -1.0059305429458618
Iteration 7251:
Training Loss: 4.215601921081543
Reconstruction Loss: -1.0075862407684326
Iteration 7301:
Training Loss: 4.1829752922058105
Reconstruction Loss: -0.9943993091583252
Iteration 7351:
Training Loss: 4.021426200866699
Reconstruction Loss: -1.0064266920089722
Iteration 7401:
Training Loss: 4.064496994018555
Reconstruction Loss: -1.0036633014678955
Iteration 7451:
Training Loss: 4.1015448570251465
Reconstruction Loss: -0.9947351813316345
Iteration 7501:
Training Loss: 4.082768440246582
Reconstruction Loss: -0.9977670907974243
Iteration 7551:
Training Loss: 4.147531032562256
Reconstruction Loss: -0.988593578338623
Iteration 7601:
Training Loss: 3.9709513187408447
Reconstruction Loss: -0.9901818037033081
Iteration 7651:
Training Loss: 4.168360710144043
Reconstruction Loss: -1.0004849433898926
Iteration 7701:
Training Loss: 4.189844608306885
Reconstruction Loss: -0.9976802468299866
Iteration 7751:
Training Loss: 4.107030868530273
Reconstruction Loss: -0.9850991368293762
Iteration 7801:
Training Loss: 3.9006540775299072
Reconstruction Loss: -0.9952374696731567
Iteration 7851:
Training Loss: 4.063316345214844
Reconstruction Loss: -0.9917564988136292
Iteration 7901:
Training Loss: 4.049373149871826
Reconstruction Loss: -0.9830082654953003
Iteration 7951:
Training Loss: 4.071052551269531
Reconstruction Loss: -0.997342586517334
Iteration 8001:
Training Loss: 4.13653564453125
Reconstruction Loss: -0.9827530384063721
Iteration 8051:
Training Loss: 4.0667948722839355
Reconstruction Loss: -0.982809841632843
Iteration 8101:
Training Loss: 4.16432523727417
Reconstruction Loss: -0.9995235800743103
Iteration 8151:
Training Loss: 4.08775520324707
Reconstruction Loss: -0.997210681438446
Iteration 8201:
Training Loss: 4.107151508331299
Reconstruction Loss: -1.0020354986190796
Iteration 8251:
Training Loss: 4.081355094909668
Reconstruction Loss: -0.9842163920402527
Iteration 8301:
Training Loss: 4.0690226554870605
Reconstruction Loss: -0.9798028469085693
Iteration 8351:
Training Loss: 4.1039347648620605
Reconstruction Loss: -0.9934501647949219
Iteration 8401:
Training Loss: 4.145249843597412
Reconstruction Loss: -0.9946528077125549
Iteration 8451:
Training Loss: 3.962373733520508
Reconstruction Loss: -0.9954355359077454
Iteration 8501:
Training Loss: 4.128314018249512
Reconstruction Loss: -0.987276017665863
Iteration 8551:
Training Loss: 4.055911064147949
Reconstruction Loss: -0.9875354766845703
Iteration 8601:
Training Loss: 3.9549031257629395
Reconstruction Loss: -0.9879751205444336
Iteration 8651:
Training Loss: 4.152270793914795
Reconstruction Loss: -0.9852885007858276
Iteration 8701:
Training Loss: 4.215055465698242
Reconstruction Loss: -0.9948095083236694
Iteration 8751:
Training Loss: 4.195383071899414
Reconstruction Loss: -0.9964794516563416
Iteration 8801:
Training Loss: 4.134311199188232
Reconstruction Loss: -0.9892834424972534
Iteration 8851:
Training Loss: 4.121090412139893
Reconstruction Loss: -0.9911966323852539
Iteration 8901:
Training Loss: 4.079324245452881
Reconstruction Loss: -0.9845250844955444
Iteration 8951:
Training Loss: 4.088366985321045
Reconstruction Loss: -0.9901447296142578
Iteration 9001:
Training Loss: 3.971867799758911
Reconstruction Loss: -0.9950135946273804
Iteration 9051:
Training Loss: 4.0761590003967285
Reconstruction Loss: -0.9838113784790039
Iteration 9101:
Training Loss: 4.06469202041626
Reconstruction Loss: -0.9935407638549805
Iteration 9151:
Training Loss: 4.0557074546813965
Reconstruction Loss: -0.9877222776412964
Iteration 9201:
Training Loss: 4.005129814147949
Reconstruction Loss: -0.9927449226379395
Iteration 9251:
Training Loss: 4.096508026123047
Reconstruction Loss: -0.9941773414611816
Iteration 9301:
Training Loss: 4.077705383300781
Reconstruction Loss: -0.9924695491790771
Iteration 9351:
Training Loss: 4.123701095581055
Reconstruction Loss: -0.9844255447387695
Iteration 9401:
Training Loss: 4.14296293258667
Reconstruction Loss: -0.9871842861175537
Iteration 9451:
Training Loss: 4.10753059387207
Reconstruction Loss: -1.001307487487793
Iteration 9501:
Training Loss: 4.176950931549072
Reconstruction Loss: -0.9910919666290283
Iteration 9551:
Training Loss: 4.061067581176758
Reconstruction Loss: -0.9876192808151245
Iteration 9601:
Training Loss: 4.215316295623779
Reconstruction Loss: -0.989466667175293
Iteration 9651:
Training Loss: 4.162992477416992
Reconstruction Loss: -0.996738612651825
Iteration 9701:
Training Loss: 3.9842522144317627
Reconstruction Loss: -0.9905420541763306
Iteration 9751:
Training Loss: 4.14426326751709
Reconstruction Loss: -0.9951671957969666
Iteration 9801:
Training Loss: 4.19829797744751
Reconstruction Loss: -0.9914811849594116
Iteration 9851:
Training Loss: 4.078673362731934
Reconstruction Loss: -1.0010923147201538
Iteration 9901:
Training Loss: 4.077140808105469
Reconstruction Loss: -0.9907593727111816
Iteration 9951:
Training Loss: 4.139313220977783
Reconstruction Loss: -0.990092933177948
Iteration 10001:
Training Loss: 4.110270977020264
Reconstruction Loss: -0.9954105615615845
Iteration 10051:
Training Loss: 4.098160743713379
Reconstruction Loss: -0.9976972341537476
Iteration 10101:
Training Loss: 4.169074058532715
Reconstruction Loss: -0.9901784658432007
Iteration 10151:
Training Loss: 4.146551132202148
Reconstruction Loss: -0.9906579256057739
Iteration 10201:
Training Loss: 4.099602699279785
Reconstruction Loss: -0.9931739568710327
Iteration 10251:
Training Loss: 4.138017654418945
Reconstruction Loss: -0.9795945286750793
Iteration 10301:
Training Loss: 4.165692329406738
Reconstruction Loss: -0.9941745400428772
Iteration 10351:
Training Loss: 4.1144256591796875
Reconstruction Loss: -1.0055437088012695
Iteration 10401:
Training Loss: 4.182862281799316
Reconstruction Loss: -0.9873799085617065
Iteration 10451:
Training Loss: 4.179952144622803
Reconstruction Loss: -0.9782437086105347
Iteration 10501:
Training Loss: 4.102257251739502
Reconstruction Loss: -0.9920104742050171
Iteration 10551:
Training Loss: 4.107456684112549
Reconstruction Loss: -1.0000050067901611
Iteration 10601:
Training Loss: 4.076265335083008
Reconstruction Loss: -1.0015642642974854
Iteration 10651:
Training Loss: 3.8982303142547607
Reconstruction Loss: -1.002448320388794
Iteration 10701:
Training Loss: 4.173369884490967
Reconstruction Loss: -0.991370677947998
Iteration 10751:
Training Loss: 4.010899543762207
Reconstruction Loss: -0.980686366558075
Iteration 10801:
Training Loss: 4.077176570892334
Reconstruction Loss: -0.9992878437042236
Iteration 10851:
Training Loss: 4.06383752822876
Reconstruction Loss: -0.9755971431732178
Iteration 10901:
Training Loss: 4.208704471588135
Reconstruction Loss: -0.9890490770339966
Iteration 10951:
Training Loss: 4.259479522705078
Reconstruction Loss: -0.9775005578994751
Iteration 11001:
Training Loss: 4.032314300537109
Reconstruction Loss: -1.0005037784576416
Iteration 11051:
Training Loss: 3.988295555114746
Reconstruction Loss: -0.9867197275161743
Iteration 11101:
Training Loss: 4.1117377281188965
Reconstruction Loss: -0.9952551126480103
Iteration 11151:
Training Loss: 4.093496322631836
Reconstruction Loss: -0.9928248524665833
Iteration 11201:
Training Loss: 4.062466621398926
Reconstruction Loss: -1.0028538703918457
Iteration 11251:
Training Loss: 4.036765098571777
Reconstruction Loss: -0.9901745915412903
Iteration 11301:
Training Loss: 4.163618564605713
Reconstruction Loss: -0.988219141960144
Iteration 11351:
Training Loss: 4.184882640838623
Reconstruction Loss: -0.9914012551307678
Iteration 11401:
Training Loss: 3.9633519649505615
Reconstruction Loss: -1.0012636184692383
Iteration 11451:
Training Loss: 4.103574752807617
Reconstruction Loss: -1.0002553462982178
Iteration 11501:
Training Loss: 4.005527019500732
Reconstruction Loss: -1.0008294582366943
Iteration 11551:
Training Loss: 4.1038665771484375
Reconstruction Loss: -0.995995283126831
Iteration 11601:
Training Loss: 4.0550856590271
Reconstruction Loss: -1.0037214756011963
Iteration 11651:
Training Loss: 3.9520151615142822
Reconstruction Loss: -0.9973515272140503
Iteration 11701:
Training Loss: 4.056358814239502
Reconstruction Loss: -0.9912755489349365
Iteration 11751:
Training Loss: 4.131259441375732
Reconstruction Loss: -0.9989132881164551
Iteration 11801:
Training Loss: 4.0762481689453125
Reconstruction Loss: -1.004047155380249
Iteration 11851:
Training Loss: 3.748152732849121
Reconstruction Loss: -1.1534860134124756
Iteration 11901:
Training Loss: 3.3029568195343018
Reconstruction Loss: -1.248626470565796
Iteration 11951:
Training Loss: 3.4709315299987793
Reconstruction Loss: -1.2426258325576782
Iteration 12001:
Training Loss: 3.459489583969116
Reconstruction Loss: -1.2390369176864624
Iteration 12051:
Training Loss: 3.3735251426696777
Reconstruction Loss: -1.2330411672592163
Iteration 12101:
Training Loss: 3.3982980251312256
Reconstruction Loss: -1.2313628196716309
Iteration 12151:
Training Loss: 3.3655385971069336
Reconstruction Loss: -1.2377495765686035
Iteration 12201:
Training Loss: 3.3327741622924805
Reconstruction Loss: -1.2402273416519165
Iteration 12251:
Training Loss: 3.355468988418579
Reconstruction Loss: -1.227904200553894
Iteration 12301:
Training Loss: 3.3017027378082275
Reconstruction Loss: -1.21689772605896
Iteration 12351:
Training Loss: 3.3236844539642334
Reconstruction Loss: -1.2243272066116333
Iteration 12401:
Training Loss: 3.442352294921875
Reconstruction Loss: -1.220845341682434
Iteration 12451:
Training Loss: 3.351318359375
Reconstruction Loss: -1.2186040878295898
Iteration 12501:
Training Loss: 3.461425304412842
Reconstruction Loss: -1.226791262626648
Iteration 12551:
Training Loss: 3.42642879486084
Reconstruction Loss: -1.2187564373016357
Iteration 12601:
Training Loss: 3.3374853134155273
Reconstruction Loss: -1.2324063777923584
Iteration 12651:
Training Loss: 3.4190568923950195
Reconstruction Loss: -1.2210477590560913
Iteration 12701:
Training Loss: 3.2968201637268066
Reconstruction Loss: -1.2152376174926758
Iteration 12751:
Training Loss: 3.4278876781463623
Reconstruction Loss: -1.2173397541046143
Iteration 12801:
Training Loss: 3.373328924179077
Reconstruction Loss: -1.2173607349395752
Iteration 12851:
Training Loss: 3.325507164001465
Reconstruction Loss: -1.2095204591751099
Iteration 12901:
Training Loss: 3.402848720550537
Reconstruction Loss: -1.2237704992294312
Iteration 12951:
Training Loss: 3.4264473915100098
Reconstruction Loss: -1.2142713069915771
Iteration 13001:
Training Loss: 3.312260389328003
Reconstruction Loss: -1.2170802354812622
Iteration 13051:
Training Loss: 3.405881881713867
Reconstruction Loss: -1.2354391813278198
Iteration 13101:
Training Loss: 3.415182113647461
Reconstruction Loss: -1.2263712882995605
Iteration 13151:
Training Loss: 3.4638776779174805
Reconstruction Loss: -1.1975113153457642
Iteration 13201:
Training Loss: 3.314545154571533
Reconstruction Loss: -1.2118186950683594
Iteration 13251:
Training Loss: 3.409334659576416
Reconstruction Loss: -1.223204255104065
Iteration 13301:
Training Loss: 3.4621095657348633
Reconstruction Loss: -1.2178694009780884
Iteration 13351:
Training Loss: 3.36378812789917
Reconstruction Loss: -1.2102713584899902
Iteration 13401:
Training Loss: 3.411257266998291
Reconstruction Loss: -1.2264213562011719
Iteration 13451:
Training Loss: 3.488032579421997
Reconstruction Loss: -1.222723126411438
Iteration 13501:
Training Loss: 3.361443281173706
Reconstruction Loss: -1.2149698734283447
Iteration 13551:
Training Loss: 3.228203058242798
Reconstruction Loss: -1.217861294746399
Iteration 13601:
Training Loss: 3.300523042678833
Reconstruction Loss: -1.2110580205917358
Iteration 13651:
Training Loss: 3.5072333812713623
Reconstruction Loss: -1.2255316972732544
Iteration 13701:
Training Loss: 3.307246685028076
Reconstruction Loss: -1.2185311317443848
Iteration 13751:
Training Loss: 3.344456911087036
Reconstruction Loss: -1.214202880859375
Iteration 13801:
Training Loss: 3.43977952003479
Reconstruction Loss: -1.2258464097976685
Iteration 13851:
Training Loss: 3.515007972717285
Reconstruction Loss: -1.212764859199524
Iteration 13901:
Training Loss: 3.3525919914245605
Reconstruction Loss: -1.2125903367996216
Iteration 13951:
Training Loss: 3.338438034057617
Reconstruction Loss: -1.217901349067688
Iteration 14001:
Training Loss: 3.3679251670837402
Reconstruction Loss: -1.2149964570999146
Iteration 14051:
Training Loss: 3.274005174636841
Reconstruction Loss: -1.2132487297058105
Iteration 14101:
Training Loss: 3.383152723312378
Reconstruction Loss: -1.2104225158691406
Iteration 14151:
Training Loss: 3.44812273979187
Reconstruction Loss: -1.2172811031341553
Iteration 14201:
Training Loss: 3.286458730697632
Reconstruction Loss: -1.2288399934768677
Iteration 14251:
Training Loss: 3.3126795291900635
Reconstruction Loss: -1.2189061641693115
Iteration 14301:
Training Loss: 3.388723373413086
Reconstruction Loss: -1.2284305095672607
Iteration 14351:
Training Loss: 3.293342113494873
Reconstruction Loss: -1.220412015914917
Iteration 14401:
Training Loss: 3.3443689346313477
Reconstruction Loss: -1.2082957029342651
Iteration 14451:
Training Loss: 3.357078790664673
Reconstruction Loss: -1.2266597747802734
Iteration 14501:
Training Loss: 3.361241102218628
Reconstruction Loss: -1.2248585224151611
Iteration 14551:
Training Loss: 3.433333396911621
Reconstruction Loss: -1.2274888753890991
Iteration 14601:
Training Loss: 3.3535654544830322
Reconstruction Loss: -1.2385228872299194
Iteration 14651:
Training Loss: 3.32038950920105
Reconstruction Loss: -1.250640869140625
Iteration 14701:
Training Loss: 3.1039223670959473
Reconstruction Loss: -1.3255867958068848
Iteration 14751:
Training Loss: 2.9113898277282715
Reconstruction Loss: -1.5112926959991455
Iteration 14801:
Training Loss: 2.691460132598877
Reconstruction Loss: -1.6322687864303589
Iteration 14851:
Training Loss: 2.7624895572662354
Reconstruction Loss: -1.7072654962539673
Iteration 14901:
Training Loss: 2.6028661727905273
Reconstruction Loss: -1.7507621049880981
Iteration 14951:
Training Loss: 2.7063381671905518
Reconstruction Loss: -1.7762728929519653
Iteration 15001:
Training Loss: 2.6231532096862793
Reconstruction Loss: -1.7858190536499023
Iteration 15051:
Training Loss: 2.7318150997161865
Reconstruction Loss: -1.7898602485656738
Iteration 15101:
Training Loss: 2.7844479084014893
Reconstruction Loss: -1.7988258600234985
Iteration 15151:
Training Loss: 2.5507924556732178
Reconstruction Loss: -1.7815921306610107
Iteration 15201:
Training Loss: 2.65293025970459
Reconstruction Loss: -1.7878953218460083
Iteration 15251:
Training Loss: 2.78277850151062
Reconstruction Loss: -1.7841838598251343
Iteration 15301:
Training Loss: 2.7031350135803223
Reconstruction Loss: -1.7980237007141113
Iteration 15351:
Training Loss: 2.44545316696167
Reconstruction Loss: -1.7699182033538818
Iteration 15401:
Training Loss: 2.6461434364318848
Reconstruction Loss: -1.7820086479187012
Iteration 15451:
Training Loss: 2.7660491466522217
Reconstruction Loss: -1.7907171249389648
Iteration 15501:
Training Loss: 2.5808639526367188
Reconstruction Loss: -1.7760852575302124
Iteration 15551:
Training Loss: 2.517171859741211
Reconstruction Loss: -1.7789498567581177
Iteration 15601:
Training Loss: 2.719729423522949
Reconstruction Loss: -1.7724632024765015
Iteration 15651:
Training Loss: 2.750854253768921
Reconstruction Loss: -1.792830467224121
Iteration 15701:
Training Loss: 2.631866931915283
Reconstruction Loss: -1.772355079650879
Iteration 15751:
Training Loss: 2.6522939205169678
Reconstruction Loss: -1.7833647727966309
Iteration 15801:
Training Loss: 2.766275644302368
Reconstruction Loss: -1.782214641571045
Iteration 15851:
Training Loss: 2.6118242740631104
Reconstruction Loss: -1.7729566097259521
Iteration 15901:
Training Loss: 2.5607686042785645
Reconstruction Loss: -1.7783080339431763
Iteration 15951:
Training Loss: 2.768026828765869
Reconstruction Loss: -1.78744637966156
Iteration 16001:
Training Loss: 2.7187719345092773
Reconstruction Loss: -1.7831716537475586
Iteration 16051:
Training Loss: 2.660489559173584
Reconstruction Loss: -1.7794846296310425
Iteration 16101:
Training Loss: 2.780362129211426
Reconstruction Loss: -1.7779597043991089
Iteration 16151:
Training Loss: 2.6074531078338623
Reconstruction Loss: -1.7752752304077148
Iteration 16201:
Training Loss: 2.6329925060272217
Reconstruction Loss: -1.7670434713363647
Iteration 16251:
Training Loss: 2.5794050693511963
Reconstruction Loss: -1.7807607650756836
Iteration 16301:
Training Loss: 2.661102294921875
Reconstruction Loss: -1.779812216758728
Iteration 16351:
Training Loss: 2.6873512268066406
Reconstruction Loss: -1.7706090211868286
Iteration 16401:
Training Loss: 2.6277787685394287
Reconstruction Loss: -1.7632092237472534
Iteration 16451:
Training Loss: 2.7568821907043457
Reconstruction Loss: -1.7823240756988525
Iteration 16501:
Training Loss: 2.592146158218384
Reconstruction Loss: -1.774523377418518
Iteration 16551:
Training Loss: 2.598705291748047
Reconstruction Loss: -1.7761573791503906
Iteration 16601:
Training Loss: 2.4954490661621094
Reconstruction Loss: -1.783297061920166
Iteration 16651:
Training Loss: 2.506723403930664
Reconstruction Loss: -1.7807612419128418
Iteration 16701:
Training Loss: 2.828219175338745
Reconstruction Loss: -1.7850970029830933
Iteration 16751:
Training Loss: 2.597851276397705
Reconstruction Loss: -1.7764840126037598
Iteration 16801:
Training Loss: 2.62149977684021
Reconstruction Loss: -1.7793816328048706
Iteration 16851:
Training Loss: 2.636486530303955
Reconstruction Loss: -1.776139259338379
Iteration 16901:
Training Loss: 2.5660321712493896
Reconstruction Loss: -1.7827675342559814
Iteration 16951:
Training Loss: 2.610438346862793
Reconstruction Loss: -1.7750462293624878
Iteration 17001:
Training Loss: 2.45686411857605
Reconstruction Loss: -1.7799720764160156
Iteration 17051:
Training Loss: 2.655710220336914
Reconstruction Loss: -1.7700698375701904
Iteration 17101:
Training Loss: 2.6268208026885986
Reconstruction Loss: -1.7753750085830688
Iteration 17151:
Training Loss: 2.6090173721313477
Reconstruction Loss: -1.7606897354125977
Iteration 17201:
Training Loss: 2.675673484802246
Reconstruction Loss: -1.781651258468628
Iteration 17251:
Training Loss: 2.640899658203125
Reconstruction Loss: -1.7706232070922852
Iteration 17301:
Training Loss: 2.541208505630493
Reconstruction Loss: -1.7739057540893555
Iteration 17351:
Training Loss: 2.488901376724243
Reconstruction Loss: -1.7777127027511597
Iteration 17401:
Training Loss: 2.4831976890563965
Reconstruction Loss: -1.7853221893310547
Iteration 17451:
Training Loss: 2.776685953140259
Reconstruction Loss: -1.7704310417175293
Iteration 17501:
Training Loss: 2.7038352489471436
Reconstruction Loss: -1.7736806869506836
Iteration 17551:
Training Loss: 2.542264461517334
Reconstruction Loss: -1.7684862613677979
Iteration 17601:
Training Loss: 2.7573890686035156
Reconstruction Loss: -1.7767269611358643
Iteration 17651:
Training Loss: 2.5312726497650146
Reconstruction Loss: -1.7808053493499756
Iteration 17701:
Training Loss: 2.7040510177612305
Reconstruction Loss: -1.7721688747406006
Iteration 17751:
Training Loss: 2.6162939071655273
Reconstruction Loss: -1.779680848121643
Iteration 17801:
Training Loss: 2.770794153213501
Reconstruction Loss: -1.7841832637786865
Iteration 17851:
Training Loss: 2.7541167736053467
Reconstruction Loss: -1.7783209085464478
Iteration 17901:
Training Loss: 2.6074130535125732
Reconstruction Loss: -1.7753844261169434
Iteration 17951:
Training Loss: 2.5332460403442383
Reconstruction Loss: -1.7760469913482666
Iteration 18001:
Training Loss: 2.527390718460083
Reconstruction Loss: -1.7760273218154907
Iteration 18051:
Training Loss: 2.7815515995025635
Reconstruction Loss: -1.7720205783843994
Iteration 18101:
Training Loss: 2.636286735534668
Reconstruction Loss: -1.7710328102111816
Iteration 18151:
Training Loss: 2.6657276153564453
Reconstruction Loss: -1.770075798034668
Iteration 18201:
Training Loss: 2.622896194458008
Reconstruction Loss: -1.7730339765548706
Iteration 18251:
Training Loss: 2.511749744415283
Reconstruction Loss: -1.77846360206604
Iteration 18301:
Training Loss: 2.517230987548828
Reconstruction Loss: -1.7782068252563477
Iteration 18351:
Training Loss: 2.618264675140381
Reconstruction Loss: -1.7743314504623413
Iteration 18401:
Training Loss: 2.701443910598755
Reconstruction Loss: -1.7817816734313965
Iteration 18451:
Training Loss: 2.660248041152954
Reconstruction Loss: -1.7758492231369019
Iteration 18501:
Training Loss: 2.653426170349121
Reconstruction Loss: -1.7817145586013794
Iteration 18551:
Training Loss: 2.5378499031066895
Reconstruction Loss: -1.7741682529449463
Iteration 18601:
Training Loss: 2.6585726737976074
Reconstruction Loss: -1.7677273750305176
Iteration 18651:
Training Loss: 2.7185287475585938
Reconstruction Loss: -1.7736021280288696
Iteration 18701:
Training Loss: 2.5506272315979004
Reconstruction Loss: -1.7824831008911133
Iteration 18751:
Training Loss: 2.539058208465576
Reconstruction Loss: -1.7740342617034912
Iteration 18801:
Training Loss: 2.5281524658203125
Reconstruction Loss: -1.774902582168579
Iteration 18851:
Training Loss: 2.4684062004089355
Reconstruction Loss: -1.778363585472107
Iteration 18901:
Training Loss: 2.6303458213806152
Reconstruction Loss: -1.7811768054962158
Iteration 18951:
Training Loss: 2.604846954345703
Reconstruction Loss: -1.7848970890045166
Iteration 19001:
Training Loss: 2.6582682132720947
Reconstruction Loss: -1.7751859426498413
Iteration 19051:
Training Loss: 2.662961959838867
Reconstruction Loss: -1.7785155773162842
Iteration 19101:
Training Loss: 2.589465618133545
Reconstruction Loss: -1.7762725353240967
Iteration 19151:
Training Loss: 2.4701404571533203
Reconstruction Loss: -1.7745928764343262
Iteration 19201:
Training Loss: 2.657250165939331
Reconstruction Loss: -1.7647954225540161
Iteration 19251:
Training Loss: 2.627668857574463
Reconstruction Loss: -1.77970552444458
Iteration 19301:
Training Loss: 2.503401279449463
Reconstruction Loss: -1.776794195175171
Iteration 19351:
Training Loss: 2.654479742050171
Reconstruction Loss: -1.7810934782028198
Iteration 19401:
Training Loss: 2.6215224266052246
Reconstruction Loss: -1.780527114868164
Iteration 19451:
Training Loss: 2.7127315998077393
Reconstruction Loss: -1.7812215089797974
Iteration 19501:
Training Loss: 2.608393669128418
Reconstruction Loss: -1.774276852607727
Iteration 19551:
Training Loss: 2.6353585720062256
Reconstruction Loss: -1.7778029441833496
Iteration 19601:
Training Loss: 2.536712884902954
Reconstruction Loss: -1.7733269929885864
Iteration 19651:
Training Loss: 2.7361323833465576
Reconstruction Loss: -1.7708157300949097
Iteration 19701:
Training Loss: 2.7259647846221924
Reconstruction Loss: -1.782551646232605
Iteration 19751:
Training Loss: 2.586606025695801
Reconstruction Loss: -1.7920383214950562
Iteration 19801:
Training Loss: 2.647463798522949
Reconstruction Loss: -1.7752947807312012
Iteration 19851:
Training Loss: 2.546962261199951
Reconstruction Loss: -1.7823549509048462
Iteration 19901:
Training Loss: 2.560821533203125
Reconstruction Loss: -1.7751455307006836
Iteration 19951:
Training Loss: 2.661142587661743
Reconstruction Loss: -1.7795438766479492
Iteration 20001:
Training Loss: 2.730954885482788
Reconstruction Loss: -1.774030327796936
Iteration 20051:
Training Loss: 2.675657272338867
Reconstruction Loss: -1.7733685970306396
Iteration 20101:
Training Loss: 2.5726821422576904
Reconstruction Loss: -1.7777681350708008
Iteration 20151:
Training Loss: 2.6350653171539307
Reconstruction Loss: -1.7812007665634155
Iteration 20201:
Training Loss: 2.669102668762207
Reconstruction Loss: -1.7747509479522705
Iteration 20251:
Training Loss: 2.6847259998321533
Reconstruction Loss: -1.7751151323318481
Iteration 20301:
Training Loss: 2.6046793460845947
Reconstruction Loss: -1.7834125757217407
Iteration 20351:
Training Loss: 2.509507179260254
Reconstruction Loss: -1.776728630065918
Iteration 20401:
Training Loss: 2.672609567642212
Reconstruction Loss: -1.7740519046783447
Iteration 20451:
Training Loss: 2.5290770530700684
Reconstruction Loss: -1.7732492685317993
Iteration 20501:
Training Loss: 2.5299582481384277
Reconstruction Loss: -1.7757835388183594
Iteration 20551:
Training Loss: 2.500967502593994
Reconstruction Loss: -1.7859491109848022
Iteration 20601:
Training Loss: 2.6703786849975586
Reconstruction Loss: -1.7729682922363281
Iteration 20651:
Training Loss: 2.6401467323303223
Reconstruction Loss: -1.7864112854003906
Iteration 20701:
Training Loss: 2.728543758392334
Reconstruction Loss: -1.7790720462799072
Iteration 20751:
Training Loss: 2.6958749294281006
Reconstruction Loss: -1.774408221244812
Iteration 20801:
Training Loss: 2.5230772495269775
Reconstruction Loss: -1.7726497650146484
Iteration 20851:
Training Loss: 2.5294957160949707
Reconstruction Loss: -1.779860258102417
Iteration 20901:
Training Loss: 2.7346813678741455
Reconstruction Loss: -1.7743419408798218
Iteration 20951:
Training Loss: 2.6292803287506104
Reconstruction Loss: -1.7708783149719238
Iteration 21001:
Training Loss: 2.5771644115448
Reconstruction Loss: -1.7766013145446777
Iteration 21051:
Training Loss: 2.659658193588257
Reconstruction Loss: -1.7824081182479858
Iteration 21101:
Training Loss: 2.608503818511963
Reconstruction Loss: -1.7645485401153564
Iteration 21151:
Training Loss: 2.5744762420654297
Reconstruction Loss: -1.780597448348999
Iteration 21201:
Training Loss: 2.570213794708252
Reconstruction Loss: -1.7727735042572021
Iteration 21251:
Training Loss: 2.5669894218444824
Reconstruction Loss: -1.771237850189209
Iteration 21301:
Training Loss: 2.6280605792999268
Reconstruction Loss: -1.7769049406051636
Iteration 21351:
Training Loss: 2.4631118774414062
Reconstruction Loss: -1.788920521736145
Iteration 21401:
Training Loss: 2.415820360183716
Reconstruction Loss: -1.7716562747955322
Iteration 21451:
Training Loss: 2.6750848293304443
Reconstruction Loss: -1.777023196220398
Iteration 21501:
Training Loss: 2.5257816314697266
Reconstruction Loss: -1.7753353118896484
Iteration 21551:
Training Loss: 2.663986921310425
Reconstruction Loss: -1.7655811309814453
Iteration 21601:
Training Loss: 2.745697498321533
Reconstruction Loss: -1.781676173210144
Iteration 21651:
Training Loss: 2.6584055423736572
Reconstruction Loss: -1.7692044973373413
Iteration 21701:
Training Loss: 2.6132147312164307
Reconstruction Loss: -1.784806728363037
Iteration 21751:
Training Loss: 2.515185594558716
Reconstruction Loss: -1.7757221460342407
Iteration 21801:
Training Loss: 2.6741554737091064
Reconstruction Loss: -1.780653953552246
Iteration 21851:
Training Loss: 2.5294227600097656
Reconstruction Loss: -1.777278184890747
Iteration 21901:
Training Loss: 2.6263344287872314
Reconstruction Loss: -1.7769672870635986
Iteration 21951:
Training Loss: 2.601663112640381
Reconstruction Loss: -1.778933048248291
Iteration 22001:
Training Loss: 2.6468145847320557
Reconstruction Loss: -1.7765510082244873
Iteration 22051:
Training Loss: 2.6549465656280518
Reconstruction Loss: -1.7746498584747314
Iteration 22101:
Training Loss: 2.6354260444641113
Reconstruction Loss: -1.7775344848632812
Iteration 22151:
Training Loss: 2.713603973388672
Reconstruction Loss: -1.7810479402542114
Iteration 22201:
Training Loss: 2.7542502880096436
Reconstruction Loss: -1.7716598510742188
Iteration 22251:
Training Loss: 2.382498264312744
Reconstruction Loss: -1.770082712173462
Iteration 22301:
Training Loss: 2.6168787479400635
Reconstruction Loss: -1.7695045471191406
Iteration 22351:
Training Loss: 2.6531829833984375
Reconstruction Loss: -1.7858901023864746
Iteration 22401:
Training Loss: 2.702181100845337
Reconstruction Loss: -1.7803407907485962
Iteration 22451:
Training Loss: 2.844965696334839
Reconstruction Loss: -1.7747448682785034
Iteration 22501:
Training Loss: 2.7946033477783203
Reconstruction Loss: -1.779442310333252
Iteration 22551:
Training Loss: 2.6610167026519775
Reconstruction Loss: -1.775002121925354
Iteration 22601:
Training Loss: 2.566833972930908
Reconstruction Loss: -1.7848374843597412
Iteration 22651:
Training Loss: 2.5973219871520996
Reconstruction Loss: -1.7672874927520752
Iteration 22701:
Training Loss: 2.5450439453125
Reconstruction Loss: -1.7838772535324097
Iteration 22751:
Training Loss: 2.6772470474243164
Reconstruction Loss: -1.7892353534698486
Iteration 22801:
Training Loss: 2.652883291244507
Reconstruction Loss: -1.7689472436904907
Iteration 22851:
Training Loss: 2.5555224418640137
Reconstruction Loss: -1.7695008516311646
Iteration 22901:
Training Loss: 2.7557857036590576
Reconstruction Loss: -1.780032992362976
Iteration 22951:
Training Loss: 2.4788284301757812
Reconstruction Loss: -1.7721765041351318
Iteration 23001:
Training Loss: 2.4404404163360596
Reconstruction Loss: -1.770124912261963
Iteration 23051:
Training Loss: 2.613908052444458
Reconstruction Loss: -1.7696844339370728
Iteration 23101:
Training Loss: 2.686286449432373
Reconstruction Loss: -1.7787764072418213
Iteration 23151:
Training Loss: 2.6010916233062744
Reconstruction Loss: -1.7790963649749756
Iteration 23201:
Training Loss: 2.7746331691741943
Reconstruction Loss: -1.7819759845733643
Iteration 23251:
Training Loss: 2.6156206130981445
Reconstruction Loss: -1.7663326263427734
Iteration 23301:
Training Loss: 2.7049062252044678
Reconstruction Loss: -1.7742106914520264
Iteration 23351:
Training Loss: 2.580432891845703
Reconstruction Loss: -1.7757796049118042
Iteration 23401:
Training Loss: 2.588995933532715
Reconstruction Loss: -1.76985764503479
Iteration 23451:
Training Loss: 2.624549150466919
Reconstruction Loss: -1.7791991233825684
Iteration 23501:
Training Loss: 2.779858112335205
Reconstruction Loss: -1.7809333801269531
Iteration 23551:
Training Loss: 2.665250301361084
Reconstruction Loss: -1.76799476146698
Iteration 23601:
Training Loss: 2.6549160480499268
Reconstruction Loss: -1.7793123722076416
Iteration 23651:
Training Loss: 2.4953978061676025
Reconstruction Loss: -1.7832627296447754
Iteration 23701:
Training Loss: 2.474484920501709
Reconstruction Loss: -1.778315782546997
Iteration 23751:
Training Loss: 2.7163639068603516
Reconstruction Loss: -1.7804250717163086
Iteration 23801:
Training Loss: 2.6022064685821533
Reconstruction Loss: -1.7635399103164673
Iteration 23851:
Training Loss: 2.644033193588257
Reconstruction Loss: -1.775254726409912
Iteration 23901:
Training Loss: 2.7713656425476074
Reconstruction Loss: -1.7810465097427368
Iteration 23951:
Training Loss: 2.6516106128692627
Reconstruction Loss: -1.7795896530151367
Iteration 24001:
Training Loss: 2.791614294052124
Reconstruction Loss: -1.7827351093292236
Iteration 24051:
Training Loss: 2.6996653079986572
Reconstruction Loss: -1.7729521989822388
Iteration 24101:
Training Loss: 2.6634557247161865
Reconstruction Loss: -1.78578782081604
Iteration 24151:
Training Loss: 2.6957716941833496
Reconstruction Loss: -1.7694145441055298
Iteration 24201:
Training Loss: 2.5537633895874023
Reconstruction Loss: -1.7731974124908447
Iteration 24251:
Training Loss: 2.575479030609131
Reconstruction Loss: -1.779428482055664
Iteration 24301:
Training Loss: 2.650178909301758
Reconstruction Loss: -1.7680293321609497
Iteration 24351:
Training Loss: 2.6821365356445312
Reconstruction Loss: -1.7779936790466309
Iteration 24401:
Training Loss: 2.671873092651367
Reconstruction Loss: -1.7789405584335327
Iteration 24451:
Training Loss: 2.5453805923461914
Reconstruction Loss: -1.7809553146362305
Iteration 24501:
Training Loss: 2.6145832538604736
Reconstruction Loss: -1.7720322608947754
Iteration 24551:
Training Loss: 2.70279598236084
Reconstruction Loss: -1.7750866413116455
Iteration 24601:
Training Loss: 2.7370173931121826
Reconstruction Loss: -1.7732936143875122
Iteration 24651:
Training Loss: 2.475985527038574
Reconstruction Loss: -1.7824885845184326
Iteration 24701:
Training Loss: 2.6598899364471436
Reconstruction Loss: -1.7782115936279297
Iteration 24751:
Training Loss: 2.6954870223999023
Reconstruction Loss: -1.7729214429855347
Iteration 24801:
Training Loss: 2.6547768115997314
Reconstruction Loss: -1.7745749950408936
Iteration 24851:
Training Loss: 2.7422542572021484
Reconstruction Loss: -1.7876070737838745
Iteration 24901:
Training Loss: 2.5867342948913574
Reconstruction Loss: -1.778592586517334
Iteration 24951:
Training Loss: 2.7033581733703613
Reconstruction Loss: -1.7732738256454468
