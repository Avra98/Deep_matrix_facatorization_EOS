5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.723267555236816
Reconstruction Loss: -0.4682890772819519
Iteration 51:
Training Loss: 4.650753974914551
Reconstruction Loss: -0.778779149055481
Iteration 101:
Training Loss: 3.5235183238983154
Reconstruction Loss: -1.402146816253662
Iteration 151:
Training Loss: 2.4396555423736572
Reconstruction Loss: -1.9695128202438354
Iteration 201:
Training Loss: 1.3556041717529297
Reconstruction Loss: -2.757979393005371
Iteration 251:
Training Loss: 0.4364297091960907
Reconstruction Loss: -3.4547557830810547
Iteration 301:
Training Loss: -0.35397282242774963
Reconstruction Loss: -3.9942665100097656
Iteration 351:
Training Loss: -0.8659809231758118
Reconstruction Loss: -4.425617218017578
Iteration 401:
Training Loss: -1.354079246520996
Reconstruction Loss: -4.774051189422607
Iteration 451:
Training Loss: -1.5465507507324219
Reconstruction Loss: -5.056393623352051
Iteration 501:
Training Loss: -1.860972285270691
Reconstruction Loss: -5.287128925323486
Iteration 551:
Training Loss: -2.160823345184326
Reconstruction Loss: -5.475461006164551
Iteration 601:
Training Loss: -2.2595620155334473
Reconstruction Loss: -5.633777141571045
Iteration 651:
Training Loss: -2.3477039337158203
Reconstruction Loss: -5.768928527832031
Iteration 701:
Training Loss: -2.671569347381592
Reconstruction Loss: -5.885275840759277
Iteration 751:
Training Loss: -2.7382524013519287
Reconstruction Loss: -5.982837677001953
Iteration 801:
Training Loss: -2.83837628364563
Reconstruction Loss: -6.07216215133667
Iteration 851:
Training Loss: -2.811483144760132
Reconstruction Loss: -6.149523735046387
Iteration 901:
Training Loss: -2.854741096496582
Reconstruction Loss: -6.219083786010742
Iteration 951:
Training Loss: -3.0658392906188965
Reconstruction Loss: -6.283499717712402
Iteration 1001:
Training Loss: -3.0908498764038086
Reconstruction Loss: -6.340229511260986
Iteration 1051:
Training Loss: -3.2280642986297607
Reconstruction Loss: -6.3939385414123535
Iteration 1101:
Training Loss: -3.1452600955963135
Reconstruction Loss: -6.442826271057129
Iteration 1151:
Training Loss: -3.202719211578369
Reconstruction Loss: -6.487766265869141
Iteration 1201:
Training Loss: -3.3971564769744873
Reconstruction Loss: -6.529816627502441
Iteration 1251:
Training Loss: -3.4755067825317383
Reconstruction Loss: -6.570195198059082
Iteration 1301:
Training Loss: -3.4827609062194824
Reconstruction Loss: -6.608834743499756
Iteration 1351:
Training Loss: -3.413027048110962
Reconstruction Loss: -6.643445014953613
Iteration 1401:
Training Loss: -3.5979630947113037
Reconstruction Loss: -6.676408767700195
Iteration 1451:
Training Loss: -3.702364921569824
Reconstruction Loss: -6.7087578773498535
Iteration 1501:
Training Loss: -3.752615213394165
Reconstruction Loss: -6.74080753326416
Iteration 1551:
Training Loss: -3.6457808017730713
Reconstruction Loss: -6.770670413970947
Iteration 1601:
Training Loss: -3.72516131401062
Reconstruction Loss: -6.797510147094727
Iteration 1651:
Training Loss: -3.786195755004883
Reconstruction Loss: -6.824932098388672
Iteration 1701:
Training Loss: -3.8138108253479004
Reconstruction Loss: -6.8523030281066895
Iteration 1751:
Training Loss: -3.7987239360809326
Reconstruction Loss: -6.876716136932373
Iteration 1801:
Training Loss: -3.838287353515625
Reconstruction Loss: -6.900424003601074
Iteration 1851:
Training Loss: -4.023985862731934
Reconstruction Loss: -6.922550201416016
Iteration 1901:
Training Loss: -4.050134658813477
Reconstruction Loss: -6.9452619552612305
Iteration 1951:
Training Loss: -4.027818202972412
Reconstruction Loss: -6.968759536743164
Iteration 2001:
Training Loss: -4.268619537353516
Reconstruction Loss: -6.989320755004883
Iteration 2051:
Training Loss: -3.9758193492889404
Reconstruction Loss: -7.009645462036133
Iteration 2101:
Training Loss: -4.148685455322266
Reconstruction Loss: -7.029003620147705
Iteration 2151:
Training Loss: -4.198523044586182
Reconstruction Loss: -7.049532413482666
Iteration 2201:
Training Loss: -4.212121486663818
Reconstruction Loss: -7.067622184753418
Iteration 2251:
Training Loss: -4.22004508972168
Reconstruction Loss: -7.0863165855407715
Iteration 2301:
Training Loss: -4.262777328491211
Reconstruction Loss: -7.10355281829834
Iteration 2351:
Training Loss: -4.158587455749512
Reconstruction Loss: -7.120851516723633
Iteration 2401:
Training Loss: -4.408291339874268
Reconstruction Loss: -7.13873291015625
Iteration 2451:
Training Loss: -4.341707229614258
Reconstruction Loss: -7.154506206512451
Iteration 2501:
Training Loss: -4.438920021057129
Reconstruction Loss: -7.171949863433838
Iteration 2551:
Training Loss: -4.531443119049072
Reconstruction Loss: -7.186608791351318
Iteration 2601:
Training Loss: -4.345507621765137
Reconstruction Loss: -7.202210903167725
Iteration 2651:
Training Loss: -4.454840183258057
Reconstruction Loss: -7.21917724609375
Iteration 2701:
Training Loss: -4.4861345291137695
Reconstruction Loss: -7.232449054718018
Iteration 2751:
Training Loss: -4.568657875061035
Reconstruction Loss: -7.246471881866455
Iteration 2801:
Training Loss: -4.587352752685547
Reconstruction Loss: -7.261268138885498
Iteration 2851:
Training Loss: -4.411337852478027
Reconstruction Loss: -7.274326324462891
Iteration 2901:
Training Loss: -4.592630386352539
Reconstruction Loss: -7.286848545074463
Iteration 2951:
Training Loss: -4.503885269165039
Reconstruction Loss: -7.301413536071777
Iteration 3001:
Training Loss: -4.5456037521362305
Reconstruction Loss: -7.314369201660156
Iteration 3051:
Training Loss: -4.618150234222412
Reconstruction Loss: -7.327882766723633
Iteration 3101:
Training Loss: -4.71111536026001
Reconstruction Loss: -7.3390116691589355
Iteration 3151:
Training Loss: -4.571604251861572
Reconstruction Loss: -7.351998805999756
Iteration 3201:
Training Loss: -4.806072235107422
Reconstruction Loss: -7.364074230194092
Iteration 3251:
Training Loss: -4.925983905792236
Reconstruction Loss: -7.376413345336914
Iteration 3301:
Training Loss: -4.734062671661377
Reconstruction Loss: -7.388453483581543
Iteration 3351:
Training Loss: -4.75045919418335
Reconstruction Loss: -7.4014458656311035
Iteration 3401:
Training Loss: -4.7945556640625
Reconstruction Loss: -7.410125255584717
Iteration 3451:
Training Loss: -4.760071754455566
Reconstruction Loss: -7.4233012199401855
Iteration 3501:
Training Loss: -4.842470645904541
Reconstruction Loss: -7.433520793914795
Iteration 3551:
Training Loss: -4.893964767456055
Reconstruction Loss: -7.4429826736450195
Iteration 3601:
Training Loss: -4.858276844024658
Reconstruction Loss: -7.454450607299805
Iteration 3651:
Training Loss: -4.866113662719727
Reconstruction Loss: -7.46505069732666
Iteration 3701:
Training Loss: -4.801061630249023
Reconstruction Loss: -7.475666522979736
Iteration 3751:
Training Loss: -4.947864532470703
Reconstruction Loss: -7.485659122467041
Iteration 3801:
Training Loss: -4.978122711181641
Reconstruction Loss: -7.495816230773926
Iteration 3851:
Training Loss: -5.01971435546875
Reconstruction Loss: -7.505919456481934
Iteration 3901:
Training Loss: -5.2223219871521
Reconstruction Loss: -7.514300346374512
Iteration 3951:
Training Loss: -4.8797149658203125
Reconstruction Loss: -7.523282527923584
Iteration 4001:
Training Loss: -4.996670722961426
Reconstruction Loss: -7.533912181854248
Iteration 4051:
Training Loss: -5.165860652923584
Reconstruction Loss: -7.542908668518066
Iteration 4101:
Training Loss: -4.96619176864624
Reconstruction Loss: -7.553267002105713
Iteration 4151:
Training Loss: -5.022204875946045
Reconstruction Loss: -7.561862468719482
Iteration 4201:
Training Loss: -5.1439924240112305
Reconstruction Loss: -7.5705156326293945
Iteration 4251:
Training Loss: -5.263600826263428
Reconstruction Loss: -7.579623222351074
Iteration 4301:
Training Loss: -5.025351047515869
Reconstruction Loss: -7.587255001068115
Iteration 4351:
Training Loss: -5.073745250701904
Reconstruction Loss: -7.596093654632568
Iteration 4401:
Training Loss: -5.209320545196533
Reconstruction Loss: -7.6049485206604
Iteration 4451:
Training Loss: -5.141014575958252
Reconstruction Loss: -7.612233638763428
Iteration 4501:
Training Loss: -5.064932823181152
Reconstruction Loss: -7.620485305786133
Iteration 4551:
Training Loss: -5.2975850105285645
Reconstruction Loss: -7.629000663757324
Iteration 4601:
Training Loss: -5.17783260345459
Reconstruction Loss: -7.636130332946777
Iteration 4651:
Training Loss: -5.181581974029541
Reconstruction Loss: -7.644117832183838
Iteration 4701:
Training Loss: -5.145982265472412
Reconstruction Loss: -7.653464317321777
Iteration 4751:
Training Loss: -5.136404991149902
Reconstruction Loss: -7.660918712615967
Iteration 4801:
Training Loss: -5.218786716461182
Reconstruction Loss: -7.667962074279785
Iteration 4851:
Training Loss: -5.328783988952637
Reconstruction Loss: -7.675568103790283
Iteration 4901:
Training Loss: -5.34909200668335
Reconstruction Loss: -7.682092189788818
Iteration 4951:
Training Loss: -5.457008361816406
Reconstruction Loss: -7.6900811195373535
Iteration 5001:
Training Loss: -5.319716453552246
Reconstruction Loss: -7.696325302124023
Iteration 5051:
Training Loss: -5.4051690101623535
Reconstruction Loss: -7.703263759613037
Iteration 5101:
Training Loss: -5.408830642700195
Reconstruction Loss: -7.710813522338867
Iteration 5151:
Training Loss: -5.522113800048828
Reconstruction Loss: -7.717475891113281
Iteration 5201:
Training Loss: -5.308615684509277
Reconstruction Loss: -7.725146770477295
Iteration 5251:
Training Loss: -5.328352928161621
Reconstruction Loss: -7.731382369995117
Iteration 5301:
Training Loss: -5.414716720581055
Reconstruction Loss: -7.7380571365356445
Iteration 5351:
Training Loss: -5.3108811378479
Reconstruction Loss: -7.745233058929443
Iteration 5401:
Training Loss: -5.470454216003418
Reconstruction Loss: -7.752445697784424
Iteration 5451:
Training Loss: -5.379976272583008
Reconstruction Loss: -7.757942199707031
Iteration 5501:
Training Loss: -5.531673431396484
Reconstruction Loss: -7.764392852783203
Iteration 5551:
Training Loss: -5.500356674194336
Reconstruction Loss: -7.771430015563965
Iteration 5601:
Training Loss: -5.46541690826416
Reconstruction Loss: -7.777426719665527
Iteration 5651:
Training Loss: -5.502824306488037
Reconstruction Loss: -7.78253173828125
Iteration 5701:
Training Loss: -5.633824348449707
Reconstruction Loss: -7.789963722229004
Iteration 5751:
Training Loss: -5.445181846618652
Reconstruction Loss: -7.795440673828125
Iteration 5801:
Training Loss: -5.546881675720215
Reconstruction Loss: -7.800677299499512
Iteration 5851:
Training Loss: -5.578969478607178
Reconstruction Loss: -7.807352542877197
Iteration 5901:
Training Loss: -5.5634379386901855
Reconstruction Loss: -7.813587665557861
Iteration 5951:
Training Loss: -5.419296741485596
Reconstruction Loss: -7.818169593811035
Iteration 6001:
Training Loss: -5.497416019439697
Reconstruction Loss: -7.8249077796936035
Iteration 6051:
Training Loss: -5.598869800567627
Reconstruction Loss: -7.831536293029785
Iteration 6101:
Training Loss: -5.586627006530762
Reconstruction Loss: -7.835787296295166
Iteration 6151:
Training Loss: -5.566002368927002
Reconstruction Loss: -7.84169340133667
Iteration 6201:
Training Loss: -5.6529107093811035
Reconstruction Loss: -7.848400115966797
Iteration 6251:
Training Loss: -5.606269359588623
Reconstruction Loss: -7.853833198547363
Iteration 6301:
Training Loss: -5.762652397155762
Reconstruction Loss: -7.858499526977539
Iteration 6351:
Training Loss: -5.566415786743164
Reconstruction Loss: -7.864727020263672
Iteration 6401:
Training Loss: -5.745538711547852
Reconstruction Loss: -7.869515895843506
Iteration 6451:
Training Loss: -5.761498928070068
Reconstruction Loss: -7.874474048614502
Iteration 6501:
Training Loss: -5.654783725738525
Reconstruction Loss: -7.879874229431152
Iteration 6551:
Training Loss: -5.654803276062012
Reconstruction Loss: -7.885624885559082
Iteration 6601:
Training Loss: -5.818514347076416
Reconstruction Loss: -7.889415740966797
Iteration 6651:
Training Loss: -5.835395336151123
Reconstruction Loss: -7.895314693450928
Iteration 6701:
Training Loss: -5.730684757232666
Reconstruction Loss: -7.900590896606445
Iteration 6751:
Training Loss: -5.808529853820801
Reconstruction Loss: -7.905126094818115
Iteration 6801:
Training Loss: -5.803488254547119
Reconstruction Loss: -7.909303188323975
Iteration 6851:
Training Loss: -6.065977573394775
Reconstruction Loss: -7.915853023529053
Iteration 6901:
Training Loss: -5.7738213539123535
Reconstruction Loss: -7.919625759124756
Iteration 6951:
Training Loss: -5.886172294616699
Reconstruction Loss: -7.925379276275635
Iteration 7001:
Training Loss: -5.841870307922363
Reconstruction Loss: -7.929900646209717
Iteration 7051:
Training Loss: -5.929553985595703
Reconstruction Loss: -7.934711456298828
Iteration 7101:
Training Loss: -5.783188343048096
Reconstruction Loss: -7.940279006958008
Iteration 7151:
Training Loss: -5.834885597229004
Reconstruction Loss: -7.943548679351807
Iteration 7201:
Training Loss: -5.841268062591553
Reconstruction Loss: -7.947478771209717
Iteration 7251:
Training Loss: -5.864011287689209
Reconstruction Loss: -7.953201770782471
Iteration 7301:
Training Loss: -6.065042495727539
Reconstruction Loss: -7.95850944519043
Iteration 7351:
Training Loss: -5.888278007507324
Reconstruction Loss: -7.962294101715088
Iteration 7401:
Training Loss: -5.917943954467773
Reconstruction Loss: -7.966910362243652
Iteration 7451:
Training Loss: -5.772968292236328
Reconstruction Loss: -7.971158981323242
