5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.4199299812316895
Reconstruction Loss: -0.5611832737922668
Iteration 51:
Training Loss: 3.4683642387390137
Reconstruction Loss: -1.3071483373641968
Iteration 101:
Training Loss: 2.1859724521636963
Reconstruction Loss: -1.9393038749694824
Iteration 151:
Training Loss: 1.1120010614395142
Reconstruction Loss: -2.514364719390869
Iteration 201:
Training Loss: 0.15571777522563934
Reconstruction Loss: -3.017571210861206
Iteration 251:
Training Loss: -0.24394144117832184
Reconstruction Loss: -3.4096994400024414
Iteration 301:
Training Loss: -0.5570448637008667
Reconstruction Loss: -3.7090141773223877
Iteration 351:
Training Loss: -0.9724379777908325
Reconstruction Loss: -3.9451851844787598
Iteration 401:
Training Loss: -1.1545569896697998
Reconstruction Loss: -4.134602069854736
Iteration 451:
Training Loss: -1.50111985206604
Reconstruction Loss: -4.293304920196533
Iteration 501:
Training Loss: -1.5684266090393066
Reconstruction Loss: -4.426929950714111
Iteration 551:
Training Loss: -1.700719952583313
Reconstruction Loss: -4.541248321533203
Iteration 601:
Training Loss: -1.8192918300628662
Reconstruction Loss: -4.639814376831055
Iteration 651:
Training Loss: -2.2693145275115967
Reconstruction Loss: -4.727181911468506
Iteration 701:
Training Loss: -2.0788614749908447
Reconstruction Loss: -4.804037570953369
Iteration 751:
Training Loss: -2.2141706943511963
Reconstruction Loss: -4.875275135040283
Iteration 801:
Training Loss: -2.3506596088409424
Reconstruction Loss: -4.933430194854736
Iteration 851:
Training Loss: -2.5884971618652344
Reconstruction Loss: -4.992147922515869
Iteration 901:
Training Loss: -2.4986321926116943
Reconstruction Loss: -5.044951915740967
Iteration 951:
Training Loss: -2.548675060272217
Reconstruction Loss: -5.09190559387207
Iteration 1001:
Training Loss: -2.7863829135894775
Reconstruction Loss: -5.135215759277344
Iteration 1051:
Training Loss: -2.762449026107788
Reconstruction Loss: -5.175729751586914
Iteration 1101:
Training Loss: -2.982247829437256
Reconstruction Loss: -5.213903427124023
Iteration 1151:
Training Loss: -3.0857346057891846
Reconstruction Loss: -5.247870922088623
Iteration 1201:
Training Loss: -2.897250175476074
Reconstruction Loss: -5.284440994262695
Iteration 1251:
Training Loss: -3.094132423400879
Reconstruction Loss: -5.31329870223999
Iteration 1301:
Training Loss: -3.147508382797241
Reconstruction Loss: -5.342631816864014
Iteration 1351:
Training Loss: -3.230771780014038
Reconstruction Loss: -5.368957042694092
Iteration 1401:
Training Loss: -3.231623649597168
Reconstruction Loss: -5.397184371948242
Iteration 1451:
Training Loss: -3.431205987930298
Reconstruction Loss: -5.422692775726318
Iteration 1501:
Training Loss: -3.260341167449951
Reconstruction Loss: -5.4475202560424805
Iteration 1551:
Training Loss: -3.4202871322631836
Reconstruction Loss: -5.469578266143799
Iteration 1601:
Training Loss: -3.3263168334960938
Reconstruction Loss: -5.4918928146362305
Iteration 1651:
Training Loss: -3.3564577102661133
Reconstruction Loss: -5.512651443481445
Iteration 1701:
Training Loss: -3.5513081550598145
Reconstruction Loss: -5.532841682434082
Iteration 1751:
Training Loss: -3.5845625400543213
Reconstruction Loss: -5.552175998687744
Iteration 1801:
Training Loss: -3.6357357501983643
Reconstruction Loss: -5.570868968963623
Iteration 1851:
Training Loss: -3.576939821243286
Reconstruction Loss: -5.588974952697754
Iteration 1901:
Training Loss: -3.905914783477783
Reconstruction Loss: -5.605627059936523
Iteration 1951:
Training Loss: -3.748385429382324
Reconstruction Loss: -5.621845722198486
Iteration 2001:
Training Loss: -3.696826934814453
Reconstruction Loss: -5.637785911560059
Iteration 2051:
Training Loss: -3.6591875553131104
Reconstruction Loss: -5.65421724319458
Iteration 2101:
Training Loss: -3.6910831928253174
Reconstruction Loss: -5.668656826019287
Iteration 2151:
Training Loss: -3.7176403999328613
Reconstruction Loss: -5.683311939239502
Iteration 2201:
Training Loss: -3.968606472015381
Reconstruction Loss: -5.697335720062256
Iteration 2251:
Training Loss: -3.884613513946533
Reconstruction Loss: -5.711521148681641
Iteration 2301:
Training Loss: -3.9106106758117676
Reconstruction Loss: -5.723855495452881
Iteration 2351:
Training Loss: -3.9086971282958984
Reconstruction Loss: -5.738084316253662
Iteration 2401:
Training Loss: -4.009912490844727
Reconstruction Loss: -5.752162456512451
Iteration 2451:
Training Loss: -3.910344123840332
Reconstruction Loss: -5.763241291046143
Iteration 2501:
Training Loss: -4.030428886413574
Reconstruction Loss: -5.774949550628662
Iteration 2551:
Training Loss: -4.088644027709961
Reconstruction Loss: -5.787428379058838
Iteration 2601:
Training Loss: -4.036891937255859
Reconstruction Loss: -5.79948616027832
Iteration 2651:
Training Loss: -4.052932262420654
Reconstruction Loss: -5.810500621795654
Iteration 2701:
Training Loss: -4.127042293548584
Reconstruction Loss: -5.8198957443237305
Iteration 2751:
Training Loss: -4.3066582679748535
Reconstruction Loss: -5.831932544708252
Iteration 2801:
Training Loss: -4.070268154144287
Reconstruction Loss: -5.842990398406982
Iteration 2851:
Training Loss: -4.08249044418335
Reconstruction Loss: -5.853658199310303
Iteration 2901:
Training Loss: -4.183834552764893
Reconstruction Loss: -5.862967491149902
Iteration 2951:
Training Loss: -4.277362823486328
Reconstruction Loss: -5.872615814208984
Iteration 3001:
Training Loss: -4.372898578643799
Reconstruction Loss: -5.882970333099365
Iteration 3051:
Training Loss: -4.293829441070557
Reconstruction Loss: -5.892947673797607
Iteration 3101:
Training Loss: -4.250389575958252
Reconstruction Loss: -5.902392864227295
Iteration 3151:
Training Loss: -4.262161731719971
Reconstruction Loss: -5.911154270172119
Iteration 3201:
Training Loss: -4.450359344482422
Reconstruction Loss: -5.920964241027832
Iteration 3251:
Training Loss: -4.441845893859863
Reconstruction Loss: -5.930478096008301
Iteration 3301:
Training Loss: -4.349701404571533
Reconstruction Loss: -5.9382147789001465
Iteration 3351:
Training Loss: -4.357259273529053
Reconstruction Loss: -5.946562767028809
Iteration 3401:
Training Loss: -4.3866448402404785
Reconstruction Loss: -5.956326007843018
Iteration 3451:
Training Loss: -4.35101318359375
Reconstruction Loss: -5.963994979858398
Iteration 3501:
Training Loss: -4.405378341674805
Reconstruction Loss: -5.971994400024414
Iteration 3551:
Training Loss: -4.409664630889893
Reconstruction Loss: -5.980178356170654
Iteration 3601:
Training Loss: -4.373508453369141
Reconstruction Loss: -5.9899749755859375
Iteration 3651:
Training Loss: -4.521866798400879
Reconstruction Loss: -5.995176315307617
Iteration 3701:
Training Loss: -4.544849872589111
Reconstruction Loss: -6.002648830413818
Iteration 3751:
Training Loss: -4.555427551269531
Reconstruction Loss: -6.010417938232422
Iteration 3801:
Training Loss: -4.545106410980225
Reconstruction Loss: -6.01929235458374
Iteration 3851:
Training Loss: -4.453100204467773
Reconstruction Loss: -6.027368068695068
Iteration 3901:
Training Loss: -4.5849103927612305
Reconstruction Loss: -6.033291816711426
Iteration 3951:
Training Loss: -4.536284446716309
Reconstruction Loss: -6.040676116943359
Iteration 4001:
Training Loss: -4.693467140197754
Reconstruction Loss: -6.047974586486816
Iteration 4051:
Training Loss: -4.755182266235352
Reconstruction Loss: -6.055205345153809
Iteration 4101:
Training Loss: -4.644522190093994
Reconstruction Loss: -6.062311172485352
Iteration 4151:
Training Loss: -4.614333152770996
Reconstruction Loss: -6.06792688369751
Iteration 4201:
Training Loss: -4.762608528137207
Reconstruction Loss: -6.0752081871032715
Iteration 4251:
Training Loss: -4.725945472717285
Reconstruction Loss: -6.082540988922119
Iteration 4301:
Training Loss: -4.660433769226074
Reconstruction Loss: -6.088745594024658
Iteration 4351:
Training Loss: -4.758488178253174
Reconstruction Loss: -6.095659255981445
Iteration 4401:
Training Loss: -4.737039566040039
Reconstruction Loss: -6.102362155914307
Iteration 4451:
Training Loss: -4.8505754470825195
Reconstruction Loss: -6.108371734619141
Iteration 4501:
Training Loss: -4.771510124206543
Reconstruction Loss: -6.114412784576416
Iteration 4551:
Training Loss: -4.797075271606445
Reconstruction Loss: -6.1196794509887695
Iteration 4601:
Training Loss: -4.798007011413574
Reconstruction Loss: -6.126087188720703
Iteration 4651:
Training Loss: -4.801661491394043
Reconstruction Loss: -6.132739067077637
Iteration 4701:
Training Loss: -4.83457612991333
Reconstruction Loss: -6.13892126083374
Iteration 4751:
Training Loss: -4.894794940948486
Reconstruction Loss: -6.144970417022705
Iteration 4801:
Training Loss: -4.996910095214844
Reconstruction Loss: -6.1508073806762695
Iteration 4851:
Training Loss: -4.910083770751953
Reconstruction Loss: -6.1555070877075195
Iteration 4901:
Training Loss: -5.0053839683532715
Reconstruction Loss: -6.162273406982422
Iteration 4951:
Training Loss: -4.858890533447266
Reconstruction Loss: -6.167561054229736
Iteration 5001:
Training Loss: -4.936279773712158
Reconstruction Loss: -6.172906398773193
Iteration 5051:
Training Loss: -4.850501537322998
Reconstruction Loss: -6.178925514221191
Iteration 5101:
Training Loss: -4.907266616821289
Reconstruction Loss: -6.184450626373291
Iteration 5151:
Training Loss: -4.810857772827148
Reconstruction Loss: -6.1900506019592285
Iteration 5201:
Training Loss: -4.9537858963012695
Reconstruction Loss: -6.195586681365967
Iteration 5251:
Training Loss: -4.877644062042236
Reconstruction Loss: -6.2007317543029785
Iteration 5301:
Training Loss: -5.033548831939697
Reconstruction Loss: -6.205467700958252
Iteration 5351:
Training Loss: -5.070172309875488
Reconstruction Loss: -6.210499286651611
Iteration 5401:
Training Loss: -4.911796569824219
Reconstruction Loss: -6.21645975112915
Iteration 5451:
Training Loss: -5.001183032989502
Reconstruction Loss: -6.221342086791992
Iteration 5501:
Training Loss: -5.031924247741699
Reconstruction Loss: -6.226284027099609
Iteration 5551:
Training Loss: -5.0741705894470215
Reconstruction Loss: -6.231564521789551
Iteration 5601:
Training Loss: -5.000441074371338
Reconstruction Loss: -6.23637580871582
Iteration 5651:
Training Loss: -5.129657745361328
Reconstruction Loss: -6.241525173187256
Iteration 5701:
Training Loss: -5.078096866607666
Reconstruction Loss: -6.246487140655518
Iteration 5751:
Training Loss: -5.128406047821045
Reconstruction Loss: -6.251464366912842
Iteration 5801:
Training Loss: -5.050777435302734
Reconstruction Loss: -6.255614757537842
Iteration 5851:
Training Loss: -5.274808883666992
Reconstruction Loss: -6.260363578796387
Iteration 5901:
Training Loss: -5.2491374015808105
Reconstruction Loss: -6.265622138977051
Iteration 5951:
Training Loss: -5.235555171966553
Reconstruction Loss: -6.270248889923096
Iteration 6001:
Training Loss: -5.110143661499023
Reconstruction Loss: -6.275204658508301
Iteration 6051:
Training Loss: -5.146554946899414
Reconstruction Loss: -6.279708385467529
Iteration 6101:
Training Loss: -5.116299629211426
Reconstruction Loss: -6.283902168273926
Iteration 6151:
Training Loss: -5.087081432342529
Reconstruction Loss: -6.289107322692871
Iteration 6201:
Training Loss: -5.269711971282959
Reconstruction Loss: -6.292738437652588
Iteration 6251:
Training Loss: -5.317695617675781
Reconstruction Loss: -6.29740047454834
Iteration 6301:
Training Loss: -5.3490705490112305
Reconstruction Loss: -6.301342487335205
Iteration 6351:
Training Loss: -5.3882222175598145
Reconstruction Loss: -6.3060126304626465
Iteration 6401:
Training Loss: -5.302089691162109
Reconstruction Loss: -6.310534477233887
Iteration 6451:
Training Loss: -5.300293922424316
Reconstruction Loss: -6.313949108123779
Iteration 6501:
Training Loss: -5.364258289337158
Reconstruction Loss: -6.3187127113342285
Iteration 6551:
Training Loss: -5.273734092712402
Reconstruction Loss: -6.322643756866455
Iteration 6601:
Training Loss: -5.437357425689697
Reconstruction Loss: -6.326762676239014
Iteration 6651:
Training Loss: -5.3065314292907715
Reconstruction Loss: -6.3308563232421875
Iteration 6701:
Training Loss: -5.290726661682129
Reconstruction Loss: -6.335291862487793
Iteration 6751:
Training Loss: -5.268499374389648
Reconstruction Loss: -6.340246200561523
Iteration 6801:
Training Loss: -5.400981903076172
Reconstruction Loss: -6.34321403503418
Iteration 6851:
Training Loss: -5.216770172119141
Reconstruction Loss: -6.3467793464660645
Iteration 6901:
Training Loss: -5.373114585876465
Reconstruction Loss: -6.3514885902404785
Iteration 6951:
Training Loss: -5.393265724182129
Reconstruction Loss: -6.354794502258301
Iteration 7001:
Training Loss: -5.353053092956543
Reconstruction Loss: -6.358592987060547
Iteration 7051:
Training Loss: -5.4258131980896
Reconstruction Loss: -6.362613677978516
Iteration 7101:
Training Loss: -5.417109489440918
Reconstruction Loss: -6.3673095703125
Iteration 7151:
Training Loss: -5.436887264251709
Reconstruction Loss: -6.370116233825684
Iteration 7201:
Training Loss: -5.51984167098999
Reconstruction Loss: -6.374453067779541
Iteration 7251:
Training Loss: -5.344862461090088
Reconstruction Loss: -6.3784708976745605
Iteration 7301:
Training Loss: -5.591136455535889
Reconstruction Loss: -6.382357120513916
Iteration 7351:
Training Loss: -5.4359846115112305
Reconstruction Loss: -6.385532379150391
Iteration 7401:
Training Loss: -5.419888973236084
Reconstruction Loss: -6.389680862426758
Iteration 7451:
Training Loss: -5.495498180389404
Reconstruction Loss: -6.393256664276123
