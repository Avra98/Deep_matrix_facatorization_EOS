5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.562243461608887
Reconstruction Loss: -0.4377681016921997
Iteration 21:
Training Loss: 5.8491902351379395
Reconstruction Loss: -0.4377681016921997
Iteration 41:
Training Loss: 5.382629871368408
Reconstruction Loss: -0.4377681016921997
Iteration 61:
Training Loss: 5.721419334411621
Reconstruction Loss: -0.4377681016921997
Iteration 81:
Training Loss: 5.84879207611084
Reconstruction Loss: -0.4377681016921997
Iteration 101:
Training Loss: 5.373045921325684
Reconstruction Loss: -0.4377681016921997
Iteration 121:
Training Loss: 5.608416557312012
Reconstruction Loss: -0.4377681016921997
Iteration 141:
Training Loss: 5.364566802978516
Reconstruction Loss: -0.4377681016921997
Iteration 161:
Training Loss: 5.4107770919799805
Reconstruction Loss: -0.4377681016921997
Iteration 181:
Training Loss: 5.656936168670654
Reconstruction Loss: -0.4377681016921997
Iteration 201:
Training Loss: 5.119808673858643
Reconstruction Loss: -0.4377681016921997
Iteration 221:
Training Loss: 5.521907806396484
Reconstruction Loss: -0.4377681016921997
Iteration 241:
Training Loss: 5.665743350982666
Reconstruction Loss: -0.4377681016921997
Iteration 261:
Training Loss: 5.725486755371094
Reconstruction Loss: -0.43776819109916687
Iteration 281:
Training Loss: 5.422910213470459
Reconstruction Loss: -0.43776819109916687
Iteration 301:
Training Loss: 5.838352203369141
Reconstruction Loss: -0.43776819109916687
Iteration 321:
Training Loss: 5.711226940155029
Reconstruction Loss: -0.43776819109916687
Iteration 341:
Training Loss: 5.303771018981934
Reconstruction Loss: -0.43776819109916687
Iteration 361:
Training Loss: 5.5361151695251465
Reconstruction Loss: -0.43776819109916687
Iteration 381:
Training Loss: 5.454128742218018
Reconstruction Loss: -0.4377681016921997
Iteration 401:
Training Loss: 5.388307094573975
Reconstruction Loss: -0.43776819109916687
Iteration 421:
Training Loss: 5.49365758895874
Reconstruction Loss: -0.43776819109916687
Iteration 441:
Training Loss: 5.617244243621826
Reconstruction Loss: -0.43776819109916687
Iteration 461:
Training Loss: 5.550838947296143
Reconstruction Loss: -0.43776819109916687
Iteration 481:
Training Loss: 5.482648849487305
Reconstruction Loss: -0.43776819109916687
Iteration 501:
Training Loss: 5.631986618041992
Reconstruction Loss: -0.43776819109916687
Iteration 521:
Training Loss: 5.2728657722473145
Reconstruction Loss: -0.43776828050613403
Iteration 541:
Training Loss: 5.455113410949707
Reconstruction Loss: -0.43776828050613403
Iteration 561:
Training Loss: 5.770805358886719
Reconstruction Loss: -0.43776828050613403
Iteration 581:
Training Loss: 5.600289344787598
Reconstruction Loss: -0.43776828050613403
Iteration 601:
Training Loss: 5.476797580718994
Reconstruction Loss: -0.43776848912239075
Iteration 621:
Training Loss: 5.7071967124938965
Reconstruction Loss: -0.43776848912239075
Iteration 641:
Training Loss: 5.611929893493652
Reconstruction Loss: -0.4377685785293579
Iteration 661:
Training Loss: 5.534852504730225
Reconstruction Loss: -0.4377685785293579
Iteration 681:
Training Loss: 5.543757438659668
Reconstruction Loss: -0.4377685785293579
Iteration 701:
Training Loss: 5.530949592590332
Reconstruction Loss: -0.4377686679363251
Iteration 721:
Training Loss: 5.7976531982421875
Reconstruction Loss: -0.4377686679363251
Iteration 741:
Training Loss: 5.407791614532471
Reconstruction Loss: -0.4377686679363251
Iteration 761:
Training Loss: 5.5852203369140625
Reconstruction Loss: -0.43776893615722656
Iteration 781:
Training Loss: 5.730225086212158
Reconstruction Loss: -0.4377690255641937
Iteration 801:
Training Loss: 5.864192962646484
Reconstruction Loss: -0.4377691149711609
Iteration 821:
Training Loss: 5.33505392074585
Reconstruction Loss: -0.4377692937850952
Iteration 841:
Training Loss: 5.692646503448486
Reconstruction Loss: -0.4377695918083191
Iteration 861:
Training Loss: 5.555384159088135
Reconstruction Loss: -0.4377697706222534
Iteration 881:
Training Loss: 5.61419153213501
Reconstruction Loss: -0.4377702474594116
Iteration 901:
Training Loss: 5.6144866943359375
Reconstruction Loss: -0.4377706050872803
Iteration 921:
Training Loss: 5.30159854888916
Reconstruction Loss: -0.43777114152908325
Iteration 941:
Training Loss: 5.65349006652832
Reconstruction Loss: -0.4377719759941101
Iteration 961:
Training Loss: 5.352863788604736
Reconstruction Loss: -0.43777337670326233
Iteration 981:
Training Loss: 5.74346923828125
Reconstruction Loss: -0.43777531385421753
Iteration 1001:
Training Loss: 5.72480583190918
Reconstruction Loss: -0.4377779960632324
Iteration 1021:
Training Loss: 5.636424541473389
Reconstruction Loss: -0.43778324127197266
Iteration 1041:
Training Loss: 5.235498905181885
Reconstruction Loss: -0.4377923905849457
Iteration 1061:
Training Loss: 5.6909499168396
Reconstruction Loss: -0.43781179189682007
Iteration 1081:
Training Loss: 5.60750150680542
Reconstruction Loss: -0.4378598928451538
Iteration 1101:
Training Loss: 5.753115653991699
Reconstruction Loss: -0.4380163550376892
Iteration 1121:
Training Loss: 5.599210262298584
Reconstruction Loss: -0.4388760030269623
Iteration 1141:
Training Loss: 5.205347061157227
Reconstruction Loss: -0.459607869386673
Iteration 1161:
Training Loss: 5.075657844543457
Reconstruction Loss: -0.4893159866333008
Iteration 1181:
Training Loss: 5.2178053855896
Reconstruction Loss: -0.5011160969734192
Iteration 1201:
Training Loss: 5.2813191413879395
Reconstruction Loss: -0.4986160099506378
Iteration 1221:
Training Loss: 4.85127592086792
Reconstruction Loss: -0.5155686140060425
Iteration 1241:
Training Loss: 5.125990867614746
Reconstruction Loss: -0.5119544267654419
Iteration 1261:
Training Loss: 5.085197448730469
Reconstruction Loss: -0.5232328176498413
Iteration 1281:
Training Loss: 5.0771074295043945
Reconstruction Loss: -0.5090785026550293
Iteration 1301:
Training Loss: 4.936248779296875
Reconstruction Loss: -0.4991319477558136
Iteration 1321:
Training Loss: 4.941069602966309
Reconstruction Loss: -0.5032577514648438
Iteration 1341:
Training Loss: 4.9267048835754395
Reconstruction Loss: -0.5066710710525513
Iteration 1361:
Training Loss: 5.121070861816406
Reconstruction Loss: -0.512315034866333
Iteration 1381:
Training Loss: 5.136277675628662
Reconstruction Loss: -0.5070641040802002
Iteration 1401:
Training Loss: 5.120724678039551
Reconstruction Loss: -0.5095813274383545
Iteration 1421:
Training Loss: 5.222602367401123
Reconstruction Loss: -0.5107995271682739
Iteration 1441:
Training Loss: 5.006868362426758
Reconstruction Loss: -0.5115246772766113
Iteration 1461:
Training Loss: 5.0397047996521
Reconstruction Loss: -0.5004740953445435
Iteration 1481:
Training Loss: 5.2096428871154785
Reconstruction Loss: -0.5066024661064148
Iteration 1501:
Training Loss: 5.290260314941406
Reconstruction Loss: -0.508953332901001
Iteration 1521:
Training Loss: 5.080749988555908
Reconstruction Loss: -0.5050891637802124
Iteration 1541:
Training Loss: 5.0249247550964355
Reconstruction Loss: -0.5111029744148254
Iteration 1561:
Training Loss: 5.1700215339660645
Reconstruction Loss: -0.5646761059761047
Iteration 1581:
Training Loss: 4.8754191398620605
Reconstruction Loss: -0.6559931039810181
Iteration 1601:
Training Loss: 4.67600679397583
Reconstruction Loss: -0.6323790550231934
Iteration 1621:
Training Loss: 4.42588996887207
Reconstruction Loss: -0.6027705073356628
Iteration 1641:
Training Loss: 4.6519999504089355
Reconstruction Loss: -0.5874846577644348
Iteration 1661:
Training Loss: 4.336084842681885
Reconstruction Loss: -0.6053466796875
Iteration 1681:
Training Loss: 4.411412239074707
Reconstruction Loss: -0.5953395366668701
Iteration 1701:
Training Loss: 4.135068893432617
Reconstruction Loss: -0.6005005240440369
Iteration 1721:
Training Loss: 4.596158504486084
Reconstruction Loss: -0.6654685735702515
Iteration 1741:
Training Loss: 4.279044151306152
Reconstruction Loss: -0.8138246536254883
Iteration 1761:
Training Loss: 3.9590115547180176
Reconstruction Loss: -0.788090705871582
Iteration 1781:
Training Loss: 4.266031265258789
Reconstruction Loss: -0.778535008430481
Iteration 1801:
Training Loss: 4.092339515686035
Reconstruction Loss: -0.7702560424804688
Iteration 1821:
Training Loss: 4.091373920440674
Reconstruction Loss: -0.7852692008018494
Iteration 1841:
Training Loss: 4.306585788726807
Reconstruction Loss: -0.779268741607666
Iteration 1861:
Training Loss: 4.070339679718018
Reconstruction Loss: -0.7875969409942627
Iteration 1881:
Training Loss: 3.976377010345459
Reconstruction Loss: -0.785331666469574
Iteration 1901:
Training Loss: 3.664867639541626
Reconstruction Loss: -0.8194500207901001
Iteration 1921:
Training Loss: 3.4538135528564453
Reconstruction Loss: -1.1336685419082642
Iteration 1941:
Training Loss: 3.671923875808716
Reconstruction Loss: -1.3368287086486816
Iteration 1961:
Training Loss: 3.2913758754730225
Reconstruction Loss: -1.4265413284301758
Iteration 1981:
Training Loss: 3.132829189300537
Reconstruction Loss: -1.476279854774475
Iteration 2001:
Training Loss: 3.093369960784912
Reconstruction Loss: -1.494283676147461
Iteration 2021:
Training Loss: 2.996804714202881
Reconstruction Loss: -1.5094531774520874
Iteration 2041:
Training Loss: 2.8942553997039795
Reconstruction Loss: -1.5211622714996338
Iteration 2061:
Training Loss: 3.2210893630981445
Reconstruction Loss: -1.5149531364440918
Iteration 2081:
Training Loss: 2.9134175777435303
Reconstruction Loss: -1.5245705842971802
Iteration 2101:
Training Loss: 3.123662233352661
Reconstruction Loss: -1.5139156579971313
Iteration 2121:
Training Loss: 3.2563910484313965
Reconstruction Loss: -1.5049042701721191
Iteration 2141:
Training Loss: 3.3102235794067383
Reconstruction Loss: -1.491189956665039
Iteration 2161:
Training Loss: 2.724904775619507
Reconstruction Loss: -1.481568694114685
Iteration 2181:
Training Loss: 2.998513698577881
Reconstruction Loss: -1.4751522541046143
Iteration 2201:
Training Loss: 3.1026811599731445
Reconstruction Loss: -1.4651175737380981
Iteration 2221:
Training Loss: 3.015977382659912
Reconstruction Loss: -1.450491189956665
Iteration 2241:
Training Loss: 3.1480650901794434
Reconstruction Loss: -1.4606772661209106
Iteration 2261:
Training Loss: 3.165180206298828
Reconstruction Loss: -1.440496563911438
Iteration 2281:
Training Loss: 2.9581379890441895
Reconstruction Loss: -1.435451865196228
Iteration 2301:
Training Loss: 2.8118252754211426
Reconstruction Loss: -1.4352384805679321
Iteration 2321:
Training Loss: 3.065392017364502
Reconstruction Loss: -1.4293451309204102
Iteration 2341:
Training Loss: 3.006592035293579
Reconstruction Loss: -1.4293262958526611
Iteration 2361:
Training Loss: 3.0412402153015137
Reconstruction Loss: -1.4248967170715332
Iteration 2381:
Training Loss: 3.29644513130188
Reconstruction Loss: -1.4231345653533936
Iteration 2401:
Training Loss: 2.994664430618286
Reconstruction Loss: -1.4197920560836792
Iteration 2421:
Training Loss: 3.0925116539001465
Reconstruction Loss: -1.413184642791748
Iteration 2441:
Training Loss: 3.0108237266540527
Reconstruction Loss: -1.4175914525985718
Iteration 2461:
Training Loss: 2.9003536701202393
Reconstruction Loss: -1.4113013744354248
Iteration 2481:
Training Loss: 3.0398430824279785
Reconstruction Loss: -1.4156389236450195
Iteration 2501:
Training Loss: 3.2238004207611084
Reconstruction Loss: -1.4075369834899902
Iteration 2521:
Training Loss: 2.9630284309387207
Reconstruction Loss: -1.4102548360824585
Iteration 2541:
Training Loss: 3.1194875240325928
Reconstruction Loss: -1.4111990928649902
Iteration 2561:
Training Loss: 2.698556900024414
Reconstruction Loss: -1.410796880722046
Iteration 2581:
Training Loss: 2.7876341342926025
Reconstruction Loss: -1.410415768623352
Iteration 2601:
Training Loss: 2.8198068141937256
Reconstruction Loss: -1.405742883682251
Iteration 2621:
Training Loss: 3.192807674407959
Reconstruction Loss: -1.406671404838562
Iteration 2641:
Training Loss: 3.113844394683838
Reconstruction Loss: -1.417773723602295
Iteration 2661:
Training Loss: 2.7280476093292236
Reconstruction Loss: -1.3966262340545654
Iteration 2681:
Training Loss: 3.301499128341675
Reconstruction Loss: -1.410111904144287
Iteration 2701:
Training Loss: 2.7701873779296875
Reconstruction Loss: -1.411572813987732
Iteration 2721:
Training Loss: 3.180142402648926
Reconstruction Loss: -1.407131314277649
Iteration 2741:
Training Loss: 2.8497908115386963
Reconstruction Loss: -1.4091346263885498
Iteration 2761:
Training Loss: 3.0024211406707764
Reconstruction Loss: -1.4133508205413818
Iteration 2781:
Training Loss: 2.982647180557251
Reconstruction Loss: -1.4320012331008911
Iteration 2801:
Training Loss: 3.069631814956665
Reconstruction Loss: -1.47828209400177
Iteration 2821:
Training Loss: 2.6850361824035645
Reconstruction Loss: -1.6585886478424072
Iteration 2841:
Training Loss: 1.8910614252090454
Reconstruction Loss: -2.1245031356811523
Iteration 2861:
Training Loss: 0.8219219446182251
Reconstruction Loss: -2.582176923751831
Iteration 2881:
Training Loss: 0.9722476005554199
Reconstruction Loss: -2.9965219497680664
Iteration 2901:
Training Loss: 0.4452683627605438
Reconstruction Loss: -3.413323402404785
Iteration 2921:
Training Loss: -0.11640357971191406
Reconstruction Loss: -3.8364498615264893
Iteration 2941:
Training Loss: -0.0554695725440979
Reconstruction Loss: -4.255871295928955
Iteration 2961:
Training Loss: -1.0267257690429688
Reconstruction Loss: -4.67262601852417
Iteration 2981:
Training Loss: -1.1748853921890259
Reconstruction Loss: -5.077075481414795
