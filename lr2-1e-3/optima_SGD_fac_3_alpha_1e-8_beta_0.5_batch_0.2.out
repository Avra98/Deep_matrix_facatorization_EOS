5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.487144470214844
Reconstruction Loss: -0.4680180549621582
Iteration 21:
Training Loss: 5.685245990753174
Reconstruction Loss: -0.4680180549621582
Iteration 41:
Training Loss: 5.603569984436035
Reconstruction Loss: -0.4680180549621582
Iteration 61:
Training Loss: 5.541825771331787
Reconstruction Loss: -0.4680180549621582
Iteration 81:
Training Loss: 5.359310150146484
Reconstruction Loss: -0.4680180549621582
Iteration 101:
Training Loss: 5.591426849365234
Reconstruction Loss: -0.4680180549621582
Iteration 121:
Training Loss: 5.758362770080566
Reconstruction Loss: -0.4680180549621582
Iteration 141:
Training Loss: 5.485239505767822
Reconstruction Loss: -0.4680180549621582
Iteration 161:
Training Loss: 5.367012977600098
Reconstruction Loss: -0.4680180549621582
Iteration 181:
Training Loss: 5.587078094482422
Reconstruction Loss: -0.4680180549621582
Iteration 201:
Training Loss: 5.720316410064697
Reconstruction Loss: -0.4680180549621582
Iteration 221:
Training Loss: 5.61567497253418
Reconstruction Loss: -0.4680180549621582
Iteration 241:
Training Loss: 5.736111164093018
Reconstruction Loss: -0.4680180549621582
Iteration 261:
Training Loss: 5.524927139282227
Reconstruction Loss: -0.4680180549621582
Iteration 281:
Training Loss: 5.42242956161499
Reconstruction Loss: -0.46801814436912537
Iteration 301:
Training Loss: 5.144622802734375
Reconstruction Loss: -0.46801814436912537
Iteration 321:
Training Loss: 5.718652248382568
Reconstruction Loss: -0.46801814436912537
Iteration 341:
Training Loss: 5.505752086639404
Reconstruction Loss: -0.4680183529853821
Iteration 361:
Training Loss: 5.528534889221191
Reconstruction Loss: -0.4680183529853821
Iteration 381:
Training Loss: 5.893144607543945
Reconstruction Loss: -0.4680183529853821
Iteration 401:
Training Loss: 5.72284460067749
Reconstruction Loss: -0.46801841259002686
Iteration 421:
Training Loss: 5.709649562835693
Reconstruction Loss: -0.46801841259002686
Iteration 441:
Training Loss: 5.371346473693848
Reconstruction Loss: -0.4680185317993164
Iteration 461:
Training Loss: 5.703990459442139
Reconstruction Loss: -0.4680185317993164
Iteration 481:
Training Loss: 5.296064853668213
Reconstruction Loss: -0.4680185317993164
Iteration 501:
Training Loss: 5.720826625823975
Reconstruction Loss: -0.46801862120628357
Iteration 521:
Training Loss: 5.332960605621338
Reconstruction Loss: -0.46801871061325073
Iteration 541:
Training Loss: 5.636049270629883
Reconstruction Loss: -0.46801888942718506
Iteration 561:
Training Loss: 5.321829795837402
Reconstruction Loss: -0.46801888942718506
Iteration 581:
Training Loss: 5.68027400970459
Reconstruction Loss: -0.4680190980434418
Iteration 601:
Training Loss: 5.381457805633545
Reconstruction Loss: -0.4680193066596985
Iteration 621:
Training Loss: 5.3282952308654785
Reconstruction Loss: -0.46801936626434326
Iteration 641:
Training Loss: 5.78032922744751
Reconstruction Loss: -0.46801966428756714
Iteration 661:
Training Loss: 5.454565048217773
Reconstruction Loss: -0.46801984310150146
Iteration 681:
Training Loss: 5.550792694091797
Reconstruction Loss: -0.46802031993865967
Iteration 701:
Training Loss: 5.291736125946045
Reconstruction Loss: -0.46802079677581787
Iteration 721:
Training Loss: 5.726393699645996
Reconstruction Loss: -0.4680214822292328
Iteration 741:
Training Loss: 5.721904277801514
Reconstruction Loss: -0.46802234649658203
Iteration 761:
Training Loss: 5.315149307250977
Reconstruction Loss: -0.4680236577987671
Iteration 781:
Training Loss: 5.39208984375
Reconstruction Loss: -0.4680255651473999
Iteration 801:
Training Loss: 5.316010475158691
Reconstruction Loss: -0.4680284261703491
Iteration 821:
Training Loss: 5.519903659820557
Reconstruction Loss: -0.46803319454193115
Iteration 841:
Training Loss: 5.741195201873779
Reconstruction Loss: -0.4680418372154236
Iteration 861:
Training Loss: 5.22349214553833
Reconstruction Loss: -0.4680592715740204
Iteration 881:
Training Loss: 5.6339335441589355
Reconstruction Loss: -0.46810057759284973
Iteration 901:
Training Loss: 5.753405570983887
Reconstruction Loss: -0.468228816986084
Iteration 921:
Training Loss: 5.284425258636475
Reconstruction Loss: -0.46891093254089355
Iteration 941:
Training Loss: 5.366899490356445
Reconstruction Loss: -0.483475923538208
Iteration 961:
Training Loss: 5.115658760070801
Reconstruction Loss: -0.5987966656684875
Iteration 981:
Training Loss: 5.015515327453613
Reconstruction Loss: -0.5728250741958618
Iteration 1001:
Training Loss: 4.888975143432617
Reconstruction Loss: -0.5559905171394348
Iteration 1021:
Training Loss: 5.433971405029297
Reconstruction Loss: -0.5656364560127258
Iteration 1041:
Training Loss: 4.945875644683838
Reconstruction Loss: -0.5396758317947388
Iteration 1061:
Training Loss: 5.17094087600708
Reconstruction Loss: -0.5470747947692871
Iteration 1081:
Training Loss: 4.516234397888184
Reconstruction Loss: -0.7483076453208923
Iteration 1101:
Training Loss: 4.788352966308594
Reconstruction Loss: -0.7932504415512085
Iteration 1121:
Training Loss: 4.528960227966309
Reconstruction Loss: -0.8148553967475891
Iteration 1141:
Training Loss: 4.587747573852539
Reconstruction Loss: -0.8137788772583008
Iteration 1161:
Training Loss: 4.492994785308838
Reconstruction Loss: -0.8495445847511292
Iteration 1181:
Training Loss: 4.502247333526611
Reconstruction Loss: -0.8570144772529602
Iteration 1201:
Training Loss: 4.293883323669434
Reconstruction Loss: -0.8601511120796204
Iteration 1221:
Training Loss: 4.258992671966553
Reconstruction Loss: -0.852962076663971
Iteration 1241:
Training Loss: 4.555537700653076
Reconstruction Loss: -0.8548312783241272
Iteration 1261:
Training Loss: 4.475647926330566
Reconstruction Loss: -0.8517334461212158
Iteration 1281:
Training Loss: 4.414306163787842
Reconstruction Loss: -0.8606418967247009
Iteration 1301:
Training Loss: 4.482588768005371
Reconstruction Loss: -0.8583180904388428
Iteration 1321:
Training Loss: 4.437918663024902
Reconstruction Loss: -0.871258556842804
Iteration 1341:
Training Loss: 4.5119805335998535
Reconstruction Loss: -0.8572233319282532
Iteration 1361:
Training Loss: 4.616947174072266
Reconstruction Loss: -0.8617451786994934
Iteration 1381:
Training Loss: 4.499996662139893
Reconstruction Loss: -0.873572587966919
Iteration 1401:
Training Loss: 4.557493209838867
Reconstruction Loss: -0.871562659740448
Iteration 1421:
Training Loss: 4.477921009063721
Reconstruction Loss: -0.8786398768424988
Iteration 1441:
Training Loss: 4.4151411056518555
Reconstruction Loss: -0.8677943348884583
Iteration 1461:
Training Loss: 4.414347171783447
Reconstruction Loss: -0.8633931875228882
Iteration 1481:
Training Loss: 4.477210998535156
Reconstruction Loss: -0.8626078367233276
Iteration 1501:
Training Loss: 4.632744312286377
Reconstruction Loss: -0.8721164464950562
Iteration 1521:
Training Loss: 4.316035747528076
Reconstruction Loss: -0.8680089116096497
Iteration 1541:
Training Loss: 4.349570274353027
Reconstruction Loss: -0.8692390322685242
Iteration 1561:
Training Loss: 4.711026668548584
Reconstruction Loss: -0.8637408018112183
Iteration 1581:
Training Loss: 4.039398670196533
Reconstruction Loss: -0.9436694383621216
Iteration 1601:
Training Loss: 4.1948466300964355
Reconstruction Loss: -1.0994518995285034
Iteration 1621:
Training Loss: 4.131492614746094
Reconstruction Loss: -1.1262565851211548
Iteration 1641:
Training Loss: 3.968163013458252
Reconstruction Loss: -1.1799429655075073
Iteration 1661:
Training Loss: 3.3686869144439697
Reconstruction Loss: -1.5396400690078735
Iteration 1681:
Training Loss: 2.946998357772827
Reconstruction Loss: -1.763150930404663
Iteration 1701:
Training Loss: 3.2174124717712402
Reconstruction Loss: -1.8329410552978516
Iteration 1721:
Training Loss: 2.9296789169311523
Reconstruction Loss: -1.854408860206604
Iteration 1741:
Training Loss: 2.8666200637817383
Reconstruction Loss: -1.8512853384017944
Iteration 1761:
Training Loss: 3.033668041229248
Reconstruction Loss: -1.8519854545593262
Iteration 1781:
Training Loss: 2.4597976207733154
Reconstruction Loss: -1.8445663452148438
Iteration 1801:
Training Loss: 3.042583465576172
Reconstruction Loss: -1.8386989831924438
Iteration 1821:
Training Loss: 2.966770887374878
Reconstruction Loss: -1.8347725868225098
Iteration 1841:
Training Loss: 2.8798153400421143
Reconstruction Loss: -1.830429196357727
Iteration 1861:
Training Loss: 2.7591712474823
Reconstruction Loss: -1.8267267942428589
Iteration 1881:
Training Loss: 2.814389944076538
Reconstruction Loss: -1.8221745491027832
Iteration 1901:
Training Loss: 2.8514957427978516
Reconstruction Loss: -1.8174282312393188
Iteration 1921:
Training Loss: 2.917558193206787
Reconstruction Loss: -1.821731448173523
Iteration 1941:
Training Loss: 3.290785312652588
Reconstruction Loss: -1.8137712478637695
Iteration 1961:
Training Loss: 2.977369546890259
Reconstruction Loss: -1.821010947227478
Iteration 1981:
Training Loss: 3.1727848052978516
Reconstruction Loss: -1.816380500793457
Iteration 2001:
Training Loss: 2.740053415298462
Reconstruction Loss: -1.8167093992233276
Iteration 2021:
Training Loss: 3.0446102619171143
Reconstruction Loss: -1.8196648359298706
Iteration 2041:
Training Loss: 2.9101922512054443
Reconstruction Loss: -1.8132989406585693
Iteration 2061:
Training Loss: 2.7423582077026367
Reconstruction Loss: -1.816444993019104
Iteration 2081:
Training Loss: 2.766143798828125
Reconstruction Loss: -1.8103952407836914
Iteration 2101:
Training Loss: 2.904233932495117
Reconstruction Loss: -1.8075556755065918
Iteration 2121:
Training Loss: 3.1969680786132812
Reconstruction Loss: -1.811349868774414
Iteration 2141:
Training Loss: 2.845299243927002
Reconstruction Loss: -1.810832142829895
Iteration 2161:
Training Loss: 2.9249067306518555
Reconstruction Loss: -1.8114607334136963
Iteration 2181:
Training Loss: 2.926393508911133
Reconstruction Loss: -1.8129240274429321
Iteration 2201:
Training Loss: 3.1267871856689453
Reconstruction Loss: -1.8093397617340088
Iteration 2221:
Training Loss: 2.7517950534820557
Reconstruction Loss: -1.8112953901290894
Iteration 2241:
Training Loss: 2.8460042476654053
Reconstruction Loss: -1.8112261295318604
Iteration 2261:
Training Loss: 2.9422731399536133
Reconstruction Loss: -1.8098511695861816
Iteration 2281:
Training Loss: 2.898033618927002
Reconstruction Loss: -1.812724232673645
Iteration 2301:
Training Loss: 2.9715895652770996
Reconstruction Loss: -1.809480905532837
Iteration 2321:
Training Loss: 2.7355282306671143
Reconstruction Loss: -1.8049864768981934
Iteration 2341:
Training Loss: 2.9323484897613525
Reconstruction Loss: -1.8096563816070557
Iteration 2361:
Training Loss: 2.899571180343628
Reconstruction Loss: -1.8127002716064453
Iteration 2381:
Training Loss: 2.8474085330963135
Reconstruction Loss: -1.8100805282592773
Iteration 2401:
Training Loss: 2.8297386169433594
Reconstruction Loss: -1.8034411668777466
Iteration 2421:
Training Loss: 2.941357374191284
Reconstruction Loss: -1.807492971420288
Iteration 2441:
Training Loss: 2.887259006500244
Reconstruction Loss: -1.8100892305374146
Iteration 2461:
Training Loss: 2.8914408683776855
Reconstruction Loss: -1.8094456195831299
Iteration 2481:
Training Loss: 2.8469622135162354
Reconstruction Loss: -1.8089925050735474
Iteration 2501:
Training Loss: 3.0252861976623535
Reconstruction Loss: -1.8085551261901855
Iteration 2521:
Training Loss: 3.002655506134033
Reconstruction Loss: -1.8076395988464355
Iteration 2541:
Training Loss: 2.9303979873657227
Reconstruction Loss: -1.8060892820358276
Iteration 2561:
Training Loss: 2.5730535984039307
Reconstruction Loss: -1.8082438707351685
Iteration 2581:
Training Loss: 2.6880903244018555
Reconstruction Loss: -1.81270432472229
Iteration 2601:
Training Loss: 3.0156991481781006
Reconstruction Loss: -1.8095934391021729
Iteration 2621:
Training Loss: 2.972705841064453
Reconstruction Loss: -1.8083478212356567
Iteration 2641:
Training Loss: 2.7557718753814697
Reconstruction Loss: -1.8078395128250122
Iteration 2661:
Training Loss: 2.8444418907165527
Reconstruction Loss: -1.8126220703125
Iteration 2681:
Training Loss: 3.1395938396453857
Reconstruction Loss: -1.8081810474395752
Iteration 2701:
Training Loss: 2.891585111618042
Reconstruction Loss: -1.8120120763778687
Iteration 2721:
Training Loss: 2.948084592819214
Reconstruction Loss: -1.805253505706787
Iteration 2741:
Training Loss: 2.8440515995025635
Reconstruction Loss: -1.813239336013794
Iteration 2761:
Training Loss: 2.8889858722686768
Reconstruction Loss: -1.820147156715393
Iteration 2781:
Training Loss: 3.1738991737365723
Reconstruction Loss: -1.8271458148956299
Iteration 2801:
Training Loss: 2.632891893386841
Reconstruction Loss: -1.8681578636169434
Iteration 2821:
Training Loss: 2.6314308643341064
Reconstruction Loss: -2.001495361328125
Iteration 2841:
Training Loss: 1.4031716585159302
Reconstruction Loss: -2.4911704063415527
Iteration 2861:
Training Loss: 1.3501282930374146
Reconstruction Loss: -3.1888680458068848
Iteration 2881:
Training Loss: 0.030531419441103935
Reconstruction Loss: -3.8174169063568115
Iteration 2901:
Training Loss: -0.32797253131866455
Reconstruction Loss: -4.382167816162109
Iteration 2921:
Training Loss: -0.9912440180778503
Reconstruction Loss: -4.8999834060668945
Iteration 2941:
Training Loss: -1.8064398765563965
Reconstruction Loss: -5.379915714263916
Iteration 2961:
Training Loss: -2.312528133392334
Reconstruction Loss: -5.837888240814209
Iteration 2981:
Training Loss: -2.6175081729888916
Reconstruction Loss: -6.2803754806518555
