5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.157123565673828
Reconstruction Loss: -0.5573747158050537
Iteration 21:
Training Loss: 5.406901836395264
Reconstruction Loss: -0.5573747158050537
Iteration 41:
Training Loss: 5.281060218811035
Reconstruction Loss: -0.5573748350143433
Iteration 61:
Training Loss: 5.277237415313721
Reconstruction Loss: -0.5573748350143433
Iteration 81:
Training Loss: 5.169713020324707
Reconstruction Loss: -0.5573748350143433
Iteration 101:
Training Loss: 5.510526180267334
Reconstruction Loss: -0.5573748350143433
Iteration 121:
Training Loss: 5.29574728012085
Reconstruction Loss: -0.557374894618988
Iteration 141:
Training Loss: 5.331202507019043
Reconstruction Loss: -0.5573750734329224
Iteration 161:
Training Loss: 5.228531837463379
Reconstruction Loss: -0.5573750734329224
Iteration 181:
Training Loss: 5.269439697265625
Reconstruction Loss: -0.5573750734329224
Iteration 201:
Training Loss: 5.453888416290283
Reconstruction Loss: -0.5573750734329224
Iteration 221:
Training Loss: 5.537230014801025
Reconstruction Loss: -0.5573753118515015
Iteration 241:
Training Loss: 5.284152507781982
Reconstruction Loss: -0.5573750734329224
Iteration 261:
Training Loss: 5.094340801239014
Reconstruction Loss: -0.5573751926422119
Iteration 281:
Training Loss: 5.276042461395264
Reconstruction Loss: -0.5573753118515015
Iteration 301:
Training Loss: 5.489538192749023
Reconstruction Loss: -0.5573753118515015
Iteration 321:
Training Loss: 5.470570087432861
Reconstruction Loss: -0.557375431060791
Iteration 341:
Training Loss: 5.074915409088135
Reconstruction Loss: -0.557375431060791
Iteration 361:
Training Loss: 5.048508644104004
Reconstruction Loss: -0.557375431060791
Iteration 381:
Training Loss: 5.253805637359619
Reconstruction Loss: -0.557375431060791
Iteration 401:
Training Loss: 5.014191627502441
Reconstruction Loss: -0.5573755502700806
Iteration 421:
Training Loss: 5.407597064971924
Reconstruction Loss: -0.5573755502700806
Iteration 441:
Training Loss: 5.163395404815674
Reconstruction Loss: -0.5573756098747253
Iteration 461:
Training Loss: 5.405364513397217
Reconstruction Loss: -0.5573756098747253
Iteration 481:
Training Loss: 5.3328728675842285
Reconstruction Loss: -0.5573757290840149
Iteration 501:
Training Loss: 5.207385063171387
Reconstruction Loss: -0.5573757290840149
Iteration 521:
Training Loss: 5.271294116973877
Reconstruction Loss: -0.5573757290840149
Iteration 541:
Training Loss: 5.0580573081970215
Reconstruction Loss: -0.5573758482933044
Iteration 561:
Training Loss: 5.178096294403076
Reconstruction Loss: -0.5573758482933044
Iteration 581:
Training Loss: 5.332005023956299
Reconstruction Loss: -0.5573758482933044
Iteration 601:
Training Loss: 5.23713493347168
Reconstruction Loss: -0.5573759078979492
Iteration 621:
Training Loss: 5.3093976974487305
Reconstruction Loss: -0.5573760271072388
Iteration 641:
Training Loss: 5.277411937713623
Reconstruction Loss: -0.5573760271072388
Iteration 661:
Training Loss: 5.284518718719482
Reconstruction Loss: -0.5573761463165283
Iteration 681:
Training Loss: 5.524285316467285
Reconstruction Loss: -0.5573761463165283
Iteration 701:
Training Loss: 5.15154504776001
Reconstruction Loss: -0.5573761463165283
Iteration 721:
Training Loss: 5.168766021728516
Reconstruction Loss: -0.5573761463165283
Iteration 741:
Training Loss: 5.347780227661133
Reconstruction Loss: -0.5573762655258179
Iteration 761:
Training Loss: 5.312076091766357
Reconstruction Loss: -0.5573763847351074
Iteration 781:
Training Loss: 5.240210056304932
Reconstruction Loss: -0.5573764443397522
Iteration 801:
Training Loss: 5.197601795196533
Reconstruction Loss: -0.5573764443397522
Iteration 821:
Training Loss: 5.28732442855835
Reconstruction Loss: -0.5573765635490417
Iteration 841:
Training Loss: 5.4136810302734375
Reconstruction Loss: -0.5573765635490417
Iteration 861:
Training Loss: 5.202514171600342
Reconstruction Loss: -0.5573765635490417
Iteration 881:
Training Loss: 5.119510173797607
Reconstruction Loss: -0.5573765635490417
Iteration 901:
Training Loss: 5.367173194885254
Reconstruction Loss: -0.5573766827583313
Iteration 921:
Training Loss: 5.304426193237305
Reconstruction Loss: -0.5573767423629761
Iteration 941:
Training Loss: 5.387671947479248
Reconstruction Loss: -0.5573767423629761
Iteration 961:
Training Loss: 5.46435022354126
Reconstruction Loss: -0.5573767423629761
Iteration 981:
Training Loss: 5.414886951446533
Reconstruction Loss: -0.5573767423629761
Iteration 1001:
Training Loss: 5.357090473175049
Reconstruction Loss: -0.5573769807815552
Iteration 1021:
Training Loss: 4.94252347946167
Reconstruction Loss: -0.5573770999908447
Iteration 1041:
Training Loss: 5.115669250488281
Reconstruction Loss: -0.5573772192001343
Iteration 1061:
Training Loss: 5.1819353103637695
Reconstruction Loss: -0.5573772192001343
Iteration 1081:
Training Loss: 5.314927101135254
Reconstruction Loss: -0.557377278804779
Iteration 1101:
Training Loss: 5.1742353439331055
Reconstruction Loss: -0.557377278804779
Iteration 1121:
Training Loss: 5.3548502922058105
Reconstruction Loss: -0.557377278804779
Iteration 1141:
Training Loss: 5.177523136138916
Reconstruction Loss: -0.5573773980140686
Iteration 1161:
Training Loss: 5.428008556365967
Reconstruction Loss: -0.5573775768280029
Iteration 1181:
Training Loss: 5.545361042022705
Reconstruction Loss: -0.5573775768280029
Iteration 1201:
Training Loss: 5.411950588226318
Reconstruction Loss: -0.5573776960372925
Iteration 1221:
Training Loss: 5.347414970397949
Reconstruction Loss: -0.5573776960372925
Iteration 1241:
Training Loss: 5.373403072357178
Reconstruction Loss: -0.557377815246582
Iteration 1261:
Training Loss: 5.509815216064453
Reconstruction Loss: -0.5573780536651611
Iteration 1281:
Training Loss: 5.312473773956299
Reconstruction Loss: -0.5573780536651611
Iteration 1301:
Training Loss: 5.3171210289001465
Reconstruction Loss: -0.5573782324790955
Iteration 1321:
Training Loss: 5.380649089813232
Reconstruction Loss: -0.557378351688385
Iteration 1341:
Training Loss: 5.303236484527588
Reconstruction Loss: -0.557378351688385
Iteration 1361:
Training Loss: 5.283047199249268
Reconstruction Loss: -0.5573784112930298
Iteration 1381:
Training Loss: 5.229677200317383
Reconstruction Loss: -0.5573785305023193
Iteration 1401:
Training Loss: 5.25816535949707
Reconstruction Loss: -0.5573787689208984
Iteration 1421:
Training Loss: 5.272681713104248
Reconstruction Loss: -0.5573787689208984
Iteration 1441:
Training Loss: 5.315864086151123
Reconstruction Loss: -0.5573787689208984
Iteration 1461:
Training Loss: 5.583187580108643
Reconstruction Loss: -0.5573791861534119
Iteration 1481:
Training Loss: 5.21005916595459
Reconstruction Loss: -0.5573792457580566
Iteration 1501:
Training Loss: 5.3126444816589355
Reconstruction Loss: -0.5573792457580566
Iteration 1521:
Training Loss: 5.382779121398926
Reconstruction Loss: -0.5573794841766357
Iteration 1541:
Training Loss: 5.2985711097717285
Reconstruction Loss: -0.5573797225952148
Iteration 1561:
Training Loss: 5.357475757598877
Reconstruction Loss: -0.5573797821998596
Iteration 1581:
Training Loss: 5.332311630249023
Reconstruction Loss: -0.5573800802230835
Iteration 1601:
Training Loss: 5.564779758453369
Reconstruction Loss: -0.5573800802230835
Iteration 1621:
Training Loss: 5.151008129119873
Reconstruction Loss: -0.5573804378509521
Iteration 1641:
Training Loss: 5.1212921142578125
Reconstruction Loss: -0.5573805570602417
Iteration 1661:
Training Loss: 5.3948211669921875
Reconstruction Loss: -0.5573808550834656
Iteration 1681:
Training Loss: 5.304473400115967
Reconstruction Loss: -0.5573809146881104
Iteration 1701:
Training Loss: 5.308506488800049
Reconstruction Loss: -0.5573811531066895
Iteration 1721:
Training Loss: 5.11618709564209
Reconstruction Loss: -0.5573813915252686
Iteration 1741:
Training Loss: 5.455329418182373
Reconstruction Loss: -0.5573817491531372
Iteration 1761:
Training Loss: 5.005437850952148
Reconstruction Loss: -0.5573819875717163
Iteration 1781:
Training Loss: 5.109607696533203
Reconstruction Loss: -0.5573821663856506
Iteration 1801:
Training Loss: 5.3251471519470215
Reconstruction Loss: -0.5573824048042297
Iteration 1821:
Training Loss: 5.210854530334473
Reconstruction Loss: -0.5573828220367432
Iteration 1841:
Training Loss: 5.1705322265625
Reconstruction Loss: -0.5573832392692566
Iteration 1861:
Training Loss: 5.388431549072266
Reconstruction Loss: -0.5573835372924805
Iteration 1881:
Training Loss: 5.284838676452637
Reconstruction Loss: -0.5573839545249939
Iteration 1901:
Training Loss: 5.167931079864502
Reconstruction Loss: -0.5573843717575073
Iteration 1921:
Training Loss: 5.268960952758789
Reconstruction Loss: -0.5573846697807312
Iteration 1941:
Training Loss: 5.4654765129089355
Reconstruction Loss: -0.5573853254318237
Iteration 1961:
Training Loss: 5.2396135330200195
Reconstruction Loss: -0.5573858022689819
Iteration 1981:
Training Loss: 5.204972267150879
Reconstruction Loss: -0.5573862791061401
Iteration 2001:
Training Loss: 5.330699920654297
Reconstruction Loss: -0.5573869943618774
Iteration 2021:
Training Loss: 5.4782795906066895
Reconstruction Loss: -0.5573874711990356
Iteration 2041:
Training Loss: 4.885417461395264
Reconstruction Loss: -0.5573882460594177
Iteration 2061:
Training Loss: 5.382363796234131
Reconstruction Loss: -0.5573891401290894
Iteration 2081:
Training Loss: 5.233323097229004
Reconstruction Loss: -0.5573898553848267
Iteration 2101:
Training Loss: 5.191557884216309
Reconstruction Loss: -0.5573908090591431
Iteration 2121:
Training Loss: 5.553191661834717
Reconstruction Loss: -0.5573920011520386
Iteration 2141:
Training Loss: 5.456091403961182
Reconstruction Loss: -0.5573931932449341
Iteration 2161:
Training Loss: 5.266360759735107
Reconstruction Loss: -0.5573945045471191
Iteration 2181:
Training Loss: 5.351468563079834
Reconstruction Loss: -0.5573960542678833
Iteration 2201:
Training Loss: 5.491508960723877
Reconstruction Loss: -0.5573977828025818
Iteration 2221:
Training Loss: 5.424368858337402
Reconstruction Loss: -0.5573996901512146
Iteration 2241:
Training Loss: 5.395917892456055
Reconstruction Loss: -0.5574021339416504
Iteration 2261:
Training Loss: 5.147905349731445
Reconstruction Loss: -0.5574051141738892
Iteration 2281:
Training Loss: 5.333690166473389
Reconstruction Loss: -0.557408332824707
Iteration 2301:
Training Loss: 5.022375583648682
Reconstruction Loss: -0.5574123859405518
Iteration 2321:
Training Loss: 5.570071220397949
Reconstruction Loss: -0.5574173927307129
Iteration 2341:
Training Loss: 5.3392462730407715
Reconstruction Loss: -0.5574237108230591
Iteration 2361:
Training Loss: 5.297197341918945
Reconstruction Loss: -0.557431697845459
Iteration 2381:
Training Loss: 5.584214687347412
Reconstruction Loss: -0.5574426651000977
Iteration 2401:
Training Loss: 4.9786295890808105
Reconstruction Loss: -0.557457447052002
Iteration 2421:
Training Loss: 5.187137603759766
Reconstruction Loss: -0.5574789047241211
Iteration 2441:
Training Loss: 5.189962863922119
Reconstruction Loss: -0.5575116872787476
Iteration 2461:
Training Loss: 5.29784631729126
Reconstruction Loss: -0.5575665235519409
Iteration 2481:
Training Loss: 5.2549591064453125
Reconstruction Loss: -0.5576708316802979
Iteration 2501:
Training Loss: 4.8812127113342285
Reconstruction Loss: -0.5579158067703247
Iteration 2521:
Training Loss: 5.349177360534668
Reconstruction Loss: -0.5587720274925232
Iteration 2541:
Training Loss: 5.141545295715332
Reconstruction Loss: -0.5709342956542969
Iteration 2561:
Training Loss: 4.919823169708252
Reconstruction Loss: -0.7107900381088257
Iteration 2581:
Training Loss: 4.8981499671936035
Reconstruction Loss: -0.7246554493904114
Iteration 2601:
Training Loss: 4.837733268737793
Reconstruction Loss: -0.7064898014068604
Iteration 2621:
Training Loss: 4.395565986633301
Reconstruction Loss: -0.7303232550621033
Iteration 2641:
Training Loss: 4.9965972900390625
Reconstruction Loss: -0.7191614508628845
Iteration 2661:
Training Loss: 4.980200290679932
Reconstruction Loss: -0.723159670829773
Iteration 2681:
Training Loss: 4.962749481201172
Reconstruction Loss: -0.7201772928237915
Iteration 2701:
Training Loss: 4.8948259353637695
Reconstruction Loss: -0.7211744785308838
Iteration 2721:
Training Loss: 4.816057205200195
Reconstruction Loss: -0.7039070129394531
Iteration 2741:
Training Loss: 4.787315845489502
Reconstruction Loss: -0.7094817161560059
Iteration 2761:
Training Loss: 5.145648002624512
Reconstruction Loss: -0.7348065376281738
Iteration 2781:
Training Loss: 4.730343818664551
Reconstruction Loss: -0.669623851776123
Iteration 2801:
Training Loss: 4.781085968017578
Reconstruction Loss: -0.6902252435684204
Iteration 2821:
Training Loss: 4.879312992095947
Reconstruction Loss: -0.7283207774162292
Iteration 2841:
Training Loss: 4.803475379943848
Reconstruction Loss: -0.7205933332443237
Iteration 2861:
Training Loss: 4.677632808685303
Reconstruction Loss: -0.720273494720459
Iteration 2881:
Training Loss: 4.857710838317871
Reconstruction Loss: -0.7226263284683228
Iteration 2901:
Training Loss: 4.740074634552002
Reconstruction Loss: -0.7125837802886963
Iteration 2921:
Training Loss: 5.007937431335449
Reconstruction Loss: -0.728079617023468
Iteration 2941:
Training Loss: 4.771109104156494
Reconstruction Loss: -0.7023753523826599
Iteration 2961:
Training Loss: 4.799292087554932
Reconstruction Loss: -0.7270240187644958
Iteration 2981:
Training Loss: 4.877211570739746
Reconstruction Loss: -0.7318434715270996
Iteration 3001:
Training Loss: 4.882174015045166
Reconstruction Loss: -0.708350658416748
Iteration 3021:
Training Loss: 4.6395440101623535
Reconstruction Loss: -0.7217811346054077
Iteration 3041:
Training Loss: 4.931492805480957
Reconstruction Loss: -0.727717936038971
Iteration 3061:
Training Loss: 4.530848503112793
Reconstruction Loss: -0.7024182677268982
Iteration 3081:
Training Loss: 4.907305717468262
Reconstruction Loss: -0.7117953896522522
Iteration 3101:
Training Loss: 5.000528335571289
Reconstruction Loss: -0.6878347992897034
Iteration 3121:
Training Loss: 4.539833068847656
Reconstruction Loss: -0.7324658632278442
Iteration 3141:
Training Loss: 4.779205322265625
Reconstruction Loss: -0.7407541871070862
Iteration 3161:
Training Loss: 4.745996952056885
Reconstruction Loss: -0.7383452653884888
Iteration 3181:
Training Loss: 5.036225318908691
Reconstruction Loss: -0.6662548184394836
Iteration 3201:
Training Loss: 4.494921684265137
Reconstruction Loss: -0.6814606189727783
Iteration 3221:
Training Loss: 4.8787713050842285
Reconstruction Loss: -0.6739005446434021
Iteration 3241:
Training Loss: 4.80068302154541
Reconstruction Loss: -0.7493746280670166
Iteration 3261:
Training Loss: 4.52139949798584
Reconstruction Loss: -0.8941918611526489
Iteration 3281:
Training Loss: 4.652010440826416
Reconstruction Loss: -0.9142674207687378
Iteration 3301:
Training Loss: 4.276420593261719
Reconstruction Loss: -0.911372184753418
Iteration 3321:
Training Loss: 4.307857513427734
Reconstruction Loss: -0.9386231899261475
Iteration 3341:
Training Loss: 4.189818859100342
Reconstruction Loss: -0.9407978057861328
Iteration 3361:
Training Loss: 4.230247974395752
Reconstruction Loss: -0.9566870927810669
Iteration 3381:
Training Loss: 4.487863540649414
Reconstruction Loss: -0.9408994913101196
Iteration 3401:
Training Loss: 4.367305278778076
Reconstruction Loss: -0.940147876739502
Iteration 3421:
Training Loss: 4.364959716796875
Reconstruction Loss: -0.9365220665931702
Iteration 3441:
Training Loss: 4.289008140563965
Reconstruction Loss: -0.9524762630462646
Iteration 3461:
Training Loss: 4.180479526519775
Reconstruction Loss: -0.929357647895813
Iteration 3481:
Training Loss: 4.121520042419434
Reconstruction Loss: -0.9245218634605408
Iteration 3501:
Training Loss: 4.186575412750244
Reconstruction Loss: -0.9537585973739624
Iteration 3521:
Training Loss: 4.58588981628418
Reconstruction Loss: -0.9251711964607239
Iteration 3541:
Training Loss: 4.256597995758057
Reconstruction Loss: -0.9092617034912109
Iteration 3561:
Training Loss: 4.278758525848389
Reconstruction Loss: -0.9395298957824707
Iteration 3581:
Training Loss: 4.4687581062316895
Reconstruction Loss: -0.926260769367218
Iteration 3601:
Training Loss: 4.291446685791016
Reconstruction Loss: -0.956853985786438
Iteration 3621:
Training Loss: 4.329443454742432
Reconstruction Loss: -0.9267575740814209
Iteration 3641:
Training Loss: 4.130791664123535
Reconstruction Loss: -0.9580119252204895
Iteration 3661:
Training Loss: 4.320610046386719
Reconstruction Loss: -0.9397690296173096
Iteration 3681:
Training Loss: 4.073057651519775
Reconstruction Loss: -0.9375197887420654
Iteration 3701:
Training Loss: 4.526284694671631
Reconstruction Loss: -0.95353102684021
Iteration 3721:
Training Loss: 4.375916957855225
Reconstruction Loss: -0.9494867920875549
Iteration 3741:
Training Loss: 4.286043643951416
Reconstruction Loss: -0.9615875482559204
Iteration 3761:
Training Loss: 4.363781929016113
Reconstruction Loss: -0.922836422920227
Iteration 3781:
Training Loss: 4.107460021972656
Reconstruction Loss: -0.941927433013916
Iteration 3801:
Training Loss: 4.517675876617432
Reconstruction Loss: -0.955467939376831
Iteration 3821:
Training Loss: 4.052392482757568
Reconstruction Loss: -0.9479340314865112
Iteration 3841:
Training Loss: 4.200136184692383
Reconstruction Loss: -0.9427887201309204
Iteration 3861:
Training Loss: 4.283669948577881
Reconstruction Loss: -0.9271187782287598
Iteration 3881:
Training Loss: 4.17454719543457
Reconstruction Loss: -0.9504188299179077
Iteration 3901:
Training Loss: 4.16073751449585
Reconstruction Loss: -0.9535554647445679
Iteration 3921:
Training Loss: 4.270596981048584
Reconstruction Loss: -0.9409260749816895
Iteration 3941:
Training Loss: 4.390884876251221
Reconstruction Loss: -0.9380258917808533
Iteration 3961:
Training Loss: 4.244058609008789
Reconstruction Loss: -0.938064455986023
Iteration 3981:
Training Loss: 4.387268543243408
Reconstruction Loss: -0.9209434986114502
Iteration 4001:
Training Loss: 4.245718955993652
Reconstruction Loss: -0.918997049331665
Iteration 4021:
Training Loss: 4.389039516448975
Reconstruction Loss: -0.9574791193008423
Iteration 4041:
Training Loss: 4.433333396911621
Reconstruction Loss: -0.9553425908088684
Iteration 4061:
Training Loss: 4.373466968536377
Reconstruction Loss: -0.9535396099090576
Iteration 4081:
Training Loss: 4.245156764984131
Reconstruction Loss: -0.9362983107566833
Iteration 4101:
Training Loss: 4.280592918395996
Reconstruction Loss: -0.9176294803619385
Iteration 4121:
Training Loss: 4.20503044128418
Reconstruction Loss: -0.9034806489944458
Iteration 4141:
Training Loss: 3.97641921043396
Reconstruction Loss: -0.9314478039741516
Iteration 4161:
Training Loss: 4.173792362213135
Reconstruction Loss: -0.9338242411613464
Iteration 4181:
Training Loss: 4.195422649383545
Reconstruction Loss: -0.9624678492546082
Iteration 4201:
Training Loss: 4.272893905639648
Reconstruction Loss: -0.9460304975509644
Iteration 4221:
Training Loss: 4.290771007537842
Reconstruction Loss: -0.9249485731124878
Iteration 4241:
Training Loss: 4.006668567657471
Reconstruction Loss: -0.9597470760345459
Iteration 4261:
Training Loss: 4.093633651733398
Reconstruction Loss: -0.9375403523445129
Iteration 4281:
Training Loss: 3.9569973945617676
Reconstruction Loss: -0.9592822790145874
Iteration 4301:
Training Loss: 4.409979820251465
Reconstruction Loss: -0.9405728578567505
Iteration 4321:
Training Loss: 3.9286866188049316
Reconstruction Loss: -0.9271207451820374
Iteration 4341:
Training Loss: 4.198244571685791
Reconstruction Loss: -0.9336047172546387
Iteration 4361:
Training Loss: 4.199041843414307
Reconstruction Loss: -0.9680745601654053
Iteration 4381:
Training Loss: 4.216736316680908
Reconstruction Loss: -0.9410116672515869
Iteration 4401:
Training Loss: 4.113946437835693
Reconstruction Loss: -0.928619921207428
Iteration 4421:
Training Loss: 4.19773006439209
Reconstruction Loss: -0.9255468845367432
Iteration 4441:
Training Loss: 4.201375961303711
Reconstruction Loss: -0.9303198456764221
Iteration 4461:
Training Loss: 4.314818859100342
Reconstruction Loss: -0.9348115921020508
Iteration 4481:
Training Loss: 4.524272441864014
Reconstruction Loss: -0.9084734916687012
Iteration 4501:
Training Loss: 4.328529357910156
Reconstruction Loss: -0.9202073216438293
Iteration 4521:
Training Loss: 4.436187744140625
Reconstruction Loss: -0.9350183010101318
Iteration 4541:
Training Loss: 4.211053371429443
Reconstruction Loss: -0.955045759677887
Iteration 4561:
Training Loss: 4.491832256317139
Reconstruction Loss: -0.9207045435905457
Iteration 4581:
Training Loss: 4.524264812469482
Reconstruction Loss: -0.9422048330307007
Iteration 4601:
Training Loss: 4.040589809417725
Reconstruction Loss: -0.9213227033615112
Iteration 4621:
Training Loss: 4.337670803070068
Reconstruction Loss: -0.9457730650901794
Iteration 4641:
Training Loss: 4.217082977294922
Reconstruction Loss: -0.9362486600875854
Iteration 4661:
Training Loss: 4.3460283279418945
Reconstruction Loss: -0.9503856897354126
Iteration 4681:
Training Loss: 4.144562721252441
Reconstruction Loss: -0.9290347099304199
Iteration 4701:
Training Loss: 3.937936782836914
Reconstruction Loss: -0.9371798038482666
Iteration 4721:
Training Loss: 4.333951950073242
Reconstruction Loss: -0.9403159022331238
Iteration 4741:
Training Loss: 4.357746124267578
Reconstruction Loss: -0.9543462991714478
Iteration 4761:
Training Loss: 4.239878177642822
Reconstruction Loss: -0.9189698696136475
Iteration 4781:
Training Loss: 4.16048526763916
Reconstruction Loss: -0.9454666972160339
Iteration 4801:
Training Loss: 4.248497486114502
Reconstruction Loss: -0.9418148994445801
Iteration 4821:
Training Loss: 4.274376392364502
Reconstruction Loss: -0.9276760816574097
Iteration 4841:
Training Loss: 4.045126914978027
Reconstruction Loss: -0.9400875568389893
Iteration 4861:
Training Loss: 4.400462627410889
Reconstruction Loss: -0.9199209213256836
Iteration 4881:
Training Loss: 4.133917808532715
Reconstruction Loss: -0.9263848662376404
Iteration 4901:
Training Loss: 4.1975016593933105
Reconstruction Loss: -0.8895248174667358
Iteration 4921:
Training Loss: 4.26337194442749
Reconstruction Loss: -0.9402018189430237
Iteration 4941:
Training Loss: 4.172104358673096
Reconstruction Loss: -0.9346763491630554
Iteration 4961:
Training Loss: 4.115771293640137
Reconstruction Loss: -0.9495595693588257
Iteration 4981:
Training Loss: 4.465094566345215
Reconstruction Loss: -0.9157053232192993
Iteration 5001:
Training Loss: 4.213513374328613
Reconstruction Loss: -0.9306585788726807
Iteration 5021:
Training Loss: 4.042616367340088
Reconstruction Loss: -0.9316291213035583
Iteration 5041:
Training Loss: 4.366562366485596
Reconstruction Loss: -0.9444993734359741
Iteration 5061:
Training Loss: 4.207901477813721
Reconstruction Loss: -0.9590809941291809
Iteration 5081:
Training Loss: 4.33746337890625
Reconstruction Loss: -0.9323943853378296
Iteration 5101:
Training Loss: 4.27780818939209
Reconstruction Loss: -0.9013619422912598
Iteration 5121:
Training Loss: 4.416029453277588
Reconstruction Loss: -0.9513039588928223
Iteration 5141:
Training Loss: 4.014641761779785
Reconstruction Loss: -0.9065937399864197
Iteration 5161:
Training Loss: 4.408774375915527
Reconstruction Loss: -0.9403169751167297
Iteration 5181:
Training Loss: 4.1497344970703125
Reconstruction Loss: -0.9380046129226685
Iteration 5201:
Training Loss: 4.3259735107421875
Reconstruction Loss: -0.912719190120697
Iteration 5221:
Training Loss: 4.251745223999023
Reconstruction Loss: -0.9271576404571533
Iteration 5241:
Training Loss: 4.195847034454346
Reconstruction Loss: -0.9353311061859131
Iteration 5261:
Training Loss: 4.498366355895996
Reconstruction Loss: -0.8957027196884155
Iteration 5281:
Training Loss: 4.037365913391113
Reconstruction Loss: -0.9405882358551025
Iteration 5301:
Training Loss: 4.176490783691406
Reconstruction Loss: -0.9383725523948669
Iteration 5321:
Training Loss: 4.506664752960205
Reconstruction Loss: -0.9036173224449158
Iteration 5341:
Training Loss: 4.380930423736572
Reconstruction Loss: -0.9371424317359924
Iteration 5361:
Training Loss: 4.323065280914307
Reconstruction Loss: -0.9133056998252869
Iteration 5381:
Training Loss: 4.255850791931152
Reconstruction Loss: -0.9502784013748169
Iteration 5401:
Training Loss: 4.116778373718262
Reconstruction Loss: -0.9348060488700867
Iteration 5421:
Training Loss: 3.8305249214172363
Reconstruction Loss: -0.9424567818641663
Iteration 5441:
Training Loss: 4.260154724121094
Reconstruction Loss: -0.9156096577644348
Iteration 5461:
Training Loss: 4.247178554534912
Reconstruction Loss: -0.9173644185066223
Iteration 5481:
Training Loss: 4.027032375335693
Reconstruction Loss: -0.9432700276374817
Iteration 5501:
Training Loss: 4.365213394165039
Reconstruction Loss: -0.9391648769378662
Iteration 5521:
Training Loss: 4.435115337371826
Reconstruction Loss: -0.9400131702423096
Iteration 5541:
Training Loss: 4.266218185424805
Reconstruction Loss: -0.9014787077903748
Iteration 5561:
Training Loss: 4.361093521118164
Reconstruction Loss: -0.9563636779785156
Iteration 5581:
Training Loss: 4.292194843292236
Reconstruction Loss: -0.8930727243423462
Iteration 5601:
Training Loss: 4.204159736633301
Reconstruction Loss: -0.9545652270317078
Iteration 5621:
Training Loss: 4.256983280181885
Reconstruction Loss: -0.9282395243644714
Iteration 5641:
Training Loss: 4.079577922821045
Reconstruction Loss: -0.9131267070770264
Iteration 5661:
Training Loss: 4.082623481750488
Reconstruction Loss: -0.9337590336799622
Iteration 5681:
Training Loss: 4.304760932922363
Reconstruction Loss: -0.9517538547515869
Iteration 5701:
Training Loss: 4.208634853363037
Reconstruction Loss: -0.8906797766685486
Iteration 5721:
Training Loss: 4.24694299697876
Reconstruction Loss: -0.9366399049758911
Iteration 5741:
Training Loss: 4.1542534828186035
Reconstruction Loss: -0.9486446380615234
Iteration 5761:
Training Loss: 4.323559284210205
Reconstruction Loss: -0.9420091509819031
Iteration 5781:
Training Loss: 4.086197853088379
Reconstruction Loss: -0.939112663269043
Iteration 5801:
Training Loss: 4.178008556365967
Reconstruction Loss: -0.9393351078033447
Iteration 5821:
Training Loss: 4.437543869018555
Reconstruction Loss: -0.9567707777023315
Iteration 5841:
Training Loss: 4.097886562347412
Reconstruction Loss: -0.9327973127365112
Iteration 5861:
Training Loss: 4.35939884185791
Reconstruction Loss: -0.9429223537445068
Iteration 5881:
Training Loss: 4.205174922943115
Reconstruction Loss: -0.9449067115783691
Iteration 5901:
Training Loss: 4.435582637786865
Reconstruction Loss: -0.9226961135864258
Iteration 5921:
Training Loss: 4.37224817276001
Reconstruction Loss: -0.9286025166511536
Iteration 5941:
Training Loss: 4.286118030548096
Reconstruction Loss: -0.9511172771453857
Iteration 5961:
Training Loss: 4.001652240753174
Reconstruction Loss: -0.9280455112457275
Iteration 5981:
Training Loss: 3.9751505851745605
Reconstruction Loss: -0.9266833066940308
Iteration 6001:
Training Loss: 4.416608810424805
Reconstruction Loss: -0.9441676735877991
Iteration 6021:
Training Loss: 4.110454559326172
Reconstruction Loss: -0.9414229989051819
Iteration 6041:
Training Loss: 4.047011852264404
Reconstruction Loss: -0.9425353407859802
Iteration 6061:
Training Loss: 4.494649887084961
Reconstruction Loss: -0.9273725748062134
Iteration 6081:
Training Loss: 4.159519195556641
Reconstruction Loss: -0.9274918437004089
Iteration 6101:
Training Loss: 4.127821922302246
Reconstruction Loss: -0.9174445271492004
Iteration 6121:
Training Loss: 4.558177471160889
Reconstruction Loss: -0.9408088326454163
Iteration 6141:
Training Loss: 3.9612956047058105
Reconstruction Loss: -0.9019966125488281
Iteration 6161:
Training Loss: 4.056852340698242
Reconstruction Loss: -0.9167723655700684
Iteration 6181:
Training Loss: 4.1703715324401855
Reconstruction Loss: -0.9598876237869263
Iteration 6201:
Training Loss: 4.1902923583984375
Reconstruction Loss: -0.942672610282898
Iteration 6221:
Training Loss: 4.519601821899414
Reconstruction Loss: -0.9298038482666016
Iteration 6241:
Training Loss: 4.043196201324463
Reconstruction Loss: -0.9365308284759521
Iteration 6261:
Training Loss: 4.275278091430664
Reconstruction Loss: -0.9537712335586548
Iteration 6281:
Training Loss: 4.207479476928711
Reconstruction Loss: -0.9612679481506348
Iteration 6301:
Training Loss: 4.116302013397217
Reconstruction Loss: -0.939016580581665
Iteration 6321:
Training Loss: 4.077049732208252
Reconstruction Loss: -0.8971649408340454
Iteration 6341:
Training Loss: 4.1264519691467285
Reconstruction Loss: -0.9517641067504883
Iteration 6361:
Training Loss: 4.377875804901123
Reconstruction Loss: -0.944108247756958
Iteration 6381:
Training Loss: 4.380971431732178
Reconstruction Loss: -0.929243266582489
Iteration 6401:
Training Loss: 4.340210914611816
Reconstruction Loss: -0.9332048892974854
Iteration 6421:
Training Loss: 4.2741241455078125
Reconstruction Loss: -0.9265278577804565
Iteration 6441:
Training Loss: 4.1444783210754395
Reconstruction Loss: -0.9358232021331787
Iteration 6461:
Training Loss: 4.530615329742432
Reconstruction Loss: -0.9134019017219543
Iteration 6481:
Training Loss: 4.2426276206970215
Reconstruction Loss: -0.9459457397460938
Iteration 6501:
Training Loss: 4.373716831207275
Reconstruction Loss: -0.9487758278846741
Iteration 6521:
Training Loss: 4.269696235656738
Reconstruction Loss: -0.9307997226715088
Iteration 6541:
Training Loss: 4.218228816986084
Reconstruction Loss: -0.9368758797645569
Iteration 6561:
Training Loss: 4.250983715057373
Reconstruction Loss: -0.9548213481903076
Iteration 6581:
Training Loss: 3.992717981338501
Reconstruction Loss: -0.9249653816223145
Iteration 6601:
Training Loss: 4.108401775360107
Reconstruction Loss: -0.9380806684494019
Iteration 6621:
Training Loss: 4.288700580596924
Reconstruction Loss: -0.9102158546447754
Iteration 6641:
Training Loss: 3.878004550933838
Reconstruction Loss: -0.9426729679107666
Iteration 6661:
Training Loss: 3.879880905151367
Reconstruction Loss: -0.9350311756134033
Iteration 6681:
Training Loss: 4.074471950531006
Reconstruction Loss: -0.9324421882629395
Iteration 6701:
Training Loss: 4.3810272216796875
Reconstruction Loss: -0.9401060342788696
Iteration 6721:
Training Loss: 4.389286994934082
Reconstruction Loss: -0.9403180480003357
Iteration 6741:
Training Loss: 4.47344446182251
Reconstruction Loss: -0.9637851715087891
Iteration 6761:
Training Loss: 4.302744388580322
Reconstruction Loss: -0.9452913999557495
Iteration 6781:
Training Loss: 4.496974945068359
Reconstruction Loss: -0.9513194561004639
Iteration 6801:
Training Loss: 4.372088432312012
Reconstruction Loss: -0.9145017266273499
Iteration 6821:
Training Loss: 4.171779632568359
Reconstruction Loss: -0.9394577741622925
Iteration 6841:
Training Loss: 4.419806003570557
Reconstruction Loss: -0.917970597743988
Iteration 6861:
Training Loss: 4.08134126663208
Reconstruction Loss: -0.960014283657074
Iteration 6881:
Training Loss: 3.9285316467285156
Reconstruction Loss: -0.986617922782898
Iteration 6901:
Training Loss: 4.002874374389648
Reconstruction Loss: -1.1172776222229004
Iteration 6921:
Training Loss: 4.014322280883789
Reconstruction Loss: -1.1875755786895752
Iteration 6941:
Training Loss: 3.886510133743286
Reconstruction Loss: -1.1829091310501099
Iteration 6961:
Training Loss: 3.8057587146759033
Reconstruction Loss: -1.1739946603775024
Iteration 6981:
Training Loss: 3.9732179641723633
Reconstruction Loss: -1.1656973361968994
Iteration 7001:
Training Loss: 3.8397746086120605
Reconstruction Loss: -1.1792691946029663
Iteration 7021:
Training Loss: 3.781120777130127
Reconstruction Loss: -1.218405842781067
Iteration 7041:
Training Loss: 3.946286201477051
Reconstruction Loss: -1.1903178691864014
Iteration 7061:
Training Loss: 3.4814412593841553
Reconstruction Loss: -1.2269660234451294
Iteration 7081:
Training Loss: 3.881229877471924
Reconstruction Loss: -1.262237787246704
Iteration 7101:
Training Loss: 3.493685245513916
Reconstruction Loss: -1.2864503860473633
Iteration 7121:
Training Loss: 3.819373846054077
Reconstruction Loss: -1.2653926610946655
Iteration 7141:
Training Loss: 4.071930408477783
Reconstruction Loss: -1.2715048789978027
Iteration 7161:
Training Loss: 3.962120771408081
Reconstruction Loss: -1.2778096199035645
Iteration 7181:
Training Loss: 3.7844955921173096
Reconstruction Loss: -1.2899636030197144
Iteration 7201:
Training Loss: 3.535301446914673
Reconstruction Loss: -1.2870471477508545
Iteration 7221:
Training Loss: 3.536041021347046
Reconstruction Loss: -1.2931365966796875
Iteration 7241:
Training Loss: 3.6138508319854736
Reconstruction Loss: -1.2656809091567993
Iteration 7261:
Training Loss: 3.7053914070129395
Reconstruction Loss: -1.29475736618042
Iteration 7281:
Training Loss: 3.738257884979248
Reconstruction Loss: -1.2761093378067017
Iteration 7301:
Training Loss: 3.6307735443115234
Reconstruction Loss: -1.2655423879623413
Iteration 7321:
Training Loss: 3.371689558029175
Reconstruction Loss: -1.2687584161758423
Iteration 7341:
Training Loss: 3.5279266834259033
Reconstruction Loss: -1.3090932369232178
Iteration 7361:
Training Loss: 3.839176893234253
Reconstruction Loss: -1.3070111274719238
Iteration 7381:
Training Loss: 3.5093467235565186
Reconstruction Loss: -1.2729601860046387
Iteration 7401:
Training Loss: 3.812854766845703
Reconstruction Loss: -1.3016235828399658
Iteration 7421:
Training Loss: 3.488643169403076
Reconstruction Loss: -1.2947516441345215
Iteration 7441:
Training Loss: 3.9132580757141113
Reconstruction Loss: -1.2661818265914917
Iteration 7461:
Training Loss: 3.8267760276794434
Reconstruction Loss: -1.295011281967163
Iteration 7481:
Training Loss: 3.723606824874878
Reconstruction Loss: -1.281611680984497
Iteration 7501:
Training Loss: 3.446948766708374
Reconstruction Loss: -1.2750400304794312
Iteration 7521:
Training Loss: 3.7017388343811035
Reconstruction Loss: -1.2732007503509521
Iteration 7541:
Training Loss: 3.7120165824890137
Reconstruction Loss: -1.2598763704299927
Iteration 7561:
Training Loss: 3.6424293518066406
Reconstruction Loss: -1.2795734405517578
Iteration 7581:
Training Loss: 3.7581470012664795
Reconstruction Loss: -1.2420951128005981
Iteration 7601:
Training Loss: 3.687511920928955
Reconstruction Loss: -1.2733831405639648
Iteration 7621:
Training Loss: 3.2391817569732666
Reconstruction Loss: -1.2713466882705688
Iteration 7641:
Training Loss: 3.7603418827056885
Reconstruction Loss: -1.256778359413147
Iteration 7661:
Training Loss: 3.6874234676361084
Reconstruction Loss: -1.236691951751709
Iteration 7681:
Training Loss: 3.7808303833007812
Reconstruction Loss: -1.2344539165496826
Iteration 7701:
Training Loss: 3.910917282104492
Reconstruction Loss: -1.2737067937850952
Iteration 7721:
Training Loss: 3.459986686706543
Reconstruction Loss: -1.2868752479553223
Iteration 7741:
Training Loss: 4.015641212463379
Reconstruction Loss: -1.2473394870758057
Iteration 7761:
Training Loss: 3.7693028450012207
Reconstruction Loss: -1.2583075761795044
Iteration 7781:
Training Loss: 3.5178942680358887
Reconstruction Loss: -1.2480945587158203
Iteration 7801:
Training Loss: 3.5153486728668213
Reconstruction Loss: -1.2542808055877686
Iteration 7821:
Training Loss: 3.6901276111602783
Reconstruction Loss: -1.27662992477417
Iteration 7841:
Training Loss: 4.008151054382324
Reconstruction Loss: -1.221215009689331
Iteration 7861:
Training Loss: 3.8103044033050537
Reconstruction Loss: -1.2549607753753662
Iteration 7881:
Training Loss: 3.388547420501709
Reconstruction Loss: -1.2478793859481812
Iteration 7901:
Training Loss: 3.54799222946167
Reconstruction Loss: -1.2455394268035889
Iteration 7921:
Training Loss: 3.5660979747772217
Reconstruction Loss: -1.2767337560653687
Iteration 7941:
Training Loss: 3.862198829650879
Reconstruction Loss: -1.2738837003707886
Iteration 7961:
Training Loss: 3.881072998046875
Reconstruction Loss: -1.25460684299469
Iteration 7981:
Training Loss: 3.7777724266052246
Reconstruction Loss: -1.2617813348770142
Iteration 8001:
Training Loss: 4.000660419464111
Reconstruction Loss: -1.2285065650939941
Iteration 8021:
Training Loss: 3.8570938110351562
Reconstruction Loss: -1.2538942098617554
Iteration 8041:
Training Loss: 3.635089635848999
Reconstruction Loss: -1.2646676301956177
Iteration 8061:
Training Loss: 3.574631452560425
Reconstruction Loss: -1.247025489807129
Iteration 8081:
Training Loss: 3.516231060028076
Reconstruction Loss: -1.2553126811981201
Iteration 8101:
Training Loss: 3.7569215297698975
Reconstruction Loss: -1.289748191833496
Iteration 8121:
Training Loss: 3.7341039180755615
Reconstruction Loss: -1.2622699737548828
Iteration 8141:
Training Loss: 3.7851977348327637
Reconstruction Loss: -1.2534652948379517
Iteration 8161:
Training Loss: 3.843048572540283
Reconstruction Loss: -1.2441974878311157
Iteration 8181:
Training Loss: 3.6276626586914062
Reconstruction Loss: -1.2539160251617432
Iteration 8201:
Training Loss: 3.4287686347961426
Reconstruction Loss: -1.2746700048446655
Iteration 8221:
Training Loss: 3.5483903884887695
Reconstruction Loss: -1.2431809902191162
Iteration 8241:
Training Loss: 3.8282382488250732
Reconstruction Loss: -1.269578218460083
Iteration 8261:
Training Loss: 3.3607256412506104
Reconstruction Loss: -1.2604448795318604
Iteration 8281:
Training Loss: 3.8991262912750244
Reconstruction Loss: -1.2543134689331055
Iteration 8301:
Training Loss: 3.461677312850952
Reconstruction Loss: -1.248241662979126
Iteration 8321:
Training Loss: 3.647714376449585
Reconstruction Loss: -1.242924690246582
Iteration 8341:
Training Loss: 3.757964849472046
Reconstruction Loss: -1.2785263061523438
Iteration 8361:
Training Loss: 3.911975145339966
Reconstruction Loss: -1.2149521112442017
Iteration 8381:
Training Loss: 3.6001758575439453
Reconstruction Loss: -1.2608354091644287
Iteration 8401:
Training Loss: 3.847461223602295
Reconstruction Loss: -1.255838394165039
Iteration 8421:
Training Loss: 3.6058480739593506
Reconstruction Loss: -1.2214871644973755
Iteration 8441:
Training Loss: 3.5327999591827393
Reconstruction Loss: -1.2422633171081543
Iteration 8461:
Training Loss: 3.6018965244293213
Reconstruction Loss: -1.269202470779419
Iteration 8481:
Training Loss: 3.412668228149414
Reconstruction Loss: -1.2533624172210693
Iteration 8501:
Training Loss: 3.807847023010254
Reconstruction Loss: -1.2522776126861572
Iteration 8521:
Training Loss: 3.889289379119873
Reconstruction Loss: -1.2698429822921753
Iteration 8541:
Training Loss: 3.640547752380371
Reconstruction Loss: -1.2402845621109009
Iteration 8561:
Training Loss: 3.47318434715271
Reconstruction Loss: -1.246071457862854
Iteration 8581:
Training Loss: 3.8849806785583496
Reconstruction Loss: -1.2264573574066162
Iteration 8601:
Training Loss: 3.7806155681610107
Reconstruction Loss: -1.2612711191177368
Iteration 8621:
Training Loss: 3.6237192153930664
Reconstruction Loss: -1.257531762123108
Iteration 8641:
Training Loss: 3.0989267826080322
Reconstruction Loss: -1.2422006130218506
Iteration 8661:
Training Loss: 3.834477424621582
Reconstruction Loss: -1.241580605506897
Iteration 8681:
Training Loss: 3.812119483947754
Reconstruction Loss: -1.2437794208526611
Iteration 8701:
Training Loss: 3.9405484199523926
Reconstruction Loss: -1.2646163702011108
Iteration 8721:
Training Loss: 3.6874547004699707
Reconstruction Loss: -1.2478442192077637
Iteration 8741:
Training Loss: 3.77620530128479
Reconstruction Loss: -1.2730400562286377
Iteration 8761:
Training Loss: 3.741140604019165
Reconstruction Loss: -1.25398850440979
Iteration 8781:
Training Loss: 3.3301050662994385
Reconstruction Loss: -1.2619452476501465
Iteration 8801:
Training Loss: 4.0243682861328125
Reconstruction Loss: -1.2165796756744385
Iteration 8821:
Training Loss: 3.706486940383911
Reconstruction Loss: -1.250440001487732
Iteration 8841:
Training Loss: 3.631984233856201
Reconstruction Loss: -1.2422009706497192
Iteration 8861:
Training Loss: 3.5159568786621094
Reconstruction Loss: -1.259283423423767
Iteration 8881:
Training Loss: 3.531616687774658
Reconstruction Loss: -1.2568150758743286
Iteration 8901:
Training Loss: 3.493061065673828
Reconstruction Loss: -1.2455623149871826
Iteration 8921:
Training Loss: 3.6758599281311035
Reconstruction Loss: -1.2748336791992188
Iteration 8941:
Training Loss: 3.8504979610443115
Reconstruction Loss: -1.2473852634429932
Iteration 8961:
Training Loss: 3.819791793823242
Reconstruction Loss: -1.2455610036849976
Iteration 8981:
Training Loss: 3.6033568382263184
Reconstruction Loss: -1.2609424591064453
Iteration 9001:
Training Loss: 3.847869873046875
Reconstruction Loss: -1.2301793098449707
Iteration 9021:
Training Loss: 3.6689670085906982
Reconstruction Loss: -1.2848539352416992
Iteration 9041:
Training Loss: 3.6733059883117676
Reconstruction Loss: -1.2491466999053955
Iteration 9061:
Training Loss: 3.7288200855255127
Reconstruction Loss: -1.2543615102767944
Iteration 9081:
Training Loss: 3.502814531326294
Reconstruction Loss: -1.238901138305664
Iteration 9101:
Training Loss: 3.639056444168091
Reconstruction Loss: -1.2630438804626465
Iteration 9121:
Training Loss: 3.604672431945801
Reconstruction Loss: -1.2636866569519043
Iteration 9141:
Training Loss: 3.613328695297241
Reconstruction Loss: -1.2591392993927002
Iteration 9161:
Training Loss: 3.3235714435577393
Reconstruction Loss: -1.2583738565444946
Iteration 9181:
Training Loss: 3.7542660236358643
Reconstruction Loss: -1.2501286268234253
Iteration 9201:
Training Loss: 3.836137056350708
Reconstruction Loss: -1.265405535697937
Iteration 9221:
Training Loss: 3.615070104598999
Reconstruction Loss: -1.2435047626495361
Iteration 9241:
Training Loss: 3.562366247177124
Reconstruction Loss: -1.24513578414917
Iteration 9261:
Training Loss: 3.307056188583374
Reconstruction Loss: -1.2634080648422241
Iteration 9281:
Training Loss: 3.701993942260742
Reconstruction Loss: -1.2289390563964844
Iteration 9301:
Training Loss: 3.75231671333313
Reconstruction Loss: -1.253922939300537
Iteration 9321:
Training Loss: 4.0264410972595215
Reconstruction Loss: -1.2400033473968506
Iteration 9341:
Training Loss: 3.4376447200775146
Reconstruction Loss: -1.2467341423034668
Iteration 9361:
Training Loss: 3.7823891639709473
Reconstruction Loss: -1.2632383108139038
Iteration 9381:
Training Loss: 3.901151180267334
Reconstruction Loss: -1.245628833770752
Iteration 9401:
Training Loss: 3.6527962684631348
Reconstruction Loss: -1.2536026239395142
Iteration 9421:
Training Loss: 3.6150550842285156
Reconstruction Loss: -1.248346209526062
Iteration 9441:
Training Loss: 3.754206895828247
Reconstruction Loss: -1.2575139999389648
Iteration 9461:
Training Loss: 3.82066011428833
Reconstruction Loss: -1.260582685470581
Iteration 9481:
Training Loss: 3.4957244396209717
Reconstruction Loss: -1.2434602975845337
Iteration 9501:
Training Loss: 3.8893465995788574
Reconstruction Loss: -1.2314718961715698
Iteration 9521:
Training Loss: 3.7426862716674805
Reconstruction Loss: -1.259209156036377
Iteration 9541:
Training Loss: 3.824742555618286
Reconstruction Loss: -1.2479970455169678
Iteration 9561:
Training Loss: 3.4159340858459473
Reconstruction Loss: -1.2345305681228638
Iteration 9581:
Training Loss: 3.582617998123169
Reconstruction Loss: -1.2437748908996582
Iteration 9601:
Training Loss: 3.668483257293701
Reconstruction Loss: -1.2219901084899902
Iteration 9621:
Training Loss: 3.4249188899993896
Reconstruction Loss: -1.2590610980987549
Iteration 9641:
Training Loss: 3.648423671722412
Reconstruction Loss: -1.2236684560775757
Iteration 9661:
Training Loss: 3.362644672393799
Reconstruction Loss: -1.263222098350525
Iteration 9681:
Training Loss: 3.7312276363372803
Reconstruction Loss: -1.2277888059616089
Iteration 9701:
Training Loss: 3.751107931137085
Reconstruction Loss: -1.246744990348816
Iteration 9721:
Training Loss: 3.7787256240844727
Reconstruction Loss: -1.2533464431762695
Iteration 9741:
Training Loss: 3.540721893310547
Reconstruction Loss: -1.243688702583313
Iteration 9761:
Training Loss: 3.9515178203582764
Reconstruction Loss: -1.2829445600509644
Iteration 9781:
Training Loss: 3.538480758666992
Reconstruction Loss: -1.262855887413025
Iteration 9801:
Training Loss: 3.502875328063965
Reconstruction Loss: -1.256540298461914
Iteration 9821:
Training Loss: 3.883397340774536
Reconstruction Loss: -1.262792706489563
Iteration 9841:
Training Loss: 3.434173583984375
Reconstruction Loss: -1.237100601196289
Iteration 9861:
Training Loss: 3.6169731616973877
Reconstruction Loss: -1.2533068656921387
Iteration 9881:
Training Loss: 3.717843770980835
Reconstruction Loss: -1.2547496557235718
Iteration 9901:
Training Loss: 3.7356836795806885
Reconstruction Loss: -1.263425350189209
Iteration 9921:
Training Loss: 3.794830083847046
Reconstruction Loss: -1.2332580089569092
Iteration 9941:
Training Loss: 3.4132235050201416
Reconstruction Loss: -1.2311524152755737
Iteration 9961:
Training Loss: 3.6284539699554443
Reconstruction Loss: -1.210341453552246
Iteration 9981:
Training Loss: 3.2279562950134277
Reconstruction Loss: -1.2648406028747559
