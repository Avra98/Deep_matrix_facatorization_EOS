5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.448322772979736
Reconstruction Loss: -0.552428126335144
Iteration 101:
Training Loss: 5.448029518127441
Reconstruction Loss: -0.5525555610656738
Iteration 201:
Training Loss: 5.447559356689453
Reconstruction Loss: -0.5527520775794983
Iteration 301:
Training Loss: 5.446286678314209
Reconstruction Loss: -0.5533219575881958
Iteration 401:
Training Loss: 5.437335014343262
Reconstruction Loss: -0.5577706694602966
Iteration 501:
Training Loss: 4.873804569244385
Reconstruction Loss: -0.697932243347168
Iteration 601:
Training Loss: 4.80474328994751
Reconstruction Loss: -0.6733153462409973
Iteration 701:
Training Loss: 4.5936384201049805
Reconstruction Loss: -0.7798356413841248
Iteration 801:
Training Loss: 4.132027626037598
Reconstruction Loss: -0.9832462072372437
Iteration 901:
Training Loss: 3.8341617584228516
Reconstruction Loss: -1.1641554832458496
Iteration 1001:
Training Loss: 3.5580689907073975
Reconstruction Loss: -1.3602708578109741
Iteration 1101:
Training Loss: 2.837420701980591
Reconstruction Loss: -1.7810508012771606
Iteration 1201:
Training Loss: 2.1509244441986084
Reconstruction Loss: -2.2426915168762207
Iteration 1301:
Training Loss: 1.3403395414352417
Reconstruction Loss: -2.855534553527832
Iteration 1401:
Training Loss: 0.5344271659851074
Reconstruction Loss: -3.505460262298584
Iteration 1501:
Training Loss: -0.21213650703430176
Reconstruction Loss: -4.119682312011719
Iteration 1601:
Training Loss: -0.881771981716156
Reconstruction Loss: -4.686334133148193
Iteration 1701:
Training Loss: -1.48758864402771
Reconstruction Loss: -5.212127208709717
Iteration 1801:
Training Loss: -2.0443475246429443
Reconstruction Loss: -5.705406665802002
Iteration 1901:
Training Loss: -2.5626649856567383
Reconstruction Loss: -6.173304557800293
Iteration 2001:
Training Loss: -3.0508174896240234
Reconstruction Loss: -6.621676921844482
Iteration 2101:
Training Loss: -3.5155608654022217
Reconstruction Loss: -7.055119514465332
Iteration 2201:
Training Loss: -3.962217092514038
Reconstruction Loss: -7.477068901062012
Iteration 2301:
Training Loss: -4.394764423370361
Reconstruction Loss: -7.8899922370910645
Iteration 2401:
Training Loss: -4.816017150878906
Reconstruction Loss: -8.295598983764648
Iteration 2501:
Training Loss: -5.227842807769775
Reconstruction Loss: -8.695013999938965
Iteration 2601:
Training Loss: -5.631280899047852
Reconstruction Loss: -9.088896751403809
Iteration 2701:
Training Loss: -6.0266547203063965
Reconstruction Loss: -9.47752571105957
Iteration 2801:
Training Loss: -6.413578987121582
Reconstruction Loss: -9.860810279846191
Iteration 2901:
Training Loss: -6.790943622589111
Reconstruction Loss: -10.238320350646973
Iteration 3001:
Training Loss: -7.156825542449951
Reconstruction Loss: -10.60920238494873
Iteration 3101:
Training Loss: -7.508374214172363
Reconstruction Loss: -10.972167015075684
Iteration 3201:
Training Loss: -7.841915130615234
Reconstruction Loss: -11.325366973876953
Iteration 3301:
Training Loss: -8.152935981750488
Reconstruction Loss: -11.66635799407959
Iteration 3401:
Training Loss: -8.43659782409668
Reconstruction Loss: -11.992050170898438
Iteration 3501:
Training Loss: -8.68844223022461
Reconstruction Loss: -12.298775672912598
Iteration 3601:
Training Loss: -8.905134201049805
Reconstruction Loss: -12.582530975341797
Iteration 3701:
Training Loss: -9.085516929626465
Reconstruction Loss: -12.839330673217773
Iteration 3801:
Training Loss: -9.23073673248291
Reconstruction Loss: -13.06600570678711
Iteration 3901:
Training Loss: -9.343985557556152
Reconstruction Loss: -13.260518074035645
Iteration 4001:
Training Loss: -9.43005657196045
Reconstruction Loss: -13.422879219055176
Iteration 4101:
Training Loss: -9.49400520324707
Reconstruction Loss: -13.554678916931152
Iteration 4201:
Training Loss: -9.540885925292969
Reconstruction Loss: -13.65903377532959
Iteration 4301:
Training Loss: -9.574809074401855
Reconstruction Loss: -13.740077018737793
Iteration 4401:
Training Loss: -9.599249839782715
Reconstruction Loss: -13.80191707611084
Iteration 4501:
Training Loss: -9.616888046264648
Reconstruction Loss: -13.848689079284668
Iteration 4601:
Training Loss: -9.629570960998535
Reconstruction Loss: -13.883697509765625
Iteration 4701:
Training Loss: -9.638861656188965
Reconstruction Loss: -13.909950256347656
Iteration 4801:
Training Loss: -9.645692825317383
Reconstruction Loss: -13.929651260375977
Iteration 4901:
Training Loss: -9.650846481323242
Reconstruction Loss: -13.944473266601562
Iteration 5001:
Training Loss: -9.654792785644531
Reconstruction Loss: -13.955609321594238
Iteration 5101:
Training Loss: -9.657943725585938
Reconstruction Loss: -13.964249610900879
Iteration 5201:
Training Loss: -9.660507202148438
Reconstruction Loss: -13.970969200134277
Iteration 5301:
Training Loss: -9.662640571594238
Reconstruction Loss: -13.976152420043945
Iteration 5401:
Training Loss: -9.664494514465332
Reconstruction Loss: -13.980365753173828
Iteration 5501:
Training Loss: -9.666122436523438
Reconstruction Loss: -13.983677864074707
Iteration 5601:
Training Loss: -9.667688369750977
Reconstruction Loss: -13.986468315124512
Iteration 5701:
Training Loss: -9.669075012207031
Reconstruction Loss: -13.988761901855469
Iteration 5801:
Training Loss: -9.670431137084961
Reconstruction Loss: -13.990710258483887
Iteration 5901:
Training Loss: -9.671661376953125
Reconstruction Loss: -13.992414474487305
Iteration 6001:
Training Loss: -9.672931671142578
Reconstruction Loss: -13.993937492370605
Iteration 6101:
Training Loss: -9.674175262451172
Reconstruction Loss: -13.995381355285645
Iteration 6201:
Training Loss: -9.675470352172852
Reconstruction Loss: -13.996769905090332
Iteration 6301:
Training Loss: -9.676631927490234
Reconstruction Loss: -13.998059272766113
Iteration 6401:
Training Loss: -9.677778244018555
Reconstruction Loss: -13.999263763427734
Iteration 6501:
Training Loss: -9.678938865661621
Reconstruction Loss: -14.00033187866211
Iteration 6601:
Training Loss: -9.680176734924316
Reconstruction Loss: -14.001389503479004
Iteration 6701:
Training Loss: -9.68136215209961
Reconstruction Loss: -14.002420425415039
Iteration 6801:
Training Loss: -9.682500839233398
Reconstruction Loss: -14.003387451171875
Iteration 6901:
Training Loss: -9.683724403381348
Reconstruction Loss: -14.004378318786621
Iteration 7001:
Training Loss: -9.684880256652832
Reconstruction Loss: -14.005351066589355
Iteration 7101:
Training Loss: -9.686031341552734
Reconstruction Loss: -14.006292343139648
Iteration 7201:
Training Loss: -9.687174797058105
Reconstruction Loss: -14.007232666015625
Iteration 7301:
Training Loss: -9.688332557678223
Reconstruction Loss: -14.008193969726562
Iteration 7401:
Training Loss: -9.689555168151855
Reconstruction Loss: -14.00912857055664
Iteration 7501:
Training Loss: -9.690681457519531
Reconstruction Loss: -14.010051727294922
Iteration 7601:
Training Loss: -9.69186019897461
Reconstruction Loss: -14.010977745056152
Iteration 7701:
Training Loss: -9.693001747131348
Reconstruction Loss: -14.011905670166016
Iteration 7801:
Training Loss: -9.694162368774414
Reconstruction Loss: -14.012834548950195
Iteration 7901:
Training Loss: -9.695300102233887
Reconstruction Loss: -14.013731002807617
Iteration 8001:
Training Loss: -9.696490287780762
Reconstruction Loss: -14.014654159545898
Iteration 8101:
Training Loss: -9.697613716125488
Reconstruction Loss: -14.015580177307129
Iteration 8201:
Training Loss: -9.698787689208984
Reconstruction Loss: -14.016474723815918
Iteration 8301:
Training Loss: -9.699928283691406
Reconstruction Loss: -14.017366409301758
Iteration 8401:
Training Loss: -9.70113468170166
Reconstruction Loss: -14.018280029296875
Iteration 8501:
Training Loss: -9.702289581298828
Reconstruction Loss: -14.019186973571777
Iteration 8601:
Training Loss: -9.703413963317871
Reconstruction Loss: -14.020071983337402
Iteration 8701:
Training Loss: -9.704548835754395
Reconstruction Loss: -14.020963668823242
Iteration 8801:
Training Loss: -9.705699920654297
Reconstruction Loss: -14.02186393737793
Iteration 8901:
Training Loss: -9.706796646118164
Reconstruction Loss: -14.022735595703125
Iteration 9001:
Training Loss: -9.708028793334961
Reconstruction Loss: -14.023611068725586
Iteration 9101:
Training Loss: -9.709098815917969
Reconstruction Loss: -14.024527549743652
Iteration 9201:
Training Loss: -9.710299491882324
Reconstruction Loss: -14.02540397644043
Iteration 9301:
Training Loss: -9.711394309997559
Reconstruction Loss: -14.026287078857422
Iteration 9401:
Training Loss: -9.712571144104004
Reconstruction Loss: -14.027166366577148
Iteration 9501:
Training Loss: -9.713690757751465
Reconstruction Loss: -14.028068542480469
Iteration 9601:
Training Loss: -9.714852333068848
Reconstruction Loss: -14.028946876525879
Iteration 9701:
Training Loss: -9.715995788574219
Reconstruction Loss: -14.029827117919922
Iteration 9801:
Training Loss: -9.717114448547363
Reconstruction Loss: -14.030717849731445
Iteration 9901:
Training Loss: -9.718318939208984
Reconstruction Loss: -14.031598091125488
Iteration 10001:
Training Loss: -9.719475746154785
Reconstruction Loss: -14.032498359680176
Iteration 10101:
Training Loss: -9.720551490783691
Reconstruction Loss: -14.03335952758789
Iteration 10201:
Training Loss: -9.72170352935791
Reconstruction Loss: -14.034259796142578
Iteration 10301:
Training Loss: -9.722825050354004
Reconstruction Loss: -14.035127639770508
Iteration 10401:
Training Loss: -9.723957061767578
Reconstruction Loss: -14.036002159118652
Iteration 10501:
Training Loss: -9.725110054016113
Reconstruction Loss: -14.036869049072266
Iteration 10601:
Training Loss: -9.726234436035156
Reconstruction Loss: -14.037750244140625
Iteration 10701:
Training Loss: -9.727361679077148
Reconstruction Loss: -14.038627624511719
Iteration 10801:
Training Loss: -9.728486061096191
Reconstruction Loss: -14.039508819580078
Iteration 10901:
Training Loss: -9.729602813720703
Reconstruction Loss: -14.04037857055664
Iteration 11001:
Training Loss: -9.730713844299316
Reconstruction Loss: -14.04124641418457
Iteration 11101:
Training Loss: -9.731876373291016
Reconstruction Loss: -14.042132377624512
Iteration 11201:
Training Loss: -9.732996940612793
Reconstruction Loss: -14.04300594329834
Iteration 11301:
Training Loss: -9.73408317565918
Reconstruction Loss: -14.043874740600586
Iteration 11401:
Training Loss: -9.735245704650879
Reconstruction Loss: -14.044744491577148
Iteration 11501:
Training Loss: -9.736404418945312
Reconstruction Loss: -14.045629501342773
Iteration 11601:
Training Loss: -9.7374906539917
Reconstruction Loss: -14.046473503112793
Iteration 11701:
Training Loss: -9.73865032196045
Reconstruction Loss: -14.047342300415039
Iteration 11801:
Training Loss: -9.739736557006836
Reconstruction Loss: -14.048194885253906
Iteration 11901:
Training Loss: -9.740880966186523
Reconstruction Loss: -14.049064636230469
Iteration 12001:
Training Loss: -9.742032051086426
Reconstruction Loss: -14.04994010925293
Iteration 12101:
Training Loss: -9.743090629577637
Reconstruction Loss: -14.050793647766113
Iteration 12201:
Training Loss: -9.744221687316895
Reconstruction Loss: -14.05166244506836
Iteration 12301:
Training Loss: -9.74534797668457
Reconstruction Loss: -14.052519798278809
Iteration 12401:
Training Loss: -9.746474266052246
Reconstruction Loss: -14.053369522094727
Iteration 12501:
Training Loss: -9.747564315795898
Reconstruction Loss: -14.054222106933594
Iteration 12601:
Training Loss: -9.748696327209473
Reconstruction Loss: -14.055095672607422
Iteration 12701:
Training Loss: -9.7498140335083
Reconstruction Loss: -14.055953979492188
Iteration 12801:
Training Loss: -9.750887870788574
Reconstruction Loss: -14.056801795959473
Iteration 12901:
Training Loss: -9.752004623413086
Reconstruction Loss: -14.057660102844238
Iteration 13001:
Training Loss: -9.753119468688965
Reconstruction Loss: -14.058485984802246
Iteration 13101:
Training Loss: -9.754271507263184
Reconstruction Loss: -14.059356689453125
Iteration 13201:
Training Loss: -9.755365371704102
Reconstruction Loss: -14.060194969177246
Iteration 13301:
Training Loss: -9.756437301635742
Reconstruction Loss: -14.06106185913086
Iteration 13401:
Training Loss: -9.757519721984863
Reconstruction Loss: -14.061895370483398
Iteration 13501:
Training Loss: -9.758650779724121
Reconstruction Loss: -14.062755584716797
Iteration 13601:
Training Loss: -9.759818077087402
Reconstruction Loss: -14.063593864440918
Iteration 13701:
Training Loss: -9.760881423950195
Reconstruction Loss: -14.064464569091797
Iteration 13801:
Training Loss: -9.761910438537598
Reconstruction Loss: -14.065303802490234
Iteration 13901:
Training Loss: -9.763033866882324
Reconstruction Loss: -14.066153526306152
Iteration 14001:
Training Loss: -9.764172554016113
Reconstruction Loss: -14.066969871520996
Iteration 14101:
Training Loss: -9.765228271484375
Reconstruction Loss: -14.067825317382812
Iteration 14201:
Training Loss: -9.76632022857666
Reconstruction Loss: -14.068646430969238
Iteration 14301:
Training Loss: -9.767443656921387
Reconstruction Loss: -14.069496154785156
Iteration 14401:
Training Loss: -9.768511772155762
Reconstruction Loss: -14.070345878601074
Iteration 14501:
Training Loss: -9.769608497619629
Reconstruction Loss: -14.07119369506836
Iteration 14601:
Training Loss: -9.770702362060547
Reconstruction Loss: -14.072017669677734
Iteration 14701:
Training Loss: -9.771771430969238
Reconstruction Loss: -14.072835922241211
Iteration 14801:
Training Loss: -9.772821426391602
Reconstruction Loss: -14.073674201965332
Iteration 14901:
Training Loss: -9.773947715759277
Reconstruction Loss: -14.074507713317871
