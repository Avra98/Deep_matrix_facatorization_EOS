5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.795061111450195
Reconstruction Loss: -0.37791258096694946
Iteration 21:
Training Loss: 5.375209331512451
Reconstruction Loss: -0.37822097539901733
Iteration 41:
Training Loss: 5.67839241027832
Reconstruction Loss: -0.37859925627708435
Iteration 61:
Training Loss: 5.432085990905762
Reconstruction Loss: -0.37943097949028015
Iteration 81:
Training Loss: 5.445810794830322
Reconstruction Loss: -0.3832702040672302
Iteration 101:
Training Loss: 5.280274391174316
Reconstruction Loss: -0.5990063548088074
Iteration 121:
Training Loss: 4.5875749588012695
Reconstruction Loss: -0.7844711542129517
Iteration 141:
Training Loss: 4.290421485900879
Reconstruction Loss: -0.8013241291046143
Iteration 161:
Training Loss: 3.853445291519165
Reconstruction Loss: -0.9583220481872559
Iteration 181:
Training Loss: 3.6824588775634766
Reconstruction Loss: -1.1834406852722168
Iteration 201:
Training Loss: 2.7428464889526367
Reconstruction Loss: -1.5458440780639648
Iteration 221:
Training Loss: 3.020824909210205
Reconstruction Loss: -1.6372169256210327
Iteration 241:
Training Loss: 2.9174082279205322
Reconstruction Loss: -1.6984388828277588
Iteration 261:
Training Loss: 2.797122001647949
Reconstruction Loss: -1.826184630393982
Iteration 281:
Training Loss: 1.9967552423477173
Reconstruction Loss: -2.4714701175689697
Iteration 301:
Training Loss: 0.6606526970863342
Reconstruction Loss: -3.5859200954437256
Iteration 321:
Training Loss: -0.434156209230423
Reconstruction Loss: -4.67680549621582
Iteration 341:
Training Loss: -1.5396605730056763
Reconstruction Loss: -5.638662338256836
Iteration 361:
Training Loss: -2.6092429161071777
Reconstruction Loss: -6.4899678230285645
Iteration 381:
Training Loss: -3.4006435871124268
Reconstruction Loss: -7.259721755981445
Iteration 401:
Training Loss: -4.48109769821167
Reconstruction Loss: -7.951820373535156
Iteration 421:
Training Loss: -5.051794052124023
Reconstruction Loss: -8.58326530456543
Iteration 441:
Training Loss: -5.257230281829834
Reconstruction Loss: -9.159828186035156
Iteration 461:
Training Loss: -6.064391613006592
Reconstruction Loss: -9.69490909576416
Iteration 481:
Training Loss: -6.462388515472412
Reconstruction Loss: -10.160510063171387
Iteration 501:
Training Loss: -6.946609973907471
Reconstruction Loss: -10.567533493041992
Iteration 521:
Training Loss: -7.572398662567139
Reconstruction Loss: -10.910666465759277
Iteration 541:
Training Loss: -7.5540876388549805
Reconstruction Loss: -11.17332935333252
Iteration 561:
Training Loss: -7.626393795013428
Reconstruction Loss: -11.370363235473633
Iteration 581:
Training Loss: -7.714714050292969
Reconstruction Loss: -11.524819374084473
Iteration 601:
Training Loss: -7.603287696838379
Reconstruction Loss: -11.627561569213867
Iteration 621:
Training Loss: -7.939366817474365
Reconstruction Loss: -11.699310302734375
Iteration 641:
Training Loss: -8.091938018798828
Reconstruction Loss: -11.74733829498291
Iteration 661:
Training Loss: -7.968569278717041
Reconstruction Loss: -11.779008865356445
Iteration 681:
Training Loss: -7.722044467926025
Reconstruction Loss: -11.806463241577148
Iteration 701:
Training Loss: -7.992206573486328
Reconstruction Loss: -11.819446563720703
Iteration 721:
Training Loss: -7.879238128662109
Reconstruction Loss: -11.833677291870117
Iteration 741:
Training Loss: -7.674679756164551
Reconstruction Loss: -11.838517189025879
Iteration 761:
Training Loss: -7.7519354820251465
Reconstruction Loss: -11.843893051147461
Iteration 781:
Training Loss: -7.839957237243652
Reconstruction Loss: -11.855951309204102
Iteration 801:
Training Loss: -7.798953056335449
Reconstruction Loss: -11.853839874267578
Iteration 821:
Training Loss: -8.01604175567627
Reconstruction Loss: -11.864645957946777
Iteration 841:
Training Loss: -7.507125377655029
Reconstruction Loss: -11.871426582336426
Iteration 861:
Training Loss: -7.728176116943359
Reconstruction Loss: -11.873153686523438
Iteration 881:
Training Loss: -8.315083503723145
Reconstruction Loss: -11.873211860656738
Iteration 901:
Training Loss: -7.870525360107422
Reconstruction Loss: -11.87472152709961
Iteration 921:
Training Loss: -7.969026565551758
Reconstruction Loss: -11.878087997436523
Iteration 941:
Training Loss: -7.837206840515137
Reconstruction Loss: -11.883600234985352
Iteration 961:
Training Loss: -8.262511253356934
Reconstruction Loss: -11.880537033081055
Iteration 981:
Training Loss: -8.149758338928223
Reconstruction Loss: -11.884845733642578
Iteration 1001:
Training Loss: -7.769640922546387
Reconstruction Loss: -11.888941764831543
Iteration 1021:
Training Loss: -7.833771228790283
Reconstruction Loss: -11.888599395751953
Iteration 1041:
Training Loss: -7.925076007843018
Reconstruction Loss: -11.896929740905762
Iteration 1061:
Training Loss: -7.937753677368164
Reconstruction Loss: -11.890915870666504
Iteration 1081:
Training Loss: -8.18899154663086
Reconstruction Loss: -11.89353084564209
Iteration 1101:
Training Loss: -8.089896202087402
Reconstruction Loss: -11.899833679199219
Iteration 1121:
Training Loss: -7.658155918121338
Reconstruction Loss: -11.903334617614746
Iteration 1141:
Training Loss: -7.918416976928711
Reconstruction Loss: -11.895159721374512
Iteration 1161:
Training Loss: -7.764954090118408
Reconstruction Loss: -11.908650398254395
Iteration 1181:
Training Loss: -7.93735408782959
Reconstruction Loss: -11.908248901367188
Iteration 1201:
Training Loss: -8.009393692016602
Reconstruction Loss: -11.91005802154541
Iteration 1221:
Training Loss: -8.094696044921875
Reconstruction Loss: -11.909652709960938
Iteration 1241:
Training Loss: -7.799315452575684
Reconstruction Loss: -11.916159629821777
Iteration 1261:
Training Loss: -8.381433486938477
Reconstruction Loss: -11.913925170898438
Iteration 1281:
Training Loss: -7.926066875457764
Reconstruction Loss: -11.912818908691406
Iteration 1301:
Training Loss: -7.905543804168701
Reconstruction Loss: -11.92138385772705
Iteration 1321:
Training Loss: -8.093276977539062
Reconstruction Loss: -11.924524307250977
Iteration 1341:
Training Loss: -7.762847900390625
Reconstruction Loss: -11.930639266967773
Iteration 1361:
Training Loss: -7.889289855957031
Reconstruction Loss: -11.916298866271973
Iteration 1381:
Training Loss: -7.879914283752441
Reconstruction Loss: -11.926629066467285
Iteration 1401:
Training Loss: -8.021690368652344
Reconstruction Loss: -11.92790412902832
Iteration 1421:
Training Loss: -7.9860687255859375
Reconstruction Loss: -11.930331230163574
Iteration 1441:
Training Loss: -7.871044635772705
Reconstruction Loss: -11.928723335266113
Iteration 1461:
Training Loss: -7.85458517074585
Reconstruction Loss: -11.933513641357422
Iteration 1481:
Training Loss: -8.039347648620605
Reconstruction Loss: -11.937569618225098
Iteration 1501:
Training Loss: -8.049148559570312
Reconstruction Loss: -11.937016487121582
Iteration 1521:
Training Loss: -7.93640661239624
Reconstruction Loss: -11.947138786315918
Iteration 1541:
Training Loss: -7.9253926277160645
Reconstruction Loss: -11.943265914916992
Iteration 1561:
Training Loss: -7.995619773864746
Reconstruction Loss: -11.942411422729492
Iteration 1581:
Training Loss: -7.645907402038574
Reconstruction Loss: -11.950098037719727
Iteration 1601:
Training Loss: -7.930026054382324
Reconstruction Loss: -11.943882942199707
Iteration 1621:
Training Loss: -7.719590187072754
Reconstruction Loss: -11.939785957336426
Iteration 1641:
Training Loss: -8.256281852722168
Reconstruction Loss: -11.946969032287598
Iteration 1661:
Training Loss: -8.019583702087402
Reconstruction Loss: -11.957208633422852
Iteration 1681:
Training Loss: -8.246052742004395
Reconstruction Loss: -11.948983192443848
Iteration 1701:
Training Loss: -8.179034233093262
Reconstruction Loss: -11.959403991699219
Iteration 1721:
Training Loss: -8.069050788879395
Reconstruction Loss: -11.961400032043457
Iteration 1741:
Training Loss: -7.888265609741211
Reconstruction Loss: -11.950692176818848
Iteration 1761:
Training Loss: -7.949862957000732
Reconstruction Loss: -11.961556434631348
Iteration 1781:
Training Loss: -8.318145751953125
Reconstruction Loss: -11.96071720123291
Iteration 1801:
Training Loss: -7.995189189910889
Reconstruction Loss: -11.967177391052246
Iteration 1821:
Training Loss: -8.333160400390625
Reconstruction Loss: -11.96630859375
Iteration 1841:
Training Loss: -8.168971061706543
Reconstruction Loss: -11.97298526763916
Iteration 1861:
Training Loss: -8.002662658691406
Reconstruction Loss: -11.970171928405762
Iteration 1881:
Training Loss: -7.8546833992004395
Reconstruction Loss: -11.971322059631348
Iteration 1901:
Training Loss: -7.915698051452637
Reconstruction Loss: -11.978660583496094
Iteration 1921:
Training Loss: -7.890356540679932
Reconstruction Loss: -11.976845741271973
Iteration 1941:
Training Loss: -7.69821834564209
Reconstruction Loss: -11.975706100463867
Iteration 1961:
Training Loss: -8.00457763671875
Reconstruction Loss: -11.978265762329102
Iteration 1981:
Training Loss: -8.024389266967773
Reconstruction Loss: -11.990553855895996
Iteration 2001:
Training Loss: -8.11391830444336
Reconstruction Loss: -11.979410171508789
Iteration 2021:
Training Loss: -7.9634928703308105
Reconstruction Loss: -11.97727108001709
Iteration 2041:
Training Loss: -8.063470840454102
Reconstruction Loss: -11.985749244689941
Iteration 2061:
Training Loss: -7.930206775665283
Reconstruction Loss: -11.988290786743164
Iteration 2081:
Training Loss: -7.945140838623047
Reconstruction Loss: -11.996192932128906
Iteration 2101:
Training Loss: -7.992990016937256
Reconstruction Loss: -11.995267868041992
Iteration 2121:
Training Loss: -8.272151947021484
Reconstruction Loss: -12.002991676330566
Iteration 2141:
Training Loss: -8.076192855834961
Reconstruction Loss: -11.992743492126465
Iteration 2161:
Training Loss: -7.862634658813477
Reconstruction Loss: -11.998695373535156
Iteration 2181:
Training Loss: -8.306598663330078
Reconstruction Loss: -11.995183944702148
Iteration 2201:
Training Loss: -7.776215076446533
Reconstruction Loss: -11.998888969421387
Iteration 2221:
Training Loss: -7.832320690155029
Reconstruction Loss: -12.004277229309082
Iteration 2241:
Training Loss: -8.291749954223633
Reconstruction Loss: -12.009397506713867
Iteration 2261:
Training Loss: -7.975785255432129
Reconstruction Loss: -12.005803108215332
Iteration 2281:
Training Loss: -7.97615385055542
Reconstruction Loss: -12.00498104095459
Iteration 2301:
Training Loss: -8.011577606201172
Reconstruction Loss: -12.006814956665039
Iteration 2321:
Training Loss: -7.911977291107178
Reconstruction Loss: -12.004770278930664
Iteration 2341:
Training Loss: -8.431340217590332
Reconstruction Loss: -12.015093803405762
Iteration 2361:
Training Loss: -8.060632705688477
Reconstruction Loss: -12.017110824584961
Iteration 2381:
Training Loss: -8.35612964630127
Reconstruction Loss: -12.021092414855957
Iteration 2401:
Training Loss: -7.939281463623047
Reconstruction Loss: -12.022279739379883
Iteration 2421:
Training Loss: -7.851257801055908
Reconstruction Loss: -12.0204496383667
Iteration 2441:
Training Loss: -8.211771011352539
Reconstruction Loss: -12.021085739135742
Iteration 2461:
Training Loss: -8.104109764099121
Reconstruction Loss: -12.034954071044922
Iteration 2481:
Training Loss: -8.162327766418457
Reconstruction Loss: -12.022235870361328
Iteration 2501:
Training Loss: -7.887082099914551
Reconstruction Loss: -12.02946662902832
Iteration 2521:
Training Loss: -8.077919006347656
Reconstruction Loss: -12.032752990722656
Iteration 2541:
Training Loss: -8.240360260009766
Reconstruction Loss: -12.037561416625977
Iteration 2561:
Training Loss: -7.831803798675537
Reconstruction Loss: -12.032587051391602
Iteration 2581:
Training Loss: -8.034738540649414
Reconstruction Loss: -12.038649559020996
Iteration 2601:
Training Loss: -8.231764793395996
Reconstruction Loss: -12.035920143127441
Iteration 2621:
Training Loss: -8.161998748779297
Reconstruction Loss: -12.030889511108398
Iteration 2641:
Training Loss: -8.318812370300293
Reconstruction Loss: -12.041407585144043
Iteration 2661:
Training Loss: -8.252754211425781
Reconstruction Loss: -12.043760299682617
Iteration 2681:
Training Loss: -8.237529754638672
Reconstruction Loss: -12.041512489318848
Iteration 2701:
Training Loss: -8.074728965759277
Reconstruction Loss: -12.050212860107422
Iteration 2721:
Training Loss: -7.985024929046631
Reconstruction Loss: -12.040728569030762
Iteration 2741:
Training Loss: -8.20535945892334
Reconstruction Loss: -12.050952911376953
Iteration 2761:
Training Loss: -7.88027811050415
Reconstruction Loss: -12.0546875
Iteration 2781:
Training Loss: -8.20236873626709
Reconstruction Loss: -12.056319236755371
Iteration 2801:
Training Loss: -8.114118576049805
Reconstruction Loss: -12.055253982543945
Iteration 2821:
Training Loss: -8.111308097839355
Reconstruction Loss: -12.057966232299805
Iteration 2841:
Training Loss: -8.156465530395508
Reconstruction Loss: -12.05980110168457
Iteration 2861:
Training Loss: -8.10886287689209
Reconstruction Loss: -12.05264949798584
Iteration 2881:
Training Loss: -7.913188934326172
Reconstruction Loss: -12.063712120056152
Iteration 2901:
Training Loss: -8.243032455444336
Reconstruction Loss: -12.058788299560547
Iteration 2921:
Training Loss: -7.848629951477051
Reconstruction Loss: -12.070332527160645
Iteration 2941:
Training Loss: -8.097631454467773
Reconstruction Loss: -12.064291000366211
Iteration 2961:
Training Loss: -8.595738410949707
Reconstruction Loss: -12.060478210449219
Iteration 2981:
Training Loss: -8.160722732543945
Reconstruction Loss: -12.060243606567383
