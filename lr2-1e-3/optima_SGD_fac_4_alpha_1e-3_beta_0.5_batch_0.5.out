5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.66370964050293
Reconstruction Loss: -0.35177528858184814
Iteration 51:
Training Loss: 3.738300085067749
Reconstruction Loss: -1.014172911643982
Iteration 101:
Training Loss: 1.7390780448913574
Reconstruction Loss: -2.0660693645477295
Iteration 151:
Training Loss: 0.5969583988189697
Reconstruction Loss: -2.7326295375823975
Iteration 201:
Training Loss: -0.016450932249426842
Reconstruction Loss: -3.1590046882629395
Iteration 251:
Training Loss: -0.4157726466655731
Reconstruction Loss: -3.461878538131714
Iteration 301:
Training Loss: -0.8507468104362488
Reconstruction Loss: -3.699615955352783
Iteration 351:
Training Loss: -1.1284934282302856
Reconstruction Loss: -3.892967462539673
Iteration 401:
Training Loss: -1.3638312816619873
Reconstruction Loss: -4.05133056640625
Iteration 451:
Training Loss: -1.5308732986450195
Reconstruction Loss: -4.190418243408203
Iteration 501:
Training Loss: -1.7525310516357422
Reconstruction Loss: -4.3106608390808105
Iteration 551:
Training Loss: -1.981844425201416
Reconstruction Loss: -4.41283655166626
Iteration 601:
Training Loss: -2.1243412494659424
Reconstruction Loss: -4.507779598236084
Iteration 651:
Training Loss: -2.1500298976898193
Reconstruction Loss: -4.5884108543396
Iteration 701:
Training Loss: -2.180621862411499
Reconstruction Loss: -4.662811279296875
Iteration 751:
Training Loss: -2.385106325149536
Reconstruction Loss: -4.729084491729736
Iteration 801:
Training Loss: -2.5109105110168457
Reconstruction Loss: -4.791715145111084
Iteration 851:
Training Loss: -2.679572820663452
Reconstruction Loss: -4.849938869476318
Iteration 901:
Training Loss: -2.701199531555176
Reconstruction Loss: -4.898293972015381
Iteration 951:
Training Loss: -2.8375160694122314
Reconstruction Loss: -4.946852207183838
Iteration 1001:
Training Loss: -2.9379420280456543
Reconstruction Loss: -4.993860244750977
Iteration 1051:
Training Loss: -2.9905002117156982
Reconstruction Loss: -5.03525447845459
Iteration 1101:
Training Loss: -3.05539870262146
Reconstruction Loss: -5.073194980621338
Iteration 1151:
Training Loss: -3.087691307067871
Reconstruction Loss: -5.109179496765137
Iteration 1201:
Training Loss: -3.017017126083374
Reconstruction Loss: -5.141944885253906
Iteration 1251:
Training Loss: -3.098820209503174
Reconstruction Loss: -5.175671100616455
Iteration 1301:
Training Loss: -3.273725986480713
Reconstruction Loss: -5.204019546508789
Iteration 1351:
Training Loss: -3.306614398956299
Reconstruction Loss: -5.2331013679504395
Iteration 1401:
Training Loss: -3.375173330307007
Reconstruction Loss: -5.259915351867676
Iteration 1451:
Training Loss: -3.465097665786743
Reconstruction Loss: -5.286619663238525
Iteration 1501:
Training Loss: -3.5944912433624268
Reconstruction Loss: -5.312298774719238
Iteration 1551:
Training Loss: -3.5827598571777344
Reconstruction Loss: -5.335047721862793
Iteration 1601:
Training Loss: -3.569206953048706
Reconstruction Loss: -5.35693883895874
Iteration 1651:
Training Loss: -3.566664695739746
Reconstruction Loss: -5.379541397094727
Iteration 1701:
Training Loss: -3.661665201187134
Reconstruction Loss: -5.400075435638428
Iteration 1751:
Training Loss: -3.6084070205688477
Reconstruction Loss: -5.417507648468018
Iteration 1801:
Training Loss: -3.6307642459869385
Reconstruction Loss: -5.438342094421387
Iteration 1851:
Training Loss: -3.701265335083008
Reconstruction Loss: -5.456000804901123
Iteration 1901:
Training Loss: -3.7301042079925537
Reconstruction Loss: -5.473150730133057
Iteration 1951:
Training Loss: -3.8250842094421387
Reconstruction Loss: -5.492077827453613
Iteration 2001:
Training Loss: -3.8355934619903564
Reconstruction Loss: -5.505944728851318
Iteration 2051:
Training Loss: -3.9178504943847656
Reconstruction Loss: -5.521520614624023
Iteration 2101:
Training Loss: -3.991835594177246
Reconstruction Loss: -5.538043975830078
Iteration 2151:
Training Loss: -4.151862144470215
Reconstruction Loss: -5.552031993865967
Iteration 2201:
Training Loss: -4.01378059387207
Reconstruction Loss: -5.5667219161987305
Iteration 2251:
Training Loss: -4.062112808227539
Reconstruction Loss: -5.581936836242676
Iteration 2301:
Training Loss: -4.032805442810059
Reconstruction Loss: -5.594415664672852
Iteration 2351:
Training Loss: -4.1085968017578125
Reconstruction Loss: -5.606693744659424
Iteration 2401:
Training Loss: -4.100128173828125
Reconstruction Loss: -5.621352195739746
Iteration 2451:
Training Loss: -4.200673580169678
Reconstruction Loss: -5.6321635246276855
Iteration 2501:
Training Loss: -4.192866325378418
Reconstruction Loss: -5.64290714263916
Iteration 2551:
Training Loss: -4.217807769775391
Reconstruction Loss: -5.657759189605713
Iteration 2601:
Training Loss: -4.2092509269714355
Reconstruction Loss: -5.66659688949585
Iteration 2651:
Training Loss: -4.294071674346924
Reconstruction Loss: -5.678025245666504
Iteration 2701:
Training Loss: -4.401642322540283
Reconstruction Loss: -5.688831329345703
Iteration 2751:
Training Loss: -4.358313083648682
Reconstruction Loss: -5.6991095542907715
Iteration 2801:
Training Loss: -4.361695289611816
Reconstruction Loss: -5.709776401519775
Iteration 2851:
Training Loss: -4.539178371429443
Reconstruction Loss: -5.720037937164307
Iteration 2901:
Training Loss: -4.543947219848633
Reconstruction Loss: -5.728909492492676
Iteration 2951:
Training Loss: -4.417014122009277
Reconstruction Loss: -5.738119125366211
Iteration 3001:
Training Loss: -4.499748706817627
Reconstruction Loss: -5.747771263122559
Iteration 3051:
Training Loss: -4.4048871994018555
Reconstruction Loss: -5.7579450607299805
Iteration 3101:
Training Loss: -4.533470153808594
Reconstruction Loss: -5.765599250793457
Iteration 3151:
Training Loss: -4.505542755126953
Reconstruction Loss: -5.775823593139648
Iteration 3201:
Training Loss: -4.57750940322876
Reconstruction Loss: -5.782376766204834
Iteration 3251:
Training Loss: -4.800037860870361
Reconstruction Loss: -5.790397644042969
Iteration 3301:
Training Loss: -4.598700046539307
Reconstruction Loss: -5.799959659576416
Iteration 3351:
Training Loss: -4.6653876304626465
Reconstruction Loss: -5.807567596435547
Iteration 3401:
Training Loss: -4.612827777862549
Reconstruction Loss: -5.815145969390869
Iteration 3451:
Training Loss: -4.663649082183838
Reconstruction Loss: -5.8230061531066895
Iteration 3501:
Training Loss: -4.749567031860352
Reconstruction Loss: -5.82914924621582
Iteration 3551:
Training Loss: -4.725921154022217
Reconstruction Loss: -5.837717056274414
Iteration 3601:
Training Loss: -4.785549640655518
Reconstruction Loss: -5.845108985900879
Iteration 3651:
Training Loss: -4.8287882804870605
Reconstruction Loss: -5.851039409637451
Iteration 3701:
Training Loss: -4.8811726570129395
Reconstruction Loss: -5.859119892120361
Iteration 3751:
Training Loss: -4.854735374450684
Reconstruction Loss: -5.866175651550293
Iteration 3801:
Training Loss: -4.889781951904297
Reconstruction Loss: -5.871840000152588
Iteration 3851:
Training Loss: -5.002425193786621
Reconstruction Loss: -5.877684116363525
Iteration 3901:
Training Loss: -4.854002475738525
Reconstruction Loss: -5.884912490844727
Iteration 3951:
Training Loss: -5.033005714416504
Reconstruction Loss: -5.891618251800537
Iteration 4001:
Training Loss: -4.858264923095703
Reconstruction Loss: -5.896042346954346
Iteration 4051:
Training Loss: -4.957523822784424
Reconstruction Loss: -5.903416633605957
Iteration 4101:
Training Loss: -5.052777290344238
Reconstruction Loss: -5.909940719604492
Iteration 4151:
Training Loss: -4.897632122039795
Reconstruction Loss: -5.916541576385498
Iteration 4201:
Training Loss: -5.002213478088379
Reconstruction Loss: -5.921786785125732
Iteration 4251:
Training Loss: -4.963755130767822
Reconstruction Loss: -5.927465915679932
Iteration 4301:
Training Loss: -5.039457321166992
Reconstruction Loss: -5.93269681930542
Iteration 4351:
Training Loss: -5.06093692779541
Reconstruction Loss: -5.938409328460693
Iteration 4401:
Training Loss: -5.122064590454102
Reconstruction Loss: -5.942563533782959
Iteration 4451:
Training Loss: -5.166490077972412
Reconstruction Loss: -5.948802947998047
Iteration 4501:
Training Loss: -5.154470920562744
Reconstruction Loss: -5.953226089477539
Iteration 4551:
Training Loss: -5.159597873687744
Reconstruction Loss: -5.960128307342529
Iteration 4601:
Training Loss: -5.181901454925537
Reconstruction Loss: -5.96386194229126
Iteration 4651:
Training Loss: -5.272110939025879
Reconstruction Loss: -5.968441963195801
Iteration 4701:
Training Loss: -5.156436920166016
Reconstruction Loss: -5.973825931549072
Iteration 4751:
Training Loss: -5.3093695640563965
Reconstruction Loss: -5.978576183319092
Iteration 4801:
Training Loss: -5.192570686340332
Reconstruction Loss: -5.982378005981445
Iteration 4851:
Training Loss: -5.198575496673584
Reconstruction Loss: -5.9886250495910645
Iteration 4901:
Training Loss: -5.186944961547852
Reconstruction Loss: -5.993961811065674
Iteration 4951:
Training Loss: -5.516222953796387
Reconstruction Loss: -5.996514797210693
Iteration 5001:
Training Loss: -5.354716777801514
Reconstruction Loss: -6.00183629989624
Iteration 5051:
Training Loss: -5.420804977416992
Reconstruction Loss: -6.0055012702941895
Iteration 5101:
Training Loss: -5.475114345550537
Reconstruction Loss: -6.010281085968018
Iteration 5151:
Training Loss: -5.394111633300781
Reconstruction Loss: -6.014706611633301
Iteration 5201:
Training Loss: -5.413008213043213
Reconstruction Loss: -6.019404411315918
Iteration 5251:
Training Loss: -5.497395992279053
Reconstruction Loss: -6.023171424865723
Iteration 5301:
Training Loss: -5.581334114074707
Reconstruction Loss: -6.0265302658081055
Iteration 5351:
Training Loss: -5.582339763641357
Reconstruction Loss: -6.031730651855469
Iteration 5401:
Training Loss: -5.560977935791016
Reconstruction Loss: -6.035350799560547
Iteration 5451:
Training Loss: -5.444616317749023
Reconstruction Loss: -6.0392584800720215
Iteration 5501:
Training Loss: -5.4043450355529785
Reconstruction Loss: -6.044126510620117
Iteration 5551:
Training Loss: -5.522249698638916
Reconstruction Loss: -6.046446800231934
Iteration 5601:
Training Loss: -5.479071617126465
Reconstruction Loss: -6.04985237121582
Iteration 5651:
Training Loss: -5.549560070037842
Reconstruction Loss: -6.0540547370910645
Iteration 5701:
Training Loss: -5.540618419647217
Reconstruction Loss: -6.058294773101807
Iteration 5751:
Training Loss: -5.69011926651001
Reconstruction Loss: -6.061574935913086
Iteration 5801:
Training Loss: -5.675247669219971
Reconstruction Loss: -6.064209461212158
Iteration 5851:
Training Loss: -5.66299295425415
Reconstruction Loss: -6.068034648895264
Iteration 5901:
Training Loss: -5.6643242835998535
Reconstruction Loss: -6.072343826293945
Iteration 5951:
Training Loss: -5.682267189025879
Reconstruction Loss: -6.075444221496582
Iteration 6001:
Training Loss: -5.659970760345459
Reconstruction Loss: -6.0790534019470215
Iteration 6051:
Training Loss: -5.709897041320801
Reconstruction Loss: -6.082573413848877
Iteration 6101:
Training Loss: -5.732912540435791
Reconstruction Loss: -6.085046291351318
Iteration 6151:
Training Loss: -5.656414031982422
Reconstruction Loss: -6.088210582733154
Iteration 6201:
Training Loss: -5.731827735900879
Reconstruction Loss: -6.092072486877441
Iteration 6251:
Training Loss: -5.716674327850342
Reconstruction Loss: -6.095304489135742
Iteration 6301:
Training Loss: -5.726202487945557
Reconstruction Loss: -6.098228454589844
Iteration 6351:
Training Loss: -5.729247093200684
Reconstruction Loss: -6.101137161254883
Iteration 6401:
Training Loss: -5.794703960418701
Reconstruction Loss: -6.10354471206665
Iteration 6451:
Training Loss: -5.792774677276611
Reconstruction Loss: -6.107214450836182
Iteration 6501:
Training Loss: -5.941288471221924
Reconstruction Loss: -6.109870910644531
Iteration 6551:
Training Loss: -5.768799781799316
Reconstruction Loss: -6.113052845001221
Iteration 6601:
Training Loss: -5.8271965980529785
Reconstruction Loss: -6.115903854370117
Iteration 6651:
Training Loss: -5.76815128326416
Reconstruction Loss: -6.119040489196777
Iteration 6701:
Training Loss: -5.856307506561279
Reconstruction Loss: -6.12114953994751
Iteration 6751:
Training Loss: -5.919919013977051
Reconstruction Loss: -6.124582290649414
Iteration 6801:
Training Loss: -5.909287452697754
Reconstruction Loss: -6.126943588256836
Iteration 6851:
Training Loss: -5.9182562828063965
Reconstruction Loss: -6.129734992980957
Iteration 6901:
Training Loss: -5.954566478729248
Reconstruction Loss: -6.132199287414551
Iteration 6951:
Training Loss: -5.957744598388672
Reconstruction Loss: -6.135276794433594
Iteration 7001:
Training Loss: -5.958677768707275
Reconstruction Loss: -6.13738489151001
Iteration 7051:
Training Loss: -6.106255054473877
Reconstruction Loss: -6.1397223472595215
Iteration 7101:
Training Loss: -5.955522060394287
Reconstruction Loss: -6.143232822418213
Iteration 7151:
Training Loss: -5.979233264923096
Reconstruction Loss: -6.145650386810303
Iteration 7201:
Training Loss: -5.929617404937744
Reconstruction Loss: -6.147862434387207
Iteration 7251:
Training Loss: -6.082546710968018
Reconstruction Loss: -6.150144100189209
Iteration 7301:
Training Loss: -6.02156925201416
Reconstruction Loss: -6.152536869049072
Iteration 7351:
Training Loss: -5.900211334228516
Reconstruction Loss: -6.156411647796631
Iteration 7401:
Training Loss: -6.028157711029053
Reconstruction Loss: -6.157744407653809
Iteration 7451:
Training Loss: -5.967001914978027
Reconstruction Loss: -6.160035610198975
