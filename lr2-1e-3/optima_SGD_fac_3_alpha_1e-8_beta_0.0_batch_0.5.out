5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.521319389343262
Reconstruction Loss: -0.5724189877510071
Iteration 51:
Training Loss: 5.5900397300720215
Reconstruction Loss: -0.5724191069602966
Iteration 101:
Training Loss: 5.576279640197754
Reconstruction Loss: -0.5724191069602966
Iteration 151:
Training Loss: 5.592796325683594
Reconstruction Loss: -0.5724191069602966
Iteration 201:
Training Loss: 5.560793876647949
Reconstruction Loss: -0.5724191069602966
Iteration 251:
Training Loss: 5.665805339813232
Reconstruction Loss: -0.5724191069602966
Iteration 301:
Training Loss: 5.738256931304932
Reconstruction Loss: -0.5724191069602966
Iteration 351:
Training Loss: 5.577994346618652
Reconstruction Loss: -0.5724191069602966
Iteration 401:
Training Loss: 5.586883544921875
Reconstruction Loss: -0.5724191069602966
Iteration 451:
Training Loss: 5.5792083740234375
Reconstruction Loss: -0.5724191069602966
Iteration 501:
Training Loss: 5.696861267089844
Reconstruction Loss: -0.5724191665649414
Iteration 551:
Training Loss: 5.5555243492126465
Reconstruction Loss: -0.5724191665649414
Iteration 601:
Training Loss: 5.591240406036377
Reconstruction Loss: -0.5724191665649414
Iteration 651:
Training Loss: 5.456867694854736
Reconstruction Loss: -0.5724191665649414
Iteration 701:
Training Loss: 5.7870049476623535
Reconstruction Loss: -0.5724191665649414
Iteration 751:
Training Loss: 5.6057820320129395
Reconstruction Loss: -0.572419285774231
Iteration 801:
Training Loss: 5.473918437957764
Reconstruction Loss: -0.572419285774231
Iteration 851:
Training Loss: 5.443118572235107
Reconstruction Loss: -0.572419285774231
Iteration 901:
Training Loss: 5.557480812072754
Reconstruction Loss: -0.5724194049835205
Iteration 951:
Training Loss: 5.52069091796875
Reconstruction Loss: -0.5724195241928101
Iteration 1001:
Training Loss: 5.655201435089111
Reconstruction Loss: -0.5724195241928101
Iteration 1051:
Training Loss: 5.525372505187988
Reconstruction Loss: -0.5724196434020996
Iteration 1101:
Training Loss: 5.58778715133667
Reconstruction Loss: -0.5724197030067444
Iteration 1151:
Training Loss: 5.556912422180176
Reconstruction Loss: -0.5724197030067444
Iteration 1201:
Training Loss: 5.653298377990723
Reconstruction Loss: -0.5724198222160339
Iteration 1251:
Training Loss: 5.604986667633057
Reconstruction Loss: -0.5724198222160339
Iteration 1301:
Training Loss: 5.611422538757324
Reconstruction Loss: -0.572420060634613
Iteration 1351:
Training Loss: 5.595950603485107
Reconstruction Loss: -0.572420060634613
Iteration 1401:
Training Loss: 5.644113540649414
Reconstruction Loss: -0.5724203586578369
Iteration 1451:
Training Loss: 5.6390838623046875
Reconstruction Loss: -0.5724204778671265
Iteration 1501:
Training Loss: 5.734291076660156
Reconstruction Loss: -0.5724207758903503
Iteration 1551:
Training Loss: 5.683298110961914
Reconstruction Loss: -0.5724211931228638
Iteration 1601:
Training Loss: 5.551044940948486
Reconstruction Loss: -0.5724215507507324
Iteration 1651:
Training Loss: 5.550342559814453
Reconstruction Loss: -0.5724221467971802
Iteration 1701:
Training Loss: 5.670441627502441
Reconstruction Loss: -0.5724228024482727
Iteration 1751:
Training Loss: 5.595611095428467
Reconstruction Loss: -0.5724238157272339
Iteration 1801:
Training Loss: 5.722681522369385
Reconstruction Loss: -0.5724253058433533
Iteration 1851:
Training Loss: 5.4993414878845215
Reconstruction Loss: -0.5724274516105652
Iteration 1901:
Training Loss: 5.507447719573975
Reconstruction Loss: -0.5724307298660278
Iteration 1951:
Training Loss: 5.645728588104248
Reconstruction Loss: -0.5724362134933472
Iteration 2001:
Training Loss: 5.657505989074707
Reconstruction Loss: -0.5724465847015381
Iteration 2051:
Training Loss: 5.696779251098633
Reconstruction Loss: -0.5724679231643677
Iteration 2101:
Training Loss: 5.430104732513428
Reconstruction Loss: -0.5725226402282715
Iteration 2151:
Training Loss: 5.712553024291992
Reconstruction Loss: -0.5727166533470154
Iteration 2201:
Training Loss: 5.566593647003174
Reconstruction Loss: -0.5741021633148193
Iteration 2251:
Training Loss: 5.465707302093506
Reconstruction Loss: -0.6835973858833313
Iteration 2301:
Training Loss: 4.982400894165039
Reconstruction Loss: -0.7153584361076355
Iteration 2351:
Training Loss: 4.906931400299072
Reconstruction Loss: -0.7167167663574219
Iteration 2401:
Training Loss: 4.921542167663574
Reconstruction Loss: -0.7180068492889404
Iteration 2451:
Training Loss: 5.005059242248535
Reconstruction Loss: -0.6994388699531555
Iteration 2501:
Training Loss: 5.015033721923828
Reconstruction Loss: -0.7085957527160645
Iteration 2551:
Training Loss: 4.889992713928223
Reconstruction Loss: -0.7135825753211975
Iteration 2601:
Training Loss: 4.908315658569336
Reconstruction Loss: -0.7493606209754944
Iteration 2651:
Training Loss: 4.2269768714904785
Reconstruction Loss: -0.913689374923706
Iteration 2701:
Training Loss: 4.162014007568359
Reconstruction Loss: -0.8812012076377869
Iteration 2751:
Training Loss: 4.249767303466797
Reconstruction Loss: -0.8606016635894775
Iteration 2801:
Training Loss: 4.171241760253906
Reconstruction Loss: -0.8478237390518188
Iteration 2851:
Training Loss: 4.09976863861084
Reconstruction Loss: -0.8242418766021729
Iteration 2901:
Training Loss: 4.278290271759033
Reconstruction Loss: -0.8145841360092163
Iteration 2951:
Training Loss: 4.250711917877197
Reconstruction Loss: -0.797227144241333
Iteration 3001:
Training Loss: 4.194785118103027
Reconstruction Loss: -0.7864941358566284
Iteration 3051:
Training Loss: 4.140969753265381
Reconstruction Loss: -0.7805556654930115
Iteration 3101:
Training Loss: 4.255125999450684
Reconstruction Loss: -0.7681901454925537
Iteration 3151:
Training Loss: 4.2746663093566895
Reconstruction Loss: -0.7675160765647888
Iteration 3201:
Training Loss: 4.273410320281982
Reconstruction Loss: -0.7594143152236938
Iteration 3251:
Training Loss: 4.1794304847717285
Reconstruction Loss: -0.7593633532524109
Iteration 3301:
Training Loss: 4.138665676116943
Reconstruction Loss: -0.7548156380653381
Iteration 3351:
Training Loss: 4.268445014953613
Reconstruction Loss: -0.7514872550964355
Iteration 3401:
Training Loss: 4.088901996612549
Reconstruction Loss: -0.7567818760871887
Iteration 3451:
Training Loss: 4.278847694396973
Reconstruction Loss: -0.7544184327125549
Iteration 3501:
Training Loss: 4.127622127532959
Reconstruction Loss: -0.7592840194702148
Iteration 3551:
Training Loss: 4.27280855178833
Reconstruction Loss: -0.7511020302772522
Iteration 3601:
Training Loss: 4.146332263946533
Reconstruction Loss: -0.7536304593086243
Iteration 3651:
Training Loss: 4.143779754638672
Reconstruction Loss: -0.7449654340744019
Iteration 3701:
Training Loss: 4.0677409172058105
Reconstruction Loss: -0.7554785013198853
Iteration 3751:
Training Loss: 4.1368207931518555
Reconstruction Loss: -0.7526894211769104
Iteration 3801:
Training Loss: 4.172074317932129
Reconstruction Loss: -0.7452935576438904
Iteration 3851:
Training Loss: 4.128279209136963
Reconstruction Loss: -0.7634454369544983
Iteration 3901:
Training Loss: 4.089320182800293
Reconstruction Loss: -0.8274644613265991
Iteration 3951:
Training Loss: 3.7126388549804688
Reconstruction Loss: -0.960615873336792
Iteration 4001:
Training Loss: 3.6469388008117676
Reconstruction Loss: -0.9980058670043945
Iteration 4051:
Training Loss: 3.64178204536438
Reconstruction Loss: -1.018811821937561
Iteration 4101:
Training Loss: 3.732123613357544
Reconstruction Loss: -1.025490641593933
Iteration 4151:
Training Loss: 3.646530866622925
Reconstruction Loss: -1.0270966291427612
Iteration 4201:
Training Loss: 3.662553548812866
Reconstruction Loss: -1.0281789302825928
Iteration 4251:
Training Loss: 3.51762318611145
Reconstruction Loss: -1.024185061454773
Iteration 4301:
Training Loss: 3.7024059295654297
Reconstruction Loss: -1.023627519607544
Iteration 4351:
Training Loss: 3.641367197036743
Reconstruction Loss: -1.0163891315460205
Iteration 4401:
Training Loss: 3.628135919570923
Reconstruction Loss: -1.007734775543213
Iteration 4451:
Training Loss: 3.654843807220459
Reconstruction Loss: -1.0111221075057983
Iteration 4501:
Training Loss: 3.342907190322876
Reconstruction Loss: -1.0055687427520752
Iteration 4551:
Training Loss: 3.5531461238861084
Reconstruction Loss: -0.9995515942573547
Iteration 4601:
Training Loss: 3.6612422466278076
Reconstruction Loss: -1.005234718322754
Iteration 4651:
Training Loss: 3.5395805835723877
Reconstruction Loss: -0.9999527931213379
Iteration 4701:
Training Loss: 3.5750620365142822
Reconstruction Loss: -1.0011849403381348
Iteration 4751:
Training Loss: 3.5126914978027344
Reconstruction Loss: -1.0064771175384521
Iteration 4801:
Training Loss: 3.665907382965088
Reconstruction Loss: -0.9988157749176025
Iteration 4851:
Training Loss: 3.6831247806549072
Reconstruction Loss: -0.9971696138381958
Iteration 4901:
Training Loss: 3.585221529006958
Reconstruction Loss: -0.997333288192749
Iteration 4951:
Training Loss: 3.470770835876465
Reconstruction Loss: -0.9951038360595703
Iteration 5001:
Training Loss: 3.6591501235961914
Reconstruction Loss: -0.9966108798980713
Iteration 5051:
Training Loss: 3.495260715484619
Reconstruction Loss: -0.9938881397247314
Iteration 5101:
Training Loss: 3.5139880180358887
Reconstruction Loss: -0.9893408417701721
Iteration 5151:
Training Loss: 3.563363552093506
Reconstruction Loss: -0.9958846569061279
Iteration 5201:
Training Loss: 3.719137191772461
Reconstruction Loss: -0.9965827465057373
Iteration 5251:
Training Loss: 3.672699213027954
Reconstruction Loss: -1.0013118982315063
Iteration 5301:
Training Loss: 3.5941555500030518
Reconstruction Loss: -0.9999687075614929
Iteration 5351:
Training Loss: 3.6331684589385986
Reconstruction Loss: -1.0011475086212158
Iteration 5401:
Training Loss: 3.658315896987915
Reconstruction Loss: -1.0034635066986084
Iteration 5451:
Training Loss: 3.4695448875427246
Reconstruction Loss: -1.0090115070343018
Iteration 5501:
Training Loss: 3.5375659465789795
Reconstruction Loss: -1.0242698192596436
Iteration 5551:
Training Loss: 3.395026683807373
Reconstruction Loss: -1.0976507663726807
Iteration 5601:
Training Loss: 3.099015474319458
Reconstruction Loss: -1.292478084564209
Iteration 5651:
Training Loss: 2.9814236164093018
Reconstruction Loss: -1.440029263496399
Iteration 5701:
Training Loss: 2.950824737548828
Reconstruction Loss: -1.520885944366455
Iteration 5751:
Training Loss: 2.895402431488037
Reconstruction Loss: -1.5608776807785034
Iteration 5801:
Training Loss: 2.8207850456237793
Reconstruction Loss: -1.5776588916778564
Iteration 5851:
Training Loss: 2.7641408443450928
Reconstruction Loss: -1.585201621055603
Iteration 5901:
Training Loss: 2.797884225845337
Reconstruction Loss: -1.5873440504074097
Iteration 5951:
Training Loss: 2.7606070041656494
Reconstruction Loss: -1.5893340110778809
Iteration 6001:
Training Loss: 2.9445888996124268
Reconstruction Loss: -1.5837510824203491
Iteration 6051:
Training Loss: 2.8687853813171387
Reconstruction Loss: -1.5792407989501953
Iteration 6101:
Training Loss: 2.703559398651123
Reconstruction Loss: -1.580249547958374
Iteration 6151:
Training Loss: 2.7794148921966553
Reconstruction Loss: -1.5798313617706299
Iteration 6201:
Training Loss: 2.781013011932373
Reconstruction Loss: -1.5780870914459229
Iteration 6251:
Training Loss: 2.6642751693725586
Reconstruction Loss: -1.5732821226119995
Iteration 6301:
Training Loss: 2.6653192043304443
Reconstruction Loss: -1.577841877937317
Iteration 6351:
Training Loss: 2.754693031311035
Reconstruction Loss: -1.577707052230835
Iteration 6401:
Training Loss: 2.8947641849517822
Reconstruction Loss: -1.5760693550109863
Iteration 6451:
Training Loss: 2.8600707054138184
Reconstruction Loss: -1.5749740600585938
Iteration 6501:
Training Loss: 2.824305295944214
Reconstruction Loss: -1.5839860439300537
Iteration 6551:
Training Loss: 2.8189189434051514
Reconstruction Loss: -1.5949218273162842
Iteration 6601:
Training Loss: 2.796677350997925
Reconstruction Loss: -1.6097075939178467
Iteration 6651:
Training Loss: 2.595608711242676
Reconstruction Loss: -1.6722334623336792
Iteration 6701:
Training Loss: 2.3470864295959473
Reconstruction Loss: -1.8875186443328857
Iteration 6751:
Training Loss: 1.436644434928894
Reconstruction Loss: -2.4556074142456055
Iteration 6801:
Training Loss: 0.821190357208252
Reconstruction Loss: -3.1046524047851562
Iteration 6851:
Training Loss: 0.04601721465587616
Reconstruction Loss: -3.689051866531372
Iteration 6901:
Training Loss: -0.49269920587539673
Reconstruction Loss: -4.239720821380615
Iteration 6951:
Training Loss: -1.0319628715515137
Reconstruction Loss: -4.762328147888184
Iteration 7001:
Training Loss: -1.6171451807022095
Reconstruction Loss: -5.264102458953857
Iteration 7051:
Training Loss: -2.010773181915283
Reconstruction Loss: -5.741603374481201
Iteration 7101:
Training Loss: -2.581129789352417
Reconstruction Loss: -6.199223518371582
Iteration 7151:
Training Loss: -2.988090753555298
Reconstruction Loss: -6.637453079223633
Iteration 7201:
Training Loss: -3.6233739852905273
Reconstruction Loss: -7.059033393859863
Iteration 7251:
Training Loss: -4.01051139831543
Reconstruction Loss: -7.463407516479492
Iteration 7301:
Training Loss: -4.55625581741333
Reconstruction Loss: -7.854128837585449
Iteration 7351:
Training Loss: -4.943406105041504
Reconstruction Loss: -8.23415470123291
Iteration 7401:
Training Loss: -5.2456793785095215
Reconstruction Loss: -8.598750114440918
Iteration 7451:
Training Loss: -5.6628618240356445
Reconstruction Loss: -8.954899787902832
