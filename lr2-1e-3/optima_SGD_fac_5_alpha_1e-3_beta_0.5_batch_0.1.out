5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.529890537261963
Reconstruction Loss: -0.5962952971458435
Iteration 11:
Training Loss: 2.7013423442840576
Reconstruction Loss: -1.751370906829834
Iteration 21:
Training Loss: 1.265636920928955
Reconstruction Loss: -2.412260055541992
Iteration 31:
Training Loss: 0.7292605042457581
Reconstruction Loss: -2.8331613540649414
Iteration 41:
Training Loss: -0.0644170269370079
Reconstruction Loss: -3.1487069129943848
Iteration 51:
Training Loss: -0.91132652759552
Reconstruction Loss: -3.3708672523498535
Iteration 61:
Training Loss: -0.6347032189369202
Reconstruction Loss: -3.557032585144043
Iteration 71:
Training Loss: -0.6535829305648804
Reconstruction Loss: -3.697885751724243
Iteration 81:
Training Loss: -1.2606409788131714
Reconstruction Loss: -3.804192304611206
Iteration 91:
Training Loss: -2.401440143585205
Reconstruction Loss: -3.9075143337249756
Iteration 101:
Training Loss: -1.643916130065918
Reconstruction Loss: -3.9778590202331543
Iteration 111:
Training Loss: -2.1286890506744385
Reconstruction Loss: -4.051488876342773
Iteration 121:
Training Loss: -1.7374968528747559
Reconstruction Loss: -4.1066203117370605
Iteration 131:
Training Loss: -2.197711229324341
Reconstruction Loss: -4.163084030151367
Iteration 141:
Training Loss: -2.4584414958953857
Reconstruction Loss: -4.212133407592773
Iteration 151:
Training Loss: -2.320103645324707
Reconstruction Loss: -4.252415180206299
Iteration 161:
Training Loss: -2.6590054035186768
Reconstruction Loss: -4.299523830413818
Iteration 171:
Training Loss: -2.46976375579834
Reconstruction Loss: -4.332585334777832
Iteration 181:
Training Loss: -2.8608832359313965
Reconstruction Loss: -4.361783504486084
Iteration 191:
Training Loss: -2.551842212677002
Reconstruction Loss: -4.393002510070801
Iteration 201:
Training Loss: -3.0306460857391357
Reconstruction Loss: -4.424770355224609
Iteration 211:
Training Loss: -2.8836472034454346
Reconstruction Loss: -4.454258918762207
Iteration 221:
Training Loss: -2.986680030822754
Reconstruction Loss: -4.483442783355713
Iteration 231:
Training Loss: -2.806971549987793
Reconstruction Loss: -4.502251148223877
Iteration 241:
Training Loss: -2.5381596088409424
Reconstruction Loss: -4.527716636657715
Iteration 251:
Training Loss: -3.2981975078582764
Reconstruction Loss: -4.548836708068848
Iteration 261:
Training Loss: -3.1651551723480225
Reconstruction Loss: -4.569292068481445
Iteration 271:
Training Loss: -3.119304895401001
Reconstruction Loss: -4.585904121398926
Iteration 281:
Training Loss: -3.4953999519348145
Reconstruction Loss: -4.604170799255371
Iteration 291:
Training Loss: -2.8918516635894775
Reconstruction Loss: -4.624505043029785
Iteration 301:
Training Loss: -3.0874826908111572
Reconstruction Loss: -4.649052619934082
Iteration 311:
Training Loss: -3.2070460319519043
Reconstruction Loss: -4.663980960845947
Iteration 321:
Training Loss: -3.229733467102051
Reconstruction Loss: -4.676975727081299
Iteration 331:
Training Loss: -3.21553635597229
Reconstruction Loss: -4.696369647979736
Iteration 341:
Training Loss: -3.396780014038086
Reconstruction Loss: -4.711287498474121
Iteration 351:
Training Loss: -3.9505720138549805
Reconstruction Loss: -4.72481107711792
Iteration 361:
Training Loss: -3.1944358348846436
Reconstruction Loss: -4.7369585037231445
Iteration 371:
Training Loss: -3.7911500930786133
Reconstruction Loss: -4.756681442260742
Iteration 381:
Training Loss: -3.168342113494873
Reconstruction Loss: -4.762682914733887
Iteration 391:
Training Loss: -3.6277358531951904
Reconstruction Loss: -4.774912357330322
Iteration 401:
Training Loss: -3.297769784927368
Reconstruction Loss: -4.791576385498047
Iteration 411:
Training Loss: -3.6145949363708496
Reconstruction Loss: -4.806646823883057
Iteration 421:
Training Loss: -3.3812522888183594
Reconstruction Loss: -4.812559604644775
Iteration 431:
Training Loss: -3.4441230297088623
Reconstruction Loss: -4.821900367736816
Iteration 441:
Training Loss: -4.079154014587402
Reconstruction Loss: -4.834918022155762
Iteration 451:
Training Loss: -3.7928762435913086
Reconstruction Loss: -4.841070652008057
Iteration 461:
Training Loss: -3.878342628479004
Reconstruction Loss: -4.852423667907715
Iteration 471:
Training Loss: -3.8737337589263916
Reconstruction Loss: -4.8656768798828125
Iteration 481:
Training Loss: -4.095144748687744
Reconstruction Loss: -4.878424167633057
Iteration 491:
Training Loss: -4.306941509246826
Reconstruction Loss: -4.882776737213135
Iteration 501:
Training Loss: -4.1115827560424805
Reconstruction Loss: -4.894510269165039
Iteration 511:
Training Loss: -3.9478495121002197
Reconstruction Loss: -4.901750087738037
Iteration 521:
Training Loss: -3.850027561187744
Reconstruction Loss: -4.9143757820129395
Iteration 531:
Training Loss: -3.822277784347534
Reconstruction Loss: -4.917447090148926
Iteration 541:
Training Loss: -3.8228530883789062
Reconstruction Loss: -4.929389476776123
Iteration 551:
Training Loss: -4.6584696769714355
Reconstruction Loss: -4.937726974487305
Iteration 561:
Training Loss: -4.433959007263184
Reconstruction Loss: -4.9474029541015625
Iteration 571:
Training Loss: -3.7014920711517334
Reconstruction Loss: -4.951458930969238
Iteration 581:
Training Loss: -4.053757667541504
Reconstruction Loss: -4.960391044616699
Iteration 591:
Training Loss: -4.7704243659973145
Reconstruction Loss: -4.964290618896484
Iteration 601:
Training Loss: -4.2624831199646
Reconstruction Loss: -4.9751505851745605
Iteration 611:
Training Loss: -4.642480850219727
Reconstruction Loss: -4.984272480010986
Iteration 621:
Training Loss: -4.422410488128662
Reconstruction Loss: -4.988236904144287
Iteration 631:
Training Loss: -4.277219295501709
Reconstruction Loss: -4.995184898376465
Iteration 641:
Training Loss: -4.316580295562744
Reconstruction Loss: -5.002629280090332
Iteration 651:
Training Loss: -4.154969215393066
Reconstruction Loss: -5.0112762451171875
Iteration 661:
Training Loss: -4.524580955505371
Reconstruction Loss: -5.016877174377441
Iteration 671:
Training Loss: -4.1849365234375
Reconstruction Loss: -5.0222392082214355
Iteration 681:
Training Loss: -4.586175441741943
Reconstruction Loss: -5.025183200836182
Iteration 691:
Training Loss: -4.789792537689209
Reconstruction Loss: -5.031828880310059
Iteration 701:
Training Loss: -3.9319071769714355
Reconstruction Loss: -5.042153835296631
Iteration 711:
Training Loss: -4.095513343811035
Reconstruction Loss: -5.043023586273193
Iteration 721:
Training Loss: -4.542087554931641
Reconstruction Loss: -5.048454284667969
Iteration 731:
Training Loss: -4.729465484619141
Reconstruction Loss: -5.056222915649414
Iteration 741:
Training Loss: -4.463731288909912
Reconstruction Loss: -5.063642978668213
Iteration 751:
Training Loss: -5.173712730407715
Reconstruction Loss: -5.071598052978516
Iteration 761:
Training Loss: -4.49418830871582
Reconstruction Loss: -5.073140621185303
Iteration 771:
Training Loss: -4.33479118347168
Reconstruction Loss: -5.078543186187744
Iteration 781:
Training Loss: -4.635088920593262
Reconstruction Loss: -5.081742763519287
Iteration 791:
Training Loss: -4.6184163093566895
Reconstruction Loss: -5.085432529449463
Iteration 801:
Training Loss: -4.854626178741455
Reconstruction Loss: -5.0934014320373535
Iteration 811:
Training Loss: -4.858639717102051
Reconstruction Loss: -5.095725059509277
Iteration 821:
Training Loss: -4.513911247253418
Reconstruction Loss: -5.1026811599731445
Iteration 831:
Training Loss: -5.1375532150268555
Reconstruction Loss: -5.101681232452393
Iteration 841:
Training Loss: -5.107888698577881
Reconstruction Loss: -5.109344482421875
Iteration 851:
Training Loss: -5.333319187164307
Reconstruction Loss: -5.117931842803955
Iteration 861:
Training Loss: -4.946293830871582
Reconstruction Loss: -5.119373321533203
Iteration 871:
Training Loss: -4.845757484436035
Reconstruction Loss: -5.123169898986816
Iteration 881:
Training Loss: -4.891290187835693
Reconstruction Loss: -5.125535011291504
Iteration 891:
Training Loss: -5.14035177230835
Reconstruction Loss: -5.129017353057861
Iteration 901:
Training Loss: -4.802304267883301
Reconstruction Loss: -5.136650085449219
Iteration 911:
Training Loss: -4.6953864097595215
Reconstruction Loss: -5.13602876663208
Iteration 921:
Training Loss: -5.164606094360352
Reconstruction Loss: -5.1411285400390625
Iteration 931:
Training Loss: -4.81553316116333
Reconstruction Loss: -5.143173694610596
Iteration 941:
Training Loss: -4.948796272277832
Reconstruction Loss: -5.149873733520508
Iteration 951:
Training Loss: -5.020930290222168
Reconstruction Loss: -5.154178619384766
Iteration 961:
Training Loss: -5.207353591918945
Reconstruction Loss: -5.159472465515137
Iteration 971:
Training Loss: -4.777256965637207
Reconstruction Loss: -5.160159587860107
Iteration 981:
Training Loss: -5.037388324737549
Reconstruction Loss: -5.163899898529053
Iteration 991:
Training Loss: -4.6183648109436035
Reconstruction Loss: -5.16772985458374
Iteration 1001:
Training Loss: -4.84267520904541
Reconstruction Loss: -5.170660018920898
Iteration 1011:
Training Loss: -5.0646796226501465
Reconstruction Loss: -5.172741889953613
Iteration 1021:
Training Loss: -5.020339012145996
Reconstruction Loss: -5.178528308868408
Iteration 1031:
Training Loss: -5.121159553527832
Reconstruction Loss: -5.181134223937988
Iteration 1041:
Training Loss: -5.148449420928955
Reconstruction Loss: -5.180603504180908
Iteration 1051:
Training Loss: -5.624291896820068
Reconstruction Loss: -5.187976837158203
Iteration 1061:
Training Loss: -4.982100486755371
Reconstruction Loss: -5.188915729522705
Iteration 1071:
Training Loss: -5.72760009765625
Reconstruction Loss: -5.194057464599609
Iteration 1081:
Training Loss: -5.557698726654053
Reconstruction Loss: -5.195307731628418
Iteration 1091:
Training Loss: -5.642844200134277
Reconstruction Loss: -5.1996612548828125
Iteration 1101:
Training Loss: -5.457268714904785
Reconstruction Loss: -5.200355529785156
Iteration 1111:
Training Loss: -5.299941062927246
Reconstruction Loss: -5.202652931213379
Iteration 1121:
Training Loss: -5.757352352142334
Reconstruction Loss: -5.206834316253662
Iteration 1131:
Training Loss: -5.329948425292969
Reconstruction Loss: -5.208908557891846
Iteration 1141:
Training Loss: -5.281749725341797
Reconstruction Loss: -5.2099738121032715
Iteration 1151:
Training Loss: -5.32633113861084
Reconstruction Loss: -5.212198734283447
Iteration 1161:
Training Loss: -5.34080696105957
Reconstruction Loss: -5.21434211730957
Iteration 1171:
Training Loss: -5.652868747711182
Reconstruction Loss: -5.217883586883545
Iteration 1181:
Training Loss: -5.439701080322266
Reconstruction Loss: -5.22268533706665
Iteration 1191:
Training Loss: -5.920555114746094
Reconstruction Loss: -5.222503185272217
Iteration 1201:
Training Loss: -5.384063720703125
Reconstruction Loss: -5.22700309753418
Iteration 1211:
Training Loss: -5.551889419555664
Reconstruction Loss: -5.226624488830566
Iteration 1221:
Training Loss: -5.754369258880615
Reconstruction Loss: -5.232595443725586
Iteration 1231:
Training Loss: -5.315223693847656
Reconstruction Loss: -5.233014106750488
Iteration 1241:
Training Loss: -5.5596418380737305
Reconstruction Loss: -5.234439373016357
Iteration 1251:
Training Loss: -5.410290241241455
Reconstruction Loss: -5.235058307647705
Iteration 1261:
Training Loss: -5.555844306945801
Reconstruction Loss: -5.237861633300781
Iteration 1271:
Training Loss: -5.449714660644531
Reconstruction Loss: -5.239975452423096
Iteration 1281:
Training Loss: -5.471222877502441
Reconstruction Loss: -5.2424092292785645
Iteration 1291:
Training Loss: -5.956394195556641
Reconstruction Loss: -5.245142936706543
Iteration 1301:
Training Loss: -5.690441131591797
Reconstruction Loss: -5.245246410369873
Iteration 1311:
Training Loss: -5.618081092834473
Reconstruction Loss: -5.2457146644592285
Iteration 1321:
Training Loss: -5.627747058868408
Reconstruction Loss: -5.248410224914551
Iteration 1331:
Training Loss: -5.880882263183594
Reconstruction Loss: -5.250940322875977
Iteration 1341:
Training Loss: -5.7498626708984375
Reconstruction Loss: -5.2533278465271
Iteration 1351:
Training Loss: -5.887546539306641
Reconstruction Loss: -5.255626201629639
Iteration 1361:
Training Loss: -5.846116065979004
Reconstruction Loss: -5.255514621734619
Iteration 1371:
Training Loss: -5.5350470542907715
Reconstruction Loss: -5.258742332458496
Iteration 1381:
Training Loss: -5.800555229187012
Reconstruction Loss: -5.262241363525391
Iteration 1391:
Training Loss: -5.978322982788086
Reconstruction Loss: -5.262351036071777
Iteration 1401:
Training Loss: -6.029005527496338
Reconstruction Loss: -5.264224052429199
Iteration 1411:
Training Loss: -5.577564716339111
Reconstruction Loss: -5.263083457946777
Iteration 1421:
Training Loss: -5.8095479011535645
Reconstruction Loss: -5.269099235534668
Iteration 1431:
Training Loss: -6.155511856079102
Reconstruction Loss: -5.265255451202393
Iteration 1441:
Training Loss: -5.58805513381958
Reconstruction Loss: -5.27028751373291
Iteration 1451:
Training Loss: -5.920629024505615
Reconstruction Loss: -5.27341365814209
Iteration 1461:
Training Loss: -6.371506214141846
Reconstruction Loss: -5.275418281555176
Iteration 1471:
Training Loss: -6.02413272857666
Reconstruction Loss: -5.274127960205078
Iteration 1481:
Training Loss: -5.978598594665527
Reconstruction Loss: -5.275417804718018
Iteration 1491:
Training Loss: -5.641136646270752
Reconstruction Loss: -5.279542446136475
Iteration 1501:
Training Loss: -5.754558563232422
Reconstruction Loss: -5.277101516723633
Iteration 1511:
Training Loss: -6.086923122406006
Reconstruction Loss: -5.281070709228516
Iteration 1521:
Training Loss: -5.990866661071777
Reconstruction Loss: -5.281859874725342
Iteration 1531:
Training Loss: -5.978212356567383
Reconstruction Loss: -5.284889221191406
Iteration 1541:
Training Loss: -5.82319450378418
Reconstruction Loss: -5.284747123718262
Iteration 1551:
Training Loss: -6.0718770027160645
Reconstruction Loss: -5.286113739013672
Iteration 1561:
Training Loss: -6.115104675292969
Reconstruction Loss: -5.28609037399292
Iteration 1571:
Training Loss: -6.319901943206787
Reconstruction Loss: -5.28851318359375
Iteration 1581:
Training Loss: -6.089792251586914
Reconstruction Loss: -5.290149688720703
Iteration 1591:
Training Loss: -6.642229080200195
Reconstruction Loss: -5.290482521057129
Iteration 1601:
Training Loss: -6.010949611663818
Reconstruction Loss: -5.291618347167969
Iteration 1611:
Training Loss: -6.301753520965576
Reconstruction Loss: -5.2937822341918945
Iteration 1621:
Training Loss: -6.924986362457275
Reconstruction Loss: -5.2928619384765625
Iteration 1631:
Training Loss: -6.177521228790283
Reconstruction Loss: -5.294275760650635
Iteration 1641:
Training Loss: -6.533604145050049
Reconstruction Loss: -5.296682834625244
Iteration 1651:
Training Loss: -6.2688446044921875
Reconstruction Loss: -5.296926975250244
Iteration 1661:
Training Loss: -6.594509601593018
Reconstruction Loss: -5.298376560211182
Iteration 1671:
Training Loss: -6.242313385009766
Reconstruction Loss: -5.299400329589844
Iteration 1681:
Training Loss: -6.543369293212891
Reconstruction Loss: -5.300212860107422
Iteration 1691:
Training Loss: -6.492327690124512
Reconstruction Loss: -5.3011040687561035
Iteration 1701:
Training Loss: -6.086399555206299
Reconstruction Loss: -5.304458141326904
Iteration 1711:
Training Loss: -6.5540690422058105
Reconstruction Loss: -5.305261135101318
Iteration 1721:
Training Loss: -6.701306343078613
Reconstruction Loss: -5.305141448974609
Iteration 1731:
Training Loss: -6.031625270843506
Reconstruction Loss: -5.306805610656738
Iteration 1741:
Training Loss: -6.240614891052246
Reconstruction Loss: -5.307845592498779
Iteration 1751:
Training Loss: -6.1873908042907715
Reconstruction Loss: -5.3084611892700195
Iteration 1761:
Training Loss: -6.665821075439453
Reconstruction Loss: -5.310491561889648
Iteration 1771:
Training Loss: -6.5918498039245605
Reconstruction Loss: -5.309879302978516
Iteration 1781:
Training Loss: -6.642955303192139
Reconstruction Loss: -5.31195592880249
Iteration 1791:
Training Loss: -6.987268447875977
Reconstruction Loss: -5.312071323394775
Iteration 1801:
Training Loss: -6.535447597503662
Reconstruction Loss: -5.3128132820129395
Iteration 1811:
Training Loss: -6.198261737823486
Reconstruction Loss: -5.31459379196167
Iteration 1821:
Training Loss: -6.328621864318848
Reconstruction Loss: -5.314144134521484
Iteration 1831:
Training Loss: -7.168697357177734
Reconstruction Loss: -5.316792964935303
Iteration 1841:
Training Loss: -6.737458229064941
Reconstruction Loss: -5.316676616668701
Iteration 1851:
Training Loss: -6.390498161315918
Reconstruction Loss: -5.316409111022949
Iteration 1861:
Training Loss: -6.6386590003967285
Reconstruction Loss: -5.317885398864746
Iteration 1871:
Training Loss: -6.631536483764648
Reconstruction Loss: -5.319860458374023
Iteration 1881:
Training Loss: -6.516477108001709
Reconstruction Loss: -5.319377899169922
Iteration 1891:
Training Loss: -6.973288059234619
Reconstruction Loss: -5.320978164672852
Iteration 1901:
Training Loss: -5.986846923828125
Reconstruction Loss: -5.320568084716797
Iteration 1911:
Training Loss: -6.466681003570557
Reconstruction Loss: -5.3221635818481445
Iteration 1921:
Training Loss: -6.348536014556885
Reconstruction Loss: -5.3230390548706055
Iteration 1931:
Training Loss: -6.635136127471924
Reconstruction Loss: -5.323030948638916
Iteration 1941:
Training Loss: -6.6407904624938965
Reconstruction Loss: -5.322980880737305
Iteration 1951:
Training Loss: -6.860136032104492
Reconstruction Loss: -5.325709342956543
Iteration 1961:
Training Loss: -6.7055983543396
Reconstruction Loss: -5.325385093688965
Iteration 1971:
Training Loss: -7.106850624084473
Reconstruction Loss: -5.325809955596924
Iteration 1981:
Training Loss: -6.6787004470825195
Reconstruction Loss: -5.327876091003418
Iteration 1991:
Training Loss: -6.666046142578125
Reconstruction Loss: -5.327085971832275
Iteration 2001:
Training Loss: -6.935832500457764
Reconstruction Loss: -5.327683448791504
Iteration 2011:
Training Loss: -6.920129776000977
Reconstruction Loss: -5.329920291900635
Iteration 2021:
Training Loss: -7.013648986816406
Reconstruction Loss: -5.330829620361328
Iteration 2031:
Training Loss: -7.323225021362305
Reconstruction Loss: -5.330718994140625
Iteration 2041:
Training Loss: -7.233570575714111
Reconstruction Loss: -5.330555438995361
Iteration 2051:
Training Loss: -6.949540615081787
Reconstruction Loss: -5.330864429473877
Iteration 2061:
Training Loss: -6.682699203491211
Reconstruction Loss: -5.3329572677612305
Iteration 2071:
Training Loss: -6.314844131469727
Reconstruction Loss: -5.333735466003418
Iteration 2081:
Training Loss: -6.718860626220703
Reconstruction Loss: -5.333178520202637
Iteration 2091:
Training Loss: -7.626460075378418
Reconstruction Loss: -5.332938194274902
Iteration 2101:
Training Loss: -6.875765323638916
Reconstruction Loss: -5.333761692047119
Iteration 2111:
Training Loss: -7.002272605895996
Reconstruction Loss: -5.335577487945557
Iteration 2121:
Training Loss: -6.621150493621826
Reconstruction Loss: -5.336337566375732
Iteration 2131:
Training Loss: -7.100728511810303
Reconstruction Loss: -5.337292671203613
Iteration 2141:
Training Loss: -7.128788471221924
Reconstruction Loss: -5.337705135345459
Iteration 2151:
Training Loss: -6.868815898895264
Reconstruction Loss: -5.336922645568848
Iteration 2161:
Training Loss: -6.884054183959961
Reconstruction Loss: -5.338986396789551
Iteration 2171:
Training Loss: -6.842816352844238
Reconstruction Loss: -5.339186191558838
Iteration 2181:
Training Loss: -7.0276265144348145
Reconstruction Loss: -5.339367866516113
Iteration 2191:
Training Loss: -7.174570560455322
Reconstruction Loss: -5.341087818145752
Iteration 2201:
Training Loss: -7.069974899291992
Reconstruction Loss: -5.341142177581787
Iteration 2211:
Training Loss: -6.963783264160156
Reconstruction Loss: -5.341479778289795
Iteration 2221:
Training Loss: -6.684957981109619
Reconstruction Loss: -5.34187126159668
Iteration 2231:
Training Loss: -7.012627601623535
Reconstruction Loss: -5.342216491699219
Iteration 2241:
Training Loss: -6.991568565368652
Reconstruction Loss: -5.343419075012207
Iteration 2251:
Training Loss: -6.9221343994140625
Reconstruction Loss: -5.344054698944092
Iteration 2261:
Training Loss: -7.086277008056641
Reconstruction Loss: -5.344316005706787
Iteration 2271:
Training Loss: -6.910469055175781
Reconstruction Loss: -5.344296932220459
Iteration 2281:
Training Loss: -6.859097957611084
Reconstruction Loss: -5.344704627990723
Iteration 2291:
Training Loss: -6.758555889129639
Reconstruction Loss: -5.3456597328186035
Iteration 2301:
Training Loss: -7.087881565093994
Reconstruction Loss: -5.34671688079834
Iteration 2311:
Training Loss: -6.801584243774414
Reconstruction Loss: -5.346616268157959
Iteration 2321:
Training Loss: -7.200060844421387
Reconstruction Loss: -5.346842288970947
Iteration 2331:
Training Loss: -6.918010711669922
Reconstruction Loss: -5.346627235412598
Iteration 2341:
Training Loss: -7.808716297149658
Reconstruction Loss: -5.347147464752197
Iteration 2351:
Training Loss: -7.25449800491333
Reconstruction Loss: -5.347883224487305
Iteration 2361:
Training Loss: -6.977773666381836
Reconstruction Loss: -5.346634387969971
Iteration 2371:
Training Loss: -7.370942115783691
Reconstruction Loss: -5.349298477172852
Iteration 2381:
Training Loss: -6.983821868896484
Reconstruction Loss: -5.348575592041016
Iteration 2391:
Training Loss: -7.0658183097839355
Reconstruction Loss: -5.35019588470459
Iteration 2401:
Training Loss: -7.5641069412231445
Reconstruction Loss: -5.350181579589844
Iteration 2411:
Training Loss: -7.456958770751953
Reconstruction Loss: -5.3500847816467285
Iteration 2421:
Training Loss: -6.7574567794799805
Reconstruction Loss: -5.350725173950195
Iteration 2431:
Training Loss: -7.14652681350708
Reconstruction Loss: -5.3524088859558105
Iteration 2441:
Training Loss: -7.100917339324951
Reconstruction Loss: -5.350741386413574
Iteration 2451:
Training Loss: -7.245057106018066
Reconstruction Loss: -5.353415489196777
Iteration 2461:
Training Loss: -7.5105180740356445
Reconstruction Loss: -5.352466106414795
Iteration 2471:
Training Loss: -7.650627136230469
Reconstruction Loss: -5.353182315826416
Iteration 2481:
Training Loss: -7.238952159881592
Reconstruction Loss: -5.352251052856445
Iteration 2491:
Training Loss: -7.247025966644287
Reconstruction Loss: -5.3534979820251465
Iteration 2501:
Training Loss: -7.375758647918701
Reconstruction Loss: -5.3537397384643555
Iteration 2511:
Training Loss: -7.231827259063721
Reconstruction Loss: -5.354404449462891
Iteration 2521:
Training Loss: -7.290853500366211
Reconstruction Loss: -5.3545918464660645
Iteration 2531:
Training Loss: -7.604320526123047
Reconstruction Loss: -5.3558878898620605
Iteration 2541:
Training Loss: -7.310132026672363
Reconstruction Loss: -5.354286193847656
Iteration 2551:
Training Loss: -7.248352527618408
Reconstruction Loss: -5.355556488037109
Iteration 2561:
Training Loss: -7.752501010894775
Reconstruction Loss: -5.3563947677612305
Iteration 2571:
Training Loss: -7.085186004638672
Reconstruction Loss: -5.355398178100586
Iteration 2581:
Training Loss: -7.523741722106934
Reconstruction Loss: -5.356472969055176
Iteration 2591:
Training Loss: -7.294365406036377
Reconstruction Loss: -5.357104301452637
Iteration 2601:
Training Loss: -7.202108383178711
Reconstruction Loss: -5.356741905212402
Iteration 2611:
Training Loss: -7.259537220001221
Reconstruction Loss: -5.357494831085205
Iteration 2621:
Training Loss: -7.298675537109375
Reconstruction Loss: -5.357926368713379
Iteration 2631:
Training Loss: -7.5874924659729
Reconstruction Loss: -5.358120918273926
Iteration 2641:
Training Loss: -7.548348903656006
Reconstruction Loss: -5.358080863952637
Iteration 2651:
Training Loss: -7.174766540527344
Reconstruction Loss: -5.357932090759277
Iteration 2661:
Training Loss: -7.4548659324646
Reconstruction Loss: -5.358789920806885
Iteration 2671:
Training Loss: -7.566338062286377
Reconstruction Loss: -5.359444618225098
Iteration 2681:
Training Loss: -7.391238212585449
Reconstruction Loss: -5.359920024871826
Iteration 2691:
Training Loss: -7.632926940917969
Reconstruction Loss: -5.359904766082764
Iteration 2701:
Training Loss: -7.141262531280518
Reconstruction Loss: -5.360570907592773
Iteration 2711:
Training Loss: -7.431379795074463
Reconstruction Loss: -5.360480785369873
Iteration 2721:
Training Loss: -7.57047700881958
Reconstruction Loss: -5.360814094543457
Iteration 2731:
Training Loss: -7.671971321105957
Reconstruction Loss: -5.361098766326904
Iteration 2741:
Training Loss: -7.910748481750488
Reconstruction Loss: -5.361345291137695
Iteration 2751:
Training Loss: -8.041930198669434
Reconstruction Loss: -5.361976623535156
Iteration 2761:
Training Loss: -7.464329719543457
Reconstruction Loss: -5.362430572509766
Iteration 2771:
Training Loss: -7.759923934936523
Reconstruction Loss: -5.361930847167969
Iteration 2781:
Training Loss: -7.424940586090088
Reconstruction Loss: -5.361855983734131
Iteration 2791:
Training Loss: -7.529074668884277
Reconstruction Loss: -5.362663269042969
Iteration 2801:
Training Loss: -7.553165435791016
Reconstruction Loss: -5.3629841804504395
Iteration 2811:
Training Loss: -7.9641499519348145
Reconstruction Loss: -5.363428115844727
Iteration 2821:
Training Loss: -7.166060447692871
Reconstruction Loss: -5.3638200759887695
Iteration 2831:
Training Loss: -7.4991044998168945
Reconstruction Loss: -5.363458156585693
Iteration 2841:
Training Loss: -7.222842216491699
Reconstruction Loss: -5.3633928298950195
Iteration 2851:
Training Loss: -7.618011474609375
Reconstruction Loss: -5.364041805267334
Iteration 2861:
Training Loss: -8.000927925109863
Reconstruction Loss: -5.364657878875732
Iteration 2871:
Training Loss: -7.633991718292236
Reconstruction Loss: -5.365476608276367
Iteration 2881:
Training Loss: -7.721814155578613
Reconstruction Loss: -5.3647637367248535
Iteration 2891:
Training Loss: -7.813548564910889
Reconstruction Loss: -5.364989757537842
Iteration 2901:
Training Loss: -7.78303861618042
Reconstruction Loss: -5.3652448654174805
Iteration 2911:
Training Loss: -8.325206756591797
Reconstruction Loss: -5.365307331085205
Iteration 2921:
Training Loss: -8.224686622619629
Reconstruction Loss: -5.366241455078125
Iteration 2931:
Training Loss: -7.890559196472168
Reconstruction Loss: -5.365153789520264
Iteration 2941:
Training Loss: -7.796464920043945
Reconstruction Loss: -5.366055011749268
Iteration 2951:
Training Loss: -7.975005626678467
Reconstruction Loss: -5.366344928741455
Iteration 2961:
Training Loss: -7.47028923034668
Reconstruction Loss: -5.367008209228516
Iteration 2971:
Training Loss: -7.681021213531494
Reconstruction Loss: -5.366819381713867
Iteration 2981:
Training Loss: -7.3935370445251465
Reconstruction Loss: -5.367103099822998
Iteration 2991:
Training Loss: -7.714068412780762
Reconstruction Loss: -5.366940498352051
Iteration 3001:
Training Loss: -7.71821403503418
Reconstruction Loss: -5.367569923400879
Iteration 3011:
Training Loss: -8.043793678283691
Reconstruction Loss: -5.367684364318848
Iteration 3021:
Training Loss: -7.86337947845459
Reconstruction Loss: -5.367921829223633
Iteration 3031:
Training Loss: -7.718866348266602
Reconstruction Loss: -5.367945194244385
Iteration 3041:
Training Loss: -7.909097194671631
Reconstruction Loss: -5.368340969085693
Iteration 3051:
Training Loss: -8.047812461853027
Reconstruction Loss: -5.368627548217773
Iteration 3061:
Training Loss: -7.741268157958984
Reconstruction Loss: -5.368744850158691
Iteration 3071:
Training Loss: -7.445898056030273
Reconstruction Loss: -5.369457244873047
Iteration 3081:
Training Loss: -7.76436185836792
Reconstruction Loss: -5.369329929351807
Iteration 3091:
Training Loss: -7.954929351806641
Reconstruction Loss: -5.36961030960083
Iteration 3101:
Training Loss: -7.774458885192871
Reconstruction Loss: -5.369224548339844
Iteration 3111:
Training Loss: -7.882920742034912
Reconstruction Loss: -5.370022296905518
Iteration 3121:
Training Loss: -8.236217498779297
Reconstruction Loss: -5.3692216873168945
Iteration 3131:
Training Loss: -7.943236351013184
Reconstruction Loss: -5.369497776031494
Iteration 3141:
Training Loss: -7.922671318054199
Reconstruction Loss: -5.370162487030029
Iteration 3151:
Training Loss: -7.52667760848999
Reconstruction Loss: -5.369497776031494
Iteration 3161:
Training Loss: -7.605916976928711
Reconstruction Loss: -5.371640682220459
Iteration 3171:
Training Loss: -7.975272178649902
Reconstruction Loss: -5.370578765869141
Iteration 3181:
Training Loss: -8.379695892333984
Reconstruction Loss: -5.370365619659424
Iteration 3191:
Training Loss: -8.219016075134277
Reconstruction Loss: -5.370031356811523
Iteration 3201:
Training Loss: -7.876974582672119
Reconstruction Loss: -5.3710737228393555
Iteration 3211:
Training Loss: -8.103707313537598
Reconstruction Loss: -5.371334075927734
Iteration 3221:
Training Loss: -7.943634033203125
Reconstruction Loss: -5.372139930725098
Iteration 3231:
Training Loss: -7.770162105560303
Reconstruction Loss: -5.372162818908691
Iteration 3241:
Training Loss: -8.18384838104248
Reconstruction Loss: -5.37152099609375
Iteration 3251:
Training Loss: -8.43433952331543
Reconstruction Loss: -5.37244176864624
Iteration 3261:
Training Loss: -7.960371971130371
Reconstruction Loss: -5.372496128082275
Iteration 3271:
Training Loss: -8.270337104797363
Reconstruction Loss: -5.372398853302002
Iteration 3281:
Training Loss: -7.9152936935424805
Reconstruction Loss: -5.3726959228515625
Iteration 3291:
Training Loss: -8.28261947631836
Reconstruction Loss: -5.372279644012451
Iteration 3301:
Training Loss: -8.090681076049805
Reconstruction Loss: -5.3733625411987305
Iteration 3311:
Training Loss: -7.974757671356201
Reconstruction Loss: -5.372623920440674
Iteration 3321:
Training Loss: -8.550162315368652
Reconstruction Loss: -5.3736162185668945
Iteration 3331:
Training Loss: -8.04891586303711
Reconstruction Loss: -5.372750282287598
Iteration 3341:
Training Loss: -8.494464874267578
Reconstruction Loss: -5.373159885406494
Iteration 3351:
Training Loss: -8.271597862243652
Reconstruction Loss: -5.374042510986328
Iteration 3361:
Training Loss: -8.360203742980957
Reconstruction Loss: -5.373896598815918
Iteration 3371:
Training Loss: -8.148261070251465
Reconstruction Loss: -5.374634742736816
Iteration 3381:
Training Loss: -8.079065322875977
Reconstruction Loss: -5.374125003814697
Iteration 3391:
Training Loss: -8.395764350891113
Reconstruction Loss: -5.374345302581787
Iteration 3401:
Training Loss: -8.243566513061523
Reconstruction Loss: -5.374752044677734
Iteration 3411:
Training Loss: -7.960271835327148
Reconstruction Loss: -5.374843597412109
Iteration 3421:
Training Loss: -8.281062126159668
Reconstruction Loss: -5.375030994415283
Iteration 3431:
Training Loss: -7.88246488571167
Reconstruction Loss: -5.3745903968811035
Iteration 3441:
Training Loss: -8.18687629699707
Reconstruction Loss: -5.375284671783447
Iteration 3451:
Training Loss: -8.576518058776855
Reconstruction Loss: -5.374912261962891
Iteration 3461:
Training Loss: -8.152396202087402
Reconstruction Loss: -5.37540340423584
Iteration 3471:
Training Loss: -8.490008354187012
Reconstruction Loss: -5.37541389465332
Iteration 3481:
Training Loss: -8.270352363586426
Reconstruction Loss: -5.375990390777588
Iteration 3491:
Training Loss: -8.584310531616211
Reconstruction Loss: -5.376387596130371
Iteration 3501:
Training Loss: -8.592875480651855
Reconstruction Loss: -5.375361919403076
Iteration 3511:
Training Loss: -8.071592330932617
Reconstruction Loss: -5.375975131988525
Iteration 3521:
Training Loss: -8.189495086669922
Reconstruction Loss: -5.376416206359863
Iteration 3531:
Training Loss: -8.70273208618164
Reconstruction Loss: -5.376069068908691
Iteration 3541:
Training Loss: -8.227641105651855
Reconstruction Loss: -5.3760271072387695
Iteration 3551:
Training Loss: -8.40323257446289
Reconstruction Loss: -5.37656307220459
Iteration 3561:
Training Loss: -8.211050033569336
Reconstruction Loss: -5.377021789550781
Iteration 3571:
Training Loss: -8.299864768981934
Reconstruction Loss: -5.376437187194824
Iteration 3581:
Training Loss: -8.620603561401367
Reconstruction Loss: -5.3765106201171875
Iteration 3591:
Training Loss: -8.529949188232422
Reconstruction Loss: -5.376890659332275
Iteration 3601:
Training Loss: -8.234221458435059
Reconstruction Loss: -5.377540111541748
Iteration 3611:
Training Loss: -8.459329605102539
Reconstruction Loss: -5.3770432472229
Iteration 3621:
Training Loss: -8.661561965942383
Reconstruction Loss: -5.377012252807617
Iteration 3631:
Training Loss: -8.739022254943848
Reconstruction Loss: -5.377077102661133
Iteration 3641:
Training Loss: -8.474414825439453
Reconstruction Loss: -5.376863956451416
Iteration 3651:
Training Loss: -8.25568675994873
Reconstruction Loss: -5.377153396606445
Iteration 3661:
Training Loss: -8.310514450073242
Reconstruction Loss: -5.376918792724609
Iteration 3671:
Training Loss: -8.157296180725098
Reconstruction Loss: -5.377802848815918
Iteration 3681:
Training Loss: -8.203184127807617
Reconstruction Loss: -5.3774518966674805
Iteration 3691:
Training Loss: -8.929483413696289
Reconstruction Loss: -5.377686023712158
Iteration 3701:
Training Loss: -8.427891731262207
Reconstruction Loss: -5.377851963043213
Iteration 3711:
Training Loss: -8.507246971130371
Reconstruction Loss: -5.377435207366943
Iteration 3721:
Training Loss: -8.257772445678711
Reconstruction Loss: -5.377650260925293
Iteration 3731:
Training Loss: -8.269463539123535
Reconstruction Loss: -5.377507209777832
Iteration 3741:
Training Loss: -9.412100791931152
Reconstruction Loss: -5.378706932067871
Iteration 3751:
Training Loss: -8.068368911743164
Reconstruction Loss: -5.378575801849365
Iteration 3761:
Training Loss: -8.812565803527832
Reconstruction Loss: -5.378250598907471
Iteration 3771:
Training Loss: -8.197938919067383
Reconstruction Loss: -5.378489017486572
Iteration 3781:
Training Loss: -9.002449035644531
Reconstruction Loss: -5.378289699554443
Iteration 3791:
Training Loss: -8.549186706542969
Reconstruction Loss: -5.3786444664001465
Iteration 3801:
Training Loss: -8.534712791442871
Reconstruction Loss: -5.378488540649414
Iteration 3811:
Training Loss: -8.984516143798828
Reconstruction Loss: -5.378889560699463
Iteration 3821:
Training Loss: -8.867209434509277
Reconstruction Loss: -5.378920078277588
Iteration 3831:
Training Loss: -8.365585327148438
Reconstruction Loss: -5.378971099853516
Iteration 3841:
Training Loss: -8.573217391967773
Reconstruction Loss: -5.37848424911499
Iteration 3851:
Training Loss: -8.781232833862305
Reconstruction Loss: -5.379319190979004
Iteration 3861:
Training Loss: -8.47973346710205
Reconstruction Loss: -5.379255771636963
Iteration 3871:
Training Loss: -8.340024948120117
Reconstruction Loss: -5.379369735717773
Iteration 3881:
Training Loss: -8.241730690002441
Reconstruction Loss: -5.379139423370361
Iteration 3891:
Training Loss: -8.716675758361816
Reconstruction Loss: -5.379827976226807
Iteration 3901:
Training Loss: -8.708730697631836
Reconstruction Loss: -5.379178047180176
Iteration 3911:
Training Loss: -8.964248657226562
Reconstruction Loss: -5.380188941955566
Iteration 3921:
Training Loss: -8.412067413330078
Reconstruction Loss: -5.379665374755859
Iteration 3931:
Training Loss: -8.859800338745117
Reconstruction Loss: -5.3797736167907715
Iteration 3941:
Training Loss: -8.176822662353516
Reconstruction Loss: -5.380000114440918
Iteration 3951:
Training Loss: -8.996894836425781
Reconstruction Loss: -5.380281925201416
Iteration 3961:
Training Loss: -8.974940299987793
Reconstruction Loss: -5.38033390045166
Iteration 3971:
Training Loss: -8.632291793823242
Reconstruction Loss: -5.380284309387207
Iteration 3981:
Training Loss: -8.476458549499512
Reconstruction Loss: -5.380362510681152
Iteration 3991:
Training Loss: -8.698671340942383
Reconstruction Loss: -5.380163669586182
Iteration 4001:
Training Loss: -8.585344314575195
Reconstruction Loss: -5.379943370819092
Iteration 4011:
Training Loss: -8.609034538269043
Reconstruction Loss: -5.381107807159424
Iteration 4021:
Training Loss: -8.600833892822266
Reconstruction Loss: -5.380948066711426
Iteration 4031:
Training Loss: -8.92166805267334
Reconstruction Loss: -5.38106107711792
Iteration 4041:
Training Loss: -8.735258102416992
Reconstruction Loss: -5.381232261657715
Iteration 4051:
Training Loss: -8.484597206115723
Reconstruction Loss: -5.380911827087402
Iteration 4061:
Training Loss: -8.785041809082031
Reconstruction Loss: -5.380598068237305
Iteration 4071:
Training Loss: -8.483152389526367
Reconstruction Loss: -5.3810625076293945
Iteration 4081:
Training Loss: -8.732890129089355
Reconstruction Loss: -5.382172107696533
Iteration 4091:
Training Loss: -9.003429412841797
Reconstruction Loss: -5.381272792816162
Iteration 4101:
Training Loss: -9.15002155303955
Reconstruction Loss: -5.381289005279541
Iteration 4111:
Training Loss: -8.795321464538574
Reconstruction Loss: -5.381406307220459
Iteration 4121:
Training Loss: -8.933723449707031
Reconstruction Loss: -5.381617546081543
Iteration 4131:
Training Loss: -8.61805534362793
Reconstruction Loss: -5.381644248962402
Iteration 4141:
Training Loss: -8.807965278625488
Reconstruction Loss: -5.38083028793335
Iteration 4151:
Training Loss: -8.33956527709961
Reconstruction Loss: -5.381604194641113
Iteration 4161:
Training Loss: -8.855911254882812
Reconstruction Loss: -5.381262302398682
Iteration 4171:
Training Loss: -8.924236297607422
Reconstruction Loss: -5.381143093109131
Iteration 4181:
Training Loss: -8.658188819885254
Reconstruction Loss: -5.381913185119629
Iteration 4191:
Training Loss: -9.032423973083496
Reconstruction Loss: -5.3818817138671875
Iteration 4201:
Training Loss: -9.072509765625
Reconstruction Loss: -5.381383895874023
Iteration 4211:
Training Loss: -8.885262489318848
Reconstruction Loss: -5.381407260894775
Iteration 4221:
Training Loss: -8.93542194366455
Reconstruction Loss: -5.381406307220459
Iteration 4231:
Training Loss: -9.141450881958008
Reconstruction Loss: -5.3819122314453125
Iteration 4241:
Training Loss: -9.256847381591797
Reconstruction Loss: -5.382599830627441
Iteration 4251:
Training Loss: -8.889068603515625
Reconstruction Loss: -5.382165908813477
Iteration 4261:
Training Loss: -9.131149291992188
Reconstruction Loss: -5.381985664367676
Iteration 4271:
Training Loss: -8.55788516998291
Reconstruction Loss: -5.381906032562256
Iteration 4281:
Training Loss: -9.11565113067627
Reconstruction Loss: -5.38192892074585
Iteration 4291:
Training Loss: -9.227599143981934
Reconstruction Loss: -5.3822455406188965
Iteration 4301:
Training Loss: -8.73110294342041
Reconstruction Loss: -5.382643222808838
Iteration 4311:
Training Loss: -8.830137252807617
Reconstruction Loss: -5.38223934173584
Iteration 4321:
Training Loss: -9.392474174499512
Reconstruction Loss: -5.382514476776123
Iteration 4331:
Training Loss: -8.981515884399414
Reconstruction Loss: -5.382713794708252
Iteration 4341:
Training Loss: -9.314833641052246
Reconstruction Loss: -5.382702350616455
Iteration 4351:
Training Loss: -8.830079078674316
Reconstruction Loss: -5.38228702545166
Iteration 4361:
Training Loss: -9.24116325378418
Reconstruction Loss: -5.382707595825195
Iteration 4371:
Training Loss: -9.001887321472168
Reconstruction Loss: -5.382512092590332
Iteration 4381:
Training Loss: -8.86400318145752
Reconstruction Loss: -5.382658004760742
Iteration 4391:
Training Loss: -8.881555557250977
Reconstruction Loss: -5.382839202880859
Iteration 4401:
Training Loss: -9.149105072021484
Reconstruction Loss: -5.382833003997803
Iteration 4411:
Training Loss: -8.578812599182129
Reconstruction Loss: -5.382692813873291
Iteration 4421:
Training Loss: -9.057761192321777
Reconstruction Loss: -5.382838726043701
Iteration 4431:
Training Loss: -8.923335075378418
Reconstruction Loss: -5.382939338684082
Iteration 4441:
Training Loss: -9.002076148986816
Reconstruction Loss: -5.383042335510254
Iteration 4451:
Training Loss: -9.180366516113281
Reconstruction Loss: -5.383760929107666
Iteration 4461:
Training Loss: -9.061039924621582
Reconstruction Loss: -5.3835930824279785
Iteration 4471:
Training Loss: -8.957948684692383
Reconstruction Loss: -5.383290767669678
Iteration 4481:
Training Loss: -9.273354530334473
Reconstruction Loss: -5.383471488952637
Iteration 4491:
Training Loss: -8.980323791503906
Reconstruction Loss: -5.38344669342041
Iteration 4501:
Training Loss: -9.225598335266113
Reconstruction Loss: -5.383713722229004
Iteration 4511:
Training Loss: -8.911699295043945
Reconstruction Loss: -5.383546829223633
Iteration 4521:
Training Loss: -8.700522422790527
Reconstruction Loss: -5.383269309997559
Iteration 4531:
Training Loss: -8.859823226928711
Reconstruction Loss: -5.383321285247803
Iteration 4541:
Training Loss: -9.127214431762695
Reconstruction Loss: -5.383139610290527
Iteration 4551:
Training Loss: -9.072088241577148
Reconstruction Loss: -5.383529186248779
Iteration 4561:
Training Loss: -9.051608085632324
Reconstruction Loss: -5.384057998657227
Iteration 4571:
Training Loss: -8.676591873168945
Reconstruction Loss: -5.383601665496826
Iteration 4581:
Training Loss: -8.950592041015625
Reconstruction Loss: -5.383752346038818
Iteration 4591:
Training Loss: -8.940381050109863
Reconstruction Loss: -5.38374662399292
Iteration 4601:
Training Loss: -9.36349105834961
Reconstruction Loss: -5.383806228637695
Iteration 4611:
Training Loss: -9.099034309387207
Reconstruction Loss: -5.384161949157715
Iteration 4621:
Training Loss: -9.053275108337402
Reconstruction Loss: -5.3840107917785645
Iteration 4631:
Training Loss: -9.729385375976562
Reconstruction Loss: -5.3838629722595215
Iteration 4641:
Training Loss: -9.0032958984375
Reconstruction Loss: -5.383847236633301
Iteration 4651:
Training Loss: -9.544200897216797
Reconstruction Loss: -5.384337902069092
Iteration 4661:
Training Loss: -9.173885345458984
Reconstruction Loss: -5.384052753448486
Iteration 4671:
Training Loss: -9.484989166259766
Reconstruction Loss: -5.383864879608154
Iteration 4681:
Training Loss: -9.003775596618652
Reconstruction Loss: -5.38401985168457
Iteration 4691:
Training Loss: -9.332005500793457
Reconstruction Loss: -5.383933067321777
Iteration 4701:
Training Loss: -9.30732250213623
Reconstruction Loss: -5.384030818939209
Iteration 4711:
Training Loss: -9.145195960998535
Reconstruction Loss: -5.384032726287842
Iteration 4721:
Training Loss: -9.589465141296387
Reconstruction Loss: -5.384144306182861
Iteration 4731:
Training Loss: -9.050860404968262
Reconstruction Loss: -5.384161949157715
Iteration 4741:
Training Loss: -9.542988777160645
Reconstruction Loss: -5.384423732757568
Iteration 4751:
Training Loss: -9.337038040161133
Reconstruction Loss: -5.384181022644043
Iteration 4761:
Training Loss: -9.111627578735352
Reconstruction Loss: -5.384358882904053
Iteration 4771:
Training Loss: -9.401853561401367
Reconstruction Loss: -5.384368419647217
Iteration 4781:
Training Loss: -9.251639366149902
Reconstruction Loss: -5.384678840637207
Iteration 4791:
Training Loss: -9.21148681640625
Reconstruction Loss: -5.384156227111816
Iteration 4801:
Training Loss: -9.455613136291504
Reconstruction Loss: -5.3844404220581055
Iteration 4811:
Training Loss: -9.435124397277832
Reconstruction Loss: -5.384712219238281
Iteration 4821:
Training Loss: -9.529769897460938
Reconstruction Loss: -5.384400367736816
Iteration 4831:
Training Loss: -9.58191204071045
Reconstruction Loss: -5.384561538696289
Iteration 4841:
Training Loss: -9.633491516113281
Reconstruction Loss: -5.385043621063232
Iteration 4851:
Training Loss: -9.528209686279297
Reconstruction Loss: -5.384490013122559
Iteration 4861:
Training Loss: -9.43839168548584
Reconstruction Loss: -5.384714126586914
Iteration 4871:
Training Loss: -8.956390380859375
Reconstruction Loss: -5.384456157684326
Iteration 4881:
Training Loss: -9.208721160888672
Reconstruction Loss: -5.384449005126953
Iteration 4891:
Training Loss: -9.65920352935791
Reconstruction Loss: -5.384472370147705
Iteration 4901:
Training Loss: -9.159576416015625
Reconstruction Loss: -5.384918689727783
Iteration 4911:
Training Loss: -9.21423053741455
Reconstruction Loss: -5.384836673736572
Iteration 4921:
Training Loss: -8.973881721496582
Reconstruction Loss: -5.3849310874938965
Iteration 4931:
Training Loss: -9.266200065612793
Reconstruction Loss: -5.384801864624023
Iteration 4941:
Training Loss: -9.287245750427246
Reconstruction Loss: -5.384610652923584
Iteration 4951:
Training Loss: -9.502341270446777
Reconstruction Loss: -5.384554386138916
Iteration 4961:
Training Loss: -8.860919952392578
Reconstruction Loss: -5.3846893310546875
Iteration 4971:
Training Loss: -9.095843315124512
Reconstruction Loss: -5.384683609008789
Iteration 4981:
Training Loss: -9.125149726867676
Reconstruction Loss: -5.3844828605651855
Iteration 4991:
Training Loss: -9.50598430633545
Reconstruction Loss: -5.38516902923584
