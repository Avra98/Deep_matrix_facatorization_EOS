5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.751535892486572
Reconstruction Loss: -0.35449835658073425
Iteration 101:
Training Loss: 5.734462738037109
Reconstruction Loss: -0.36372870206832886
Iteration 201:
Training Loss: 5.0936055183410645
Reconstruction Loss: -0.5300405621528625
Iteration 301:
Training Loss: 4.2087860107421875
Reconstruction Loss: -0.9593491554260254
Iteration 401:
Training Loss: 3.8865463733673096
Reconstruction Loss: -1.2002323865890503
Iteration 501:
Training Loss: 3.839569568634033
Reconstruction Loss: -1.1698018312454224
Iteration 601:
Training Loss: 3.8209340572357178
Reconstruction Loss: -1.1440017223358154
Iteration 701:
Training Loss: 3.793086290359497
Reconstruction Loss: -1.135400414466858
Iteration 801:
Training Loss: 3.6160125732421875
Reconstruction Loss: -1.1868038177490234
Iteration 901:
Training Loss: 3.1365036964416504
Reconstruction Loss: -1.4606118202209473
Iteration 1001:
Training Loss: 2.842599630355835
Reconstruction Loss: -1.6483898162841797
Iteration 1101:
Training Loss: 2.669301748275757
Reconstruction Loss: -1.6943732500076294
Iteration 1201:
Training Loss: 2.4492075443267822
Reconstruction Loss: -1.763379693031311
Iteration 1301:
Training Loss: 2.0109763145446777
Reconstruction Loss: -2.03108286857605
Iteration 1401:
Training Loss: 0.8148682117462158
Reconstruction Loss: -2.885502815246582
Iteration 1501:
Training Loss: -0.4161880910396576
Reconstruction Loss: -3.7411696910858154
Iteration 1601:
Training Loss: -1.1359903812408447
Reconstruction Loss: -4.285927772521973
Iteration 1701:
Training Loss: -1.5664269924163818
Reconstruction Loss: -4.655259609222412
Iteration 1801:
Training Loss: -1.8610846996307373
Reconstruction Loss: -4.935172080993652
Iteration 1901:
Training Loss: -2.0884296894073486
Reconstruction Loss: -5.164621829986572
Iteration 2001:
Training Loss: -2.276946544647217
Reconstruction Loss: -5.360986709594727
Iteration 2101:
Training Loss: -2.439450263977051
Reconstruction Loss: -5.532957077026367
Iteration 2201:
Training Loss: -2.5826966762542725
Reconstruction Loss: -5.685661792755127
Iteration 2301:
Training Loss: -2.7108421325683594
Reconstruction Loss: -5.822584629058838
Iteration 2401:
Training Loss: -2.826744556427002
Reconstruction Loss: -5.946322441101074
Iteration 2501:
Training Loss: -2.93249249458313
Reconstruction Loss: -6.058903217315674
Iteration 2601:
Training Loss: -3.0296823978424072
Reconstruction Loss: -6.161953449249268
Iteration 2701:
Training Loss: -3.11957049369812
Reconstruction Loss: -6.25680685043335
Iteration 2801:
Training Loss: -3.203160524368286
Reconstruction Loss: -6.344559669494629
Iteration 2901:
Training Loss: -3.2812659740448
Reconstruction Loss: -6.426119327545166
Iteration 3001:
Training Loss: -3.3545587062835693
Reconstruction Loss: -6.5022478103637695
Iteration 3101:
Training Loss: -3.423591136932373
Reconstruction Loss: -6.573582172393799
Iteration 3201:
Training Loss: -3.4888362884521484
Reconstruction Loss: -6.640664577484131
Iteration 3301:
Training Loss: -3.5506842136383057
Reconstruction Loss: -6.703949928283691
Iteration 3401:
Training Loss: -3.6094770431518555
Reconstruction Loss: -6.7638349533081055
Iteration 3501:
Training Loss: -3.665503740310669
Reconstruction Loss: -6.820650100708008
Iteration 3601:
Training Loss: -3.719015121459961
Reconstruction Loss: -6.874690055847168
Iteration 3701:
Training Loss: -3.7702274322509766
Reconstruction Loss: -6.926208972930908
Iteration 3801:
Training Loss: -3.8193373680114746
Reconstruction Loss: -6.975422382354736
Iteration 3901:
Training Loss: -3.8665103912353516
Reconstruction Loss: -7.022525310516357
Iteration 4001:
Training Loss: -3.9118967056274414
Reconstruction Loss: -7.067691326141357
Iteration 4101:
Training Loss: -3.955623149871826
Reconstruction Loss: -7.111067295074463
Iteration 4201:
Training Loss: -3.9978113174438477
Reconstruction Loss: -7.15278959274292
Iteration 4301:
Training Loss: -4.038570880889893
Reconstruction Loss: -7.19297456741333
Iteration 4401:
Training Loss: -4.0779948234558105
Reconstruction Loss: -7.231736183166504
Iteration 4501:
Training Loss: -4.11616849899292
Reconstruction Loss: -7.2691650390625
Iteration 4601:
Training Loss: -4.153170108795166
Reconstruction Loss: -7.305355548858643
Iteration 4701:
Training Loss: -4.189072132110596
Reconstruction Loss: -7.340377330780029
Iteration 4801:
Training Loss: -4.223933219909668
Reconstruction Loss: -7.374305725097656
Iteration 4901:
Training Loss: -4.257814884185791
Reconstruction Loss: -7.40720796585083
Iteration 5001:
Training Loss: -4.290774822235107
Reconstruction Loss: -7.4391398429870605
Iteration 5101:
Training Loss: -4.32286262512207
Reconstruction Loss: -7.470163822174072
Iteration 5201:
Training Loss: -4.354122638702393
Reconstruction Loss: -7.500324249267578
Iteration 5301:
Training Loss: -4.384592533111572
Reconstruction Loss: -7.529662609100342
Iteration 5401:
Training Loss: -4.414318084716797
Reconstruction Loss: -7.558226585388184
Iteration 5501:
Training Loss: -4.443326473236084
Reconstruction Loss: -7.586053848266602
Iteration 5601:
Training Loss: -4.471668243408203
Reconstruction Loss: -7.613188743591309
Iteration 5701:
Training Loss: -4.499354839324951
Reconstruction Loss: -7.6396613121032715
Iteration 5801:
Training Loss: -4.526425361633301
Reconstruction Loss: -7.665491580963135
Iteration 5901:
Training Loss: -4.552907466888428
Reconstruction Loss: -7.6907219886779785
Iteration 6001:
Training Loss: -4.578822135925293
Reconstruction Loss: -7.715373992919922
Iteration 6101:
Training Loss: -4.6041975021362305
Reconstruction Loss: -7.739477157592773
Iteration 6201:
Training Loss: -4.629058361053467
Reconstruction Loss: -7.763050556182861
Iteration 6301:
Training Loss: -4.653412342071533
Reconstruction Loss: -7.786116123199463
Iteration 6401:
Training Loss: -4.677287578582764
Reconstruction Loss: -7.80869722366333
Iteration 6501:
Training Loss: -4.700716018676758
Reconstruction Loss: -7.830811500549316
Iteration 6601:
Training Loss: -4.723694801330566
Reconstruction Loss: -7.852485179901123
Iteration 6701:
Training Loss: -4.746244430541992
Reconstruction Loss: -7.873726844787598
Iteration 6801:
Training Loss: -4.768392086029053
Reconstruction Loss: -7.894559383392334
Iteration 6901:
Training Loss: -4.7901387214660645
Reconstruction Loss: -7.91499137878418
Iteration 7001:
Training Loss: -4.811506748199463
Reconstruction Loss: -7.935040473937988
Iteration 7101:
Training Loss: -4.83250617980957
Reconstruction Loss: -7.954721927642822
Iteration 7201:
Training Loss: -4.853147029876709
Reconstruction Loss: -7.974054336547852
Iteration 7301:
Training Loss: -4.873448848724365
Reconstruction Loss: -7.993039608001709
Iteration 7401:
Training Loss: -4.893411159515381
Reconstruction Loss: -8.011687278747559
Iteration 7501:
Training Loss: -4.913061618804932
Reconstruction Loss: -8.030017852783203
Iteration 7601:
Training Loss: -4.932394504547119
Reconstruction Loss: -8.048038482666016
Iteration 7701:
Training Loss: -4.951418399810791
Reconstruction Loss: -8.06576919555664
Iteration 7801:
Training Loss: -4.970168590545654
Reconstruction Loss: -8.083194732666016
Iteration 7901:
Training Loss: -4.988624095916748
Reconstruction Loss: -8.100342750549316
Iteration 8001:
Training Loss: -5.00679874420166
Reconstruction Loss: -8.117226600646973
Iteration 8101:
Training Loss: -5.024713516235352
Reconstruction Loss: -8.133840560913086
Iteration 8201:
Training Loss: -5.042369842529297
Reconstruction Loss: -8.150206565856934
Iteration 8301:
Training Loss: -5.05977201461792
Reconstruction Loss: -8.166322708129883
Iteration 8401:
Training Loss: -5.076930522918701
Reconstruction Loss: -8.182188034057617
Iteration 8501:
Training Loss: -5.093852996826172
Reconstruction Loss: -8.197834968566895
Iteration 8601:
Training Loss: -5.110535621643066
Reconstruction Loss: -8.213249206542969
Iteration 8701:
Training Loss: -5.127003192901611
Reconstruction Loss: -8.228446006774902
Iteration 8801:
Training Loss: -5.14324951171875
Reconstruction Loss: -8.243420600891113
Iteration 8901:
Training Loss: -5.159276485443115
Reconstruction Loss: -8.258188247680664
Iteration 9001:
Training Loss: -5.175099849700928
Reconstruction Loss: -8.272749900817871
Iteration 9101:
Training Loss: -5.190722465515137
Reconstruction Loss: -8.287114143371582
Iteration 9201:
Training Loss: -5.206146240234375
Reconstruction Loss: -8.301279067993164
Iteration 9301:
Training Loss: -5.221369743347168
Reconstruction Loss: -8.315272331237793
Iteration 9401:
Training Loss: -5.236414909362793
Reconstruction Loss: -8.32908821105957
Iteration 9501:
Training Loss: -5.251273155212402
Reconstruction Loss: -8.34272289276123
Iteration 9601:
Training Loss: -5.265948295593262
Reconstruction Loss: -8.356169700622559
Iteration 9701:
Training Loss: -5.280457496643066
Reconstruction Loss: -8.369446754455566
Iteration 9801:
Training Loss: -5.29479455947876
Reconstruction Loss: -8.382550239562988
Iteration 9901:
Training Loss: -5.308963775634766
Reconstruction Loss: -8.395515441894531
Iteration 10001:
Training Loss: -5.322962284088135
Reconstruction Loss: -8.408310890197754
Iteration 10101:
Training Loss: -5.336819648742676
Reconstruction Loss: -8.420966148376465
Iteration 10201:
Training Loss: -5.350511074066162
Reconstruction Loss: -8.433470726013184
Iteration 10301:
Training Loss: -5.364048004150391
Reconstruction Loss: -8.445831298828125
Iteration 10401:
Training Loss: -5.377441883087158
Reconstruction Loss: -8.458035469055176
Iteration 10501:
Training Loss: -5.390683174133301
Reconstruction Loss: -8.470097541809082
Iteration 10601:
Training Loss: -5.4037909507751465
Reconstruction Loss: -8.48202133178711
Iteration 10701:
Training Loss: -5.4167561531066895
Reconstruction Loss: -8.49382209777832
Iteration 10801:
Training Loss: -5.429582118988037
Reconstruction Loss: -8.505480766296387
Iteration 10901:
Training Loss: -5.442280292510986
Reconstruction Loss: -8.517014503479004
Iteration 11001:
Training Loss: -5.45484733581543
Reconstruction Loss: -8.528427124023438
Iteration 11101:
Training Loss: -5.467280864715576
Reconstruction Loss: -8.539717674255371
Iteration 11201:
Training Loss: -5.479594707489014
Reconstruction Loss: -8.550885200500488
Iteration 11301:
Training Loss: -5.4917778968811035
Reconstruction Loss: -8.561931610107422
Iteration 11401:
Training Loss: -5.503846168518066
Reconstruction Loss: -8.572856903076172
Iteration 11501:
Training Loss: -5.5157999992370605
Reconstruction Loss: -8.583673477172852
Iteration 11601:
Training Loss: -5.527625560760498
Reconstruction Loss: -8.594375610351562
Iteration 11701:
Training Loss: -5.539342880249023
Reconstruction Loss: -8.604991912841797
Iteration 11801:
Training Loss: -5.5509562492370605
Reconstruction Loss: -8.615483283996582
Iteration 11901:
Training Loss: -5.56246280670166
Reconstruction Loss: -8.625873565673828
Iteration 12001:
Training Loss: -5.573855876922607
Reconstruction Loss: -8.636160850524902
Iteration 12101:
Training Loss: -5.585136413574219
Reconstruction Loss: -8.64635181427002
Iteration 12201:
Training Loss: -5.596315383911133
Reconstruction Loss: -8.656464576721191
Iteration 12301:
Training Loss: -5.607390403747559
Reconstruction Loss: -8.666455268859863
Iteration 12401:
Training Loss: -5.618367671966553
Reconstruction Loss: -8.676346778869629
Iteration 12501:
Training Loss: -5.629261493682861
Reconstruction Loss: -8.686150550842285
Iteration 12601:
Training Loss: -5.640045642852783
Reconstruction Loss: -8.695862770080566
Iteration 12701:
Training Loss: -5.650742053985596
Reconstruction Loss: -8.705486297607422
Iteration 12801:
Training Loss: -5.661332130432129
Reconstruction Loss: -8.715022087097168
Iteration 12901:
Training Loss: -5.671842098236084
Reconstruction Loss: -8.724474906921387
Iteration 13001:
Training Loss: -5.682261943817139
Reconstruction Loss: -8.73383617401123
Iteration 13101:
Training Loss: -5.692597389221191
Reconstruction Loss: -8.743112564086914
Iteration 13201:
Training Loss: -5.702832221984863
Reconstruction Loss: -8.75231647491455
Iteration 13301:
Training Loss: -5.7129950523376465
Reconstruction Loss: -8.761427879333496
Iteration 13401:
Training Loss: -5.7230706214904785
Reconstruction Loss: -8.770475387573242
Iteration 13501:
Training Loss: -5.733063220977783
Reconstruction Loss: -8.779440879821777
Iteration 13601:
Training Loss: -5.742975234985352
Reconstruction Loss: -8.788326263427734
Iteration 13701:
Training Loss: -5.752804279327393
Reconstruction Loss: -8.797123908996582
Iteration 13801:
Training Loss: -5.7625603675842285
Reconstruction Loss: -8.805854797363281
Iteration 13901:
Training Loss: -5.7722392082214355
Reconstruction Loss: -8.814515113830566
Iteration 14001:
Training Loss: -5.781842231750488
Reconstruction Loss: -8.823108673095703
Iteration 14101:
Training Loss: -5.791359901428223
Reconstruction Loss: -8.83162784576416
Iteration 14201:
Training Loss: -5.800808429718018
Reconstruction Loss: -8.84006118774414
Iteration 14301:
Training Loss: -5.810190200805664
Reconstruction Loss: -8.848426818847656
Iteration 14401:
Training Loss: -5.819492816925049
Reconstruction Loss: -8.856718063354492
Iteration 14501:
Training Loss: -5.828725814819336
Reconstruction Loss: -8.864959716796875
Iteration 14601:
Training Loss: -5.837897777557373
Reconstruction Loss: -8.87314510345459
Iteration 14701:
Training Loss: -5.846983909606934
Reconstruction Loss: -8.88125991821289
Iteration 14801:
Training Loss: -5.856016635894775
Reconstruction Loss: -8.88930892944336
Iteration 14901:
Training Loss: -5.864977836608887
Reconstruction Loss: -8.897293090820312
Iteration 15001:
Training Loss: -5.873875141143799
Reconstruction Loss: -8.905219078063965
Iteration 15101:
Training Loss: -5.88271427154541
Reconstruction Loss: -8.913086891174316
Iteration 15201:
Training Loss: -5.891485214233398
Reconstruction Loss: -8.92089557647705
Iteration 15301:
Training Loss: -5.900195598602295
Reconstruction Loss: -8.928641319274902
Iteration 15401:
Training Loss: -5.908850193023682
Reconstruction Loss: -8.9363374710083
Iteration 15501:
Training Loss: -5.91743803024292
Reconstruction Loss: -8.943967819213867
Iteration 15601:
Training Loss: -5.925971508026123
Reconstruction Loss: -8.951536178588867
Iteration 15701:
Training Loss: -5.93442964553833
Reconstruction Loss: -8.95905876159668
Iteration 15801:
Training Loss: -5.942831516265869
Reconstruction Loss: -8.966527938842773
Iteration 15901:
Training Loss: -5.951190948486328
Reconstruction Loss: -8.97393798828125
Iteration 16001:
Training Loss: -5.959483623504639
Reconstruction Loss: -8.981306076049805
Iteration 16101:
Training Loss: -5.967726230621338
Reconstruction Loss: -8.988628387451172
Iteration 16201:
Training Loss: -5.975896835327148
Reconstruction Loss: -8.995899200439453
Iteration 16301:
Training Loss: -5.984031677246094
Reconstruction Loss: -9.003121376037598
Iteration 16401:
Training Loss: -5.992102146148682
Reconstruction Loss: -9.010289192199707
Iteration 16501:
Training Loss: -6.000134468078613
Reconstruction Loss: -9.017401695251465
Iteration 16601:
Training Loss: -6.008101463317871
Reconstruction Loss: -9.024456024169922
Iteration 16701:
Training Loss: -6.016027450561523
Reconstruction Loss: -9.031464576721191
Iteration 16801:
Training Loss: -6.023890495300293
Reconstruction Loss: -9.038421630859375
Iteration 16901:
Training Loss: -6.031707763671875
Reconstruction Loss: -9.045323371887207
Iteration 17001:
Training Loss: -6.039474010467529
Reconstruction Loss: -9.052188873291016
Iteration 17101:
Training Loss: -6.047183513641357
Reconstruction Loss: -9.059003829956055
Iteration 17201:
Training Loss: -6.054848670959473
Reconstruction Loss: -9.06577205657959
Iteration 17301:
Training Loss: -6.062473297119141
Reconstruction Loss: -9.07249927520752
Iteration 17401:
Training Loss: -6.070046901702881
Reconstruction Loss: -9.079187393188477
Iteration 17501:
Training Loss: -6.077573299407959
Reconstruction Loss: -9.085832595825195
Iteration 17601:
Training Loss: -6.085058212280273
Reconstruction Loss: -9.09244155883789
Iteration 17701:
Training Loss: -6.092493534088135
Reconstruction Loss: -9.099002838134766
Iteration 17801:
Training Loss: -6.099877834320068
Reconstruction Loss: -9.105520248413086
Iteration 17901:
Training Loss: -6.1072282791137695
Reconstruction Loss: -9.111979484558105
Iteration 18001:
Training Loss: -6.114539623260498
Reconstruction Loss: -9.118412017822266
Iteration 18101:
Training Loss: -6.121801376342773
Reconstruction Loss: -9.124810218811035
Iteration 18201:
Training Loss: -6.129012107849121
Reconstruction Loss: -9.131162643432617
Iteration 18301:
Training Loss: -6.136178016662598
Reconstruction Loss: -9.137470245361328
Iteration 18401:
Training Loss: -6.143322467803955
Reconstruction Loss: -9.143743515014648
Iteration 18501:
Training Loss: -6.150414943695068
Reconstruction Loss: -9.14998722076416
Iteration 18601:
Training Loss: -6.157456874847412
Reconstruction Loss: -9.156196594238281
Iteration 18701:
Training Loss: -6.164468765258789
Reconstruction Loss: -9.16236400604248
Iteration 18801:
Training Loss: -6.171443939208984
Reconstruction Loss: -9.168499946594238
Iteration 18901:
Training Loss: -6.178370475769043
Reconstruction Loss: -9.174592971801758
Iteration 19001:
Training Loss: -6.185267448425293
Reconstruction Loss: -9.180647850036621
Iteration 19101:
Training Loss: -6.192109107971191
Reconstruction Loss: -9.186660766601562
Iteration 19201:
Training Loss: -6.1989288330078125
Reconstruction Loss: -9.192648887634277
Iteration 19301:
Training Loss: -6.2057061195373535
Reconstruction Loss: -9.198594093322754
Iteration 19401:
Training Loss: -6.212447166442871
Reconstruction Loss: -9.204504013061523
Iteration 19501:
Training Loss: -6.219146728515625
Reconstruction Loss: -9.210367202758789
Iteration 19601:
Training Loss: -6.2258148193359375
Reconstruction Loss: -9.216187477111816
Iteration 19701:
Training Loss: -6.232448577880859
Reconstruction Loss: -9.221979141235352
Iteration 19801:
Training Loss: -6.23903226852417
Reconstruction Loss: -9.22774600982666
Iteration 19901:
Training Loss: -6.245591163635254
Reconstruction Loss: -9.233491897583008
Iteration 20001:
Training Loss: -6.252106189727783
Reconstruction Loss: -9.239201545715332
Iteration 20101:
Training Loss: -6.258596420288086
Reconstruction Loss: -9.24487590789795
Iteration 20201:
Training Loss: -6.265044689178467
Reconstruction Loss: -9.250521659851074
Iteration 20301:
Training Loss: -6.271478176116943
Reconstruction Loss: -9.256133079528809
Iteration 20401:
Training Loss: -6.2778472900390625
Reconstruction Loss: -9.261711120605469
Iteration 20501:
Training Loss: -6.284209728240967
Reconstruction Loss: -9.267271041870117
Iteration 20601:
Training Loss: -6.290537357330322
Reconstruction Loss: -9.272802352905273
Iteration 20701:
Training Loss: -6.296819686889648
Reconstruction Loss: -9.278288841247559
Iteration 20801:
Training Loss: -6.303074359893799
Reconstruction Loss: -9.283740997314453
Iteration 20901:
Training Loss: -6.309291839599609
Reconstruction Loss: -9.289166450500488
Iteration 21001:
Training Loss: -6.315497875213623
Reconstruction Loss: -9.294562339782715
Iteration 21101:
Training Loss: -6.321654796600342
Reconstruction Loss: -9.299930572509766
Iteration 21201:
Training Loss: -6.327789306640625
Reconstruction Loss: -9.30527400970459
Iteration 21301:
Training Loss: -6.333892822265625
Reconstruction Loss: -9.310587882995605
Iteration 21401:
Training Loss: -6.339960098266602
Reconstruction Loss: -9.315890312194824
Iteration 21501:
Training Loss: -6.346003532409668
Reconstruction Loss: -9.321157455444336
Iteration 21601:
Training Loss: -6.35202169418335
Reconstruction Loss: -9.32638931274414
Iteration 21701:
Training Loss: -6.358006000518799
Reconstruction Loss: -9.33159351348877
Iteration 21801:
Training Loss: -6.363962173461914
Reconstruction Loss: -9.336759567260742
Iteration 21901:
Training Loss: -6.369877338409424
Reconstruction Loss: -9.341902732849121
Iteration 22001:
Training Loss: -6.37576961517334
Reconstruction Loss: -9.347029685974121
Iteration 22101:
Training Loss: -6.381642818450928
Reconstruction Loss: -9.352121353149414
Iteration 22201:
Training Loss: -6.387478351593018
Reconstruction Loss: -9.357202529907227
Iteration 22301:
Training Loss: -6.393284320831299
Reconstruction Loss: -9.362253189086914
Iteration 22401:
Training Loss: -6.399071216583252
Reconstruction Loss: -9.36727523803711
Iteration 22501:
Training Loss: -6.404829502105713
Reconstruction Loss: -9.372273445129395
Iteration 22601:
Training Loss: -6.410552978515625
Reconstruction Loss: -9.377243995666504
Iteration 22701:
Training Loss: -6.416258811950684
Reconstruction Loss: -9.382186889648438
Iteration 22801:
Training Loss: -6.421940803527832
Reconstruction Loss: -9.387094497680664
Iteration 22901:
Training Loss: -6.427582740783691
Reconstruction Loss: -9.391979217529297
Iteration 23001:
Training Loss: -6.43320369720459
Reconstruction Loss: -9.3968505859375
Iteration 23101:
Training Loss: -6.438804626464844
Reconstruction Loss: -9.401705741882324
Iteration 23201:
Training Loss: -6.444368839263916
Reconstruction Loss: -9.406542778015137
Iteration 23301:
Training Loss: -6.44992733001709
Reconstruction Loss: -9.41135025024414
Iteration 23401:
Training Loss: -6.455451011657715
Reconstruction Loss: -9.416130065917969
Iteration 23501:
Training Loss: -6.46095085144043
Reconstruction Loss: -9.420888900756836
Iteration 23601:
Training Loss: -6.466433525085449
Reconstruction Loss: -9.42562198638916
Iteration 23701:
Training Loss: -6.471874713897705
Reconstruction Loss: -9.430341720581055
Iteration 23801:
Training Loss: -6.47730827331543
Reconstruction Loss: -9.435039520263672
Iteration 23901:
Training Loss: -6.482701301574707
Reconstruction Loss: -9.439718246459961
Iteration 24001:
Training Loss: -6.488087177276611
Reconstruction Loss: -9.444369316101074
Iteration 24101:
Training Loss: -6.493441104888916
Reconstruction Loss: -9.448997497558594
Iteration 24201:
Training Loss: -6.498767375946045
Reconstruction Loss: -9.45360279083252
Iteration 24301:
Training Loss: -6.504072189331055
Reconstruction Loss: -9.4581880569458
Iteration 24401:
Training Loss: -6.509374141693115
Reconstruction Loss: -9.46275520324707
Iteration 24501:
Training Loss: -6.514632701873779
Reconstruction Loss: -9.467297554016113
Iteration 24601:
Training Loss: -6.519885063171387
Reconstruction Loss: -9.471827507019043
Iteration 24701:
Training Loss: -6.525083065032959
Reconstruction Loss: -9.476327896118164
Iteration 24801:
Training Loss: -6.530289173126221
Reconstruction Loss: -9.480802536010742
Iteration 24901:
Training Loss: -6.535462379455566
Reconstruction Loss: -9.485258102416992
Iteration 25001:
Training Loss: -6.540598392486572
Reconstruction Loss: -9.489700317382812
Iteration 25101:
Training Loss: -6.545732021331787
Reconstruction Loss: -9.494112968444824
Iteration 25201:
Training Loss: -6.5508317947387695
Reconstruction Loss: -9.498519897460938
Iteration 25301:
Training Loss: -6.555926322937012
Reconstruction Loss: -9.502911567687988
Iteration 25401:
Training Loss: -6.5609893798828125
Reconstruction Loss: -9.507282257080078
Iteration 25501:
Training Loss: -6.566032886505127
Reconstruction Loss: -9.511632919311523
Iteration 25601:
Training Loss: -6.5710601806640625
Reconstruction Loss: -9.515961647033691
Iteration 25701:
Training Loss: -6.5760626792907715
Reconstruction Loss: -9.52027416229248
Iteration 25801:
Training Loss: -6.581053256988525
Reconstruction Loss: -9.524560928344727
Iteration 25901:
Training Loss: -6.586021900177002
Reconstruction Loss: -9.528834342956543
Iteration 26001:
Training Loss: -6.590968132019043
Reconstruction Loss: -9.533085823059082
Iteration 26101:
Training Loss: -6.595890998840332
Reconstruction Loss: -9.537325859069824
Iteration 26201:
Training Loss: -6.6007981300354
Reconstruction Loss: -9.541542053222656
Iteration 26301:
Training Loss: -6.60568380355835
Reconstruction Loss: -9.545734405517578
Iteration 26401:
Training Loss: -6.6105546951293945
Reconstruction Loss: -9.549914360046387
Iteration 26501:
Training Loss: -6.615398406982422
Reconstruction Loss: -9.554075241088867
Iteration 26601:
Training Loss: -6.620240211486816
Reconstruction Loss: -9.558213233947754
Iteration 26701:
Training Loss: -6.625034332275391
Reconstruction Loss: -9.562336921691895
Iteration 26801:
Training Loss: -6.629835605621338
Reconstruction Loss: -9.566439628601074
Iteration 26901:
Training Loss: -6.634599208831787
Reconstruction Loss: -9.570526123046875
Iteration 27001:
Training Loss: -6.639354705810547
Reconstruction Loss: -9.57459545135498
Iteration 27101:
Training Loss: -6.644097805023193
Reconstruction Loss: -9.578648567199707
Iteration 27201:
Training Loss: -6.648805141448975
Reconstruction Loss: -9.58267879486084
Iteration 27301:
Training Loss: -6.653488636016846
Reconstruction Loss: -9.586691856384277
Iteration 27401:
Training Loss: -6.658174514770508
Reconstruction Loss: -9.590690612792969
Iteration 27501:
Training Loss: -6.662840366363525
Reconstruction Loss: -9.594674110412598
Iteration 27601:
Training Loss: -6.667501926422119
Reconstruction Loss: -9.598649024963379
Iteration 27701:
Training Loss: -6.672116756439209
Reconstruction Loss: -9.602612495422363
Iteration 27801:
Training Loss: -6.676727294921875
Reconstruction Loss: -9.606561660766602
Iteration 27901:
Training Loss: -6.68134069442749
Reconstruction Loss: -9.610491752624512
Iteration 28001:
Training Loss: -6.685909748077393
Reconstruction Loss: -9.614416122436523
Iteration 28101:
Training Loss: -6.690478801727295
Reconstruction Loss: -9.61832046508789
Iteration 28201:
Training Loss: -6.695014476776123
Reconstruction Loss: -9.62220573425293
Iteration 28301:
Training Loss: -6.699538230895996
Reconstruction Loss: -9.626075744628906
Iteration 28401:
Training Loss: -6.704052925109863
Reconstruction Loss: -9.629922866821289
Iteration 28501:
Training Loss: -6.708542823791504
Reconstruction Loss: -9.633757591247559
Iteration 28601:
Training Loss: -6.713012218475342
Reconstruction Loss: -9.6375732421875
Iteration 28701:
Training Loss: -6.717468738555908
Reconstruction Loss: -9.641377449035645
Iteration 28801:
Training Loss: -6.721909046173096
Reconstruction Loss: -9.645161628723145
Iteration 28901:
Training Loss: -6.726335048675537
Reconstruction Loss: -9.648941040039062
Iteration 29001:
Training Loss: -6.730747699737549
Reconstruction Loss: -9.652706146240234
Iteration 29101:
Training Loss: -6.7351531982421875
Reconstruction Loss: -9.656455993652344
Iteration 29201:
Training Loss: -6.739530086517334
Reconstruction Loss: -9.66019058227539
Iteration 29301:
Training Loss: -6.743897438049316
Reconstruction Loss: -9.663914680480957
Iteration 29401:
Training Loss: -6.748263835906982
Reconstruction Loss: -9.667617797851562
Iteration 29501:
Training Loss: -6.752594947814941
Reconstruction Loss: -9.671311378479004
Iteration 29601:
Training Loss: -6.756930351257324
Reconstruction Loss: -9.6749849319458
Iteration 29701:
Training Loss: -6.76123571395874
Reconstruction Loss: -9.678651809692383
Iteration 29801:
Training Loss: -6.765521049499512
Reconstruction Loss: -9.68230152130127
Iteration 29901:
Training Loss: -6.769796848297119
Reconstruction Loss: -9.685951232910156
Iteration 30001:
Training Loss: -6.774075984954834
Reconstruction Loss: -9.68958568572998
Iteration 30101:
Training Loss: -6.77830696105957
Reconstruction Loss: -9.693196296691895
Iteration 30201:
Training Loss: -6.7825541496276855
Reconstruction Loss: -9.696785926818848
Iteration 30301:
Training Loss: -6.786770820617676
Reconstruction Loss: -9.700366973876953
Iteration 30401:
Training Loss: -6.790970802307129
Reconstruction Loss: -9.703933715820312
Iteration 30501:
Training Loss: -6.795154571533203
Reconstruction Loss: -9.707489967346191
Iteration 30601:
Training Loss: -6.799330711364746
Reconstruction Loss: -9.711026191711426
Iteration 30701:
Training Loss: -6.803482532501221
Reconstruction Loss: -9.714558601379395
Iteration 30801:
Training Loss: -6.80764102935791
Reconstruction Loss: -9.718070983886719
Iteration 30901:
Training Loss: -6.811764240264893
Reconstruction Loss: -9.721576690673828
Iteration 31001:
Training Loss: -6.8158721923828125
Reconstruction Loss: -9.725074768066406
Iteration 31101:
Training Loss: -6.819983959197998
Reconstruction Loss: -9.72855281829834
Iteration 31201:
Training Loss: -6.8240790367126465
Reconstruction Loss: -9.732017517089844
Iteration 31301:
Training Loss: -6.828155040740967
Reconstruction Loss: -9.735471725463867
Iteration 31401:
Training Loss: -6.832231044769287
Reconstruction Loss: -9.738912582397461
Iteration 31501:
Training Loss: -6.836268424987793
Reconstruction Loss: -9.742342948913574
Iteration 31601:
Training Loss: -6.840301036834717
Reconstruction Loss: -9.74576187133789
Iteration 31701:
Training Loss: -6.8443403244018555
Reconstruction Loss: -9.74916934967041
Iteration 31801:
Training Loss: -6.848343372344971
Reconstruction Loss: -9.752571105957031
Iteration 31901:
Training Loss: -6.852367401123047
Reconstruction Loss: -9.755959510803223
Iteration 32001:
Training Loss: -6.856340408325195
Reconstruction Loss: -9.759334564208984
Iteration 32101:
Training Loss: -6.860318660736084
Reconstruction Loss: -9.762694358825684
Iteration 32201:
Training Loss: -6.8642754554748535
Reconstruction Loss: -9.766043663024902
Iteration 32301:
Training Loss: -6.868232727050781
Reconstruction Loss: -9.769379615783691
Iteration 32401:
Training Loss: -6.872157096862793
Reconstruction Loss: -9.772705078125
Iteration 32501:
Training Loss: -6.876090049743652
Reconstruction Loss: -9.776016235351562
Iteration 32601:
Training Loss: -6.8800225257873535
Reconstruction Loss: -9.779325485229492
Iteration 32701:
Training Loss: -6.883913516998291
Reconstruction Loss: -9.782617568969727
Iteration 32801:
Training Loss: -6.887784481048584
Reconstruction Loss: -9.785896301269531
Iteration 32901:
Training Loss: -6.891666412353516
Reconstruction Loss: -9.789164543151855
Iteration 33001:
Training Loss: -6.895528793334961
Reconstruction Loss: -9.792425155639648
Iteration 33101:
Training Loss: -6.899395942687988
Reconstruction Loss: -9.795680046081543
Iteration 33201:
Training Loss: -6.90322732925415
Reconstruction Loss: -9.798918724060059
Iteration 33301:
Training Loss: -6.907057285308838
Reconstruction Loss: -9.80215072631836
Iteration 33401:
Training Loss: -6.910890579223633
Reconstruction Loss: -9.805363655090332
Iteration 33501:
Training Loss: -6.9146857261657715
Reconstruction Loss: -9.80856704711914
Iteration 33601:
Training Loss: -6.918481826782227
Reconstruction Loss: -9.811758995056152
Iteration 33701:
Training Loss: -6.9222540855407715
Reconstruction Loss: -9.814942359924316
Iteration 33801:
Training Loss: -6.926008224487305
Reconstruction Loss: -9.81811237335205
Iteration 33901:
Training Loss: -6.929789066314697
Reconstruction Loss: -9.821276664733887
Iteration 34001:
Training Loss: -6.933502197265625
Reconstruction Loss: -9.824421882629395
Iteration 34101:
Training Loss: -6.937248229980469
Reconstruction Loss: -9.827561378479004
Iteration 34201:
Training Loss: -6.940974712371826
Reconstruction Loss: -9.830696105957031
Iteration 34301:
Training Loss: -6.944675445556641
Reconstruction Loss: -9.833815574645996
Iteration 34401:
Training Loss: -6.948390007019043
Reconstruction Loss: -9.836931228637695
Iteration 34501:
Training Loss: -6.9520649909973145
Reconstruction Loss: -9.840032577514648
Iteration 34601:
Training Loss: -6.955740928649902
Reconstruction Loss: -9.843127250671387
Iteration 34701:
Training Loss: -6.959426403045654
Reconstruction Loss: -9.846212387084961
Iteration 34801:
Training Loss: -6.963079452514648
Reconstruction Loss: -9.849285125732422
Iteration 34901:
Training Loss: -6.9667229652404785
Reconstruction Loss: -9.852343559265137
Iteration 35001:
Training Loss: -6.970359802246094
Reconstruction Loss: -9.85539436340332
Iteration 35101:
Training Loss: -6.973979473114014
Reconstruction Loss: -9.858429908752441
Iteration 35201:
Training Loss: -6.977573394775391
Reconstruction Loss: -9.86146068572998
Iteration 35301:
Training Loss: -6.981193542480469
Reconstruction Loss: -9.86447811126709
Iteration 35401:
Training Loss: -6.984766960144043
Reconstruction Loss: -9.867490768432617
Iteration 35501:
Training Loss: -6.9883317947387695
Reconstruction Loss: -9.870488166809082
Iteration 35601:
Training Loss: -6.991917133331299
Reconstruction Loss: -9.8734769821167
Iteration 35701:
Training Loss: -6.995475769042969
Reconstruction Loss: -9.876460075378418
Iteration 35801:
Training Loss: -6.9990129470825195
Reconstruction Loss: -9.879438400268555
Iteration 35901:
Training Loss: -7.002554893493652
Reconstruction Loss: -9.882408142089844
Iteration 36001:
Training Loss: -7.0060954093933105
Reconstruction Loss: -9.885371208190918
Iteration 36101:
Training Loss: -7.009617805480957
Reconstruction Loss: -9.888323783874512
Iteration 36201:
Training Loss: -7.0131330490112305
Reconstruction Loss: -9.891265869140625
Iteration 36301:
Training Loss: -7.01664400100708
Reconstruction Loss: -9.894200325012207
Iteration 36401:
Training Loss: -7.020102500915527
Reconstruction Loss: -9.897120475769043
Iteration 36501:
Training Loss: -7.023603916168213
Reconstruction Loss: -9.900036811828613
Iteration 36601:
Training Loss: -7.0270795822143555
Reconstruction Loss: -9.90294075012207
Iteration 36701:
Training Loss: -7.030538558959961
Reconstruction Loss: -9.905840873718262
Iteration 36801:
Training Loss: -7.033998489379883
Reconstruction Loss: -9.90872573852539
Iteration 36901:
Training Loss: -7.037436485290527
Reconstruction Loss: -9.91159725189209
Iteration 37001:
Training Loss: -7.040874481201172
Reconstruction Loss: -9.914460182189941
Iteration 37101:
Training Loss: -7.0442914962768555
Reconstruction Loss: -9.917314529418945
Iteration 37201:
Training Loss: -7.047702789306641
Reconstruction Loss: -9.920159339904785
Iteration 37301:
Training Loss: -7.051132678985596
Reconstruction Loss: -9.922998428344727
Iteration 37401:
Training Loss: -7.054527759552002
Reconstruction Loss: -9.925826072692871
Iteration 37501:
Training Loss: -7.05790376663208
Reconstruction Loss: -9.928644180297852
Iteration 37601:
Training Loss: -7.061271667480469
Reconstruction Loss: -9.9314546585083
Iteration 37701:
Training Loss: -7.064640998840332
Reconstruction Loss: -9.934256553649902
Iteration 37801:
Training Loss: -7.067988872528076
Reconstruction Loss: -9.937051773071289
Iteration 37901:
Training Loss: -7.071349620819092
Reconstruction Loss: -9.939840316772461
Iteration 38001:
Training Loss: -7.074676513671875
Reconstruction Loss: -9.942623138427734
Iteration 38101:
Training Loss: -7.078001022338867
Reconstruction Loss: -9.94540023803711
Iteration 38201:
Training Loss: -7.081330299377441
Reconstruction Loss: -9.948163986206055
Iteration 38301:
Training Loss: -7.084648132324219
Reconstruction Loss: -9.950920104980469
Iteration 38401:
Training Loss: -7.0879292488098145
Reconstruction Loss: -9.953668594360352
Iteration 38501:
Training Loss: -7.091254234313965
Reconstruction Loss: -9.956411361694336
Iteration 38601:
Training Loss: -7.09453010559082
Reconstruction Loss: -9.959146499633789
Iteration 38701:
Training Loss: -7.097806453704834
Reconstruction Loss: -9.961874961853027
Iteration 38801:
Training Loss: -7.101085662841797
Reconstruction Loss: -9.964604377746582
Iteration 38901:
Training Loss: -7.104344844818115
Reconstruction Loss: -9.967327117919922
Iteration 39001:
Training Loss: -7.107602119445801
Reconstruction Loss: -9.970040321350098
Iteration 39101:
Training Loss: -7.110856533050537
Reconstruction Loss: -9.972746849060059
Iteration 39201:
Training Loss: -7.114089488983154
Reconstruction Loss: -9.975444793701172
Iteration 39301:
Training Loss: -7.117339611053467
Reconstruction Loss: -9.978134155273438
Iteration 39401:
Training Loss: -7.1205596923828125
Reconstruction Loss: -9.980817794799805
Iteration 39501:
Training Loss: -7.123781204223633
Reconstruction Loss: -9.983499526977539
Iteration 39601:
Training Loss: -7.1269941329956055
Reconstruction Loss: -9.986166954040527
Iteration 39701:
Training Loss: -7.130192279815674
Reconstruction Loss: -9.988819122314453
Iteration 39801:
Training Loss: -7.133396148681641
Reconstruction Loss: -9.991469383239746
Iteration 39901:
Training Loss: -7.136568069458008
Reconstruction Loss: -9.99411392211914
Iteration 40001:
Training Loss: -7.13973331451416
Reconstruction Loss: -9.99675178527832
Iteration 40101:
Training Loss: -7.1429057121276855
Reconstruction Loss: -9.999383926391602
Iteration 40201:
Training Loss: -7.146055221557617
Reconstruction Loss: -10.002006530761719
Iteration 40301:
Training Loss: -7.14921236038208
Reconstruction Loss: -10.00462818145752
Iteration 40401:
Training Loss: -7.152378082275391
Reconstruction Loss: -10.00723648071289
Iteration 40501:
Training Loss: -7.155498504638672
Reconstruction Loss: -10.00984001159668
Iteration 40601:
Training Loss: -7.158618450164795
Reconstruction Loss: -10.012438774108887
Iteration 40701:
Training Loss: -7.161758899688721
Reconstruction Loss: -10.015023231506348
Iteration 40801:
Training Loss: -7.164869785308838
Reconstruction Loss: -10.017609596252441
Iteration 40901:
Training Loss: -7.167969226837158
Reconstruction Loss: -10.02017879486084
Iteration 41001:
Training Loss: -7.1710638999938965
Reconstruction Loss: -10.022749900817871
Iteration 41101:
Training Loss: -7.174149036407471
Reconstruction Loss: -10.025312423706055
Iteration 41201:
Training Loss: -7.177237033843994
Reconstruction Loss: -10.02786922454834
Iteration 41301:
Training Loss: -7.180328369140625
Reconstruction Loss: -10.030415534973145
Iteration 41401:
Training Loss: -7.183383941650391
Reconstruction Loss: -10.032958030700684
Iteration 41501:
Training Loss: -7.186453819274902
Reconstruction Loss: -10.03549861907959
Iteration 41601:
Training Loss: -7.189482688903809
Reconstruction Loss: -10.038021087646484
Iteration 41701:
Training Loss: -7.192539691925049
Reconstruction Loss: -10.040545463562012
Iteration 41801:
Training Loss: -7.195568561553955
Reconstruction Loss: -10.043061256408691
Iteration 41901:
Training Loss: -7.198607444763184
Reconstruction Loss: -10.045565605163574
Iteration 42001:
Training Loss: -7.2016282081604
Reconstruction Loss: -10.048065185546875
Iteration 42101:
Training Loss: -7.2046332359313965
Reconstruction Loss: -10.050559997558594
Iteration 42201:
Training Loss: -7.207633972167969
Reconstruction Loss: -10.0530424118042
Iteration 42301:
Training Loss: -7.210647106170654
Reconstruction Loss: -10.055517196655273
Iteration 42401:
Training Loss: -7.213632583618164
Reconstruction Loss: -10.057992935180664
Iteration 42501:
Training Loss: -7.216635227203369
Reconstruction Loss: -10.060461044311523
Iteration 42601:
Training Loss: -7.219593524932861
Reconstruction Loss: -10.062921524047852
Iteration 42701:
Training Loss: -7.222566604614258
Reconstruction Loss: -10.065372467041016
Iteration 42801:
Training Loss: -7.22553825378418
Reconstruction Loss: -10.067819595336914
Iteration 42901:
Training Loss: -7.22849178314209
Reconstruction Loss: -10.070258140563965
Iteration 43001:
Training Loss: -7.231448650360107
Reconstruction Loss: -10.072693824768066
Iteration 43101:
Training Loss: -7.23439884185791
Reconstruction Loss: -10.075115203857422
Iteration 43201:
Training Loss: -7.2373127937316895
Reconstruction Loss: -10.077537536621094
Iteration 43301:
Training Loss: -7.240266799926758
Reconstruction Loss: -10.079949378967285
Iteration 43401:
Training Loss: -7.243184566497803
Reconstruction Loss: -10.082356452941895
Iteration 43501:
Training Loss: -7.246096611022949
Reconstruction Loss: -10.084763526916504
Iteration 43601:
Training Loss: -7.249009132385254
Reconstruction Loss: -10.087154388427734
Iteration 43701:
Training Loss: -7.251910209655762
Reconstruction Loss: -10.089548110961914
Iteration 43801:
Training Loss: -7.254807472229004
Reconstruction Loss: -10.091931343078613
Iteration 43901:
Training Loss: -7.257694244384766
Reconstruction Loss: -10.094306945800781
Iteration 44001:
Training Loss: -7.260583877563477
Reconstruction Loss: -10.096673965454102
Iteration 44101:
Training Loss: -7.263459205627441
Reconstruction Loss: -10.099034309387207
Iteration 44201:
Training Loss: -7.266307830810547
Reconstruction Loss: -10.101391792297363
Iteration 44301:
Training Loss: -7.26917028427124
Reconstruction Loss: -10.103742599487305
Iteration 44401:
Training Loss: -7.2720232009887695
Reconstruction Loss: -10.106081008911133
Iteration 44501:
Training Loss: -7.274885177612305
Reconstruction Loss: -10.108420372009277
Iteration 44601:
Training Loss: -7.277720928192139
Reconstruction Loss: -10.110747337341309
Iteration 44701:
Training Loss: -7.2805562019348145
Reconstruction Loss: -10.113075256347656
Iteration 44801:
Training Loss: -7.283400535583496
Reconstruction Loss: -10.115396499633789
Iteration 44901:
Training Loss: -7.286221027374268
Reconstruction Loss: -10.117709159851074
Iteration 45001:
Training Loss: -7.289046764373779
Reconstruction Loss: -10.12001895904541
Iteration 45101:
Training Loss: -7.291853904724121
Reconstruction Loss: -10.122323989868164
Iteration 45201:
Training Loss: -7.294656753540039
Reconstruction Loss: -10.124615669250488
Iteration 45301:
Training Loss: -7.297458648681641
Reconstruction Loss: -10.126901626586914
Iteration 45401:
Training Loss: -7.300252914428711
Reconstruction Loss: -10.129190444946289
Iteration 45501:
Training Loss: -7.303028583526611
Reconstruction Loss: -10.131464958190918
Iteration 45601:
Training Loss: -7.305801868438721
Reconstruction Loss: -10.13374137878418
Iteration 45701:
Training Loss: -7.308592796325684
Reconstruction Loss: -10.136005401611328
Iteration 45801:
Training Loss: -7.3113484382629395
Reconstruction Loss: -10.138274192810059
Iteration 45901:
Training Loss: -7.314116954803467
Reconstruction Loss: -10.140534400939941
Iteration 46001:
Training Loss: -7.316861629486084
Reconstruction Loss: -10.14278793334961
Iteration 46101:
Training Loss: -7.319622039794922
Reconstruction Loss: -10.14503002166748
Iteration 46201:
Training Loss: -7.322359561920166
Reconstruction Loss: -10.147272109985352
Iteration 46301:
Training Loss: -7.325112342834473
Reconstruction Loss: -10.149506568908691
Iteration 46401:
Training Loss: -7.327831268310547
Reconstruction Loss: -10.151737213134766
Iteration 46501:
Training Loss: -7.330551624298096
Reconstruction Loss: -10.153964042663574
Iteration 46601:
Training Loss: -7.333262920379639
Reconstruction Loss: -10.156180381774902
Iteration 46701:
Training Loss: -7.335960388183594
Reconstruction Loss: -10.158392906188965
Iteration 46801:
Training Loss: -7.338693618774414
Reconstruction Loss: -10.160595893859863
Iteration 46901:
Training Loss: -7.341402530670166
Reconstruction Loss: -10.162800788879395
Iteration 47001:
Training Loss: -7.344090461730957
Reconstruction Loss: -10.164994239807129
Iteration 47101:
Training Loss: -7.34680700302124
Reconstruction Loss: -10.167193412780762
Iteration 47201:
Training Loss: -7.3494696617126465
Reconstruction Loss: -10.169382095336914
Iteration 47301:
Training Loss: -7.3521599769592285
Reconstruction Loss: -10.17156982421875
Iteration 47401:
Training Loss: -7.354853630065918
Reconstruction Loss: -10.173750877380371
Iteration 47501:
Training Loss: -7.357498645782471
Reconstruction Loss: -10.17592716217041
Iteration 47601:
Training Loss: -7.360166549682617
Reconstruction Loss: -10.178096771240234
Iteration 47701:
Training Loss: -7.3628387451171875
Reconstruction Loss: -10.180261611938477
Iteration 47801:
Training Loss: -7.365469932556152
Reconstruction Loss: -10.182417869567871
Iteration 47901:
Training Loss: -7.368109226226807
Reconstruction Loss: -10.184578895568848
Iteration 48001:
Training Loss: -7.370758533477783
Reconstruction Loss: -10.186729431152344
Iteration 48101:
Training Loss: -7.373407363891602
Reconstruction Loss: -10.188873291015625
Iteration 48201:
Training Loss: -7.376032829284668
Reconstruction Loss: -10.191014289855957
Iteration 48301:
Training Loss: -7.378643035888672
Reconstruction Loss: -10.19315242767334
Iteration 48401:
Training Loss: -7.38127326965332
Reconstruction Loss: -10.195283889770508
Iteration 48501:
Training Loss: -7.38389253616333
Reconstruction Loss: -10.197409629821777
Iteration 48601:
Training Loss: -7.3864874839782715
Reconstruction Loss: -10.199535369873047
Iteration 48701:
Training Loss: -7.3890700340271
Reconstruction Loss: -10.201648712158203
Iteration 48801:
Training Loss: -7.391690254211426
Reconstruction Loss: -10.20376205444336
Iteration 48901:
Training Loss: -7.394275665283203
Reconstruction Loss: -10.205872535705566
Iteration 49001:
Training Loss: -7.3968658447265625
Reconstruction Loss: -10.207971572875977
Iteration 49101:
Training Loss: -7.399434566497803
Reconstruction Loss: -10.210067749023438
Iteration 49201:
Training Loss: -7.402022361755371
Reconstruction Loss: -10.212160110473633
Iteration 49301:
Training Loss: -7.404580116271973
Reconstruction Loss: -10.214245796203613
Iteration 49401:
Training Loss: -7.407132625579834
Reconstruction Loss: -10.216324806213379
Iteration 49501:
Training Loss: -7.409687042236328
Reconstruction Loss: -10.21840763092041
Iteration 49601:
Training Loss: -7.41223669052124
Reconstruction Loss: -10.220479011535645
Iteration 49701:
Training Loss: -7.414814472198486
Reconstruction Loss: -10.222549438476562
Iteration 49801:
Training Loss: -7.417325019836426
Reconstruction Loss: -10.224610328674316
Iteration 49901:
Training Loss: -7.4198760986328125
Reconstruction Loss: -10.226673126220703
