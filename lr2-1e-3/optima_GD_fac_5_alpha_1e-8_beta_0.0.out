5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.661978721618652
Reconstruction Loss: -0.47944605350494385
Iteration 101:
Training Loss: 5.661978244781494
Reconstruction Loss: -0.4794461727142334
Iteration 201:
Training Loss: 5.661978244781494
Reconstruction Loss: -0.4794462323188782
Iteration 301:
Training Loss: 5.661978244781494
Reconstruction Loss: -0.4794463515281677
Iteration 401:
Training Loss: 5.661977767944336
Reconstruction Loss: -0.4794463515281677
Iteration 501:
Training Loss: 5.661977767944336
Reconstruction Loss: -0.47944653034210205
Iteration 601:
Training Loss: 5.661977767944336
Reconstruction Loss: -0.47944653034210205
Iteration 701:
Training Loss: 5.661977291107178
Reconstruction Loss: -0.4794466495513916
Iteration 801:
Training Loss: 5.661977291107178
Reconstruction Loss: -0.4794467091560364
Iteration 901:
Training Loss: 5.661977291107178
Reconstruction Loss: -0.4794468283653259
Iteration 1001:
Training Loss: 5.6619768142700195
Reconstruction Loss: -0.4794468283653259
Iteration 1101:
Training Loss: 5.6619768142700195
Reconstruction Loss: -0.47944700717926025
Iteration 1201:
Training Loss: 5.6619768142700195
Reconstruction Loss: -0.4794471263885498
Iteration 1301:
Training Loss: 5.661976337432861
Reconstruction Loss: -0.47944721579551697
Iteration 1401:
Training Loss: 5.661976337432861
Reconstruction Loss: -0.47944721579551697
Iteration 1501:
Training Loss: 5.661976337432861
Reconstruction Loss: -0.47944721579551697
Iteration 1601:
Training Loss: 5.661975860595703
Reconstruction Loss: -0.4794474244117737
Iteration 1701:
Training Loss: 5.661975860595703
Reconstruction Loss: -0.47944748401641846
Iteration 1801:
Training Loss: 5.661975860595703
Reconstruction Loss: -0.479447603225708
Iteration 1901:
Training Loss: 5.661975383758545
Reconstruction Loss: -0.479447603225708
Iteration 2001:
Training Loss: 5.661975383758545
Reconstruction Loss: -0.47944769263267517
Iteration 2101:
Training Loss: 5.661975383758545
Reconstruction Loss: -0.47944778203964233
Iteration 2201:
Training Loss: 5.661974906921387
Reconstruction Loss: -0.4794479012489319
Iteration 2301:
Training Loss: 5.661974906921387
Reconstruction Loss: -0.47944796085357666
Iteration 2401:
Training Loss: 5.6619744300842285
Reconstruction Loss: -0.4794480800628662
Iteration 2501:
Training Loss: 5.6619744300842285
Reconstruction Loss: -0.4794480800628662
Iteration 2601:
Training Loss: 5.6619744300842285
Reconstruction Loss: -0.47944825887680054
Iteration 2701:
Training Loss: 5.66197395324707
Reconstruction Loss: -0.47944825887680054
Iteration 2801:
Training Loss: 5.66197395324707
Reconstruction Loss: -0.47944846749305725
Iteration 2901:
Training Loss: 5.66197395324707
Reconstruction Loss: -0.47944846749305725
Iteration 3001:
Training Loss: 5.661973476409912
Reconstruction Loss: -0.47944867610931396
Iteration 3101:
Training Loss: 5.661973476409912
Reconstruction Loss: -0.47944873571395874
Iteration 3201:
Training Loss: 5.661972999572754
Reconstruction Loss: -0.47944873571395874
Iteration 3301:
Training Loss: 5.661972522735596
Reconstruction Loss: -0.47944894433021545
Iteration 3401:
Training Loss: 5.661972522735596
Reconstruction Loss: -0.4794490337371826
Iteration 3501:
Training Loss: 5.661972522735596
Reconstruction Loss: -0.47944915294647217
Iteration 3601:
Training Loss: 5.6619720458984375
Reconstruction Loss: -0.4794493317604065
Iteration 3701:
Training Loss: 5.6619720458984375
Reconstruction Loss: -0.47944942116737366
Iteration 3801:
Training Loss: 5.661971569061279
Reconstruction Loss: -0.4794495105743408
Iteration 3901:
Training Loss: 5.661971569061279
Reconstruction Loss: -0.4794495105743408
Iteration 4001:
Training Loss: 5.661971092224121
Reconstruction Loss: -0.47944962978363037
Iteration 4101:
Training Loss: 5.661970615386963
Reconstruction Loss: -0.47944989800453186
Iteration 4201:
Training Loss: 5.661970615386963
Reconstruction Loss: -0.479449987411499
Iteration 4301:
Training Loss: 5.661970138549805
Reconstruction Loss: -0.47945019602775574
Iteration 4401:
Training Loss: 5.661970138549805
Reconstruction Loss: -0.47945019602775574
Iteration 4501:
Training Loss: 5.6619696617126465
Reconstruction Loss: -0.47945040464401245
Iteration 4601:
Training Loss: 5.661969184875488
Reconstruction Loss: -0.4794504642486572
Iteration 4701:
Training Loss: 5.66196870803833
Reconstruction Loss: -0.47945067286491394
Iteration 4801:
Training Loss: 5.66196870803833
Reconstruction Loss: -0.47945088148117065
Iteration 4901:
Training Loss: 5.661968231201172
Reconstruction Loss: -0.4794509708881378
Iteration 5001:
Training Loss: 5.661967754364014
Reconstruction Loss: -0.4794512391090393
Iteration 5101:
Training Loss: 5.661967754364014
Reconstruction Loss: -0.47945135831832886
Iteration 5201:
Training Loss: 5.6619672775268555
Reconstruction Loss: -0.4794515371322632
Iteration 5301:
Training Loss: 5.661966800689697
Reconstruction Loss: -0.4794517159461975
Iteration 5401:
Training Loss: 5.661965847015381
Reconstruction Loss: -0.4794519245624542
Iteration 5501:
Training Loss: 5.661965847015381
Reconstruction Loss: -0.47945213317871094
Iteration 5601:
Training Loss: 5.6619648933410645
Reconstruction Loss: -0.4794524013996124
Iteration 5701:
Training Loss: 5.6619648933410645
Reconstruction Loss: -0.47945261001586914
Iteration 5801:
Training Loss: 5.661963939666748
Reconstruction Loss: -0.47945278882980347
Iteration 5901:
Training Loss: 5.66196346282959
Reconstruction Loss: -0.47945308685302734
Iteration 6001:
Training Loss: 5.661962985992432
Reconstruction Loss: -0.4794533848762512
Iteration 6101:
Training Loss: 5.661962032318115
Reconstruction Loss: -0.4794537425041199
Iteration 6201:
Training Loss: 5.661961555480957
Reconstruction Loss: -0.47945404052734375
Iteration 6301:
Training Loss: 5.661960601806641
Reconstruction Loss: -0.4794543385505676
Iteration 6401:
Training Loss: 5.661959648132324
Reconstruction Loss: -0.4794546365737915
Iteration 6501:
Training Loss: 5.661959171295166
Reconstruction Loss: -0.47945499420166016
Iteration 6601:
Training Loss: 5.661957740783691
Reconstruction Loss: -0.4794554114341736
Iteration 6701:
Training Loss: 5.661956787109375
Reconstruction Loss: -0.47945576906204224
Iteration 6801:
Training Loss: 5.661955833435059
Reconstruction Loss: -0.47945624589920044
Iteration 6901:
Training Loss: 5.661954402923584
Reconstruction Loss: -0.4794568419456482
Iteration 7001:
Training Loss: 5.661953449249268
Reconstruction Loss: -0.4794573187828064
Iteration 7101:
Training Loss: 5.661952018737793
Reconstruction Loss: -0.47945791482925415
Iteration 7201:
Training Loss: 5.661950588226318
Reconstruction Loss: -0.47945839166641235
Iteration 7301:
Training Loss: 5.6619486808776855
Reconstruction Loss: -0.4794592261314392
Iteration 7401:
Training Loss: 5.661946773529053
Reconstruction Loss: -0.4794599115848541
Iteration 7501:
Training Loss: 5.66194486618042
Reconstruction Loss: -0.4794606864452362
Iteration 7601:
Training Loss: 5.661942481994629
Reconstruction Loss: -0.479461669921875
Iteration 7701:
Training Loss: 5.66193962097168
Reconstruction Loss: -0.47946271300315857
Iteration 7801:
Training Loss: 5.6619367599487305
Reconstruction Loss: -0.4794638752937317
Iteration 7901:
Training Loss: 5.661933422088623
Reconstruction Loss: -0.479465126991272
Iteration 8001:
Training Loss: 5.661929607391357
Reconstruction Loss: -0.4794667363166809
Iteration 8101:
Training Loss: 5.661925315856934
Reconstruction Loss: -0.4794684648513794
Iteration 8201:
Training Loss: 5.661920070648193
Reconstruction Loss: -0.47947049140930176
Iteration 8301:
Training Loss: 5.661913871765137
Reconstruction Loss: -0.47947293519973755
Iteration 8401:
Training Loss: 5.6619062423706055
Reconstruction Loss: -0.47947579622268677
Iteration 8501:
Training Loss: 5.6618971824646
Reconstruction Loss: -0.47947925329208374
Iteration 8601:
Training Loss: 5.661885738372803
Reconstruction Loss: -0.47948378324508667
Iteration 8701:
Training Loss: 5.661871433258057
Reconstruction Loss: -0.47948938608169556
Iteration 8801:
Training Loss: 5.6618523597717285
Reconstruction Loss: -0.47949689626693726
Iteration 8901:
Training Loss: 5.6618266105651855
Reconstruction Loss: -0.4795072078704834
Iteration 9001:
Training Loss: 5.661789894104004
Reconstruction Loss: -0.4795219302177429
Iteration 9101:
Training Loss: 5.661734580993652
Reconstruction Loss: -0.4795442521572113
Iteration 9201:
Training Loss: 5.661644458770752
Reconstruction Loss: -0.47958141565322876
Iteration 9301:
Training Loss: 5.661478519439697
Reconstruction Loss: -0.47965142130851746
Iteration 9401:
Training Loss: 5.661107540130615
Reconstruction Loss: -0.4798121452331543
Iteration 9501:
Training Loss: 5.65989351272583
Reconstruction Loss: -0.4803580641746521
Iteration 9601:
Training Loss: 5.646348476409912
Reconstruction Loss: -0.4867936372756958
Iteration 9701:
Training Loss: 5.184040069580078
Reconstruction Loss: -0.5245859622955322
Iteration 9801:
Training Loss: 5.1662702560424805
Reconstruction Loss: -0.5239279866218567
Iteration 9901:
Training Loss: 5.16513204574585
Reconstruction Loss: -0.5260442495346069
Iteration 10001:
Training Loss: 5.164608001708984
Reconstruction Loss: -0.5278218388557434
Iteration 10101:
Training Loss: 5.164343357086182
Reconstruction Loss: -0.5289846062660217
Iteration 10201:
Training Loss: 5.164210796356201
Reconstruction Loss: -0.5297571420669556
Iteration 10301:
Training Loss: 5.16414213180542
Reconstruction Loss: -0.5302795171737671
Iteration 10401:
Training Loss: 5.164105415344238
Reconstruction Loss: -0.5306383371353149
Iteration 10501:
Training Loss: 5.164083957672119
Reconstruction Loss: -0.5308893918991089
Iteration 10601:
Training Loss: 5.164071083068848
Reconstruction Loss: -0.531067967414856
Iteration 10701:
Training Loss: 5.1640625
Reconstruction Loss: -0.531197190284729
Iteration 10801:
Training Loss: 5.164056301116943
Reconstruction Loss: -0.5312926173210144
Iteration 10901:
Training Loss: 5.164051055908203
Reconstruction Loss: -0.531363844871521
Iteration 11001:
Training Loss: 5.164046287536621
Reconstruction Loss: -0.531417965888977
Iteration 11101:
Training Loss: 5.164041519165039
Reconstruction Loss: -0.5314597487449646
Iteration 11201:
Training Loss: 5.164036273956299
Reconstruction Loss: -0.5314924120903015
Iteration 11301:
Training Loss: 5.164031028747559
Reconstruction Loss: -0.5315185785293579
Iteration 11401:
Training Loss: 5.16402530670166
Reconstruction Loss: -0.531540036201477
Iteration 11501:
Training Loss: 5.1640191078186035
Reconstruction Loss: -0.5315580368041992
Iteration 11601:
Training Loss: 5.1640119552612305
Reconstruction Loss: -0.5315736532211304
Iteration 11701:
Training Loss: 5.164003372192383
Reconstruction Loss: -0.5315878391265869
Iteration 11801:
Training Loss: 5.163994312286377
Reconstruction Loss: -0.5316011309623718
Iteration 11901:
Training Loss: 5.163983345031738
Reconstruction Loss: -0.5316141843795776
Iteration 12001:
Training Loss: 5.163970947265625
Reconstruction Loss: -0.5316277146339417
Iteration 12101:
Training Loss: 5.163956165313721
Reconstruction Loss: -0.5316420793533325
Iteration 12201:
Training Loss: 5.163938999176025
Reconstruction Loss: -0.5316581130027771
Iteration 12301:
Training Loss: 5.1639180183410645
Reconstruction Loss: -0.5316764116287231
Iteration 12401:
Training Loss: 5.1638922691345215
Reconstruction Loss: -0.5316979885101318
Iteration 12501:
Training Loss: 5.163860321044922
Reconstruction Loss: -0.5317243337631226
Iteration 12601:
Training Loss: 5.163819313049316
Reconstruction Loss: -0.5317573547363281
Iteration 12701:
Training Loss: 5.163765907287598
Reconstruction Loss: -0.5318000316619873
Iteration 12801:
Training Loss: 5.163692951202393
Reconstruction Loss: -0.5318572521209717
Iteration 12901:
Training Loss: 5.1635894775390625
Reconstruction Loss: -0.5319380164146423
Iteration 13001:
Training Loss: 5.1634321212768555
Reconstruction Loss: -0.5320593118667603
Iteration 13101:
Training Loss: 5.1631693840026855
Reconstruction Loss: -0.5322589874267578
Iteration 13201:
Training Loss: 5.162661552429199
Reconstruction Loss: -0.5326379537582397
Iteration 13301:
Training Loss: 5.161384582519531
Reconstruction Loss: -0.5335676074028015
Iteration 13401:
Training Loss: 5.155421257019043
Reconstruction Loss: -0.5377100110054016
Iteration 13501:
Training Loss: 4.726000785827637
Reconstruction Loss: -0.7221007943153381
Iteration 13601:
Training Loss: 4.476120948791504
Reconstruction Loss: -0.7752820253372192
Iteration 13701:
Training Loss: 4.447256088256836
Reconstruction Loss: -0.7452372312545776
Iteration 13801:
Training Loss: 4.4438157081604
Reconstruction Loss: -0.7328172326087952
Iteration 13901:
Training Loss: 4.443100929260254
Reconstruction Loss: -0.7286322116851807
Iteration 14001:
Training Loss: 4.442509174346924
Reconstruction Loss: -0.7275453805923462
Iteration 14101:
Training Loss: 4.441470623016357
Reconstruction Loss: -0.7278999090194702
Iteration 14201:
Training Loss: 4.439058780670166
Reconstruction Loss: -0.7297782897949219
Iteration 14301:
Training Loss: 4.430542469024658
Reconstruction Loss: -0.7367407083511353
Iteration 14401:
Training Loss: 4.326815128326416
Reconstruction Loss: -0.8153231143951416
Iteration 14501:
Training Loss: 4.034292697906494
Reconstruction Loss: -1.056091070175171
Iteration 14601:
Training Loss: 3.9855470657348633
Reconstruction Loss: -1.0509928464889526
Iteration 14701:
Training Loss: 3.929506778717041
Reconstruction Loss: -1.014965534210205
Iteration 14801:
Training Loss: 3.8990869522094727
Reconstruction Loss: -0.9970510005950928
Iteration 14901:
Training Loss: 3.8842737674713135
Reconstruction Loss: -1.006221055984497
Iteration 15001:
Training Loss: 3.8700127601623535
Reconstruction Loss: -1.0271680355072021
Iteration 15101:
Training Loss: 3.8525631427764893
Reconstruction Loss: -1.0521667003631592
Iteration 15201:
Training Loss: 3.8333144187927246
Reconstruction Loss: -1.0755095481872559
Iteration 15301:
Training Loss: 3.816631555557251
Reconstruction Loss: -1.0934438705444336
Iteration 15401:
Training Loss: 3.8041741847991943
Reconstruction Loss: -1.1059370040893555
Iteration 15501:
Training Loss: 3.7937326431274414
Reconstruction Loss: -1.1131261587142944
Iteration 15601:
Training Loss: 3.782536029815674
Reconstruction Loss: -1.1135427951812744
Iteration 15701:
Training Loss: 3.7686429023742676
Reconstruction Loss: -1.1038994789123535
Iteration 15801:
Training Loss: 3.7523036003112793
Reconstruction Loss: -1.0816357135772705
Iteration 15901:
Training Loss: 3.7371621131896973
Reconstruction Loss: -1.0511393547058105
Iteration 16001:
Training Loss: 3.7267932891845703
Reconstruction Loss: -1.0232493877410889
Iteration 16101:
Training Loss: 3.720853805541992
Reconstruction Loss: -1.0040661096572876
Iteration 16201:
Training Loss: 3.717463493347168
Reconstruction Loss: -0.9922999143600464
Iteration 16301:
Training Loss: 3.715420961380005
Reconstruction Loss: -0.9850122332572937
Iteration 16401:
Training Loss: 3.714143991470337
Reconstruction Loss: -0.9802924394607544
Iteration 16501:
Training Loss: 3.713327646255493
Reconstruction Loss: -0.9771423935890198
Iteration 16601:
Training Loss: 3.7127954959869385
Reconstruction Loss: -0.97501540184021
Iteration 16701:
Training Loss: 3.7124416828155518
Reconstruction Loss: -0.9735747575759888
Iteration 16801:
Training Loss: 3.7122013568878174
Reconstruction Loss: -0.9725967049598694
Iteration 16901:
Training Loss: 3.7120349407196045
Reconstruction Loss: -0.9719294309616089
Iteration 17001:
Training Loss: 3.7119178771972656
Reconstruction Loss: -0.971469521522522
Iteration 17101:
Training Loss: 3.711834192276001
Reconstruction Loss: -0.9711485505104065
Iteration 17201:
Training Loss: 3.711772918701172
Reconstruction Loss: -0.9709208011627197
Iteration 17301:
Training Loss: 3.7117278575897217
Reconstruction Loss: -0.9707560539245605
Iteration 17401:
Training Loss: 3.7116944789886475
Reconstruction Loss: -0.970634400844574
Iteration 17501:
Training Loss: 3.711669445037842
Reconstruction Loss: -0.9705426096916199
Iteration 17601:
Training Loss: 3.7116501331329346
Reconstruction Loss: -0.9704717397689819
Iteration 17701:
Training Loss: 3.7116355895996094
Reconstruction Loss: -0.9704160690307617
Iteration 17801:
Training Loss: 3.7116241455078125
Reconstruction Loss: -0.9703714847564697
Iteration 17901:
Training Loss: 3.7116150856018066
Reconstruction Loss: -0.9703351259231567
Iteration 18001:
Training Loss: 3.7116081714630127
Reconstruction Loss: -0.9703050851821899
Iteration 18101:
Training Loss: 3.7116026878356934
Reconstruction Loss: -0.9702797532081604
Iteration 18201:
Training Loss: 3.7115981578826904
Reconstruction Loss: -0.9702585339546204
Iteration 18301:
Training Loss: 3.711594343185425
Reconstruction Loss: -0.9702403545379639
Iteration 18401:
Training Loss: 3.7115914821624756
Reconstruction Loss: -0.9702247977256775
Iteration 18501:
Training Loss: 3.7115888595581055
Reconstruction Loss: -0.9702112078666687
Iteration 18601:
Training Loss: 3.7115867137908936
Reconstruction Loss: -0.970199465751648
Iteration 18701:
Training Loss: 3.7115848064422607
Reconstruction Loss: -0.9701892137527466
Iteration 18801:
Training Loss: 3.711583375930786
Reconstruction Loss: -0.9701802730560303
Iteration 18901:
Training Loss: 3.7115819454193115
Reconstruction Loss: -0.9701725244522095
Iteration 19001:
Training Loss: 3.711580276489258
Reconstruction Loss: -0.9701659083366394
Iteration 19101:
Training Loss: 3.7115793228149414
Reconstruction Loss: -0.9701597690582275
Iteration 19201:
Training Loss: 3.711578130722046
Reconstruction Loss: -0.9701546430587769
Iteration 19301:
Training Loss: 3.7115771770477295
Reconstruction Loss: -0.9701499938964844
Iteration 19401:
Training Loss: 3.711575984954834
Reconstruction Loss: -0.9701460599899292
Iteration 19501:
Training Loss: 3.7115750312805176
Reconstruction Loss: -0.9701426029205322
Iteration 19601:
Training Loss: 3.711574077606201
Reconstruction Loss: -0.9701398015022278
Iteration 19701:
Training Loss: 3.7115731239318848
Reconstruction Loss: -0.9701371192932129
Iteration 19801:
Training Loss: 3.7115719318389893
Reconstruction Loss: -0.9701350927352905
Iteration 19901:
Training Loss: 3.711570978164673
Reconstruction Loss: -0.9701333045959473
Iteration 20001:
Training Loss: 3.7115697860717773
Reconstruction Loss: -0.9701317548751831
Iteration 20101:
Training Loss: 3.711568832397461
Reconstruction Loss: -0.970130443572998
Iteration 20201:
Training Loss: 3.7115676403045654
Reconstruction Loss: -0.9701293706893921
Iteration 20301:
Training Loss: 3.71156644821167
Reconstruction Loss: -0.9701287746429443
Iteration 20401:
Training Loss: 3.7115652561187744
Reconstruction Loss: -0.9701282382011414
Iteration 20501:
Training Loss: 3.711564064025879
Reconstruction Loss: -0.9701277017593384
Iteration 20601:
Training Loss: 3.7115628719329834
Reconstruction Loss: -0.9701277017593384
Iteration 20701:
Training Loss: 3.711561441421509
Reconstruction Loss: -0.9701277017593384
Iteration 20801:
Training Loss: 3.711560010910034
Reconstruction Loss: -0.9701278209686279
Iteration 20901:
Training Loss: 3.7115585803985596
Reconstruction Loss: -0.9701281785964966
Iteration 21001:
Training Loss: 3.711556911468506
Reconstruction Loss: -0.9701286554336548
Iteration 21101:
Training Loss: 3.7115554809570312
Reconstruction Loss: -0.9701292514801025
Iteration 21201:
Training Loss: 3.7115538120269775
Reconstruction Loss: -0.9701298475265503
Iteration 21301:
Training Loss: 3.711552143096924
Reconstruction Loss: -0.9701308012008667
Iteration 21401:
Training Loss: 3.71155047416687
Reconstruction Loss: -0.9701321125030518
Iteration 21501:
Training Loss: 3.711548328399658
Reconstruction Loss: -0.9701329469680786
Iteration 21601:
Training Loss: 3.7115464210510254
Reconstruction Loss: -0.9701341986656189
Iteration 21701:
Training Loss: 3.7115445137023926
Reconstruction Loss: -0.9701354503631592
Iteration 21801:
Training Loss: 3.7115423679351807
Reconstruction Loss: -0.9701370000839233
Iteration 21901:
Training Loss: 3.7115399837493896
Reconstruction Loss: -0.9701385498046875
Iteration 22001:
Training Loss: 3.7115375995635986
Reconstruction Loss: -0.9701403379440308
Iteration 22101:
Training Loss: 3.7115349769592285
Reconstruction Loss: -0.9701422452926636
Iteration 22201:
Training Loss: 3.7115323543548584
Reconstruction Loss: -0.9701442718505859
Iteration 22301:
Training Loss: 3.711529493331909
Reconstruction Loss: -0.9701464176177979
Iteration 22401:
Training Loss: 3.711526393890381
Reconstruction Loss: -0.9701489210128784
Iteration 22501:
Training Loss: 3.7115232944488525
Reconstruction Loss: -0.9701511859893799
Iteration 22601:
Training Loss: 3.711519956588745
Reconstruction Loss: -0.97015380859375
Iteration 22701:
Training Loss: 3.7115163803100586
Reconstruction Loss: -0.9701566100120544
Iteration 22801:
Training Loss: 3.711512565612793
Reconstruction Loss: -0.9701597690582275
Iteration 22901:
Training Loss: 3.7115087509155273
Reconstruction Loss: -0.9701629877090454
Iteration 23001:
Training Loss: 3.7115042209625244
Reconstruction Loss: -0.9701663255691528
Iteration 23101:
Training Loss: 3.7114996910095215
Reconstruction Loss: -0.970170259475708
Iteration 23201:
Training Loss: 3.7114949226379395
Reconstruction Loss: -0.9701743125915527
Iteration 23301:
Training Loss: 3.711489677429199
Reconstruction Loss: -0.9701786041259766
Iteration 23401:
Training Loss: 3.711483955383301
Reconstruction Loss: -0.9701831936836243
Iteration 23501:
Training Loss: 3.7114779949188232
Reconstruction Loss: -0.9701881408691406
Iteration 23601:
Training Loss: 3.7114713191986084
Reconstruction Loss: -0.9701937437057495
Iteration 23701:
Training Loss: 3.7114641666412354
Reconstruction Loss: -0.970199465751648
Iteration 23801:
Training Loss: 3.711456537246704
Reconstruction Loss: -0.9702058434486389
Iteration 23901:
Training Loss: 3.7114481925964355
Reconstruction Loss: -0.9702126979827881
Iteration 24001:
Training Loss: 3.7114391326904297
Reconstruction Loss: -0.9702202081680298
Iteration 24101:
Training Loss: 3.7114293575286865
Reconstruction Loss: -0.9702284336090088
Iteration 24201:
Training Loss: 3.711418628692627
Reconstruction Loss: -0.9702373743057251
Iteration 24301:
Training Loss: 3.711406707763672
Reconstruction Loss: -0.9702472686767578
Iteration 24401:
Training Loss: 3.711393356323242
Reconstruction Loss: -0.9702580571174622
Iteration 24501:
Training Loss: 3.711379051208496
Reconstruction Loss: -0.9702702760696411
Iteration 24601:
Training Loss: 3.711362838745117
Reconstruction Loss: -0.9702835083007812
Iteration 24701:
Training Loss: 3.7113449573516846
Reconstruction Loss: -0.9702985286712646
Iteration 24801:
Training Loss: 3.711324453353882
Reconstruction Loss: -0.9703152179718018
Iteration 24901:
Training Loss: 3.7113020420074463
Reconstruction Loss: -0.9703340530395508
Iteration 25001:
Training Loss: 3.7112762928009033
Reconstruction Loss: -0.9703550934791565
Iteration 25101:
Training Loss: 3.711246967315674
Reconstruction Loss: -0.9703794717788696
Iteration 25201:
Training Loss: 3.7112131118774414
Reconstruction Loss: -0.9704071283340454
Iteration 25301:
Training Loss: 3.7111740112304688
Reconstruction Loss: -0.9704392552375793
Iteration 25401:
Training Loss: 3.7111287117004395
Reconstruction Loss: -0.9704766273498535
Iteration 25501:
Training Loss: 3.7110753059387207
Reconstruction Loss: -0.9705204963684082
Iteration 25601:
Training Loss: 3.7110114097595215
Reconstruction Loss: -0.9705726504325867
Iteration 25701:
Training Loss: 3.71093487739563
Reconstruction Loss: -0.9706353545188904
Iteration 25801:
Training Loss: 3.7108407020568848
Reconstruction Loss: -0.9707114100456238
Iteration 25901:
Training Loss: 3.710724115371704
Reconstruction Loss: -0.970805823802948
Iteration 26001:
Training Loss: 3.7105765342712402
Reconstruction Loss: -0.9709246158599854
Iteration 26101:
Training Loss: 3.710385322570801
Reconstruction Loss: -0.9710778594017029
Iteration 26201:
Training Loss: 3.7101306915283203
Reconstruction Loss: -0.9712806940078735
Iteration 26301:
Training Loss: 3.7097795009613037
Reconstruction Loss: -0.9715580940246582
Iteration 26401:
Training Loss: 3.709273338317871
Reconstruction Loss: -0.9719542860984802
Iteration 26501:
Training Loss: 3.7084996700286865
Reconstruction Loss: -0.9725522398948669
Iteration 26601:
Training Loss: 3.7072179317474365
Reconstruction Loss: -0.9735270738601685
Iteration 26701:
Training Loss: 3.704827308654785
Reconstruction Loss: -0.9753046035766602
Iteration 26801:
Training Loss: 3.6994216442108154
Reconstruction Loss: -0.9791942834854126
Iteration 26901:
Training Loss: 3.681771993637085
Reconstruction Loss: -0.9912109375
Iteration 27001:
Training Loss: 3.538160562515259
Reconstruction Loss: -1.0790804624557495
Iteration 27101:
Training Loss: 3.0561540126800537
Reconstruction Loss: -1.4672718048095703
Iteration 27201:
Training Loss: 2.9580206871032715
Reconstruction Loss: -1.6160013675689697
Iteration 27301:
Training Loss: 2.9210989475250244
Reconstruction Loss: -1.664431095123291
Iteration 27401:
Training Loss: 2.9060959815979004
Reconstruction Loss: -1.673203706741333
Iteration 27501:
Training Loss: 2.900235176086426
Reconstruction Loss: -1.6714673042297363
Iteration 27601:
Training Loss: 2.897826671600342
Reconstruction Loss: -1.668790340423584
Iteration 27701:
Training Loss: 2.896724224090576
Reconstruction Loss: -1.666934847831726
Iteration 27801:
Training Loss: 2.89615535736084
Reconstruction Loss: -1.6657981872558594
Iteration 27901:
Training Loss: 2.8958282470703125
Reconstruction Loss: -1.6650854349136353
Iteration 28001:
Training Loss: 2.8956236839294434
Reconstruction Loss: -1.664599895477295
Iteration 28101:
Training Loss: 2.895487070083618
Reconstruction Loss: -1.6642361879348755
Iteration 28201:
Training Loss: 2.8953921794891357
Reconstruction Loss: -1.6639418601989746
Iteration 28301:
Training Loss: 2.8953239917755127
Reconstruction Loss: -1.6636905670166016
Iteration 28401:
Training Loss: 2.8952736854553223
Reconstruction Loss: -1.6634693145751953
Iteration 28501:
Training Loss: 2.8952362537384033
Reconstruction Loss: -1.6632707118988037
Iteration 28601:
Training Loss: 2.8952081203460693
Reconstruction Loss: -1.6630911827087402
Iteration 28701:
Training Loss: 2.8951869010925293
Reconstruction Loss: -1.6629279851913452
Iteration 28801:
Training Loss: 2.8951704502105713
Reconstruction Loss: -1.6627798080444336
Iteration 28901:
Training Loss: 2.895158052444458
Reconstruction Loss: -1.6626452207565308
Iteration 29001:
Training Loss: 2.895148515701294
Reconstruction Loss: -1.6625230312347412
Iteration 29101:
Training Loss: 2.895141124725342
Reconstruction Loss: -1.662412405014038
Iteration 29201:
Training Loss: 2.8951354026794434
Reconstruction Loss: -1.662312626838684
Iteration 29301:
Training Loss: 2.8951311111450195
Reconstruction Loss: -1.6622226238250732
Iteration 29401:
Training Loss: 2.895127534866333
Reconstruction Loss: -1.6621415615081787
Iteration 29501:
Training Loss: 2.895124912261963
Reconstruction Loss: -1.6620686054229736
Iteration 29601:
Training Loss: 2.895122766494751
Reconstruction Loss: -1.6620032787322998
Iteration 29701:
Training Loss: 2.8951213359832764
Reconstruction Loss: -1.6619449853897095
Iteration 29801:
Training Loss: 2.895120143890381
Reconstruction Loss: -1.6618928909301758
Iteration 29901:
Training Loss: 2.8951191902160645
Reconstruction Loss: -1.66184663772583
Iteration 30001:
Training Loss: 2.895118236541748
Reconstruction Loss: -1.6618051528930664
Iteration 30101:
Training Loss: 2.89511775970459
Reconstruction Loss: -1.6617681980133057
Iteration 30201:
Training Loss: 2.8951172828674316
Reconstruction Loss: -1.6617355346679688
Iteration 30301:
Training Loss: 2.8951168060302734
Reconstruction Loss: -1.6617064476013184
Iteration 30401:
Training Loss: 2.8951165676116943
Reconstruction Loss: -1.6616806983947754
Iteration 30501:
Training Loss: 2.8951165676116943
Reconstruction Loss: -1.6616578102111816
Iteration 30601:
Training Loss: 2.8951163291931152
Reconstruction Loss: -1.6616376638412476
Iteration 30701:
Training Loss: 2.895116090774536
Reconstruction Loss: -1.6616201400756836
Iteration 30801:
Training Loss: 2.895115852355957
Reconstruction Loss: -1.661604404449463
Iteration 30901:
Training Loss: 2.895115613937378
Reconstruction Loss: -1.661590337753296
Iteration 31001:
Training Loss: 2.895115613937378
Reconstruction Loss: -1.6615779399871826
Iteration 31101:
Training Loss: 2.895115613937378
Reconstruction Loss: -1.661567211151123
Iteration 31201:
Training Loss: 2.895115613937378
Reconstruction Loss: -1.6615575551986694
Iteration 31301:
Training Loss: 2.895115613937378
Reconstruction Loss: -1.6615493297576904
Iteration 31401:
Training Loss: 2.895115375518799
Reconstruction Loss: -1.6615418195724487
Iteration 31501:
Training Loss: 2.895115375518799
Reconstruction Loss: -1.661535382270813
Iteration 31601:
Training Loss: 2.8951151371002197
Reconstruction Loss: -1.6615294218063354
Iteration 31701:
Training Loss: 2.8951151371002197
Reconstruction Loss: -1.6615245342254639
Iteration 31801:
Training Loss: 2.8951151371002197
Reconstruction Loss: -1.661520004272461
Iteration 31901:
Training Loss: 2.8951148986816406
Reconstruction Loss: -1.6615160703659058
Iteration 32001:
Training Loss: 2.8951151371002197
Reconstruction Loss: -1.661512851715088
Iteration 32101:
Training Loss: 2.8951148986816406
Reconstruction Loss: -1.6615098714828491
Iteration 32201:
Training Loss: 2.8951151371002197
Reconstruction Loss: -1.6615071296691895
Iteration 32301:
Training Loss: 2.8951146602630615
Reconstruction Loss: -1.6615045070648193
Iteration 32401:
Training Loss: 2.8951148986816406
Reconstruction Loss: -1.6615023612976074
Iteration 32501:
Training Loss: 2.8951148986816406
Reconstruction Loss: -1.6615004539489746
Iteration 32601:
Training Loss: 2.8951146602630615
Reconstruction Loss: -1.661498785018921
Iteration 32701:
Training Loss: 2.8951146602630615
Reconstruction Loss: -1.6614975929260254
Iteration 32801:
Training Loss: 2.8951146602630615
Reconstruction Loss: -1.6614964008331299
Iteration 32901:
Training Loss: 2.8951146602630615
Reconstruction Loss: -1.661495327949524
Iteration 33001:
Training Loss: 2.8951144218444824
Reconstruction Loss: -1.661494493484497
Iteration 33101:
Training Loss: 2.8951144218444824
Reconstruction Loss: -1.6614937782287598
Iteration 33201:
Training Loss: 2.8951146602630615
Reconstruction Loss: -1.6614930629730225
Iteration 33301:
Training Loss: 2.8951146602630615
Reconstruction Loss: -1.6614923477172852
Iteration 33401:
Training Loss: 2.8951144218444824
Reconstruction Loss: -1.661491870880127
Iteration 33501:
Training Loss: 2.8951141834259033
Reconstruction Loss: -1.6614915132522583
Iteration 33601:
Training Loss: 2.8951141834259033
Reconstruction Loss: -1.6614911556243896
Iteration 33701:
Training Loss: 2.8951141834259033
Reconstruction Loss: -1.6614906787872314
Iteration 33801:
Training Loss: 2.8951141834259033
Reconstruction Loss: -1.6614904403686523
Iteration 33901:
Training Loss: 2.8951141834259033
Reconstruction Loss: -1.6614902019500732
Iteration 34001:
Training Loss: 2.8951141834259033
Reconstruction Loss: -1.6614898443222046
Iteration 34101:
Training Loss: 2.895113945007324
Reconstruction Loss: -1.6614896059036255
Iteration 34201:
Training Loss: 2.895113945007324
Reconstruction Loss: -1.661489486694336
Iteration 34301:
Training Loss: 2.8951141834259033
Reconstruction Loss: -1.6614892482757568
Iteration 34401:
Training Loss: 2.895113706588745
Reconstruction Loss: -1.6614891290664673
Iteration 34501:
Training Loss: 2.895113706588745
Reconstruction Loss: -1.6614891290664673
Iteration 34601:
Training Loss: 2.895113706588745
Reconstruction Loss: -1.6614890098571777
Iteration 34701:
Training Loss: 2.895113706588745
Reconstruction Loss: -1.6614887714385986
Iteration 34801:
Training Loss: 2.895113706588745
Reconstruction Loss: -1.6614887714385986
Iteration 34901:
Training Loss: 2.895113706588745
Reconstruction Loss: -1.661488652229309
Iteration 35001:
Training Loss: 2.895113468170166
Reconstruction Loss: -1.661488652229309
Iteration 35101:
Training Loss: 2.895113468170166
Reconstruction Loss: -1.661488652229309
Iteration 35201:
Training Loss: 2.895113468170166
Reconstruction Loss: -1.661488652229309
Iteration 35301:
Training Loss: 2.895113229751587
Reconstruction Loss: -1.661488652229309
Iteration 35401:
Training Loss: 2.895113468170166
Reconstruction Loss: -1.6614885330200195
Iteration 35501:
Training Loss: 2.895113229751587
Reconstruction Loss: -1.6614885330200195
Iteration 35601:
Training Loss: 2.895113229751587
Reconstruction Loss: -1.6614885330200195
Iteration 35701:
Training Loss: 2.895112991333008
Reconstruction Loss: -1.66148841381073
Iteration 35801:
Training Loss: 2.895112991333008
Reconstruction Loss: -1.6614882946014404
Iteration 35901:
Training Loss: 2.895112991333008
Reconstruction Loss: -1.6614882946014404
Iteration 36001:
Training Loss: 2.895112991333008
Reconstruction Loss: -1.6614882946014404
Iteration 36101:
Training Loss: 2.895112991333008
Reconstruction Loss: -1.6614881753921509
Iteration 36201:
Training Loss: 2.8951127529144287
Reconstruction Loss: -1.6614881753921509
Iteration 36301:
Training Loss: 2.8951127529144287
Reconstruction Loss: -1.6614881753921509
Iteration 36401:
Training Loss: 2.895112991333008
Reconstruction Loss: -1.6614881753921509
Iteration 36501:
Training Loss: 2.8951125144958496
Reconstruction Loss: -1.6614881753921509
Iteration 36601:
Training Loss: 2.8951127529144287
Reconstruction Loss: -1.6614881753921509
Iteration 36701:
Training Loss: 2.8951125144958496
Reconstruction Loss: -1.6614881753921509
Iteration 36801:
Training Loss: 2.8951122760772705
Reconstruction Loss: -1.6614881753921509
Iteration 36901:
Training Loss: 2.8951122760772705
Reconstruction Loss: -1.6614881753921509
Iteration 37001:
Training Loss: 2.8951122760772705
Reconstruction Loss: -1.6614882946014404
Iteration 37101:
Training Loss: 2.8951125144958496
Reconstruction Loss: -1.6614882946014404
Iteration 37201:
Training Loss: 2.8951122760772705
Reconstruction Loss: -1.6614882946014404
Iteration 37301:
Training Loss: 2.8951122760772705
Reconstruction Loss: -1.6614882946014404
Iteration 37401:
Training Loss: 2.8951120376586914
Reconstruction Loss: -1.6614885330200195
Iteration 37501:
Training Loss: 2.8951120376586914
Reconstruction Loss: -1.66148841381073
Iteration 37601:
Training Loss: 2.8951120376586914
Reconstruction Loss: -1.6614885330200195
Iteration 37701:
Training Loss: 2.8951120376586914
Reconstruction Loss: -1.661488652229309
Iteration 37801:
Training Loss: 2.8951117992401123
Reconstruction Loss: -1.6614885330200195
Iteration 37901:
Training Loss: 2.8951117992401123
Reconstruction Loss: -1.6614887714385986
Iteration 38001:
Training Loss: 2.8951117992401123
Reconstruction Loss: -1.6614887714385986
Iteration 38101:
Training Loss: 2.8951117992401123
Reconstruction Loss: -1.6614887714385986
Iteration 38201:
Training Loss: 2.8951117992401123
Reconstruction Loss: -1.6614888906478882
Iteration 38301:
Training Loss: 2.8951117992401123
Reconstruction Loss: -1.6614888906478882
Iteration 38401:
Training Loss: 2.895111560821533
Reconstruction Loss: -1.6614888906478882
Iteration 38501:
Training Loss: 2.895111560821533
Reconstruction Loss: -1.6614888906478882
Iteration 38601:
Training Loss: 2.895111322402954
Reconstruction Loss: -1.6614888906478882
Iteration 38701:
Training Loss: 2.895111322402954
Reconstruction Loss: -1.6614890098571777
Iteration 38801:
Training Loss: 2.895111560821533
Reconstruction Loss: -1.6614890098571777
Iteration 38901:
Training Loss: 2.895111083984375
Reconstruction Loss: -1.6614891290664673
Iteration 39001:
Training Loss: 2.895111083984375
Reconstruction Loss: -1.6614890098571777
Iteration 39101:
Training Loss: 2.895111083984375
Reconstruction Loss: -1.6614891290664673
Iteration 39201:
Training Loss: 2.895111083984375
Reconstruction Loss: -1.6614892482757568
Iteration 39301:
Training Loss: 2.895111083984375
Reconstruction Loss: -1.6614893674850464
Iteration 39401:
Training Loss: 2.895110845565796
Reconstruction Loss: -1.661489486694336
Iteration 39501:
Training Loss: 2.895110845565796
Reconstruction Loss: -1.661489486694336
Iteration 39601:
Training Loss: 2.895110845565796
Reconstruction Loss: -1.661489486694336
Iteration 39701:
Training Loss: 2.895110607147217
Reconstruction Loss: -1.661489486694336
Iteration 39801:
Training Loss: 2.895110607147217
Reconstruction Loss: -1.6614896059036255
Iteration 39901:
Training Loss: 2.895110607147217
Reconstruction Loss: -1.661489725112915
Iteration 40001:
Training Loss: 2.8951103687286377
Reconstruction Loss: -1.6614896059036255
Iteration 40101:
Training Loss: 2.8951103687286377
Reconstruction Loss: -1.6614896059036255
Iteration 40201:
Training Loss: 2.8951103687286377
Reconstruction Loss: -1.661489725112915
Iteration 40301:
Training Loss: 2.8951101303100586
Reconstruction Loss: -1.661489725112915
Iteration 40401:
Training Loss: 2.8951101303100586
Reconstruction Loss: -1.661489725112915
Iteration 40501:
Training Loss: 2.8951101303100586
Reconstruction Loss: -1.6614899635314941
Iteration 40601:
Training Loss: 2.8951101303100586
Reconstruction Loss: -1.6614898443222046
Iteration 40701:
Training Loss: 2.8951098918914795
Reconstruction Loss: -1.6614899635314941
Iteration 40801:
Training Loss: 2.8951098918914795
Reconstruction Loss: -1.6614899635314941
Iteration 40901:
Training Loss: 2.8951096534729004
Reconstruction Loss: -1.6614899635314941
Iteration 41001:
Training Loss: 2.8951096534729004
Reconstruction Loss: -1.6614899635314941
Iteration 41101:
Training Loss: 2.8951096534729004
Reconstruction Loss: -1.6614899635314941
Iteration 41201:
Training Loss: 2.8951094150543213
Reconstruction Loss: -1.6614900827407837
Iteration 41301:
Training Loss: 2.8951096534729004
Reconstruction Loss: -1.6614899635314941
Iteration 41401:
Training Loss: 2.8951094150543213
Reconstruction Loss: -1.6614900827407837
Iteration 41501:
Training Loss: 2.8951094150543213
Reconstruction Loss: -1.6614900827407837
Iteration 41601:
Training Loss: 2.895109176635742
Reconstruction Loss: -1.6614902019500732
Iteration 41701:
Training Loss: 2.895109176635742
Reconstruction Loss: -1.6614902019500732
Iteration 41801:
Training Loss: 2.895109176635742
Reconstruction Loss: -1.6614902019500732
Iteration 41901:
Training Loss: 2.895108938217163
Reconstruction Loss: -1.6614903211593628
Iteration 42001:
Training Loss: 2.895109176635742
Reconstruction Loss: -1.6614903211593628
Iteration 42101:
Training Loss: 2.895108938217163
Reconstruction Loss: -1.6614904403686523
Iteration 42201:
Training Loss: 2.895108699798584
Reconstruction Loss: -1.6614903211593628
Iteration 42301:
Training Loss: 2.895108699798584
Reconstruction Loss: -1.6614904403686523
Iteration 42401:
Training Loss: 2.895108699798584
Reconstruction Loss: -1.6614904403686523
Iteration 42501:
Training Loss: 2.895108461380005
Reconstruction Loss: -1.6614904403686523
Iteration 42601:
Training Loss: 2.895108461380005
Reconstruction Loss: -1.6614906787872314
Iteration 42701:
Training Loss: 2.895108222961426
Reconstruction Loss: -1.6614906787872314
Iteration 42801:
Training Loss: 2.895108222961426
Reconstruction Loss: -1.6614909172058105
Iteration 42901:
Training Loss: 2.895108222961426
Reconstruction Loss: -1.6614909172058105
Iteration 43001:
Training Loss: 2.895108222961426
Reconstruction Loss: -1.6614909172058105
Iteration 43101:
Training Loss: 2.8951079845428467
Reconstruction Loss: -1.6614911556243896
Iteration 43201:
Training Loss: 2.8951079845428467
Reconstruction Loss: -1.6614911556243896
Iteration 43301:
Training Loss: 2.8951079845428467
Reconstruction Loss: -1.6614912748336792
Iteration 43401:
Training Loss: 2.8951077461242676
Reconstruction Loss: -1.6614912748336792
Iteration 43501:
Training Loss: 2.8951077461242676
Reconstruction Loss: -1.6614913940429688
Iteration 43601:
Training Loss: 2.8951075077056885
Reconstruction Loss: -1.6614912748336792
Iteration 43701:
Training Loss: 2.8951075077056885
Reconstruction Loss: -1.6614915132522583
Iteration 43801:
Training Loss: 2.8951072692871094
Reconstruction Loss: -1.6614915132522583
Iteration 43901:
Training Loss: 2.8951072692871094
Reconstruction Loss: -1.6614915132522583
Iteration 44001:
Training Loss: 2.8951070308685303
Reconstruction Loss: -1.6614916324615479
Iteration 44101:
Training Loss: 2.8951070308685303
Reconstruction Loss: -1.6614916324615479
Iteration 44201:
Training Loss: 2.8951070308685303
Reconstruction Loss: -1.6614916324615479
Iteration 44301:
Training Loss: 2.8951070308685303
Reconstruction Loss: -1.661491870880127
Iteration 44401:
Training Loss: 2.895106554031372
Reconstruction Loss: -1.661491870880127
Iteration 44501:
Training Loss: 2.895106792449951
Reconstruction Loss: -1.661491870880127
Iteration 44601:
Training Loss: 2.895106554031372
Reconstruction Loss: -1.661491870880127
Iteration 44701:
Training Loss: 2.895106315612793
Reconstruction Loss: -1.6614919900894165
Iteration 44801:
Training Loss: 2.895106315612793
Reconstruction Loss: -1.6614922285079956
Iteration 44901:
Training Loss: 2.895106315612793
Reconstruction Loss: -1.6614919900894165
Iteration 45001:
Training Loss: 2.895106077194214
Reconstruction Loss: -1.661492109298706
Iteration 45101:
Training Loss: 2.895106077194214
Reconstruction Loss: -1.6614922285079956
Iteration 45201:
Training Loss: 2.895106077194214
Reconstruction Loss: -1.6614922285079956
Iteration 45301:
Training Loss: 2.8951058387756348
Reconstruction Loss: -1.6614922285079956
Iteration 45401:
Training Loss: 2.8951058387756348
Reconstruction Loss: -1.6614923477172852
Iteration 45501:
Training Loss: 2.8951056003570557
Reconstruction Loss: -1.6614923477172852
Iteration 45601:
Training Loss: 2.8951056003570557
Reconstruction Loss: -1.6614925861358643
Iteration 45701:
Training Loss: 2.8951056003570557
Reconstruction Loss: -1.6614928245544434
Iteration 45801:
Training Loss: 2.8951053619384766
Reconstruction Loss: -1.661492943763733
Iteration 45901:
Training Loss: 2.8951056003570557
Reconstruction Loss: -1.661492943763733
Iteration 46001:
Training Loss: 2.8951051235198975
Reconstruction Loss: -1.661492943763733
Iteration 46101:
Training Loss: 2.8951051235198975
Reconstruction Loss: -1.661492943763733
Iteration 46201:
Training Loss: 2.8951048851013184
Reconstruction Loss: -1.661492943763733
Iteration 46301:
Training Loss: 2.8951046466827393
Reconstruction Loss: -1.6614930629730225
Iteration 46401:
Training Loss: 2.8951046466827393
Reconstruction Loss: -1.6614930629730225
Iteration 46501:
Training Loss: 2.8951046466827393
Reconstruction Loss: -1.6614930629730225
Iteration 46601:
Training Loss: 2.89510440826416
Reconstruction Loss: -1.6614930629730225
Iteration 46701:
Training Loss: 2.89510440826416
Reconstruction Loss: -1.6614930629730225
Iteration 46801:
Training Loss: 2.89510440826416
Reconstruction Loss: -1.6614930629730225
Iteration 46901:
Training Loss: 2.895104169845581
Reconstruction Loss: -1.6614930629730225
Iteration 47001:
Training Loss: 2.895104169845581
Reconstruction Loss: -1.6614930629730225
Iteration 47101:
Training Loss: 2.895103693008423
Reconstruction Loss: -1.6614930629730225
Iteration 47201:
Training Loss: 2.895103693008423
Reconstruction Loss: -1.6614930629730225
Iteration 47301:
Training Loss: 2.895103693008423
Reconstruction Loss: -1.661493182182312
Iteration 47401:
Training Loss: 2.895103693008423
Reconstruction Loss: -1.6614930629730225
Iteration 47501:
Training Loss: 2.8951034545898438
Reconstruction Loss: -1.661493182182312
Iteration 47601:
Training Loss: 2.8951032161712646
Reconstruction Loss: -1.6614930629730225
Iteration 47701:
Training Loss: 2.8951032161712646
Reconstruction Loss: -1.6614930629730225
Iteration 47801:
Training Loss: 2.8951029777526855
Reconstruction Loss: -1.6614933013916016
Iteration 47901:
Training Loss: 2.8951027393341064
Reconstruction Loss: -1.6614933013916016
Iteration 48001:
Training Loss: 2.8951027393341064
Reconstruction Loss: -1.6614933013916016
Iteration 48101:
Training Loss: 2.8951027393341064
Reconstruction Loss: -1.6614934206008911
Iteration 48201:
Training Loss: 2.8951022624969482
Reconstruction Loss: -1.6614935398101807
Iteration 48301:
Training Loss: 2.8951025009155273
Reconstruction Loss: -1.6614935398101807
Iteration 48401:
Training Loss: 2.895102024078369
Reconstruction Loss: -1.6614937782287598
Iteration 48501:
Training Loss: 2.895102024078369
Reconstruction Loss: -1.6614938974380493
Iteration 48601:
Training Loss: 2.895102024078369
Reconstruction Loss: -1.6614940166473389
Iteration 48701:
Training Loss: 2.89510178565979
Reconstruction Loss: -1.6614941358566284
Iteration 48801:
Training Loss: 2.89510178565979
Reconstruction Loss: -1.661494255065918
Iteration 48901:
Training Loss: 2.895101547241211
Reconstruction Loss: -1.661494493484497
Iteration 49001:
Training Loss: 2.895101547241211
Reconstruction Loss: -1.661494493484497
Iteration 49101:
Training Loss: 2.895101308822632
Reconstruction Loss: -1.6614946126937866
Iteration 49201:
Training Loss: 2.895101308822632
Reconstruction Loss: -1.6614950895309448
Iteration 49301:
Training Loss: 2.8951010704040527
Reconstruction Loss: -1.6614950895309448
Iteration 49401:
Training Loss: 2.8951008319854736
Reconstruction Loss: -1.6614952087402344
Iteration 49501:
Training Loss: 2.8951005935668945
Reconstruction Loss: -1.6614952087402344
Iteration 49601:
Training Loss: 2.8951005935668945
Reconstruction Loss: -1.661495327949524
Iteration 49701:
Training Loss: 2.8951003551483154
Reconstruction Loss: -1.661495566368103
Iteration 49801:
Training Loss: 2.8951001167297363
Reconstruction Loss: -1.6614956855773926
Iteration 49901:
Training Loss: 2.8951001167297363
Reconstruction Loss: -1.6614959239959717
