5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.2438130378723145
Reconstruction Loss: -0.5194416046142578
Iteration 21:
Training Loss: 5.01940393447876
Reconstruction Loss: -0.5198950171470642
Iteration 41:
Training Loss: 5.364081859588623
Reconstruction Loss: -0.5206334590911865
Iteration 61:
Training Loss: 5.3517165184021
Reconstruction Loss: -0.523561418056488
Iteration 81:
Training Loss: 4.8124260902404785
Reconstruction Loss: -0.633948028087616
Iteration 101:
Training Loss: 4.518614292144775
Reconstruction Loss: -0.7870636582374573
Iteration 121:
Training Loss: 4.368126392364502
Reconstruction Loss: -0.7311518788337708
Iteration 141:
Training Loss: 4.490349769592285
Reconstruction Loss: -0.8424984216690063
Iteration 161:
Training Loss: 4.069699764251709
Reconstruction Loss: -1.1495782136917114
Iteration 181:
Training Loss: 3.3580005168914795
Reconstruction Loss: -1.192119836807251
Iteration 201:
Training Loss: 3.682084321975708
Reconstruction Loss: -1.2154786586761475
Iteration 221:
Training Loss: 3.437195301055908
Reconstruction Loss: -1.2444658279418945
Iteration 241:
Training Loss: 3.3391757011413574
Reconstruction Loss: -1.4180837869644165
Iteration 261:
Training Loss: 3.1098694801330566
Reconstruction Loss: -1.7124004364013672
Iteration 281:
Training Loss: 2.3977949619293213
Reconstruction Loss: -1.9643535614013672
Iteration 301:
Training Loss: 1.6638985872268677
Reconstruction Loss: -2.5173981189727783
Iteration 321:
Training Loss: 1.080354928970337
Reconstruction Loss: -3.243361473083496
Iteration 341:
Training Loss: 0.0650101974606514
Reconstruction Loss: -3.9856033325195312
Iteration 361:
Training Loss: -0.8821722269058228
Reconstruction Loss: -4.697340488433838
Iteration 381:
Training Loss: -1.812300443649292
Reconstruction Loss: -5.371443271636963
Iteration 401:
Training Loss: -2.260953426361084
Reconstruction Loss: -6.005328178405762
Iteration 421:
Training Loss: -2.72672176361084
Reconstruction Loss: -6.605639934539795
Iteration 441:
Training Loss: -3.7453765869140625
Reconstruction Loss: -7.184781074523926
Iteration 461:
Training Loss: -4.3742170333862305
Reconstruction Loss: -7.746409893035889
Iteration 481:
Training Loss: -4.895086765289307
Reconstruction Loss: -8.290128707885742
Iteration 501:
Training Loss: -5.16192626953125
Reconstruction Loss: -8.81783390045166
Iteration 521:
Training Loss: -6.030650615692139
Reconstruction Loss: -9.339136123657227
Iteration 541:
Training Loss: -6.491329193115234
Reconstruction Loss: -9.849658966064453
Iteration 561:
Training Loss: -6.8263654708862305
Reconstruction Loss: -10.337907791137695
Iteration 581:
Training Loss: -7.5246100425720215
Reconstruction Loss: -10.821088790893555
Iteration 601:
Training Loss: -7.714344024658203
Reconstruction Loss: -11.272738456726074
Iteration 621:
Training Loss: -8.049121856689453
Reconstruction Loss: -11.688240051269531
Iteration 641:
Training Loss: -8.281634330749512
Reconstruction Loss: -12.062490463256836
Iteration 661:
Training Loss: -8.403427124023438
Reconstruction Loss: -12.388943672180176
Iteration 681:
Training Loss: -8.749702453613281
Reconstruction Loss: -12.630925178527832
Iteration 701:
Training Loss: -8.710537910461426
Reconstruction Loss: -12.81924057006836
Iteration 721:
Training Loss: -8.676274299621582
Reconstruction Loss: -12.959884643554688
Iteration 741:
Training Loss: -8.955089569091797
Reconstruction Loss: -13.042963981628418
Iteration 761:
Training Loss: -9.402525901794434
Reconstruction Loss: -13.10560131072998
Iteration 781:
Training Loss: -9.084101676940918
Reconstruction Loss: -13.138163566589355
Iteration 801:
Training Loss: -9.026422500610352
Reconstruction Loss: -13.163019180297852
Iteration 821:
Training Loss: -8.97822093963623
Reconstruction Loss: -13.175256729125977
Iteration 841:
Training Loss: -8.974693298339844
Reconstruction Loss: -13.185807228088379
Iteration 861:
Training Loss: -8.8350191116333
Reconstruction Loss: -13.187005996704102
Iteration 881:
Training Loss: -9.14209270477295
Reconstruction Loss: -13.1922607421875
Iteration 901:
Training Loss: -8.818326950073242
Reconstruction Loss: -13.175365447998047
Iteration 921:
Training Loss: -9.287582397460938
Reconstruction Loss: -13.183467864990234
Iteration 941:
Training Loss: -8.99993896484375
Reconstruction Loss: -13.191290855407715
Iteration 961:
Training Loss: -8.810001373291016
Reconstruction Loss: -13.188011169433594
Iteration 981:
Training Loss: -8.977373123168945
Reconstruction Loss: -13.201303482055664
Iteration 1001:
Training Loss: -9.109768867492676
Reconstruction Loss: -13.19548511505127
Iteration 1021:
Training Loss: -9.207481384277344
Reconstruction Loss: -13.191245079040527
Iteration 1041:
Training Loss: -9.05196475982666
Reconstruction Loss: -13.20330810546875
Iteration 1061:
Training Loss: -9.301231384277344
Reconstruction Loss: -13.199808120727539
Iteration 1081:
Training Loss: -9.027928352355957
Reconstruction Loss: -13.189485549926758
Iteration 1101:
Training Loss: -9.122102737426758
Reconstruction Loss: -13.194239616394043
Iteration 1121:
Training Loss: -9.129866600036621
Reconstruction Loss: -13.192449569702148
Iteration 1141:
Training Loss: -9.025464057922363
Reconstruction Loss: -13.190930366516113
Iteration 1161:
Training Loss: -9.23954963684082
Reconstruction Loss: -13.193580627441406
Iteration 1181:
Training Loss: -9.167531967163086
Reconstruction Loss: -13.190400123596191
Iteration 1201:
Training Loss: -8.863356590270996
Reconstruction Loss: -13.19730281829834
Iteration 1221:
Training Loss: -8.943856239318848
Reconstruction Loss: -13.197741508483887
Iteration 1241:
Training Loss: -9.276253700256348
Reconstruction Loss: -13.1972074508667
Iteration 1261:
Training Loss: -9.221951484680176
Reconstruction Loss: -13.196720123291016
Iteration 1281:
Training Loss: -9.107644081115723
Reconstruction Loss: -13.204449653625488
Iteration 1301:
Training Loss: -9.060431480407715
Reconstruction Loss: -13.197234153747559
Iteration 1321:
Training Loss: -8.989962577819824
Reconstruction Loss: -13.194354057312012
Iteration 1341:
Training Loss: -9.254076957702637
Reconstruction Loss: -13.194525718688965
Iteration 1361:
Training Loss: -8.830094337463379
Reconstruction Loss: -13.207514762878418
Iteration 1381:
Training Loss: -9.156766891479492
Reconstruction Loss: -13.206888198852539
Iteration 1401:
Training Loss: -9.071328163146973
Reconstruction Loss: -13.198129653930664
Iteration 1421:
Training Loss: -9.101652145385742
Reconstruction Loss: -13.198725700378418
Iteration 1441:
Training Loss: -9.31732177734375
Reconstruction Loss: -13.210421562194824
Iteration 1461:
Training Loss: -9.111381530761719
Reconstruction Loss: -13.20335578918457
Iteration 1481:
Training Loss: -9.053245544433594
Reconstruction Loss: -13.20421314239502
Iteration 1501:
Training Loss: -9.151262283325195
Reconstruction Loss: -13.207446098327637
Iteration 1521:
Training Loss: -8.986602783203125
Reconstruction Loss: -13.211893081665039
Iteration 1541:
Training Loss: -9.15417194366455
Reconstruction Loss: -13.197732925415039
Iteration 1561:
Training Loss: -9.428844451904297
Reconstruction Loss: -13.205570220947266
Iteration 1581:
Training Loss: -8.751879692077637
Reconstruction Loss: -13.201725959777832
Iteration 1601:
Training Loss: -9.144783973693848
Reconstruction Loss: -13.213397026062012
Iteration 1621:
Training Loss: -8.944377899169922
Reconstruction Loss: -13.210391998291016
Iteration 1641:
Training Loss: -9.117301940917969
Reconstruction Loss: -13.205059051513672
Iteration 1661:
Training Loss: -9.239392280578613
Reconstruction Loss: -13.212804794311523
Iteration 1681:
Training Loss: -8.88686752319336
Reconstruction Loss: -13.208964347839355
Iteration 1701:
Training Loss: -8.898402214050293
Reconstruction Loss: -13.200495719909668
Iteration 1721:
Training Loss: -9.247222900390625
Reconstruction Loss: -13.21929931640625
Iteration 1741:
Training Loss: -9.002420425415039
Reconstruction Loss: -13.205150604248047
Iteration 1761:
Training Loss: -9.375969886779785
Reconstruction Loss: -13.218290328979492
Iteration 1781:
Training Loss: -9.290777206420898
Reconstruction Loss: -13.21624755859375
Iteration 1801:
Training Loss: -8.994060516357422
Reconstruction Loss: -13.216941833496094
Iteration 1821:
Training Loss: -9.14694595336914
Reconstruction Loss: -13.219620704650879
Iteration 1841:
Training Loss: -9.207212448120117
Reconstruction Loss: -13.215936660766602
Iteration 1861:
Training Loss: -9.252187728881836
Reconstruction Loss: -13.214314460754395
Iteration 1881:
Training Loss: -9.39629077911377
Reconstruction Loss: -13.219676971435547
Iteration 1901:
Training Loss: -9.374537467956543
Reconstruction Loss: -13.215617179870605
Iteration 1921:
Training Loss: -9.29091739654541
Reconstruction Loss: -13.223638534545898
Iteration 1941:
Training Loss: -8.894294738769531
Reconstruction Loss: -13.226347923278809
Iteration 1961:
Training Loss: -9.098905563354492
Reconstruction Loss: -13.221077919006348
Iteration 1981:
Training Loss: -8.899121284484863
Reconstruction Loss: -13.221572875976562
Iteration 2001:
Training Loss: -9.061174392700195
Reconstruction Loss: -13.214634895324707
Iteration 2021:
Training Loss: -9.079190254211426
Reconstruction Loss: -13.220163345336914
Iteration 2041:
Training Loss: -9.283220291137695
Reconstruction Loss: -13.222148895263672
Iteration 2061:
Training Loss: -9.018510818481445
Reconstruction Loss: -13.219883918762207
Iteration 2081:
Training Loss: -9.068168640136719
Reconstruction Loss: -13.224678039550781
Iteration 2101:
Training Loss: -9.288986206054688
Reconstruction Loss: -13.228826522827148
Iteration 2121:
Training Loss: -9.020360946655273
Reconstruction Loss: -13.222347259521484
Iteration 2141:
Training Loss: -8.886409759521484
Reconstruction Loss: -13.224976539611816
Iteration 2161:
Training Loss: -9.113999366760254
Reconstruction Loss: -13.237765312194824
Iteration 2181:
Training Loss: -9.11788272857666
Reconstruction Loss: -13.228079795837402
Iteration 2201:
Training Loss: -9.054449081420898
Reconstruction Loss: -13.219161033630371
Iteration 2221:
Training Loss: -9.273025512695312
Reconstruction Loss: -13.233229637145996
Iteration 2241:
Training Loss: -9.232757568359375
Reconstruction Loss: -13.227505683898926
Iteration 2261:
Training Loss: -9.043484687805176
Reconstruction Loss: -13.22562026977539
Iteration 2281:
Training Loss: -9.103975296020508
Reconstruction Loss: -13.225310325622559
Iteration 2301:
Training Loss: -9.186219215393066
Reconstruction Loss: -13.235299110412598
Iteration 2321:
Training Loss: -9.132241249084473
Reconstruction Loss: -13.239188194274902
Iteration 2341:
Training Loss: -9.20789623260498
Reconstruction Loss: -13.236352920532227
Iteration 2361:
Training Loss: -9.326422691345215
Reconstruction Loss: -13.230915069580078
Iteration 2381:
Training Loss: -8.966018676757812
Reconstruction Loss: -13.226101875305176
Iteration 2401:
Training Loss: -9.140027046203613
Reconstruction Loss: -13.237833976745605
Iteration 2421:
Training Loss: -9.283598899841309
Reconstruction Loss: -13.240005493164062
Iteration 2441:
Training Loss: -9.214383125305176
Reconstruction Loss: -13.23644733428955
Iteration 2461:
Training Loss: -9.123988151550293
Reconstruction Loss: -13.236125946044922
Iteration 2481:
Training Loss: -9.277260780334473
Reconstruction Loss: -13.238208770751953
Iteration 2501:
Training Loss: -8.901524543762207
Reconstruction Loss: -13.23976993560791
Iteration 2521:
Training Loss: -9.108579635620117
Reconstruction Loss: -13.24200439453125
Iteration 2541:
Training Loss: -9.187712669372559
Reconstruction Loss: -13.237444877624512
Iteration 2561:
Training Loss: -9.333410263061523
Reconstruction Loss: -13.24522590637207
Iteration 2581:
Training Loss: -9.096969604492188
Reconstruction Loss: -13.253206253051758
Iteration 2601:
Training Loss: -9.133413314819336
Reconstruction Loss: -13.243042945861816
Iteration 2621:
Training Loss: -9.105504035949707
Reconstruction Loss: -13.238884925842285
Iteration 2641:
Training Loss: -9.056252479553223
Reconstruction Loss: -13.243707656860352
Iteration 2661:
Training Loss: -9.20790958404541
Reconstruction Loss: -13.24588394165039
Iteration 2681:
Training Loss: -8.914186477661133
Reconstruction Loss: -13.241406440734863
Iteration 2701:
Training Loss: -8.848910331726074
Reconstruction Loss: -13.250247955322266
Iteration 2721:
Training Loss: -9.557493209838867
Reconstruction Loss: -13.25064468383789
Iteration 2741:
Training Loss: -9.081610679626465
Reconstruction Loss: -13.238818168640137
Iteration 2761:
Training Loss: -9.293951034545898
Reconstruction Loss: -13.257246971130371
Iteration 2781:
Training Loss: -9.431045532226562
Reconstruction Loss: -13.248132705688477
Iteration 2801:
Training Loss: -8.894192695617676
Reconstruction Loss: -13.247024536132812
Iteration 2821:
Training Loss: -9.196152687072754
Reconstruction Loss: -13.242658615112305
Iteration 2841:
Training Loss: -9.13525104522705
Reconstruction Loss: -13.246077537536621
Iteration 2861:
Training Loss: -9.315421104431152
Reconstruction Loss: -13.251882553100586
Iteration 2881:
Training Loss: -9.28915786743164
Reconstruction Loss: -13.257638931274414
Iteration 2901:
Training Loss: -9.067266464233398
Reconstruction Loss: -13.252129554748535
Iteration 2921:
Training Loss: -9.00475025177002
Reconstruction Loss: -13.246493339538574
Iteration 2941:
Training Loss: -9.179471015930176
Reconstruction Loss: -13.25456714630127
Iteration 2961:
Training Loss: -9.177719116210938
Reconstruction Loss: -13.259357452392578
Iteration 2981:
Training Loss: -9.35548210144043
Reconstruction Loss: -13.25503921508789
