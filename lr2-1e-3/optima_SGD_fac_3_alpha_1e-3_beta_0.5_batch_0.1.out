5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 4.815575122833252
Reconstruction Loss: -0.44674384593963623
Iteration 11:
Training Loss: 4.772891998291016
Reconstruction Loss: -0.795199453830719
Iteration 21:
Training Loss: 3.5489134788513184
Reconstruction Loss: -1.3941186666488647
Iteration 31:
Training Loss: 2.5149614810943604
Reconstruction Loss: -1.8857120275497437
Iteration 41:
Training Loss: 1.5488710403442383
Reconstruction Loss: -2.4302141666412354
Iteration 51:
Training Loss: 1.0415124893188477
Reconstruction Loss: -2.9533092975616455
Iteration 61:
Training Loss: 0.1704363077878952
Reconstruction Loss: -3.3923659324645996
Iteration 71:
Training Loss: -0.20708316564559937
Reconstruction Loss: -3.7202646732330322
Iteration 81:
Training Loss: -0.9091588258743286
Reconstruction Loss: -3.969876766204834
Iteration 91:
Training Loss: -0.8818361759185791
Reconstruction Loss: -4.169070243835449
Iteration 101:
Training Loss: -0.6576127409934998
Reconstruction Loss: -4.33510684967041
Iteration 111:
Training Loss: -1.3474875688552856
Reconstruction Loss: -4.473792552947998
Iteration 121:
Training Loss: -1.148293375968933
Reconstruction Loss: -4.591245174407959
Iteration 131:
Training Loss: -1.4482629299163818
Reconstruction Loss: -4.701554298400879
Iteration 141:
Training Loss: -1.438830018043518
Reconstruction Loss: -4.795151710510254
Iteration 151:
Training Loss: -1.6456729173660278
Reconstruction Loss: -4.876514911651611
Iteration 161:
Training Loss: -1.9198315143585205
Reconstruction Loss: -4.950108528137207
Iteration 171:
Training Loss: -1.5905883312225342
Reconstruction Loss: -5.024691581726074
Iteration 181:
Training Loss: -2.1480071544647217
Reconstruction Loss: -5.091372489929199
Iteration 191:
Training Loss: -1.9406092166900635
Reconstruction Loss: -5.1436076164245605
Iteration 201:
Training Loss: -1.9477784633636475
Reconstruction Loss: -5.206879138946533
Iteration 211:
Training Loss: -2.321028232574463
Reconstruction Loss: -5.258443355560303
Iteration 221:
Training Loss: -1.9945523738861084
Reconstruction Loss: -5.303385257720947
Iteration 231:
Training Loss: -2.3052752017974854
Reconstruction Loss: -5.345633029937744
Iteration 241:
Training Loss: -2.805138349533081
Reconstruction Loss: -5.391773700714111
Iteration 251:
Training Loss: -2.4131343364715576
Reconstruction Loss: -5.433037757873535
Iteration 261:
Training Loss: -2.6745376586914062
Reconstruction Loss: -5.4755635261535645
Iteration 271:
Training Loss: -2.2777328491210938
Reconstruction Loss: -5.508720874786377
Iteration 281:
Training Loss: -2.7164721488952637
Reconstruction Loss: -5.5411200523376465
Iteration 291:
Training Loss: -2.8084259033203125
Reconstruction Loss: -5.577148914337158
Iteration 301:
Training Loss: -2.6085784435272217
Reconstruction Loss: -5.609560966491699
Iteration 311:
Training Loss: -2.6558997631073
Reconstruction Loss: -5.639004707336426
Iteration 321:
Training Loss: -2.3994133472442627
Reconstruction Loss: -5.667505741119385
Iteration 331:
Training Loss: -2.6603434085845947
Reconstruction Loss: -5.697268962860107
Iteration 341:
Training Loss: -3.54103946685791
Reconstruction Loss: -5.724770545959473
Iteration 351:
Training Loss: -3.253645181655884
Reconstruction Loss: -5.753143310546875
Iteration 361:
Training Loss: -3.171478271484375
Reconstruction Loss: -5.7782158851623535
Iteration 371:
Training Loss: -3.34173321723938
Reconstruction Loss: -5.801730155944824
Iteration 381:
Training Loss: -3.5702474117279053
Reconstruction Loss: -5.8245849609375
Iteration 391:
Training Loss: -2.879150629043579
Reconstruction Loss: -5.846599102020264
Iteration 401:
Training Loss: -3.398224115371704
Reconstruction Loss: -5.875300884246826
Iteration 411:
Training Loss: -3.58406662940979
Reconstruction Loss: -5.896180152893066
Iteration 421:
Training Loss: -3.2054898738861084
Reconstruction Loss: -5.91628360748291
Iteration 431:
Training Loss: -3.528057813644409
Reconstruction Loss: -5.938517093658447
Iteration 441:
Training Loss: -3.012204885482788
Reconstruction Loss: -5.957458019256592
Iteration 451:
Training Loss: -3.754401206970215
Reconstruction Loss: -5.977360248565674
Iteration 461:
Training Loss: -3.463182210922241
Reconstruction Loss: -5.996451377868652
Iteration 471:
Training Loss: -3.701796054840088
Reconstruction Loss: -6.014437675476074
Iteration 481:
Training Loss: -4.059261322021484
Reconstruction Loss: -6.034336566925049
Iteration 491:
Training Loss: -3.9725422859191895
Reconstruction Loss: -6.051615238189697
Iteration 501:
Training Loss: -3.452437162399292
Reconstruction Loss: -6.0679097175598145
Iteration 511:
Training Loss: -3.5070676803588867
Reconstruction Loss: -6.090242385864258
Iteration 521:
Training Loss: -3.6536638736724854
Reconstruction Loss: -6.099958419799805
Iteration 531:
Training Loss: -4.007833957672119
Reconstruction Loss: -6.120055675506592
Iteration 541:
Training Loss: -3.768941640853882
Reconstruction Loss: -6.13386344909668
Iteration 551:
Training Loss: -3.322038412094116
Reconstruction Loss: -6.147592544555664
Iteration 561:
Training Loss: -3.781259775161743
Reconstruction Loss: -6.166349411010742
Iteration 571:
Training Loss: -3.603205919265747
Reconstruction Loss: -6.178205966949463
Iteration 581:
Training Loss: -4.279290676116943
Reconstruction Loss: -6.1955342292785645
Iteration 591:
Training Loss: -3.9969773292541504
Reconstruction Loss: -6.213067054748535
Iteration 601:
Training Loss: -3.7282931804656982
Reconstruction Loss: -6.224540710449219
Iteration 611:
Training Loss: -4.205191135406494
Reconstruction Loss: -6.235808372497559
Iteration 621:
Training Loss: -3.9443423748016357
Reconstruction Loss: -6.249922752380371
Iteration 631:
Training Loss: -4.024570465087891
Reconstruction Loss: -6.262588024139404
Iteration 641:
Training Loss: -3.6227264404296875
Reconstruction Loss: -6.278718948364258
Iteration 651:
Training Loss: -4.0125732421875
Reconstruction Loss: -6.290970325469971
Iteration 661:
Training Loss: -4.12843656539917
Reconstruction Loss: -6.302767753601074
Iteration 671:
Training Loss: -4.354770183563232
Reconstruction Loss: -6.318378448486328
Iteration 681:
Training Loss: -3.977959632873535
Reconstruction Loss: -6.324304580688477
Iteration 691:
Training Loss: -3.8610544204711914
Reconstruction Loss: -6.335320472717285
Iteration 701:
Training Loss: -4.187252521514893
Reconstruction Loss: -6.348860740661621
Iteration 711:
Training Loss: -4.0257768630981445
Reconstruction Loss: -6.361047267913818
Iteration 721:
Training Loss: -4.2616777420043945
Reconstruction Loss: -6.373647689819336
Iteration 731:
Training Loss: -4.661556243896484
Reconstruction Loss: -6.3839335441589355
Iteration 741:
Training Loss: -4.222832679748535
Reconstruction Loss: -6.394330978393555
Iteration 751:
Training Loss: -3.9612948894500732
Reconstruction Loss: -6.404329299926758
Iteration 761:
Training Loss: -4.216670513153076
Reconstruction Loss: -6.417351245880127
Iteration 771:
Training Loss: -3.9742279052734375
Reconstruction Loss: -6.426881313323975
Iteration 781:
Training Loss: -4.179934978485107
Reconstruction Loss: -6.436819553375244
Iteration 791:
Training Loss: -4.513230323791504
Reconstruction Loss: -6.4482526779174805
Iteration 801:
Training Loss: -4.501507759094238
Reconstruction Loss: -6.45682430267334
Iteration 811:
Training Loss: -4.165677547454834
Reconstruction Loss: -6.466146945953369
Iteration 821:
Training Loss: -4.078530311584473
Reconstruction Loss: -6.475015640258789
Iteration 831:
Training Loss: -4.261355876922607
Reconstruction Loss: -6.486613750457764
Iteration 841:
Training Loss: -4.459840297698975
Reconstruction Loss: -6.496535301208496
Iteration 851:
Training Loss: -4.434756278991699
Reconstruction Loss: -6.505147933959961
Iteration 861:
Training Loss: -4.600561618804932
Reconstruction Loss: -6.512374401092529
Iteration 871:
Training Loss: -4.652681350708008
Reconstruction Loss: -6.522943496704102
Iteration 881:
Training Loss: -4.335659027099609
Reconstruction Loss: -6.532461643218994
Iteration 891:
Training Loss: -4.371473789215088
Reconstruction Loss: -6.544306755065918
Iteration 901:
Training Loss: -4.352628231048584
Reconstruction Loss: -6.547673225402832
Iteration 911:
Training Loss: -4.194519519805908
Reconstruction Loss: -6.557684421539307
Iteration 921:
Training Loss: -5.142910003662109
Reconstruction Loss: -6.563790321350098
Iteration 931:
Training Loss: -4.3519368171691895
Reconstruction Loss: -6.572145938873291
Iteration 941:
Training Loss: -5.0398383140563965
Reconstruction Loss: -6.584184646606445
Iteration 951:
Training Loss: -4.573153972625732
Reconstruction Loss: -6.592065811157227
Iteration 961:
Training Loss: -4.500205039978027
Reconstruction Loss: -6.600611209869385
Iteration 971:
Training Loss: -4.672068119049072
Reconstruction Loss: -6.607950687408447
Iteration 981:
Training Loss: -4.784834384918213
Reconstruction Loss: -6.6132707595825195
Iteration 991:
Training Loss: -4.62797212600708
Reconstruction Loss: -6.621040344238281
Iteration 1001:
Training Loss: -4.706284046173096
Reconstruction Loss: -6.627999305725098
Iteration 1011:
Training Loss: -4.746061325073242
Reconstruction Loss: -6.6351494789123535
Iteration 1021:
Training Loss: -4.330698013305664
Reconstruction Loss: -6.642980575561523
Iteration 1031:
Training Loss: -5.498232364654541
Reconstruction Loss: -6.650365352630615
Iteration 1041:
Training Loss: -4.745965480804443
Reconstruction Loss: -6.658843994140625
Iteration 1051:
Training Loss: -4.952814102172852
Reconstruction Loss: -6.665133476257324
Iteration 1061:
Training Loss: -5.02341890335083
Reconstruction Loss: -6.673524856567383
Iteration 1071:
Training Loss: -4.937893867492676
Reconstruction Loss: -6.677978515625
Iteration 1081:
Training Loss: -5.069796562194824
Reconstruction Loss: -6.683879852294922
Iteration 1091:
Training Loss: -4.8160929679870605
Reconstruction Loss: -6.689914226531982
Iteration 1101:
Training Loss: -5.12167501449585
Reconstruction Loss: -6.700253009796143
Iteration 1111:
Training Loss: -5.011257648468018
Reconstruction Loss: -6.7053327560424805
Iteration 1121:
Training Loss: -4.467979431152344
Reconstruction Loss: -6.711664199829102
Iteration 1131:
Training Loss: -4.772163391113281
Reconstruction Loss: -6.718369483947754
Iteration 1141:
Training Loss: -4.849712371826172
Reconstruction Loss: -6.723384380340576
Iteration 1151:
Training Loss: -5.231189250946045
Reconstruction Loss: -6.730876922607422
Iteration 1161:
Training Loss: -5.098581790924072
Reconstruction Loss: -6.737705707550049
Iteration 1171:
Training Loss: -4.808994770050049
Reconstruction Loss: -6.744399070739746
Iteration 1181:
Training Loss: -4.7558441162109375
Reconstruction Loss: -6.746735095977783
Iteration 1191:
Training Loss: -5.445849895477295
Reconstruction Loss: -6.755023002624512
Iteration 1201:
Training Loss: -5.640715599060059
Reconstruction Loss: -6.76185417175293
Iteration 1211:
Training Loss: -5.296982288360596
Reconstruction Loss: -6.766829490661621
Iteration 1221:
Training Loss: -4.7348175048828125
Reconstruction Loss: -6.770758628845215
Iteration 1231:
Training Loss: -4.496990203857422
Reconstruction Loss: -6.776183605194092
Iteration 1241:
Training Loss: -5.140467643737793
Reconstruction Loss: -6.785223007202148
Iteration 1251:
Training Loss: -4.914790153503418
Reconstruction Loss: -6.787809371948242
Iteration 1261:
Training Loss: -5.349261283874512
Reconstruction Loss: -6.7957987785339355
Iteration 1271:
Training Loss: -5.45216178894043
Reconstruction Loss: -6.802375793457031
Iteration 1281:
Training Loss: -4.520708084106445
Reconstruction Loss: -6.806236743927002
Iteration 1291:
Training Loss: -4.996570587158203
Reconstruction Loss: -6.812229633331299
Iteration 1301:
Training Loss: -4.937686443328857
Reconstruction Loss: -6.817062854766846
Iteration 1311:
Training Loss: -5.12456750869751
Reconstruction Loss: -6.820088863372803
Iteration 1321:
Training Loss: -5.155820846557617
Reconstruction Loss: -6.82573127746582
Iteration 1331:
Training Loss: -5.162065029144287
Reconstruction Loss: -6.833349227905273
Iteration 1341:
Training Loss: -4.603645324707031
Reconstruction Loss: -6.838000297546387
Iteration 1351:
Training Loss: -5.309621810913086
Reconstruction Loss: -6.841777324676514
Iteration 1361:
Training Loss: -5.001466751098633
Reconstruction Loss: -6.847668647766113
Iteration 1371:
Training Loss: -5.5222296714782715
Reconstruction Loss: -6.854193210601807
Iteration 1381:
Training Loss: -5.056675434112549
Reconstruction Loss: -6.857817649841309
Iteration 1391:
Training Loss: -5.291571140289307
Reconstruction Loss: -6.86344575881958
Iteration 1401:
Training Loss: -5.2534990310668945
Reconstruction Loss: -6.866719722747803
Iteration 1411:
Training Loss: -4.860091686248779
Reconstruction Loss: -6.871605396270752
Iteration 1421:
Training Loss: -5.3982391357421875
Reconstruction Loss: -6.876352787017822
Iteration 1431:
Training Loss: -5.077272891998291
Reconstruction Loss: -6.880645751953125
Iteration 1441:
Training Loss: -5.725849151611328
Reconstruction Loss: -6.8855462074279785
Iteration 1451:
Training Loss: -5.955122470855713
Reconstruction Loss: -6.888705253601074
Iteration 1461:
Training Loss: -5.3408331871032715
Reconstruction Loss: -6.892658710479736
Iteration 1471:
Training Loss: -5.234054088592529
Reconstruction Loss: -6.899813175201416
Iteration 1481:
Training Loss: -5.103148460388184
Reconstruction Loss: -6.903875827789307
Iteration 1491:
Training Loss: -6.212423801422119
Reconstruction Loss: -6.907661437988281
