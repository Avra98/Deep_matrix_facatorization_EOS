5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.343181610107422
Reconstruction Loss: -0.560074508190155
Iteration 21:
Training Loss: 3.565363883972168
Reconstruction Loss: -1.3203625679016113
Iteration 41:
Training Loss: 2.5508604049682617
Reconstruction Loss: -1.9119114875793457
Iteration 61:
Training Loss: 1.243490219116211
Reconstruction Loss: -2.5572571754455566
Iteration 81:
Training Loss: 0.3137863278388977
Reconstruction Loss: -3.1584739685058594
Iteration 101:
Training Loss: -0.06020868942141533
Reconstruction Loss: -3.6686105728149414
Iteration 121:
Training Loss: -0.39807116985321045
Reconstruction Loss: -4.055188179016113
Iteration 141:
Training Loss: -0.9697651863098145
Reconstruction Loss: -4.358956336975098
Iteration 161:
Training Loss: -1.208164095878601
Reconstruction Loss: -4.594971179962158
Iteration 181:
Training Loss: -1.7282872200012207
Reconstruction Loss: -4.782577991485596
Iteration 201:
Training Loss: -1.64646315574646
Reconstruction Loss: -4.936307430267334
Iteration 221:
Training Loss: -1.7133129835128784
Reconstruction Loss: -5.072154521942139
Iteration 241:
Training Loss: -2.055525541305542
Reconstruction Loss: -5.192288398742676
Iteration 261:
Training Loss: -1.9794857501983643
Reconstruction Loss: -5.295234680175781
Iteration 281:
Training Loss: -2.4278125762939453
Reconstruction Loss: -5.391970157623291
Iteration 301:
Training Loss: -2.4370806217193604
Reconstruction Loss: -5.476511001586914
Iteration 321:
Training Loss: -2.3532016277313232
Reconstruction Loss: -5.547027111053467
Iteration 341:
Training Loss: -2.965482711791992
Reconstruction Loss: -5.617478847503662
Iteration 361:
Training Loss: -2.772599697113037
Reconstruction Loss: -5.687318325042725
Iteration 381:
Training Loss: -2.7388203144073486
Reconstruction Loss: -5.749065399169922
Iteration 401:
Training Loss: -2.914626121520996
Reconstruction Loss: -5.798139572143555
Iteration 421:
Training Loss: -2.8938040733337402
Reconstruction Loss: -5.845475196838379
Iteration 441:
Training Loss: -3.300663709640503
Reconstruction Loss: -5.89517068862915
Iteration 461:
Training Loss: -3.039860486984253
Reconstruction Loss: -5.940566062927246
Iteration 481:
Training Loss: -3.2092630863189697
Reconstruction Loss: -5.981225967407227
Iteration 501:
Training Loss: -3.212404251098633
Reconstruction Loss: -6.019065856933594
Iteration 521:
Training Loss: -3.426164388656616
Reconstruction Loss: -6.058194637298584
Iteration 541:
Training Loss: -3.3922975063323975
Reconstruction Loss: -6.093474864959717
Iteration 561:
Training Loss: -3.4291164875030518
Reconstruction Loss: -6.125917911529541
Iteration 581:
Training Loss: -3.520585775375366
Reconstruction Loss: -6.156335353851318
Iteration 601:
Training Loss: -3.9075841903686523
Reconstruction Loss: -6.180049419403076
Iteration 621:
Training Loss: -3.547245502471924
Reconstruction Loss: -6.214867115020752
Iteration 641:
Training Loss: -3.5352399349212646
Reconstruction Loss: -6.238758563995361
Iteration 661:
Training Loss: -3.627950429916382
Reconstruction Loss: -6.263664722442627
Iteration 681:
Training Loss: -3.829955816268921
Reconstruction Loss: -6.2925028800964355
Iteration 701:
Training Loss: -3.6073498725891113
Reconstruction Loss: -6.31799840927124
Iteration 721:
Training Loss: -3.620791435241699
Reconstruction Loss: -6.338581562042236
Iteration 741:
Training Loss: -4.00274658203125
Reconstruction Loss: -6.35736608505249
Iteration 761:
Training Loss: -4.108063220977783
Reconstruction Loss: -6.381101131439209
Iteration 781:
Training Loss: -3.9116408824920654
Reconstruction Loss: -6.402798652648926
Iteration 801:
Training Loss: -3.917302370071411
Reconstruction Loss: -6.418586254119873
Iteration 821:
Training Loss: -3.9314346313476562
Reconstruction Loss: -6.437370777130127
Iteration 841:
Training Loss: -4.17141580581665
Reconstruction Loss: -6.458207130432129
Iteration 861:
Training Loss: -4.1510844230651855
Reconstruction Loss: -6.476293563842773
Iteration 881:
Training Loss: -3.9869444370269775
Reconstruction Loss: -6.493317604064941
Iteration 901:
Training Loss: -3.953566789627075
Reconstruction Loss: -6.5085906982421875
Iteration 921:
Training Loss: -4.144510746002197
Reconstruction Loss: -6.523417949676514
Iteration 941:
Training Loss: -4.184440612792969
Reconstruction Loss: -6.541437149047852
Iteration 961:
Training Loss: -3.9883785247802734
Reconstruction Loss: -6.557964324951172
Iteration 981:
Training Loss: -4.456568241119385
Reconstruction Loss: -6.569355010986328
Iteration 1001:
Training Loss: -3.9405412673950195
Reconstruction Loss: -6.5815935134887695
Iteration 1021:
Training Loss: -4.452732563018799
Reconstruction Loss: -6.596678256988525
Iteration 1041:
Training Loss: -4.132225513458252
Reconstruction Loss: -6.6109938621521
Iteration 1061:
Training Loss: -4.359663486480713
Reconstruction Loss: -6.62178373336792
Iteration 1081:
Training Loss: -4.551490306854248
Reconstruction Loss: -6.636292934417725
Iteration 1101:
Training Loss: -4.4148268699646
Reconstruction Loss: -6.647282123565674
Iteration 1121:
Training Loss: -4.458914756774902
Reconstruction Loss: -6.663671970367432
Iteration 1141:
Training Loss: -4.50244665145874
Reconstruction Loss: -6.672460556030273
Iteration 1161:
Training Loss: -4.1911444664001465
Reconstruction Loss: -6.684650897979736
Iteration 1181:
Training Loss: -4.449688911437988
Reconstruction Loss: -6.695824146270752
Iteration 1201:
Training Loss: -4.776745796203613
Reconstruction Loss: -6.706655502319336
Iteration 1221:
Training Loss: -4.460257053375244
Reconstruction Loss: -6.7167439460754395
Iteration 1241:
Training Loss: -4.43675422668457
Reconstruction Loss: -6.728312015533447
Iteration 1261:
Training Loss: -4.263507843017578
Reconstruction Loss: -6.738585948944092
Iteration 1281:
Training Loss: -4.310054779052734
Reconstruction Loss: -6.74809455871582
Iteration 1301:
Training Loss: -4.920611381530762
Reconstruction Loss: -6.757236957550049
Iteration 1321:
Training Loss: -4.732774257659912
Reconstruction Loss: -6.764997959136963
Iteration 1341:
Training Loss: -5.068296432495117
Reconstruction Loss: -6.776740074157715
Iteration 1361:
Training Loss: -5.00620174407959
Reconstruction Loss: -6.785900115966797
Iteration 1381:
Training Loss: -4.894561290740967
Reconstruction Loss: -6.794016361236572
Iteration 1401:
Training Loss: -4.951537609100342
Reconstruction Loss: -6.804123878479004
Iteration 1421:
Training Loss: -4.898820400238037
Reconstruction Loss: -6.813478946685791
Iteration 1441:
Training Loss: -4.884251117706299
Reconstruction Loss: -6.820417404174805
Iteration 1461:
Training Loss: -4.900119781494141
Reconstruction Loss: -6.826772689819336
Iteration 1481:
Training Loss: -5.0112810134887695
Reconstruction Loss: -6.836193561553955
Iteration 1501:
Training Loss: -4.811212062835693
Reconstruction Loss: -6.846683979034424
Iteration 1521:
Training Loss: -4.968067169189453
Reconstruction Loss: -6.85538911819458
Iteration 1541:
Training Loss: -4.798994064331055
Reconstruction Loss: -6.862166881561279
Iteration 1561:
Training Loss: -4.908742427825928
Reconstruction Loss: -6.869300842285156
Iteration 1581:
Training Loss: -5.1872663497924805
Reconstruction Loss: -6.875980377197266
Iteration 1601:
Training Loss: -4.790892124176025
Reconstruction Loss: -6.883404731750488
Iteration 1621:
Training Loss: -4.790063381195068
Reconstruction Loss: -6.890252113342285
Iteration 1641:
Training Loss: -5.032825469970703
Reconstruction Loss: -6.897547721862793
Iteration 1661:
Training Loss: -4.8300700187683105
Reconstruction Loss: -6.904198169708252
Iteration 1681:
Training Loss: -4.938675880432129
Reconstruction Loss: -6.909921646118164
Iteration 1701:
Training Loss: -4.962679862976074
Reconstruction Loss: -6.916869163513184
Iteration 1721:
Training Loss: -4.7568583488464355
Reconstruction Loss: -6.925073623657227
Iteration 1741:
Training Loss: -5.261857032775879
Reconstruction Loss: -6.931725978851318
Iteration 1761:
Training Loss: -4.8847150802612305
Reconstruction Loss: -6.937512397766113
Iteration 1781:
Training Loss: -5.6603240966796875
Reconstruction Loss: -6.941650867462158
Iteration 1801:
Training Loss: -5.235906600952148
Reconstruction Loss: -6.952929496765137
Iteration 1821:
Training Loss: -5.146820545196533
Reconstruction Loss: -6.956852436065674
Iteration 1841:
Training Loss: -5.37420129776001
Reconstruction Loss: -6.96285343170166
Iteration 1861:
Training Loss: -5.184221267700195
Reconstruction Loss: -6.9684739112854
Iteration 1881:
Training Loss: -5.306020259857178
Reconstruction Loss: -6.974146842956543
Iteration 1901:
Training Loss: -5.14137601852417
Reconstruction Loss: -6.979917526245117
Iteration 1921:
Training Loss: -5.191740989685059
Reconstruction Loss: -6.983742713928223
Iteration 1941:
Training Loss: -5.427349090576172
Reconstruction Loss: -6.9905686378479
Iteration 1961:
Training Loss: -5.475570201873779
Reconstruction Loss: -6.996311187744141
Iteration 1981:
Training Loss: -5.213221073150635
Reconstruction Loss: -7.001495361328125
Iteration 2001:
Training Loss: -5.3691325187683105
Reconstruction Loss: -7.007032871246338
Iteration 2021:
Training Loss: -5.4094557762146
Reconstruction Loss: -7.011285305023193
Iteration 2041:
Training Loss: -5.312770366668701
Reconstruction Loss: -7.016915798187256
Iteration 2061:
Training Loss: -5.289372444152832
Reconstruction Loss: -7.022478103637695
Iteration 2081:
Training Loss: -5.272496700286865
Reconstruction Loss: -7.025790214538574
Iteration 2101:
Training Loss: -5.406524658203125
Reconstruction Loss: -7.031283378601074
Iteration 2121:
Training Loss: -5.206341743469238
Reconstruction Loss: -7.037170886993408
Iteration 2141:
Training Loss: -5.676719665527344
Reconstruction Loss: -7.040039539337158
Iteration 2161:
Training Loss: -5.548342704772949
Reconstruction Loss: -7.047212600708008
Iteration 2181:
Training Loss: -5.316084861755371
Reconstruction Loss: -7.050379753112793
Iteration 2201:
Training Loss: -5.49030065536499
Reconstruction Loss: -7.056070327758789
Iteration 2221:
Training Loss: -5.160828590393066
Reconstruction Loss: -7.061242580413818
Iteration 2241:
Training Loss: -5.672988414764404
Reconstruction Loss: -7.062436580657959
Iteration 2261:
Training Loss: -5.232966423034668
Reconstruction Loss: -7.06710147857666
Iteration 2281:
Training Loss: -5.732419967651367
Reconstruction Loss: -7.0711541175842285
Iteration 2301:
Training Loss: -5.548677444458008
Reconstruction Loss: -7.077622413635254
Iteration 2321:
Training Loss: -5.3888044357299805
Reconstruction Loss: -7.079890727996826
Iteration 2341:
Training Loss: -5.75431489944458
Reconstruction Loss: -7.084262847900391
Iteration 2361:
Training Loss: -5.376342296600342
Reconstruction Loss: -7.08972692489624
Iteration 2381:
Training Loss: -5.542196273803711
Reconstruction Loss: -7.093545436859131
Iteration 2401:
Training Loss: -5.468855381011963
Reconstruction Loss: -7.096120357513428
Iteration 2421:
Training Loss: -5.2191853523254395
Reconstruction Loss: -7.099441051483154
Iteration 2441:
Training Loss: -5.37684965133667
Reconstruction Loss: -7.104814052581787
Iteration 2461:
Training Loss: -5.6481428146362305
Reconstruction Loss: -7.107736110687256
Iteration 2481:
Training Loss: -5.52707576751709
Reconstruction Loss: -7.11118745803833
Iteration 2501:
Training Loss: -5.802453994750977
Reconstruction Loss: -7.1164774894714355
Iteration 2521:
Training Loss: -5.784822463989258
Reconstruction Loss: -7.120126247406006
Iteration 2541:
Training Loss: -5.795464038848877
Reconstruction Loss: -7.122288703918457
Iteration 2561:
Training Loss: -5.656156063079834
Reconstruction Loss: -7.126992225646973
Iteration 2581:
Training Loss: -5.7249650955200195
Reconstruction Loss: -7.132289886474609
Iteration 2601:
Training Loss: -5.871287822723389
Reconstruction Loss: -7.134093761444092
Iteration 2621:
Training Loss: -5.416317939758301
Reconstruction Loss: -7.137054920196533
Iteration 2641:
Training Loss: -5.59998083114624
Reconstruction Loss: -7.140919208526611
Iteration 2661:
Training Loss: -5.8785600662231445
Reconstruction Loss: -7.145042419433594
Iteration 2681:
Training Loss: -5.605778217315674
Reconstruction Loss: -7.147262096405029
Iteration 2701:
Training Loss: -5.925477027893066
Reconstruction Loss: -7.150583267211914
Iteration 2721:
Training Loss: -5.875544548034668
Reconstruction Loss: -7.153595447540283
Iteration 2741:
Training Loss: -5.527734279632568
Reconstruction Loss: -7.156556606292725
Iteration 2761:
Training Loss: -5.869538307189941
Reconstruction Loss: -7.1613359451293945
Iteration 2781:
Training Loss: -5.776401042938232
Reconstruction Loss: -7.163748741149902
Iteration 2801:
Training Loss: -5.702582836151123
Reconstruction Loss: -7.166777610778809
Iteration 2821:
Training Loss: -5.707395076751709
Reconstruction Loss: -7.169015884399414
Iteration 2841:
Training Loss: -5.789267063140869
Reconstruction Loss: -7.172351360321045
Iteration 2861:
Training Loss: -5.798345565795898
Reconstruction Loss: -7.176461696624756
Iteration 2881:
Training Loss: -5.80685567855835
Reconstruction Loss: -7.178675651550293
Iteration 2901:
Training Loss: -6.187727451324463
Reconstruction Loss: -7.181221961975098
Iteration 2921:
Training Loss: -6.0652079582214355
Reconstruction Loss: -7.184561252593994
Iteration 2941:
Training Loss: -5.918388366699219
Reconstruction Loss: -7.18803071975708
Iteration 2961:
Training Loss: -5.6855573654174805
Reconstruction Loss: -7.1929192543029785
Iteration 2981:
Training Loss: -6.046560287475586
Reconstruction Loss: -7.193674087524414
