5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.537933349609375
Reconstruction Loss: -0.415642112493515
Iteration 101:
Training Loss: 4.592720985412598
Reconstruction Loss: -0.6896525025367737
Iteration 201:
Training Loss: 3.5592339038848877
Reconstruction Loss: -1.0839811563491821
Iteration 301:
Training Loss: 2.4938793182373047
Reconstruction Loss: -1.5445407629013062
Iteration 401:
Training Loss: 1.680542230606079
Reconstruction Loss: -1.9643365144729614
Iteration 501:
Training Loss: 1.057802438735962
Reconstruction Loss: -2.3413572311401367
Iteration 601:
Training Loss: 0.5367234349250793
Reconstruction Loss: -2.681471347808838
Iteration 701:
Training Loss: 0.07951132953166962
Reconstruction Loss: -2.9855687618255615
Iteration 801:
Training Loss: -0.31443315744400024
Reconstruction Loss: -3.251796245574951
Iteration 901:
Training Loss: -0.6478972434997559
Reconstruction Loss: -3.4819209575653076
Iteration 1001:
Training Loss: -0.9299613237380981
Reconstruction Loss: -3.6803455352783203
Iteration 1101:
Training Loss: -1.171013593673706
Reconstruction Loss: -3.8520801067352295
Iteration 1201:
Training Loss: -1.3799347877502441
Reconstruction Loss: -4.001712322235107
Iteration 1301:
Training Loss: -1.5635583400726318
Reconstruction Loss: -4.133105754852295
Iteration 1401:
Training Loss: -1.7269797325134277
Reconstruction Loss: -4.249411582946777
Iteration 1501:
Training Loss: -1.8739923238754272
Reconstruction Loss: -4.353163719177246
Iteration 1601:
Training Loss: -2.007451295852661
Reconstruction Loss: -4.446398735046387
Iteration 1701:
Training Loss: -2.129547357559204
Reconstruction Loss: -4.5307536125183105
Iteration 1801:
Training Loss: -2.2419798374176025
Reconstruction Loss: -4.607552528381348
Iteration 1901:
Training Loss: -2.3461050987243652
Reconstruction Loss: -4.677872180938721
Iteration 2001:
Training Loss: -2.443009614944458
Reconstruction Loss: -4.742595672607422
Iteration 2101:
Training Loss: -2.5335872173309326
Reconstruction Loss: -4.802450656890869
Iteration 2201:
Training Loss: -2.6185719966888428
Reconstruction Loss: -4.858041763305664
Iteration 2301:
Training Loss: -2.698579788208008
Reconstruction Loss: -4.909875869750977
Iteration 2401:
Training Loss: -2.7741360664367676
Reconstruction Loss: -4.958381652832031
Iteration 2501:
Training Loss: -2.845679521560669
Reconstruction Loss: -5.003920555114746
Iteration 2601:
Training Loss: -2.913599967956543
Reconstruction Loss: -5.046804428100586
Iteration 2701:
Training Loss: -2.9782259464263916
Reconstruction Loss: -5.087300777435303
Iteration 2801:
Training Loss: -3.0398483276367188
Reconstruction Loss: -5.125639915466309
Iteration 2901:
Training Loss: -3.098721504211426
Reconstruction Loss: -5.162022590637207
Iteration 3001:
Training Loss: -3.1550703048706055
Reconstruction Loss: -5.196624755859375
Iteration 3101:
Training Loss: -3.209099769592285
Reconstruction Loss: -5.229600429534912
Iteration 3201:
Training Loss: -3.2609808444976807
Reconstruction Loss: -5.26108455657959
Iteration 3301:
Training Loss: -3.3108794689178467
Reconstruction Loss: -5.291199207305908
Iteration 3401:
Training Loss: -3.3589372634887695
Reconstruction Loss: -5.320052623748779
Iteration 3501:
Training Loss: -3.4052839279174805
Reconstruction Loss: -5.347735404968262
Iteration 3601:
Training Loss: -3.450035333633423
Reconstruction Loss: -5.374338626861572
Iteration 3701:
Training Loss: -3.4933018684387207
Reconstruction Loss: -5.399934768676758
Iteration 3801:
Training Loss: -3.53517746925354
Reconstruction Loss: -5.424595832824707
Iteration 3901:
Training Loss: -3.575749158859253
Reconstruction Loss: -5.448385238647461
Iteration 4001:
Training Loss: -3.615100383758545
Reconstruction Loss: -5.471355438232422
Iteration 4101:
Training Loss: -3.653305768966675
Reconstruction Loss: -5.493560791015625
Iteration 4201:
Training Loss: -3.6904265880584717
Reconstruction Loss: -5.515047550201416
Iteration 4301:
Training Loss: -3.726532459259033
Reconstruction Loss: -5.53585958480835
Iteration 4401:
Training Loss: -3.76167368888855
Reconstruction Loss: -5.556033611297607
Iteration 4501:
Training Loss: -3.795905590057373
Reconstruction Loss: -5.575610160827637
Iteration 4601:
Training Loss: -3.829275608062744
Reconstruction Loss: -5.594616889953613
Iteration 4701:
Training Loss: -3.8618357181549072
Reconstruction Loss: -5.613086223602295
Iteration 4801:
Training Loss: -3.8936166763305664
Reconstruction Loss: -5.631046295166016
Iteration 4901:
Training Loss: -3.9246609210968018
Reconstruction Loss: -5.648523330688477
Iteration 5001:
Training Loss: -3.9550085067749023
Reconstruction Loss: -5.665544033050537
Iteration 5101:
Training Loss: -3.9846866130828857
Reconstruction Loss: -5.682128429412842
Iteration 5201:
Training Loss: -4.013728141784668
Reconstruction Loss: -5.698296070098877
Iteration 5301:
Training Loss: -4.042158126831055
Reconstruction Loss: -5.714069843292236
Iteration 5401:
Training Loss: -4.070016384124756
Reconstruction Loss: -5.729465961456299
Iteration 5501:
Training Loss: -4.0973124504089355
Reconstruction Loss: -5.744497776031494
Iteration 5601:
Training Loss: -4.124079704284668
Reconstruction Loss: -5.759185314178467
Iteration 5701:
Training Loss: -4.15033483505249
Reconstruction Loss: -5.773543357849121
Iteration 5801:
Training Loss: -4.176103115081787
Reconstruction Loss: -5.787586212158203
Iteration 5901:
Training Loss: -4.201401233673096
Reconstruction Loss: -5.801324844360352
Iteration 6001:
Training Loss: -4.2262420654296875
Reconstruction Loss: -5.8147735595703125
Iteration 6101:
Training Loss: -4.250656604766846
Reconstruction Loss: -5.827940940856934
Iteration 6201:
Training Loss: -4.2746453285217285
Reconstruction Loss: -5.840840816497803
Iteration 6301:
Training Loss: -4.298232555389404
Reconstruction Loss: -5.853483200073242
Iteration 6401:
Training Loss: -4.321434497833252
Reconstruction Loss: -5.865875720977783
Iteration 6501:
Training Loss: -4.344260215759277
Reconstruction Loss: -5.878026008605957
Iteration 6601:
Training Loss: -4.366722583770752
Reconstruction Loss: -5.88994836807251
Iteration 6701:
Training Loss: -4.388833522796631
Reconstruction Loss: -5.901646137237549
Iteration 6801:
Training Loss: -4.4106059074401855
Reconstruction Loss: -5.913135051727295
Iteration 6901:
Training Loss: -4.432048797607422
Reconstruction Loss: -5.924410343170166
Iteration 7001:
Training Loss: -4.453178405761719
Reconstruction Loss: -5.935481548309326
Iteration 7101:
Training Loss: -4.474000930786133
Reconstruction Loss: -5.946365833282471
Iteration 7201:
Training Loss: -4.4945197105407715
Reconstruction Loss: -5.957060813903809
Iteration 7301:
Training Loss: -4.5147528648376465
Reconstruction Loss: -5.967573165893555
Iteration 7401:
Training Loss: -4.534701824188232
Reconstruction Loss: -5.977910995483398
Iteration 7501:
Training Loss: -4.554380893707275
Reconstruction Loss: -5.9880781173706055
Iteration 7601:
Training Loss: -4.573798656463623
Reconstruction Loss: -5.998081207275391
Iteration 7701:
Training Loss: -4.592952728271484
Reconstruction Loss: -6.0079216957092285
Iteration 7801:
Training Loss: -4.6118574142456055
Reconstruction Loss: -6.017606735229492
Iteration 7901:
Training Loss: -4.63052225112915
Reconstruction Loss: -6.027142524719238
Iteration 8001:
Training Loss: -4.64894962310791
Reconstruction Loss: -6.036528587341309
Iteration 8101:
Training Loss: -4.667144298553467
Reconstruction Loss: -6.045775890350342
Iteration 8201:
Training Loss: -4.685121536254883
Reconstruction Loss: -6.0548810958862305
Iteration 8301:
Training Loss: -4.70286750793457
Reconstruction Loss: -6.0638532638549805
Iteration 8401:
Training Loss: -4.720405578613281
Reconstruction Loss: -6.072691917419434
Iteration 8501:
Training Loss: -4.73773193359375
Reconstruction Loss: -6.081404209136963
Iteration 8601:
Training Loss: -4.754858016967773
Reconstruction Loss: -6.089999675750732
Iteration 8701:
Training Loss: -4.771791934967041
Reconstruction Loss: -6.098470211029053
Iteration 8801:
Training Loss: -4.788519382476807
Reconstruction Loss: -6.106818199157715
Iteration 8901:
Training Loss: -4.805063724517822
Reconstruction Loss: -6.115048885345459
Iteration 9001:
Training Loss: -4.821420192718506
Reconstruction Loss: -6.123167991638184
Iteration 9101:
Training Loss: -4.8376007080078125
Reconstruction Loss: -6.131186008453369
Iteration 9201:
Training Loss: -4.853608131408691
Reconstruction Loss: -6.139092445373535
Iteration 9301:
Training Loss: -4.86943244934082
Reconstruction Loss: -6.146900177001953
Iteration 9401:
Training Loss: -4.885092258453369
Reconstruction Loss: -6.154608249664307
Iteration 9501:
Training Loss: -4.900585174560547
Reconstruction Loss: -6.162215709686279
Iteration 9601:
Training Loss: -4.915919303894043
Reconstruction Loss: -6.169728755950928
Iteration 9701:
Training Loss: -4.931097030639648
Reconstruction Loss: -6.177144527435303
Iteration 9801:
Training Loss: -4.946115016937256
Reconstruction Loss: -6.184467792510986
Iteration 9901:
Training Loss: -4.9609832763671875
Reconstruction Loss: -6.191699028015137
Iteration 10001:
Training Loss: -4.975695610046387
Reconstruction Loss: -6.198841571807861
Iteration 10101:
Training Loss: -4.99027156829834
Reconstruction Loss: -6.205897808074951
Iteration 10201:
Training Loss: -5.004697799682617
Reconstruction Loss: -6.212869644165039
Iteration 10301:
Training Loss: -5.01899528503418
Reconstruction Loss: -6.219761848449707
Iteration 10401:
Training Loss: -5.033144950866699
Reconstruction Loss: -6.226574420928955
Iteration 10501:
Training Loss: -5.047164440155029
Reconstruction Loss: -6.233309745788574
Iteration 10601:
Training Loss: -5.061039447784424
Reconstruction Loss: -6.2399582862854
Iteration 10701:
Training Loss: -5.074796676635742
Reconstruction Loss: -6.246538162231445
Iteration 10801:
Training Loss: -5.088424205780029
Reconstruction Loss: -6.253046035766602
Iteration 10901:
Training Loss: -5.101922035217285
Reconstruction Loss: -6.259472370147705
Iteration 11001:
Training Loss: -5.115296363830566
Reconstruction Loss: -6.2658233642578125
Iteration 11101:
Training Loss: -5.128551006317139
Reconstruction Loss: -6.272112846374512
Iteration 11201:
Training Loss: -5.141690731048584
Reconstruction Loss: -6.278335094451904
Iteration 11301:
Training Loss: -5.154709339141846
Reconstruction Loss: -6.284488677978516
Iteration 11401:
Training Loss: -5.1676225662231445
Reconstruction Loss: -6.290573596954346
Iteration 11501:
Training Loss: -5.180413246154785
Reconstruction Loss: -6.296596527099609
Iteration 11601:
Training Loss: -5.193097114562988
Reconstruction Loss: -6.302550315856934
Iteration 11701:
Training Loss: -5.205669403076172
Reconstruction Loss: -6.308441638946533
Iteration 11801:
Training Loss: -5.218139171600342
Reconstruction Loss: -6.314277172088623
Iteration 11901:
Training Loss: -5.230508327484131
Reconstruction Loss: -6.320051193237305
Iteration 12001:
Training Loss: -5.242757797241211
Reconstruction Loss: -6.325766086578369
Iteration 12101:
Training Loss: -5.254917144775391
Reconstruction Loss: -6.331427097320557
Iteration 12201:
Training Loss: -5.2669782638549805
Reconstruction Loss: -6.3370280265808105
Iteration 12301:
Training Loss: -5.2789306640625
Reconstruction Loss: -6.3425679206848145
Iteration 12401:
Training Loss: -5.290787696838379
Reconstruction Loss: -6.348058223724365
Iteration 12501:
Training Loss: -5.302553653717041
Reconstruction Loss: -6.3534932136535645
Iteration 12601:
Training Loss: -5.314228057861328
Reconstruction Loss: -6.358875274658203
Iteration 12701:
Training Loss: -5.325804233551025
Reconstruction Loss: -6.364194869995117
Iteration 12801:
Training Loss: -5.33729887008667
Reconstruction Loss: -6.3694748878479
Iteration 12901:
Training Loss: -5.34869384765625
Reconstruction Loss: -6.3747076988220215
Iteration 13001:
Training Loss: -5.360013008117676
Reconstruction Loss: -6.379882335662842
Iteration 13101:
Training Loss: -5.371230125427246
Reconstruction Loss: -6.385007858276367
Iteration 13201:
Training Loss: -5.3823652267456055
Reconstruction Loss: -6.390087604522705
Iteration 13301:
Training Loss: -5.393420696258545
Reconstruction Loss: -6.395124435424805
Iteration 13401:
Training Loss: -5.404389381408691
Reconstruction Loss: -6.400112152099609
Iteration 13501:
Training Loss: -5.415277481079102
Reconstruction Loss: -6.405055046081543
Iteration 13601:
Training Loss: -5.426091194152832
Reconstruction Loss: -6.409956932067871
Iteration 13701:
Training Loss: -5.436816215515137
Reconstruction Loss: -6.4148125648498535
Iteration 13801:
Training Loss: -5.447465896606445
Reconstruction Loss: -6.419626235961914
Iteration 13901:
Training Loss: -5.4580302238464355
Reconstruction Loss: -6.424393653869629
Iteration 14001:
Training Loss: -5.468537330627441
Reconstruction Loss: -6.429116249084473
Iteration 14101:
Training Loss: -5.4789581298828125
Reconstruction Loss: -6.4337968826293945
Iteration 14201:
Training Loss: -5.489304065704346
Reconstruction Loss: -6.438438415527344
Iteration 14301:
Training Loss: -5.499576091766357
Reconstruction Loss: -6.443039894104004
Iteration 14401:
Training Loss: -5.509775161743164
Reconstruction Loss: -6.447601795196533
Iteration 14501:
Training Loss: -5.519906520843506
Reconstruction Loss: -6.45212459564209
Iteration 14601:
Training Loss: -5.529966831207275
Reconstruction Loss: -6.456612586975098
Iteration 14701:
Training Loss: -5.539961338043213
Reconstruction Loss: -6.461060523986816
Iteration 14801:
Training Loss: -5.549884796142578
Reconstruction Loss: -6.465470790863037
Iteration 14901:
Training Loss: -5.559741020202637
Reconstruction Loss: -6.469836711883545
