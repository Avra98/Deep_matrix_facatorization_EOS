5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.765046119689941
Reconstruction Loss: -0.6687555313110352
Iteration 11:
Training Loss: 2.5758237838745117
Reconstruction Loss: -2.033777952194214
Iteration 21:
Training Loss: 0.7477778196334839
Reconstruction Loss: -2.8101258277893066
Iteration 31:
Training Loss: 0.3037243187427521
Reconstruction Loss: -3.370713472366333
Iteration 41:
Training Loss: -0.39345017075538635
Reconstruction Loss: -3.7737772464752197
Iteration 51:
Training Loss: -0.25322628021240234
Reconstruction Loss: -4.033242702484131
Iteration 61:
Training Loss: -1.137186050415039
Reconstruction Loss: -4.226670742034912
Iteration 71:
Training Loss: -1.0710804462432861
Reconstruction Loss: -4.387049198150635
Iteration 81:
Training Loss: -1.4409853219985962
Reconstruction Loss: -4.499235153198242
Iteration 91:
Training Loss: -1.5287055969238281
Reconstruction Loss: -4.58090353012085
Iteration 101:
Training Loss: -1.587525486946106
Reconstruction Loss: -4.675779819488525
Iteration 111:
Training Loss: -2.2038538455963135
Reconstruction Loss: -4.7460527420043945
Iteration 121:
Training Loss: -2.045487403869629
Reconstruction Loss: -4.797976493835449
Iteration 131:
Training Loss: -2.708841323852539
Reconstruction Loss: -4.844247341156006
Iteration 141:
Training Loss: -2.5541577339172363
Reconstruction Loss: -4.8964338302612305
Iteration 151:
Training Loss: -2.2501962184906006
Reconstruction Loss: -4.932410717010498
Iteration 161:
Training Loss: -2.477484941482544
Reconstruction Loss: -4.969564914703369
Iteration 171:
Training Loss: -2.5027177333831787
Reconstruction Loss: -5.000988483428955
Iteration 181:
Training Loss: -2.6455705165863037
Reconstruction Loss: -5.032631874084473
Iteration 191:
Training Loss: -3.0680441856384277
Reconstruction Loss: -5.062532901763916
Iteration 201:
Training Loss: -2.9057633876800537
Reconstruction Loss: -5.078200340270996
Iteration 211:
Training Loss: -2.6357052326202393
Reconstruction Loss: -5.104099273681641
Iteration 221:
Training Loss: -3.2155661582946777
Reconstruction Loss: -5.134328365325928
Iteration 231:
Training Loss: -3.8134846687316895
Reconstruction Loss: -5.152161598205566
Iteration 241:
Training Loss: -3.058439016342163
Reconstruction Loss: -5.182514190673828
Iteration 251:
Training Loss: -3.5135374069213867
Reconstruction Loss: -5.194518089294434
Iteration 261:
Training Loss: -3.3623502254486084
Reconstruction Loss: -5.2048492431640625
Iteration 271:
Training Loss: -3.5492212772369385
Reconstruction Loss: -5.227728366851807
Iteration 281:
Training Loss: -4.0530619621276855
Reconstruction Loss: -5.243576526641846
Iteration 291:
Training Loss: -3.3476738929748535
Reconstruction Loss: -5.261236667633057
Iteration 301:
Training Loss: -3.102837324142456
Reconstruction Loss: -5.275064468383789
Iteration 311:
Training Loss: -3.4275169372558594
Reconstruction Loss: -5.293487071990967
Iteration 321:
Training Loss: -3.798837900161743
Reconstruction Loss: -5.299502849578857
Iteration 331:
Training Loss: -4.0115885734558105
Reconstruction Loss: -5.317072868347168
Iteration 341:
Training Loss: -3.5509285926818848
Reconstruction Loss: -5.324347972869873
Iteration 351:
Training Loss: -3.9153270721435547
Reconstruction Loss: -5.337209224700928
Iteration 361:
Training Loss: -3.924034595489502
Reconstruction Loss: -5.339104175567627
Iteration 371:
Training Loss: -3.7553038597106934
Reconstruction Loss: -5.35528564453125
Iteration 381:
Training Loss: -3.7164456844329834
Reconstruction Loss: -5.365782260894775
Iteration 391:
Training Loss: -3.8492722511291504
Reconstruction Loss: -5.378003120422363
Iteration 401:
Training Loss: -3.7830936908721924
Reconstruction Loss: -5.384304523468018
Iteration 411:
Training Loss: -3.677342653274536
Reconstruction Loss: -5.395419120788574
Iteration 421:
Training Loss: -3.920966148376465
Reconstruction Loss: -5.408215522766113
Iteration 431:
Training Loss: -3.9378561973571777
Reconstruction Loss: -5.416043758392334
Iteration 441:
Training Loss: -3.8038337230682373
Reconstruction Loss: -5.421616554260254
Iteration 451:
Training Loss: -4.287763595581055
Reconstruction Loss: -5.427281379699707
Iteration 461:
Training Loss: -3.9263992309570312
Reconstruction Loss: -5.438198566436768
Iteration 471:
Training Loss: -4.118916034698486
Reconstruction Loss: -5.44638204574585
Iteration 481:
Training Loss: -4.411853313446045
Reconstruction Loss: -5.451831340789795
Iteration 491:
Training Loss: -4.672109603881836
Reconstruction Loss: -5.459866046905518
Iteration 501:
Training Loss: -4.140726089477539
Reconstruction Loss: -5.4684553146362305
Iteration 511:
Training Loss: -4.1395111083984375
Reconstruction Loss: -5.4755778312683105
Iteration 521:
Training Loss: -4.242527961730957
Reconstruction Loss: -5.479873180389404
Iteration 531:
Training Loss: -4.611013889312744
Reconstruction Loss: -5.4882893562316895
Iteration 541:
Training Loss: -4.699847221374512
Reconstruction Loss: -5.494478225708008
Iteration 551:
Training Loss: -4.378032207489014
Reconstruction Loss: -5.499828815460205
Iteration 561:
Training Loss: -4.7863383293151855
Reconstruction Loss: -5.502640247344971
Iteration 571:
Training Loss: -4.800239562988281
Reconstruction Loss: -5.512842178344727
Iteration 581:
Training Loss: -4.627128601074219
Reconstruction Loss: -5.515845775604248
Iteration 591:
Training Loss: -4.717268943786621
Reconstruction Loss: -5.523482799530029
Iteration 601:
Training Loss: -4.508109092712402
Reconstruction Loss: -5.5221991539001465
Iteration 611:
Training Loss: -4.574297904968262
Reconstruction Loss: -5.530185222625732
Iteration 621:
Training Loss: -4.296051979064941
Reconstruction Loss: -5.539884090423584
Iteration 631:
Training Loss: -4.805338382720947
Reconstruction Loss: -5.541836738586426
Iteration 641:
Training Loss: -4.759454250335693
Reconstruction Loss: -5.54703950881958
Iteration 651:
Training Loss: -4.58341121673584
Reconstruction Loss: -5.555600643157959
Iteration 661:
Training Loss: -4.205495357513428
Reconstruction Loss: -5.558377265930176
Iteration 671:
Training Loss: -5.256829738616943
Reconstruction Loss: -5.563377857208252
Iteration 681:
Training Loss: -4.718950271606445
Reconstruction Loss: -5.566281795501709
Iteration 691:
Training Loss: -4.960447311401367
Reconstruction Loss: -5.572904586791992
Iteration 701:
Training Loss: -5.273443222045898
Reconstruction Loss: -5.577879905700684
Iteration 711:
Training Loss: -5.31917142868042
Reconstruction Loss: -5.581794738769531
Iteration 721:
Training Loss: -5.194472312927246
Reconstruction Loss: -5.584804058074951
Iteration 731:
Training Loss: -5.229660987854004
Reconstruction Loss: -5.589271068572998
Iteration 741:
Training Loss: -5.24928617477417
Reconstruction Loss: -5.590721130371094
Iteration 751:
Training Loss: -4.702183723449707
Reconstruction Loss: -5.594559669494629
Iteration 761:
Training Loss: -4.776558876037598
Reconstruction Loss: -5.6021575927734375
Iteration 771:
Training Loss: -4.820572376251221
Reconstruction Loss: -5.600330829620361
Iteration 781:
Training Loss: -4.894266605377197
Reconstruction Loss: -5.60500955581665
Iteration 791:
Training Loss: -4.437376022338867
Reconstruction Loss: -5.612179756164551
Iteration 801:
Training Loss: -5.023797035217285
Reconstruction Loss: -5.616970062255859
Iteration 811:
Training Loss: -4.837517261505127
Reconstruction Loss: -5.618956565856934
Iteration 821:
Training Loss: -4.836226463317871
Reconstruction Loss: -5.620700836181641
Iteration 831:
Training Loss: -5.162578105926514
Reconstruction Loss: -5.625603199005127
Iteration 841:
Training Loss: -4.785984992980957
Reconstruction Loss: -5.629189491271973
Iteration 851:
Training Loss: -4.964203357696533
Reconstruction Loss: -5.636933326721191
Iteration 861:
Training Loss: -5.063138961791992
Reconstruction Loss: -5.633130073547363
Iteration 871:
Training Loss: -5.052696228027344
Reconstruction Loss: -5.638465881347656
Iteration 881:
Training Loss: -5.07645320892334
Reconstruction Loss: -5.64560079574585
Iteration 891:
Training Loss: -5.188774108886719
Reconstruction Loss: -5.648374080657959
Iteration 901:
Training Loss: -5.340557098388672
Reconstruction Loss: -5.648818492889404
Iteration 911:
Training Loss: -4.8441877365112305
Reconstruction Loss: -5.651434421539307
Iteration 921:
Training Loss: -5.16365909576416
Reconstruction Loss: -5.655878067016602
Iteration 931:
Training Loss: -5.202058792114258
Reconstruction Loss: -5.659102439880371
Iteration 941:
Training Loss: -5.401817321777344
Reconstruction Loss: -5.662332534790039
Iteration 951:
Training Loss: -5.063384056091309
Reconstruction Loss: -5.666665077209473
Iteration 961:
Training Loss: -5.384449481964111
Reconstruction Loss: -5.671814918518066
Iteration 971:
Training Loss: -5.15248966217041
Reconstruction Loss: -5.669893741607666
Iteration 981:
Training Loss: -5.244088649749756
Reconstruction Loss: -5.672646999359131
Iteration 991:
Training Loss: -4.908191204071045
Reconstruction Loss: -5.676995277404785
Iteration 1001:
Training Loss: -4.850905418395996
Reconstruction Loss: -5.676767826080322
Iteration 1011:
Training Loss: -5.159205436706543
Reconstruction Loss: -5.6804399490356445
Iteration 1021:
Training Loss: -5.184664249420166
Reconstruction Loss: -5.682277202606201
Iteration 1031:
Training Loss: -5.330037593841553
Reconstruction Loss: -5.686065673828125
Iteration 1041:
Training Loss: -4.921733379364014
Reconstruction Loss: -5.692484378814697
Iteration 1051:
Training Loss: -5.123284339904785
Reconstruction Loss: -5.694818019866943
Iteration 1061:
Training Loss: -5.130753040313721
Reconstruction Loss: -5.698323726654053
Iteration 1071:
Training Loss: -5.094948768615723
Reconstruction Loss: -5.694674491882324
Iteration 1081:
Training Loss: -5.212053298950195
Reconstruction Loss: -5.706752777099609
Iteration 1091:
Training Loss: -5.316334247589111
Reconstruction Loss: -5.705854415893555
Iteration 1101:
Training Loss: -5.467842102050781
Reconstruction Loss: -5.706876277923584
Iteration 1111:
Training Loss: -5.8677520751953125
Reconstruction Loss: -5.706341743469238
Iteration 1121:
Training Loss: -5.3079328536987305
Reconstruction Loss: -5.713157653808594
Iteration 1131:
Training Loss: -5.393490791320801
Reconstruction Loss: -5.714893341064453
Iteration 1141:
Training Loss: -5.381443500518799
Reconstruction Loss: -5.715194225311279
Iteration 1151:
Training Loss: -5.637135982513428
Reconstruction Loss: -5.720613479614258
Iteration 1161:
Training Loss: -5.396245956420898
Reconstruction Loss: -5.719285488128662
Iteration 1171:
Training Loss: -5.206926345825195
Reconstruction Loss: -5.7193193435668945
Iteration 1181:
Training Loss: -5.305256366729736
Reconstruction Loss: -5.7230634689331055
Iteration 1191:
Training Loss: -5.448282241821289
Reconstruction Loss: -5.724360466003418
Iteration 1201:
Training Loss: -6.1382622718811035
Reconstruction Loss: -5.730483531951904
Iteration 1211:
Training Loss: -5.601069450378418
Reconstruction Loss: -5.729798316955566
Iteration 1221:
Training Loss: -5.369791507720947
Reconstruction Loss: -5.732542991638184
Iteration 1231:
Training Loss: -5.857388496398926
Reconstruction Loss: -5.736997127532959
Iteration 1241:
Training Loss: -5.198615074157715
Reconstruction Loss: -5.736854076385498
Iteration 1251:
Training Loss: -5.67455530166626
Reconstruction Loss: -5.736410140991211
Iteration 1261:
Training Loss: -5.863712310791016
Reconstruction Loss: -5.743195056915283
Iteration 1271:
Training Loss: -5.722929954528809
Reconstruction Loss: -5.741539001464844
Iteration 1281:
Training Loss: -5.510325908660889
Reconstruction Loss: -5.743000507354736
Iteration 1291:
Training Loss: -5.643682956695557
Reconstruction Loss: -5.747946739196777
Iteration 1301:
Training Loss: -6.112938404083252
Reconstruction Loss: -5.751929759979248
Iteration 1311:
Training Loss: -5.429091453552246
Reconstruction Loss: -5.7514448165893555
Iteration 1321:
Training Loss: -5.499431133270264
Reconstruction Loss: -5.754336357116699
Iteration 1331:
Training Loss: -5.910123348236084
Reconstruction Loss: -5.7538604736328125
Iteration 1341:
Training Loss: -5.663583278656006
Reconstruction Loss: -5.758014678955078
Iteration 1351:
Training Loss: -5.71798038482666
Reconstruction Loss: -5.760138511657715
Iteration 1361:
Training Loss: -5.371201992034912
Reconstruction Loss: -5.758218765258789
Iteration 1371:
Training Loss: -5.352328777313232
Reconstruction Loss: -5.761927127838135
Iteration 1381:
Training Loss: -5.397254467010498
Reconstruction Loss: -5.7636399269104
Iteration 1391:
Training Loss: -5.537116050720215
Reconstruction Loss: -5.762450218200684
Iteration 1401:
Training Loss: -5.490715026855469
Reconstruction Loss: -5.768341541290283
Iteration 1411:
Training Loss: -5.603155136108398
Reconstruction Loss: -5.769440650939941
Iteration 1421:
Training Loss: -5.875746250152588
Reconstruction Loss: -5.7697930335998535
Iteration 1431:
Training Loss: -6.068634510040283
Reconstruction Loss: -5.775449275970459
Iteration 1441:
Training Loss: -6.204278945922852
Reconstruction Loss: -5.774721145629883
Iteration 1451:
Training Loss: -5.75544548034668
Reconstruction Loss: -5.777551651000977
Iteration 1461:
Training Loss: -5.6083478927612305
Reconstruction Loss: -5.780466079711914
Iteration 1471:
Training Loss: -6.093858242034912
Reconstruction Loss: -5.7797369956970215
Iteration 1481:
Training Loss: -5.633996963500977
Reconstruction Loss: -5.7848076820373535
Iteration 1491:
Training Loss: -5.956303596496582
Reconstruction Loss: -5.78428316116333
Iteration 1501:
Training Loss: -5.946752548217773
Reconstruction Loss: -5.783557415008545
Iteration 1511:
Training Loss: -5.547330856323242
Reconstruction Loss: -5.784436225891113
Iteration 1521:
Training Loss: -5.349390029907227
Reconstruction Loss: -5.788301944732666
Iteration 1531:
Training Loss: -5.554892539978027
Reconstruction Loss: -5.788796424865723
Iteration 1541:
Training Loss: -6.2118682861328125
Reconstruction Loss: -5.790327548980713
Iteration 1551:
Training Loss: -5.433356285095215
Reconstruction Loss: -5.792030334472656
Iteration 1561:
Training Loss: -5.651905536651611
Reconstruction Loss: -5.795348644256592
Iteration 1571:
Training Loss: -6.557172775268555
Reconstruction Loss: -5.794964790344238
Iteration 1581:
Training Loss: -5.91230583190918
Reconstruction Loss: -5.798291206359863
Iteration 1591:
Training Loss: -5.893289566040039
Reconstruction Loss: -5.800511360168457
Iteration 1601:
Training Loss: -5.947606563568115
Reconstruction Loss: -5.799587249755859
Iteration 1611:
Training Loss: -6.145434379577637
Reconstruction Loss: -5.802206516265869
Iteration 1621:
Training Loss: -6.3042521476745605
Reconstruction Loss: -5.803679466247559
Iteration 1631:
Training Loss: -6.086878299713135
Reconstruction Loss: -5.807401657104492
Iteration 1641:
Training Loss: -6.015998840332031
Reconstruction Loss: -5.806718349456787
Iteration 1651:
Training Loss: -5.9988579750061035
Reconstruction Loss: -5.806023120880127
Iteration 1661:
Training Loss: -5.939539909362793
Reconstruction Loss: -5.806903839111328
Iteration 1671:
Training Loss: -6.330750465393066
Reconstruction Loss: -5.810885429382324
Iteration 1681:
Training Loss: -5.559311866760254
Reconstruction Loss: -5.810710906982422
Iteration 1691:
Training Loss: -6.258272647857666
Reconstruction Loss: -5.813019275665283
Iteration 1701:
Training Loss: -6.425044059753418
Reconstruction Loss: -5.81630802154541
Iteration 1711:
Training Loss: -5.75129508972168
Reconstruction Loss: -5.814589023590088
Iteration 1721:
Training Loss: -6.109583377838135
Reconstruction Loss: -5.819555282592773
Iteration 1731:
Training Loss: -6.390356063842773
Reconstruction Loss: -5.817732810974121
Iteration 1741:
Training Loss: -6.2945475578308105
Reconstruction Loss: -5.820891380310059
Iteration 1751:
Training Loss: -5.83720064163208
Reconstruction Loss: -5.8184814453125
Iteration 1761:
Training Loss: -6.099095344543457
Reconstruction Loss: -5.824751377105713
Iteration 1771:
Training Loss: -6.090803623199463
Reconstruction Loss: -5.827411651611328
Iteration 1781:
Training Loss: -6.746706962585449
Reconstruction Loss: -5.828195095062256
Iteration 1791:
Training Loss: -6.517195701599121
Reconstruction Loss: -5.826948165893555
Iteration 1801:
Training Loss: -5.616970539093018
Reconstruction Loss: -5.828849792480469
Iteration 1811:
Training Loss: -6.24751615524292
Reconstruction Loss: -5.829326152801514
Iteration 1821:
Training Loss: -5.836921215057373
Reconstruction Loss: -5.829052925109863
Iteration 1831:
Training Loss: -6.101858139038086
Reconstruction Loss: -5.8307085037231445
Iteration 1841:
Training Loss: -6.348507881164551
Reconstruction Loss: -5.834245204925537
Iteration 1851:
Training Loss: -6.3589324951171875
Reconstruction Loss: -5.834971904754639
Iteration 1861:
Training Loss: -6.07933235168457
Reconstruction Loss: -5.836359024047852
Iteration 1871:
Training Loss: -6.028297424316406
Reconstruction Loss: -5.838741302490234
Iteration 1881:
Training Loss: -5.808943748474121
Reconstruction Loss: -5.839911460876465
Iteration 1891:
Training Loss: -5.874274730682373
Reconstruction Loss: -5.840619087219238
Iteration 1901:
Training Loss: -6.580657482147217
Reconstruction Loss: -5.840826988220215
Iteration 1911:
Training Loss: -6.128892421722412
Reconstruction Loss: -5.844141483306885
Iteration 1921:
Training Loss: -5.870086669921875
Reconstruction Loss: -5.8444671630859375
Iteration 1931:
Training Loss: -6.292481422424316
Reconstruction Loss: -5.8469133377075195
Iteration 1941:
Training Loss: -5.73045015335083
Reconstruction Loss: -5.843577861785889
Iteration 1951:
Training Loss: -6.406405925750732
Reconstruction Loss: -5.844696521759033
Iteration 1961:
Training Loss: -6.092151165008545
Reconstruction Loss: -5.849369049072266
Iteration 1971:
Training Loss: -6.299069404602051
Reconstruction Loss: -5.848106384277344
Iteration 1981:
Training Loss: -6.218469619750977
Reconstruction Loss: -5.851036071777344
Iteration 1991:
Training Loss: -6.172676086425781
Reconstruction Loss: -5.850515365600586
Iteration 2001:
Training Loss: -6.407867431640625
Reconstruction Loss: -5.851551055908203
Iteration 2011:
Training Loss: -5.9880876541137695
Reconstruction Loss: -5.851562023162842
Iteration 2021:
Training Loss: -6.338039398193359
Reconstruction Loss: -5.854596138000488
Iteration 2031:
Training Loss: -6.68348503112793
Reconstruction Loss: -5.856268882751465
Iteration 2041:
Training Loss: -6.268768787384033
Reconstruction Loss: -5.857605934143066
Iteration 2051:
Training Loss: -6.346971035003662
Reconstruction Loss: -5.860422611236572
Iteration 2061:
Training Loss: -6.205190181732178
Reconstruction Loss: -5.856751441955566
Iteration 2071:
Training Loss: -6.393805980682373
Reconstruction Loss: -5.8600664138793945
Iteration 2081:
Training Loss: -6.052700996398926
Reconstruction Loss: -5.860697269439697
Iteration 2091:
Training Loss: -6.516754150390625
Reconstruction Loss: -5.861898899078369
Iteration 2101:
Training Loss: -6.330832481384277
Reconstruction Loss: -5.864512920379639
Iteration 2111:
Training Loss: -6.457927703857422
Reconstruction Loss: -5.863771438598633
Iteration 2121:
Training Loss: -6.334713935852051
Reconstruction Loss: -5.865555286407471
Iteration 2131:
Training Loss: -6.266483306884766
Reconstruction Loss: -5.867673873901367
Iteration 2141:
Training Loss: -6.201191425323486
Reconstruction Loss: -5.869617462158203
Iteration 2151:
Training Loss: -6.2395429611206055
Reconstruction Loss: -5.8692474365234375
Iteration 2161:
Training Loss: -6.129014015197754
Reconstruction Loss: -5.869203567504883
Iteration 2171:
Training Loss: -6.185081958770752
Reconstruction Loss: -5.869385719299316
Iteration 2181:
Training Loss: -6.702207565307617
Reconstruction Loss: -5.8742218017578125
Iteration 2191:
Training Loss: -6.363308429718018
Reconstruction Loss: -5.8733038902282715
Iteration 2201:
Training Loss: -6.757962226867676
Reconstruction Loss: -5.875606060028076
Iteration 2211:
Training Loss: -6.631991863250732
Reconstruction Loss: -5.87567138671875
Iteration 2221:
Training Loss: -6.346043109893799
Reconstruction Loss: -5.874460220336914
Iteration 2231:
Training Loss: -6.018777370452881
Reconstruction Loss: -5.878648281097412
Iteration 2241:
Training Loss: -6.976726531982422
Reconstruction Loss: -5.878188133239746
Iteration 2251:
Training Loss: -6.554862976074219
Reconstruction Loss: -5.880591869354248
Iteration 2261:
Training Loss: -6.848393440246582
Reconstruction Loss: -5.881503105163574
Iteration 2271:
Training Loss: -6.533438682556152
Reconstruction Loss: -5.881521701812744
Iteration 2281:
Training Loss: -6.024734973907471
Reconstruction Loss: -5.88215970993042
Iteration 2291:
Training Loss: -6.531821250915527
Reconstruction Loss: -5.8839521408081055
Iteration 2301:
Training Loss: -6.23991584777832
Reconstruction Loss: -5.881357669830322
Iteration 2311:
Training Loss: -6.7268900871276855
Reconstruction Loss: -5.886599540710449
Iteration 2321:
Training Loss: -6.5152974128723145
Reconstruction Loss: -5.886346817016602
Iteration 2331:
Training Loss: -6.7044477462768555
Reconstruction Loss: -5.88706636428833
Iteration 2341:
Training Loss: -6.718547344207764
Reconstruction Loss: -5.888018608093262
Iteration 2351:
Training Loss: -6.750450134277344
Reconstruction Loss: -5.889164924621582
Iteration 2361:
Training Loss: -6.347254753112793
Reconstruction Loss: -5.888786315917969
Iteration 2371:
Training Loss: -6.262299060821533
Reconstruction Loss: -5.888846397399902
Iteration 2381:
Training Loss: -6.4554009437561035
Reconstruction Loss: -5.892202377319336
Iteration 2391:
Training Loss: -6.397806167602539
Reconstruction Loss: -5.894168853759766
Iteration 2401:
Training Loss: -6.432734489440918
Reconstruction Loss: -5.892559051513672
Iteration 2411:
Training Loss: -6.681028842926025
Reconstruction Loss: -5.8950042724609375
Iteration 2421:
Training Loss: -6.631484031677246
Reconstruction Loss: -5.893738746643066
Iteration 2431:
Training Loss: -6.512528896331787
Reconstruction Loss: -5.8942694664001465
Iteration 2441:
Training Loss: -6.929718017578125
Reconstruction Loss: -5.897406578063965
Iteration 2451:
Training Loss: -6.172616481781006
Reconstruction Loss: -5.899293899536133
Iteration 2461:
Training Loss: -6.323934555053711
Reconstruction Loss: -5.898878574371338
Iteration 2471:
Training Loss: -6.523758411407471
Reconstruction Loss: -5.901439189910889
Iteration 2481:
Training Loss: -6.717477798461914
Reconstruction Loss: -5.899937629699707
Iteration 2491:
Training Loss: -6.488256454467773
Reconstruction Loss: -5.901217937469482
Iteration 2501:
Training Loss: -6.987631320953369
Reconstruction Loss: -5.904704570770264
Iteration 2511:
Training Loss: -6.839212894439697
Reconstruction Loss: -5.90049409866333
Iteration 2521:
Training Loss: -6.505049705505371
Reconstruction Loss: -5.903926372528076
Iteration 2531:
Training Loss: -6.244228839874268
Reconstruction Loss: -5.905206203460693
Iteration 2541:
Training Loss: -6.7254462242126465
Reconstruction Loss: -5.905574798583984
Iteration 2551:
Training Loss: -6.281550407409668
Reconstruction Loss: -5.906020641326904
Iteration 2561:
Training Loss: -6.6178879737854
Reconstruction Loss: -5.907515048980713
Iteration 2571:
Training Loss: -6.87578010559082
Reconstruction Loss: -5.908684253692627
Iteration 2581:
Training Loss: -6.634474754333496
Reconstruction Loss: -5.909576892852783
Iteration 2591:
Training Loss: -6.413011074066162
Reconstruction Loss: -5.9122748374938965
Iteration 2601:
Training Loss: -7.019204139709473
Reconstruction Loss: -5.909946918487549
Iteration 2611:
Training Loss: -6.690725803375244
Reconstruction Loss: -5.911890506744385
Iteration 2621:
Training Loss: -6.531778335571289
Reconstruction Loss: -5.9111175537109375
Iteration 2631:
Training Loss: -7.165774345397949
Reconstruction Loss: -5.912822723388672
Iteration 2641:
Training Loss: -6.995195388793945
Reconstruction Loss: -5.914662837982178
Iteration 2651:
Training Loss: -6.905908584594727
Reconstruction Loss: -5.915624141693115
Iteration 2661:
Training Loss: -6.508604049682617
Reconstruction Loss: -5.915024280548096
Iteration 2671:
Training Loss: -6.379536151885986
Reconstruction Loss: -5.91748046875
Iteration 2681:
Training Loss: -6.491735458374023
Reconstruction Loss: -5.917012691497803
Iteration 2691:
Training Loss: -6.890811443328857
Reconstruction Loss: -5.918481349945068
Iteration 2701:
Training Loss: -6.928952693939209
Reconstruction Loss: -5.918569564819336
Iteration 2711:
Training Loss: -6.4592108726501465
Reconstruction Loss: -5.919499397277832
Iteration 2721:
Training Loss: -6.639955043792725
Reconstruction Loss: -5.9199419021606445
Iteration 2731:
Training Loss: -7.058222770690918
Reconstruction Loss: -5.920101642608643
Iteration 2741:
Training Loss: -6.529200553894043
Reconstruction Loss: -5.92256498336792
Iteration 2751:
Training Loss: -6.372768878936768
Reconstruction Loss: -5.921305179595947
Iteration 2761:
Training Loss: -6.828378677368164
Reconstruction Loss: -5.92282247543335
Iteration 2771:
Training Loss: -6.731711387634277
Reconstruction Loss: -5.923276901245117
Iteration 2781:
Training Loss: -6.975769519805908
Reconstruction Loss: -5.924037456512451
Iteration 2791:
Training Loss: -6.670477390289307
Reconstruction Loss: -5.925908088684082
Iteration 2801:
Training Loss: -6.986884593963623
Reconstruction Loss: -5.927630424499512
Iteration 2811:
Training Loss: -6.6859283447265625
Reconstruction Loss: -5.9276885986328125
Iteration 2821:
Training Loss: -6.712558746337891
Reconstruction Loss: -5.9258623123168945
Iteration 2831:
Training Loss: -7.39304780960083
Reconstruction Loss: -5.92806339263916
Iteration 2841:
Training Loss: -6.762070178985596
Reconstruction Loss: -5.927676677703857
Iteration 2851:
Training Loss: -6.896409034729004
Reconstruction Loss: -5.929284572601318
Iteration 2861:
Training Loss: -6.915472507476807
Reconstruction Loss: -5.930920124053955
Iteration 2871:
Training Loss: -7.198604583740234
Reconstruction Loss: -5.933907985687256
Iteration 2881:
Training Loss: -6.926485061645508
Reconstruction Loss: -5.933106422424316
Iteration 2891:
Training Loss: -6.478903293609619
Reconstruction Loss: -5.931201457977295
Iteration 2901:
Training Loss: -6.5205912590026855
Reconstruction Loss: -5.931792259216309
Iteration 2911:
Training Loss: -6.792750358581543
Reconstruction Loss: -5.935103416442871
Iteration 2921:
Training Loss: -7.0944342613220215
Reconstruction Loss: -5.934643268585205
Iteration 2931:
Training Loss: -7.046741485595703
Reconstruction Loss: -5.935533046722412
Iteration 2941:
Training Loss: -7.051537036895752
Reconstruction Loss: -5.937772274017334
Iteration 2951:
Training Loss: -7.1007399559021
Reconstruction Loss: -5.938605308532715
Iteration 2961:
Training Loss: -7.100649356842041
Reconstruction Loss: -5.938076019287109
Iteration 2971:
Training Loss: -6.489007949829102
Reconstruction Loss: -5.9378981590271
Iteration 2981:
Training Loss: -6.914310455322266
Reconstruction Loss: -5.939024925231934
Iteration 2991:
Training Loss: -6.6521100997924805
Reconstruction Loss: -5.941650867462158
Iteration 3001:
Training Loss: -7.012079238891602
Reconstruction Loss: -5.9408159255981445
Iteration 3011:
Training Loss: -6.918310165405273
Reconstruction Loss: -5.940613269805908
Iteration 3021:
Training Loss: -7.172665119171143
Reconstruction Loss: -5.940219879150391
Iteration 3031:
Training Loss: -6.8048014640808105
Reconstruction Loss: -5.941922187805176
Iteration 3041:
Training Loss: -6.229218482971191
Reconstruction Loss: -5.942018032073975
Iteration 3051:
Training Loss: -7.174685478210449
Reconstruction Loss: -5.943690299987793
Iteration 3061:
Training Loss: -6.815576553344727
Reconstruction Loss: -5.943587779998779
Iteration 3071:
Training Loss: -6.938220500946045
Reconstruction Loss: -5.943283557891846
Iteration 3081:
Training Loss: -7.015596389770508
Reconstruction Loss: -5.9459614753723145
Iteration 3091:
Training Loss: -6.9509406089782715
Reconstruction Loss: -5.945426940917969
Iteration 3101:
Training Loss: -6.74556827545166
Reconstruction Loss: -5.943709373474121
Iteration 3111:
Training Loss: -6.921391487121582
Reconstruction Loss: -5.946852684020996
Iteration 3121:
Training Loss: -6.95726203918457
Reconstruction Loss: -5.947295188903809
Iteration 3131:
Training Loss: -6.441068172454834
Reconstruction Loss: -5.949424743652344
Iteration 3141:
Training Loss: -7.292000770568848
Reconstruction Loss: -5.948318004608154
Iteration 3151:
Training Loss: -7.051395416259766
Reconstruction Loss: -5.9503936767578125
Iteration 3161:
Training Loss: -7.060496807098389
Reconstruction Loss: -5.949681758880615
Iteration 3171:
Training Loss: -7.211913108825684
Reconstruction Loss: -5.949456214904785
Iteration 3181:
Training Loss: -6.837340354919434
Reconstruction Loss: -5.9502644538879395
Iteration 3191:
Training Loss: -7.040924549102783
Reconstruction Loss: -5.952134609222412
Iteration 3201:
Training Loss: -7.147477149963379
Reconstruction Loss: -5.952243328094482
Iteration 3211:
Training Loss: -6.860976219177246
Reconstruction Loss: -5.955289840698242
Iteration 3221:
Training Loss: -7.563324451446533
Reconstruction Loss: -5.953225135803223
Iteration 3231:
Training Loss: -6.613217353820801
Reconstruction Loss: -5.953217029571533
Iteration 3241:
Training Loss: -6.947271823883057
Reconstruction Loss: -5.955345153808594
Iteration 3251:
Training Loss: -7.0885515213012695
Reconstruction Loss: -5.957257270812988
Iteration 3261:
Training Loss: -7.018552303314209
Reconstruction Loss: -5.957765102386475
Iteration 3271:
Training Loss: -6.855062484741211
Reconstruction Loss: -5.956729412078857
Iteration 3281:
Training Loss: -7.087950229644775
Reconstruction Loss: -5.957077503204346
Iteration 3291:
Training Loss: -7.572049140930176
Reconstruction Loss: -5.9584641456604
Iteration 3301:
Training Loss: -6.9617204666137695
Reconstruction Loss: -5.958873271942139
Iteration 3311:
Training Loss: -7.295565128326416
Reconstruction Loss: -5.95998477935791
Iteration 3321:
Training Loss: -6.773756980895996
Reconstruction Loss: -5.960845470428467
Iteration 3331:
Training Loss: -7.158729076385498
Reconstruction Loss: -5.960092544555664
Iteration 3341:
Training Loss: -6.964095115661621
Reconstruction Loss: -5.959960460662842
Iteration 3351:
Training Loss: -7.271563529968262
Reconstruction Loss: -5.960073471069336
Iteration 3361:
Training Loss: -7.0211181640625
Reconstruction Loss: -5.961706638336182
Iteration 3371:
Training Loss: -7.405623912811279
Reconstruction Loss: -5.961982250213623
Iteration 3381:
Training Loss: -6.906131744384766
Reconstruction Loss: -5.9614152908325195
Iteration 3391:
Training Loss: -7.151133060455322
Reconstruction Loss: -5.963022232055664
Iteration 3401:
Training Loss: -6.835447788238525
Reconstruction Loss: -5.963089942932129
Iteration 3411:
Training Loss: -6.789147853851318
Reconstruction Loss: -5.964724063873291
Iteration 3421:
Training Loss: -6.864383697509766
Reconstruction Loss: -5.965339183807373
Iteration 3431:
Training Loss: -7.248867988586426
Reconstruction Loss: -5.9640913009643555
Iteration 3441:
Training Loss: -6.802655220031738
Reconstruction Loss: -5.965104579925537
Iteration 3451:
Training Loss: -6.812872409820557
Reconstruction Loss: -5.967868804931641
Iteration 3461:
Training Loss: -7.454153537750244
Reconstruction Loss: -5.966751575469971
Iteration 3471:
Training Loss: -7.2005696296691895
Reconstruction Loss: -5.968171119689941
Iteration 3481:
Training Loss: -7.363977909088135
Reconstruction Loss: -5.967174530029297
Iteration 3491:
Training Loss: -6.791330814361572
Reconstruction Loss: -5.968933582305908
Iteration 3501:
Training Loss: -6.8135552406311035
Reconstruction Loss: -5.968643665313721
Iteration 3511:
Training Loss: -7.466159820556641
Reconstruction Loss: -5.969440460205078
Iteration 3521:
Training Loss: -7.214548587799072
Reconstruction Loss: -5.96810245513916
Iteration 3531:
Training Loss: -7.473196506500244
Reconstruction Loss: -5.968511581420898
Iteration 3541:
Training Loss: -6.825465202331543
Reconstruction Loss: -5.970314979553223
Iteration 3551:
Training Loss: -7.1738409996032715
Reconstruction Loss: -5.971699237823486
Iteration 3561:
Training Loss: -7.288923263549805
Reconstruction Loss: -5.972355365753174
Iteration 3571:
Training Loss: -7.328757286071777
Reconstruction Loss: -5.9723381996154785
Iteration 3581:
Training Loss: -7.517819881439209
Reconstruction Loss: -5.971834182739258
Iteration 3591:
Training Loss: -7.063544750213623
Reconstruction Loss: -5.97238826751709
Iteration 3601:
Training Loss: -7.3000054359436035
Reconstruction Loss: -5.9735846519470215
Iteration 3611:
Training Loss: -7.254946231842041
Reconstruction Loss: -5.974335193634033
Iteration 3621:
Training Loss: -6.8761467933654785
Reconstruction Loss: -5.975025177001953
Iteration 3631:
Training Loss: -7.520768642425537
Reconstruction Loss: -5.976080894470215
Iteration 3641:
Training Loss: -7.0288214683532715
Reconstruction Loss: -5.976324081420898
Iteration 3651:
Training Loss: -6.907093524932861
Reconstruction Loss: -5.975346565246582
Iteration 3661:
Training Loss: -7.956750869750977
Reconstruction Loss: -5.977083683013916
Iteration 3671:
Training Loss: -7.162621974945068
Reconstruction Loss: -5.977395057678223
Iteration 3681:
Training Loss: -6.997684001922607
Reconstruction Loss: -5.97756814956665
Iteration 3691:
Training Loss: -7.453363418579102
Reconstruction Loss: -5.977016925811768
Iteration 3701:
Training Loss: -7.253442764282227
Reconstruction Loss: -5.979335308074951
Iteration 3711:
Training Loss: -6.945133686065674
Reconstruction Loss: -5.9784770011901855
Iteration 3721:
Training Loss: -7.452914714813232
Reconstruction Loss: -5.980809211730957
Iteration 3731:
Training Loss: -6.86531925201416
Reconstruction Loss: -5.980325222015381
Iteration 3741:
Training Loss: -7.739046096801758
Reconstruction Loss: -5.980118274688721
Iteration 3751:
Training Loss: -7.242734909057617
Reconstruction Loss: -5.982068061828613
Iteration 3761:
Training Loss: -7.201874732971191
Reconstruction Loss: -5.982460975646973
Iteration 3771:
Training Loss: -7.206020355224609
Reconstruction Loss: -5.9831037521362305
Iteration 3781:
Training Loss: -7.308913707733154
Reconstruction Loss: -5.9824299812316895
Iteration 3791:
Training Loss: -7.584010601043701
Reconstruction Loss: -5.984175682067871
Iteration 3801:
Training Loss: -7.434055805206299
Reconstruction Loss: -5.982913494110107
Iteration 3811:
Training Loss: -7.229940891265869
Reconstruction Loss: -5.984447479248047
Iteration 3821:
Training Loss: -7.144435882568359
Reconstruction Loss: -5.985390663146973
Iteration 3831:
Training Loss: -7.684747219085693
Reconstruction Loss: -5.984070777893066
Iteration 3841:
Training Loss: -7.132279872894287
Reconstruction Loss: -5.9850077629089355
Iteration 3851:
Training Loss: -7.064149379730225
Reconstruction Loss: -5.9847846031188965
Iteration 3861:
Training Loss: -7.77748441696167
Reconstruction Loss: -5.986358642578125
Iteration 3871:
Training Loss: -7.2908935546875
Reconstruction Loss: -5.985757827758789
Iteration 3881:
Training Loss: -7.130909442901611
Reconstruction Loss: -5.986901760101318
Iteration 3891:
Training Loss: -7.053681373596191
Reconstruction Loss: -5.988712310791016
Iteration 3901:
Training Loss: -7.39039945602417
Reconstruction Loss: -5.988274097442627
Iteration 3911:
Training Loss: -7.247339725494385
Reconstruction Loss: -5.987293720245361
Iteration 3921:
Training Loss: -7.241546154022217
Reconstruction Loss: -5.98956298828125
Iteration 3931:
Training Loss: -6.987288951873779
Reconstruction Loss: -5.988900184631348
Iteration 3941:
Training Loss: -7.830400466918945
Reconstruction Loss: -5.990111351013184
Iteration 3951:
Training Loss: -7.425410270690918
Reconstruction Loss: -5.990034580230713
Iteration 3961:
Training Loss: -7.587351322174072
Reconstruction Loss: -5.990889072418213
Iteration 3971:
Training Loss: -7.699060440063477
Reconstruction Loss: -5.992129325866699
Iteration 3981:
Training Loss: -7.954276084899902
Reconstruction Loss: -5.991339206695557
Iteration 3991:
Training Loss: -8.141711235046387
Reconstruction Loss: -5.991912841796875
Iteration 4001:
Training Loss: -7.78070068359375
Reconstruction Loss: -5.993196964263916
Iteration 4011:
Training Loss: -7.678768157958984
Reconstruction Loss: -5.993756294250488
Iteration 4021:
Training Loss: -7.583038330078125
Reconstruction Loss: -5.993834495544434
Iteration 4031:
Training Loss: -7.44158935546875
Reconstruction Loss: -5.994569301605225
Iteration 4041:
Training Loss: -7.536917209625244
Reconstruction Loss: -5.99351692199707
Iteration 4051:
Training Loss: -7.411382675170898
Reconstruction Loss: -5.99452543258667
Iteration 4061:
Training Loss: -7.530401229858398
Reconstruction Loss: -5.994166374206543
Iteration 4071:
Training Loss: -7.241107940673828
Reconstruction Loss: -5.996399402618408
Iteration 4081:
Training Loss: -7.9957146644592285
Reconstruction Loss: -5.995947360992432
Iteration 4091:
Training Loss: -7.512070655822754
Reconstruction Loss: -5.996853828430176
Iteration 4101:
Training Loss: -7.843442916870117
Reconstruction Loss: -5.995813846588135
Iteration 4111:
Training Loss: -7.0669355392456055
Reconstruction Loss: -5.997329235076904
Iteration 4121:
Training Loss: -7.55363655090332
Reconstruction Loss: -5.998712539672852
Iteration 4131:
Training Loss: -7.164865016937256
Reconstruction Loss: -5.99791955947876
Iteration 4141:
Training Loss: -8.134198188781738
Reconstruction Loss: -5.9969258308410645
Iteration 4151:
Training Loss: -7.815723896026611
Reconstruction Loss: -5.99924898147583
Iteration 4161:
Training Loss: -7.294830322265625
Reconstruction Loss: -6.000086307525635
Iteration 4171:
Training Loss: -7.465401649475098
Reconstruction Loss: -5.999660491943359
Iteration 4181:
Training Loss: -7.360404014587402
Reconstruction Loss: -5.998498439788818
Iteration 4191:
Training Loss: -7.843948841094971
Reconstruction Loss: -6.001380920410156
Iteration 4201:
Training Loss: -7.356697082519531
Reconstruction Loss: -6.000283718109131
Iteration 4211:
Training Loss: -7.506272315979004
Reconstruction Loss: -6.000885009765625
Iteration 4221:
Training Loss: -7.476019859313965
Reconstruction Loss: -6.000866889953613
Iteration 4231:
Training Loss: -7.5689473152160645
Reconstruction Loss: -6.002009868621826
Iteration 4241:
Training Loss: -7.4782795906066895
Reconstruction Loss: -5.999697208404541
Iteration 4251:
Training Loss: -7.374581336975098
Reconstruction Loss: -6.003143787384033
Iteration 4261:
Training Loss: -7.393535614013672
Reconstruction Loss: -6.002769470214844
Iteration 4271:
Training Loss: -7.516894817352295
Reconstruction Loss: -6.0039215087890625
Iteration 4281:
Training Loss: -7.618345737457275
Reconstruction Loss: -6.0033440589904785
Iteration 4291:
Training Loss: -7.5268073081970215
Reconstruction Loss: -6.002640724182129
Iteration 4301:
Training Loss: -7.554497241973877
Reconstruction Loss: -6.005115985870361
Iteration 4311:
Training Loss: -7.153916835784912
Reconstruction Loss: -6.004539966583252
Iteration 4321:
Training Loss: -7.242827415466309
Reconstruction Loss: -6.005731105804443
Iteration 4331:
Training Loss: -7.5392866134643555
Reconstruction Loss: -6.005914211273193
Iteration 4341:
Training Loss: -7.750156402587891
Reconstruction Loss: -6.004927635192871
Iteration 4351:
Training Loss: -7.550867080688477
Reconstruction Loss: -6.005076885223389
Iteration 4361:
Training Loss: -7.714771747589111
Reconstruction Loss: -6.006533145904541
Iteration 4371:
Training Loss: -7.122056007385254
Reconstruction Loss: -6.006529331207275
Iteration 4381:
Training Loss: -7.563324451446533
Reconstruction Loss: -6.0067009925842285
Iteration 4391:
Training Loss: -7.3502197265625
Reconstruction Loss: -6.0078840255737305
Iteration 4401:
Training Loss: -7.753629207611084
Reconstruction Loss: -6.007192611694336
Iteration 4411:
Training Loss: -7.600507736206055
Reconstruction Loss: -6.007358551025391
Iteration 4421:
Training Loss: -7.417030334472656
Reconstruction Loss: -6.009557247161865
Iteration 4431:
Training Loss: -7.809985637664795
Reconstruction Loss: -6.008693218231201
Iteration 4441:
Training Loss: -7.403618335723877
Reconstruction Loss: -6.0095319747924805
Iteration 4451:
Training Loss: -7.463468551635742
Reconstruction Loss: -6.010368347167969
Iteration 4461:
Training Loss: -7.224588871002197
Reconstruction Loss: -6.0081634521484375
Iteration 4471:
Training Loss: -7.861656188964844
Reconstruction Loss: -6.0102620124816895
Iteration 4481:
Training Loss: -7.626668453216553
Reconstruction Loss: -6.011285781860352
Iteration 4491:
Training Loss: -7.593525409698486
Reconstruction Loss: -6.011410713195801
Iteration 4501:
Training Loss: -7.705714702606201
Reconstruction Loss: -6.012344837188721
Iteration 4511:
Training Loss: -7.923154830932617
Reconstruction Loss: -6.0120015144348145
Iteration 4521:
Training Loss: -8.05594539642334
Reconstruction Loss: -6.012606143951416
Iteration 4531:
Training Loss: -7.358829498291016
Reconstruction Loss: -6.012740135192871
Iteration 4541:
Training Loss: -7.44565486907959
Reconstruction Loss: -6.01274299621582
Iteration 4551:
Training Loss: -8.015953063964844
Reconstruction Loss: -6.0122809410095215
Iteration 4561:
Training Loss: -7.842738151550293
Reconstruction Loss: -6.012895107269287
Iteration 4571:
Training Loss: -7.520469665527344
Reconstruction Loss: -6.013340473175049
Iteration 4581:
Training Loss: -7.462298393249512
Reconstruction Loss: -6.015284538269043
Iteration 4591:
Training Loss: -7.927550792694092
Reconstruction Loss: -6.013851642608643
Iteration 4601:
Training Loss: -7.501662731170654
Reconstruction Loss: -6.013930797576904
Iteration 4611:
Training Loss: -7.6533122062683105
Reconstruction Loss: -6.015799045562744
Iteration 4621:
Training Loss: -7.599230766296387
Reconstruction Loss: -6.014959812164307
Iteration 4631:
Training Loss: -7.517906665802002
Reconstruction Loss: -6.016339302062988
Iteration 4641:
Training Loss: -7.601548194885254
Reconstruction Loss: -6.016162395477295
Iteration 4651:
Training Loss: -8.146307945251465
Reconstruction Loss: -6.014865398406982
Iteration 4661:
Training Loss: -8.144611358642578
Reconstruction Loss: -6.017503261566162
Iteration 4671:
Training Loss: -7.930792808532715
Reconstruction Loss: -6.0166168212890625
Iteration 4681:
Training Loss: -7.875877380371094
Reconstruction Loss: -6.01789665222168
Iteration 4691:
Training Loss: -7.848420143127441
Reconstruction Loss: -6.017484188079834
Iteration 4701:
Training Loss: -7.558398723602295
Reconstruction Loss: -6.017276763916016
Iteration 4711:
Training Loss: -7.407289981842041
Reconstruction Loss: -6.0183563232421875
Iteration 4721:
Training Loss: -7.560070514678955
Reconstruction Loss: -6.018208026885986
Iteration 4731:
Training Loss: -7.448243141174316
Reconstruction Loss: -6.019175052642822
Iteration 4741:
Training Loss: -7.714034557342529
Reconstruction Loss: -6.018564224243164
Iteration 4751:
Training Loss: -7.8056960105896
Reconstruction Loss: -6.01980447769165
Iteration 4761:
Training Loss: -7.820308685302734
Reconstruction Loss: -6.018727779388428
Iteration 4771:
Training Loss: -8.166257858276367
Reconstruction Loss: -6.019077301025391
Iteration 4781:
Training Loss: -7.578474521636963
Reconstruction Loss: -6.018761157989502
Iteration 4791:
Training Loss: -7.747675895690918
Reconstruction Loss: -6.020040512084961
Iteration 4801:
Training Loss: -7.776300430297852
Reconstruction Loss: -6.021073341369629
Iteration 4811:
Training Loss: -8.114718437194824
Reconstruction Loss: -6.02170467376709
Iteration 4821:
Training Loss: -7.72098445892334
Reconstruction Loss: -6.022507190704346
Iteration 4831:
Training Loss: -7.835309028625488
Reconstruction Loss: -6.022011756896973
Iteration 4841:
Training Loss: -7.893627643585205
Reconstruction Loss: -6.023453712463379
Iteration 4851:
Training Loss: -7.5916008949279785
Reconstruction Loss: -6.022306442260742
Iteration 4861:
Training Loss: -7.648813724517822
Reconstruction Loss: -6.022560119628906
Iteration 4871:
Training Loss: -7.979717254638672
Reconstruction Loss: -6.02376651763916
Iteration 4881:
Training Loss: -7.942902088165283
Reconstruction Loss: -6.022369861602783
Iteration 4891:
Training Loss: -7.481233596801758
Reconstruction Loss: -6.024398326873779
Iteration 4901:
Training Loss: -7.912646770477295
Reconstruction Loss: -6.02376651763916
Iteration 4911:
Training Loss: -7.886671543121338
Reconstruction Loss: -6.024524688720703
Iteration 4921:
Training Loss: -7.9011640548706055
Reconstruction Loss: -6.0243353843688965
Iteration 4931:
Training Loss: -7.814688205718994
Reconstruction Loss: -6.024676322937012
Iteration 4941:
Training Loss: -7.798715591430664
Reconstruction Loss: -6.025236129760742
Iteration 4951:
Training Loss: -7.810720920562744
Reconstruction Loss: -6.024479389190674
Iteration 4961:
Training Loss: -8.051405906677246
Reconstruction Loss: -6.025522708892822
Iteration 4971:
Training Loss: -7.8556952476501465
Reconstruction Loss: -6.026658058166504
Iteration 4981:
Training Loss: -7.8797783851623535
Reconstruction Loss: -6.026331424713135
Iteration 4991:
Training Loss: -8.115506172180176
Reconstruction Loss: -6.0265679359436035
