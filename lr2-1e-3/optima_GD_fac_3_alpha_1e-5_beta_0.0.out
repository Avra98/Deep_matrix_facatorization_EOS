5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.456722259521484
Reconstruction Loss: -0.49219220876693726
Iteration 101:
Training Loss: 5.456503868103027
Reconstruction Loss: -0.4922807812690735
Iteration 201:
Training Loss: 5.456173419952393
Reconstruction Loss: -0.49242743849754333
Iteration 301:
Training Loss: 5.45546293258667
Reconstruction Loss: -0.492777943611145
Iteration 401:
Training Loss: 5.453117370605469
Reconstruction Loss: -0.49404484033584595
Iteration 501:
Training Loss: 5.434410572052002
Reconstruction Loss: -0.5047355890274048
Iteration 601:
Training Loss: 4.784468650817871
Reconstruction Loss: -0.7291579842567444
Iteration 701:
Training Loss: 4.283824920654297
Reconstruction Loss: -0.844701886177063
Iteration 801:
Training Loss: 4.015097618103027
Reconstruction Loss: -0.9311131238937378
Iteration 901:
Training Loss: 3.7254388332366943
Reconstruction Loss: -1.0984240770339966
Iteration 1001:
Training Loss: 3.6491055488586426
Reconstruction Loss: -1.1356971263885498
Iteration 1101:
Training Loss: 3.609121322631836
Reconstruction Loss: -1.146089792251587
Iteration 1201:
Training Loss: 3.547663927078247
Reconstruction Loss: -1.1709262132644653
Iteration 1301:
Training Loss: 3.32381534576416
Reconstruction Loss: -1.2824301719665527
Iteration 1401:
Training Loss: 3.024029016494751
Reconstruction Loss: -1.471380591392517
Iteration 1501:
Training Loss: 2.8428852558135986
Reconstruction Loss: -1.6200436353683472
Iteration 1601:
Training Loss: 2.43658447265625
Reconstruction Loss: -1.9057449102401733
Iteration 1701:
Training Loss: 1.6753472089767456
Reconstruction Loss: -2.4902584552764893
Iteration 1801:
Training Loss: 0.8871588706970215
Reconstruction Loss: -3.162919759750366
Iteration 1901:
Training Loss: 0.0930090919137001
Reconstruction Loss: -3.8379130363464355
Iteration 2001:
Training Loss: -0.6742712259292603
Reconstruction Loss: -4.484932899475098
Iteration 2101:
Training Loss: -1.3812015056610107
Reconstruction Loss: -5.087169170379639
Iteration 2201:
Training Loss: -2.0204663276672363
Reconstruction Loss: -5.643250465393066
Iteration 2301:
Training Loss: -2.596517562866211
Reconstruction Loss: -6.157171726226807
Iteration 2401:
Training Loss: -3.113771677017212
Reconstruction Loss: -6.6326823234558105
Iteration 2501:
Training Loss: -3.5738682746887207
Reconstruction Loss: -7.071872711181641
Iteration 2601:
Training Loss: -3.9765045642852783
Reconstruction Loss: -7.475132465362549
Iteration 2701:
Training Loss: -4.321116924285889
Reconstruction Loss: -7.841501712799072
Iteration 2801:
Training Loss: -4.608526229858398
Reconstruction Loss: -8.169220924377441
Iteration 2901:
Training Loss: -4.841994285583496
Reconstruction Loss: -8.456515312194824
Iteration 3001:
Training Loss: -5.0273118019104
Reconstruction Loss: -8.70250415802002
Iteration 3101:
Training Loss: -5.171996593475342
Reconstruction Loss: -8.907909393310547
Iteration 3201:
Training Loss: -5.284062385559082
Reconstruction Loss: -9.075373649597168
Iteration 3301:
Training Loss: -5.371018409729004
Reconstruction Loss: -9.209190368652344
Iteration 3401:
Training Loss: -5.439222812652588
Reconstruction Loss: -9.314615249633789
Iteration 3501:
Training Loss: -5.493736743927002
Reconstruction Loss: -9.39712142944336
Iteration 3601:
Training Loss: -5.538360595703125
Reconstruction Loss: -9.461786270141602
Iteration 3701:
Training Loss: -5.5759100914001465
Reconstruction Loss: -9.512930870056152
Iteration 3801:
Training Loss: -5.608381748199463
Reconstruction Loss: -9.554055213928223
Iteration 3901:
Training Loss: -5.6372389793396
Reconstruction Loss: -9.587836265563965
Iteration 4001:
Training Loss: -5.663457870483398
Reconstruction Loss: -9.61631965637207
Iteration 4101:
Training Loss: -5.687747478485107
Reconstruction Loss: -9.640987396240234
Iteration 4201:
Training Loss: -5.710623741149902
Reconstruction Loss: -9.66291618347168
Iteration 4301:
Training Loss: -5.732407569885254
Reconstruction Loss: -9.682868957519531
Iteration 4401:
Training Loss: -5.753364086151123
Reconstruction Loss: -9.701417922973633
Iteration 4501:
Training Loss: -5.773662090301514
Reconstruction Loss: -9.71890926361084
Iteration 4601:
Training Loss: -5.793417453765869
Reconstruction Loss: -9.735658645629883
Iteration 4701:
Training Loss: -5.812710285186768
Reconstruction Loss: -9.751843452453613
Iteration 4801:
Training Loss: -5.8316330909729
Reconstruction Loss: -9.767590522766113
Iteration 4901:
Training Loss: -5.850200653076172
Reconstruction Loss: -9.783018112182617
Iteration 5001:
Training Loss: -5.868475437164307
Reconstruction Loss: -9.798155784606934
Iteration 5101:
Training Loss: -5.886465549468994
Reconstruction Loss: -9.81307315826416
Iteration 5201:
Training Loss: -5.904203414916992
Reconstruction Loss: -9.82779598236084
Iteration 5301:
Training Loss: -5.921693325042725
Reconstruction Loss: -9.842347145080566
Iteration 5401:
Training Loss: -5.938948631286621
Reconstruction Loss: -9.856727600097656
Iteration 5501:
Training Loss: -5.955979347229004
Reconstruction Loss: -9.870950698852539
Iteration 5601:
Training Loss: -5.9728102684021
Reconstruction Loss: -9.88502025604248
Iteration 5701:
Training Loss: -5.989422798156738
Reconstruction Loss: -9.898966789245605
Iteration 5801:
Training Loss: -6.005834102630615
Reconstruction Loss: -9.912747383117676
Iteration 5901:
Training Loss: -6.022069454193115
Reconstruction Loss: -9.926419258117676
Iteration 6001:
Training Loss: -6.038094997406006
Reconstruction Loss: -9.939952850341797
Iteration 6101:
Training Loss: -6.053941249847412
Reconstruction Loss: -9.953338623046875
Iteration 6201:
Training Loss: -6.069609642028809
Reconstruction Loss: -9.96658706665039
Iteration 6301:
Training Loss: -6.085094928741455
Reconstruction Loss: -9.979671478271484
Iteration 6401:
Training Loss: -6.100403308868408
Reconstruction Loss: -9.99262809753418
Iteration 6501:
Training Loss: -6.11557149887085
Reconstruction Loss: -10.005476951599121
Iteration 6601:
Training Loss: -6.130552768707275
Reconstruction Loss: -10.01817798614502
Iteration 6701:
Training Loss: -6.14539098739624
Reconstruction Loss: -10.030770301818848
Iteration 6801:
Training Loss: -6.160062789916992
Reconstruction Loss: -10.043259620666504
Iteration 6901:
Training Loss: -6.1745805740356445
Reconstruction Loss: -10.055596351623535
Iteration 7001:
Training Loss: -6.1889519691467285
Reconstruction Loss: -10.067800521850586
Iteration 7101:
Training Loss: -6.203166484832764
Reconstruction Loss: -10.079889297485352
Iteration 7201:
Training Loss: -6.2172322273254395
Reconstruction Loss: -10.091812133789062
Iteration 7301:
Training Loss: -6.23116397857666
Reconstruction Loss: -10.103612899780273
Iteration 7401:
Training Loss: -6.244964122772217
Reconstruction Loss: -10.115279197692871
Iteration 7501:
Training Loss: -6.258610725402832
Reconstruction Loss: -10.12682819366455
Iteration 7601:
Training Loss: -6.272135257720947
Reconstruction Loss: -10.138267517089844
Iteration 7701:
Training Loss: -6.285521984100342
Reconstruction Loss: -10.14963150024414
Iteration 7801:
Training Loss: -6.298778057098389
Reconstruction Loss: -10.160882949829102
Iteration 7901:
Training Loss: -6.311910629272461
Reconstruction Loss: -10.172017097473145
Iteration 8001:
Training Loss: -6.324926853179932
Reconstruction Loss: -10.183038711547852
Iteration 8101:
Training Loss: -6.337802886962891
Reconstruction Loss: -10.193939208984375
Iteration 8201:
Training Loss: -6.350576877593994
Reconstruction Loss: -10.204732894897461
Iteration 8301:
Training Loss: -6.363216400146484
Reconstruction Loss: -10.215453147888184
Iteration 8401:
Training Loss: -6.375744819641113
Reconstruction Loss: -10.226043701171875
Iteration 8501:
Training Loss: -6.388156414031982
Reconstruction Loss: -10.236542701721191
Iteration 8601:
Training Loss: -6.400466442108154
Reconstruction Loss: -10.246932029724121
Iteration 8701:
Training Loss: -6.412657737731934
Reconstruction Loss: -10.257218360900879
Iteration 8801:
Training Loss: -6.4247283935546875
Reconstruction Loss: -10.267443656921387
Iteration 8901:
Training Loss: -6.436721324920654
Reconstruction Loss: -10.277567863464355
Iteration 9001:
Training Loss: -6.448594570159912
Reconstruction Loss: -10.287583351135254
Iteration 9101:
Training Loss: -6.460360527038574
Reconstruction Loss: -10.297481536865234
Iteration 9201:
Training Loss: -6.472035884857178
Reconstruction Loss: -10.307296752929688
Iteration 9301:
Training Loss: -6.483607769012451
Reconstruction Loss: -10.31704330444336
Iteration 9401:
Training Loss: -6.495079040527344
Reconstruction Loss: -10.326687812805176
Iteration 9501:
Training Loss: -6.506447792053223
Reconstruction Loss: -10.336260795593262
Iteration 9601:
Training Loss: -6.517733573913574
Reconstruction Loss: -10.345708847045898
Iteration 9701:
Training Loss: -6.528918743133545
Reconstruction Loss: -10.355081558227539
Iteration 9801:
Training Loss: -6.540016174316406
Reconstruction Loss: -10.364405632019043
Iteration 9901:
Training Loss: -6.551020622253418
Reconstruction Loss: -10.373685836791992
Iteration 10001:
Training Loss: -6.561927795410156
Reconstruction Loss: -10.38286304473877
Iteration 10101:
Training Loss: -6.572755336761475
Reconstruction Loss: -10.391932487487793
Iteration 10201:
Training Loss: -6.583499431610107
Reconstruction Loss: -10.400919914245605
Iteration 10301:
Training Loss: -6.594148635864258
Reconstruction Loss: -10.409838676452637
Iteration 10401:
Training Loss: -6.604705333709717
Reconstruction Loss: -10.418681144714355
Iteration 10501:
Training Loss: -6.615201473236084
Reconstruction Loss: -10.427452087402344
Iteration 10601:
Training Loss: -6.625611305236816
Reconstruction Loss: -10.436140060424805
Iteration 10701:
Training Loss: -6.635923385620117
Reconstruction Loss: -10.444725036621094
Iteration 10801:
Training Loss: -6.646183967590332
Reconstruction Loss: -10.453274726867676
Iteration 10901:
Training Loss: -6.656342506408691
Reconstruction Loss: -10.461743354797363
Iteration 11001:
Training Loss: -6.666433811187744
Reconstruction Loss: -10.470170021057129
Iteration 11101:
Training Loss: -6.676454544067383
Reconstruction Loss: -10.478540420532227
Iteration 11201:
Training Loss: -6.686390399932861
Reconstruction Loss: -10.48684024810791
Iteration 11301:
Training Loss: -6.6962666511535645
Reconstruction Loss: -10.495059967041016
Iteration 11401:
Training Loss: -6.7060394287109375
Reconstruction Loss: -10.503209114074707
Iteration 11501:
Training Loss: -6.715770244598389
Reconstruction Loss: -10.511320114135742
Iteration 11601:
Training Loss: -6.725425720214844
Reconstruction Loss: -10.519366264343262
Iteration 11701:
Training Loss: -6.735013008117676
Reconstruction Loss: -10.527325630187988
Iteration 11801:
Training Loss: -6.744534492492676
Reconstruction Loss: -10.535222053527832
Iteration 11901:
Training Loss: -6.753984451293945
Reconstruction Loss: -10.54308795928955
Iteration 12001:
Training Loss: -6.763352870941162
Reconstruction Loss: -10.550882339477539
Iteration 12101:
Training Loss: -6.772663593292236
Reconstruction Loss: -10.558589935302734
Iteration 12201:
Training Loss: -6.781907081604004
Reconstruction Loss: -10.566252708435059
Iteration 12301:
Training Loss: -6.791094779968262
Reconstruction Loss: -10.5738525390625
Iteration 12401:
Training Loss: -6.800205230712891
Reconstruction Loss: -10.581417083740234
Iteration 12501:
Training Loss: -6.80927848815918
Reconstruction Loss: -10.58893871307373
Iteration 12601:
Training Loss: -6.818264007568359
Reconstruction Loss: -10.596413612365723
Iteration 12701:
Training Loss: -6.8272013664245605
Reconstruction Loss: -10.603830337524414
Iteration 12801:
Training Loss: -6.836071968078613
Reconstruction Loss: -10.611190795898438
Iteration 12901:
Training Loss: -6.844879627227783
Reconstruction Loss: -10.618468284606934
Iteration 13001:
Training Loss: -6.853630542755127
Reconstruction Loss: -10.625694274902344
Iteration 13101:
Training Loss: -6.862335681915283
Reconstruction Loss: -10.63288688659668
Iteration 13201:
Training Loss: -6.870976448059082
Reconstruction Loss: -10.640030860900879
Iteration 13301:
Training Loss: -6.879572868347168
Reconstruction Loss: -10.64713191986084
Iteration 13401:
Training Loss: -6.888095855712891
Reconstruction Loss: -10.654199600219727
Iteration 13501:
Training Loss: -6.896583080291748
Reconstruction Loss: -10.66122055053711
Iteration 13601:
Training Loss: -6.904971599578857
Reconstruction Loss: -10.668176651000977
Iteration 13701:
Training Loss: -6.913350582122803
Reconstruction Loss: -10.675095558166504
Iteration 13801:
Training Loss: -6.921659469604492
Reconstruction Loss: -10.681934356689453
Iteration 13901:
Training Loss: -6.929920196533203
Reconstruction Loss: -10.688706398010254
Iteration 14001:
Training Loss: -6.938114643096924
Reconstruction Loss: -10.695462226867676
Iteration 14101:
Training Loss: -6.9462666511535645
Reconstruction Loss: -10.702166557312012
Iteration 14201:
Training Loss: -6.954363822937012
Reconstruction Loss: -10.708819389343262
Iteration 14301:
Training Loss: -6.962423324584961
Reconstruction Loss: -10.715433120727539
Iteration 14401:
Training Loss: -6.970400333404541
Reconstruction Loss: -10.72201919555664
Iteration 14501:
Training Loss: -6.978360176086426
Reconstruction Loss: -10.728567123413086
Iteration 14601:
Training Loss: -6.986263275146484
Reconstruction Loss: -10.735076904296875
Iteration 14701:
Training Loss: -6.994123935699463
Reconstruction Loss: -10.741534233093262
Iteration 14801:
Training Loss: -7.001953125
Reconstruction Loss: -10.74795913696289
Iteration 14901:
Training Loss: -7.009716987609863
Reconstruction Loss: -10.754327774047852
