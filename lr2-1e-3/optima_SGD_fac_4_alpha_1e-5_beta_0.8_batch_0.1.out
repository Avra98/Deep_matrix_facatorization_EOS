5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.585485458374023
Reconstruction Loss: -0.483157217502594
Iteration 11:
Training Loss: 5.236610412597656
Reconstruction Loss: -0.4834926724433899
Iteration 21:
Training Loss: 5.526723384857178
Reconstruction Loss: -0.4839702844619751
Iteration 31:
Training Loss: 5.173932075500488
Reconstruction Loss: -0.48519760370254517
Iteration 41:
Training Loss: 5.338475227355957
Reconstruction Loss: -0.49363216757774353
Iteration 51:
Training Loss: 5.110888957977295
Reconstruction Loss: -0.5631066560745239
Iteration 61:
Training Loss: 3.8304407596588135
Reconstruction Loss: -0.9397569894790649
Iteration 71:
Training Loss: 4.036716461181641
Reconstruction Loss: -1.1516348123550415
Iteration 81:
Training Loss: 4.355441570281982
Reconstruction Loss: -1.1609671115875244
Iteration 91:
Training Loss: 3.8877952098846436
Reconstruction Loss: -1.1904782056808472
Iteration 101:
Training Loss: 3.102445602416992
Reconstruction Loss: -1.420504093170166
Iteration 111:
Training Loss: 3.268331527709961
Reconstruction Loss: -1.6245324611663818
Iteration 121:
Training Loss: 2.85115122795105
Reconstruction Loss: -1.7387235164642334
Iteration 131:
Training Loss: 3.2544925212860107
Reconstruction Loss: -1.7737377882003784
Iteration 141:
Training Loss: 3.3998031616210938
Reconstruction Loss: -1.8192641735076904
Iteration 151:
Training Loss: 2.9242358207702637
Reconstruction Loss: -1.8387248516082764
Iteration 161:
Training Loss: 3.144211530685425
Reconstruction Loss: -1.8798493146896362
Iteration 171:
Training Loss: 2.902559518814087
Reconstruction Loss: -1.9218649864196777
Iteration 181:
Training Loss: 2.887686252593994
Reconstruction Loss: -1.995295524597168
Iteration 191:
Training Loss: 2.0452680587768555
Reconstruction Loss: -2.3023505210876465
Iteration 201:
Training Loss: 0.9965476393699646
Reconstruction Loss: -3.303069829940796
Iteration 211:
Training Loss: 0.0899978056550026
Reconstruction Loss: -4.1417765617370605
Iteration 221:
Training Loss: -1.2405037879943848
Reconstruction Loss: -4.833742141723633
Iteration 231:
Training Loss: -1.7596888542175293
Reconstruction Loss: -5.433376789093018
Iteration 241:
Training Loss: -2.3053174018859863
Reconstruction Loss: -5.97452449798584
Iteration 251:
Training Loss: -2.9776132106781006
Reconstruction Loss: -6.488148212432861
Iteration 261:
Training Loss: -3.229189395904541
Reconstruction Loss: -6.975122451782227
Iteration 271:
Training Loss: -4.276859283447266
Reconstruction Loss: -7.437972068786621
Iteration 281:
Training Loss: -4.241994857788086
Reconstruction Loss: -7.869699478149414
Iteration 291:
Training Loss: -4.544163703918457
Reconstruction Loss: -8.28596305847168
Iteration 301:
Training Loss: -4.716864109039307
Reconstruction Loss: -8.66042423248291
Iteration 311:
Training Loss: -5.362947463989258
Reconstruction Loss: -9.012886047363281
Iteration 321:
Training Loss: -5.124319553375244
Reconstruction Loss: -9.326133728027344
Iteration 331:
Training Loss: -5.205199241638184
Reconstruction Loss: -9.602607727050781
Iteration 341:
Training Loss: -6.241176605224609
Reconstruction Loss: -9.836337089538574
Iteration 351:
Training Loss: -5.790100574493408
Reconstruction Loss: -10.003161430358887
Iteration 361:
Training Loss: -5.769479274749756
Reconstruction Loss: -10.151122093200684
Iteration 371:
Training Loss: -6.047118663787842
Reconstruction Loss: -10.273930549621582
Iteration 381:
Training Loss: -6.078439712524414
Reconstruction Loss: -10.367477416992188
Iteration 391:
Training Loss: -6.113998889923096
Reconstruction Loss: -10.422188758850098
Iteration 401:
Training Loss: -6.050645351409912
Reconstruction Loss: -10.462413787841797
Iteration 411:
Training Loss: -6.202521800994873
Reconstruction Loss: -10.511144638061523
Iteration 421:
Training Loss: -5.898104190826416
Reconstruction Loss: -10.55274772644043
Iteration 431:
Training Loss: -6.136191368103027
Reconstruction Loss: -10.562127113342285
Iteration 441:
Training Loss: -6.255349636077881
Reconstruction Loss: -10.572871208190918
Iteration 451:
Training Loss: -5.919026851654053
Reconstruction Loss: -10.589274406433105
Iteration 461:
Training Loss: -6.398376941680908
Reconstruction Loss: -10.585942268371582
Iteration 471:
Training Loss: -6.505108833312988
Reconstruction Loss: -10.616228103637695
Iteration 481:
Training Loss: -6.107750415802002
Reconstruction Loss: -10.614729881286621
Iteration 491:
Training Loss: -6.011909008026123
Reconstruction Loss: -10.622522354125977
Iteration 501:
Training Loss: -6.2641472816467285
Reconstruction Loss: -10.636565208435059
Iteration 511:
Training Loss: -6.292830467224121
Reconstruction Loss: -10.657912254333496
Iteration 521:
Training Loss: -6.279803276062012
Reconstruction Loss: -10.653618812561035
Iteration 531:
Training Loss: -6.3670172691345215
Reconstruction Loss: -10.660074234008789
Iteration 541:
Training Loss: -6.131424427032471
Reconstruction Loss: -10.675052642822266
Iteration 551:
Training Loss: -6.12841796875
Reconstruction Loss: -10.683429718017578
Iteration 561:
Training Loss: -6.422362804412842
Reconstruction Loss: -10.69150161743164
Iteration 571:
Training Loss: -6.091097354888916
Reconstruction Loss: -10.671849250793457
Iteration 581:
Training Loss: -6.322247505187988
Reconstruction Loss: -10.713473320007324
Iteration 591:
Training Loss: -6.159960746765137
Reconstruction Loss: -10.70029067993164
Iteration 601:
Training Loss: -6.418491363525391
Reconstruction Loss: -10.720701217651367
Iteration 611:
Training Loss: -6.7527289390563965
Reconstruction Loss: -10.724138259887695
Iteration 621:
Training Loss: -6.307937145233154
Reconstruction Loss: -10.712906837463379
Iteration 631:
Training Loss: -6.136473655700684
Reconstruction Loss: -10.741666793823242
Iteration 641:
Training Loss: -6.262340545654297
Reconstruction Loss: -10.722721099853516
Iteration 651:
Training Loss: -6.318513870239258
Reconstruction Loss: -10.738302230834961
Iteration 661:
Training Loss: -6.133514881134033
Reconstruction Loss: -10.742389678955078
Iteration 671:
Training Loss: -6.57443904876709
Reconstruction Loss: -10.768912315368652
Iteration 681:
Training Loss: -6.438327312469482
Reconstruction Loss: -10.78373908996582
Iteration 691:
Training Loss: -6.2999982833862305
Reconstruction Loss: -10.772988319396973
Iteration 701:
Training Loss: -6.849982738494873
Reconstruction Loss: -10.776833534240723
Iteration 711:
Training Loss: -6.596686363220215
Reconstruction Loss: -10.785438537597656
Iteration 721:
Training Loss: -6.138797760009766
Reconstruction Loss: -10.798849105834961
Iteration 731:
Training Loss: -5.977929592132568
Reconstruction Loss: -10.796622276306152
Iteration 741:
Training Loss: -6.6270952224731445
Reconstruction Loss: -10.791862487792969
Iteration 751:
Training Loss: -6.430270671844482
Reconstruction Loss: -10.81233024597168
Iteration 761:
Training Loss: -6.16475772857666
Reconstruction Loss: -10.819774627685547
Iteration 771:
Training Loss: -6.35311222076416
Reconstruction Loss: -10.819046020507812
Iteration 781:
Training Loss: -6.345929145812988
Reconstruction Loss: -10.826001167297363
Iteration 791:
Training Loss: -6.2909159660339355
Reconstruction Loss: -10.834973335266113
Iteration 801:
Training Loss: -6.174716472625732
Reconstruction Loss: -10.843719482421875
Iteration 811:
Training Loss: -6.83282995223999
Reconstruction Loss: -10.835043907165527
Iteration 821:
Training Loss: -6.735818862915039
Reconstruction Loss: -10.843585968017578
Iteration 831:
Training Loss: -6.957635879516602
Reconstruction Loss: -10.85738468170166
Iteration 841:
Training Loss: -6.148806095123291
Reconstruction Loss: -10.87352180480957
Iteration 851:
Training Loss: -6.572173118591309
Reconstruction Loss: -10.852520942687988
Iteration 861:
Training Loss: -6.362104415893555
Reconstruction Loss: -10.869546890258789
Iteration 871:
Training Loss: -6.456644058227539
Reconstruction Loss: -10.862903594970703
Iteration 881:
Training Loss: -6.657229900360107
Reconstruction Loss: -10.878948211669922
Iteration 891:
Training Loss: -6.613977432250977
Reconstruction Loss: -10.879121780395508
Iteration 901:
Training Loss: -6.360389709472656
Reconstruction Loss: -10.896101951599121
Iteration 911:
Training Loss: -6.249630928039551
Reconstruction Loss: -10.910977363586426
Iteration 921:
Training Loss: -6.259484767913818
Reconstruction Loss: -10.908754348754883
Iteration 931:
Training Loss: -6.467163562774658
Reconstruction Loss: -10.915337562561035
Iteration 941:
Training Loss: -6.305656909942627
Reconstruction Loss: -10.929437637329102
Iteration 951:
Training Loss: -6.8784871101379395
Reconstruction Loss: -10.922582626342773
Iteration 961:
Training Loss: -6.406414031982422
Reconstruction Loss: -10.927046775817871
Iteration 971:
Training Loss: -6.431101322174072
Reconstruction Loss: -10.943550109863281
Iteration 981:
Training Loss: -6.866713523864746
Reconstruction Loss: -10.957883834838867
Iteration 991:
Training Loss: -6.642172336578369
Reconstruction Loss: -10.953943252563477
Iteration 1001:
Training Loss: -6.327408790588379
Reconstruction Loss: -10.940362930297852
Iteration 1011:
Training Loss: -6.670041084289551
Reconstruction Loss: -10.954771995544434
Iteration 1021:
Training Loss: -6.504418849945068
Reconstruction Loss: -10.975424766540527
Iteration 1031:
Training Loss: -6.646574020385742
Reconstruction Loss: -10.963425636291504
Iteration 1041:
Training Loss: -6.5843186378479
Reconstruction Loss: -10.966965675354004
Iteration 1051:
Training Loss: -6.705438137054443
Reconstruction Loss: -10.972833633422852
Iteration 1061:
Training Loss: -6.683854103088379
Reconstruction Loss: -10.96949291229248
Iteration 1071:
Training Loss: -7.075744152069092
Reconstruction Loss: -11.00261116027832
Iteration 1081:
Training Loss: -7.12909460067749
Reconstruction Loss: -10.991707801818848
Iteration 1091:
Training Loss: -6.525379657745361
Reconstruction Loss: -10.971712112426758
Iteration 1101:
Training Loss: -7.058027744293213
Reconstruction Loss: -11.000061988830566
Iteration 1111:
Training Loss: -6.56561803817749
Reconstruction Loss: -11.017027854919434
Iteration 1121:
Training Loss: -6.495046138763428
Reconstruction Loss: -11.008480072021484
Iteration 1131:
Training Loss: -6.407752990722656
Reconstruction Loss: -11.009185791015625
Iteration 1141:
Training Loss: -6.714147090911865
Reconstruction Loss: -11.015316009521484
Iteration 1151:
Training Loss: -6.781787872314453
Reconstruction Loss: -11.026657104492188
Iteration 1161:
Training Loss: -6.5974555015563965
Reconstruction Loss: -11.0352783203125
Iteration 1171:
Training Loss: -6.219867706298828
Reconstruction Loss: -11.02193832397461
Iteration 1181:
Training Loss: -6.3810133934021
Reconstruction Loss: -11.051531791687012
Iteration 1191:
Training Loss: -6.6834564208984375
Reconstruction Loss: -11.032489776611328
Iteration 1201:
Training Loss: -6.5085272789001465
Reconstruction Loss: -11.041617393493652
Iteration 1211:
Training Loss: -6.785096168518066
Reconstruction Loss: -11.049219131469727
Iteration 1221:
Training Loss: -7.090864658355713
Reconstruction Loss: -11.0601167678833
Iteration 1231:
Training Loss: -6.682207107543945
Reconstruction Loss: -11.065256118774414
Iteration 1241:
Training Loss: -6.898278713226318
Reconstruction Loss: -11.069579124450684
Iteration 1251:
Training Loss: -6.6020002365112305
Reconstruction Loss: -11.062101364135742
Iteration 1261:
Training Loss: -6.779743671417236
Reconstruction Loss: -11.087804794311523
Iteration 1271:
Training Loss: -6.527047634124756
Reconstruction Loss: -11.085314750671387
Iteration 1281:
Training Loss: -6.760612964630127
Reconstruction Loss: -11.090563774108887
Iteration 1291:
Training Loss: -6.835826873779297
Reconstruction Loss: -11.089138984680176
Iteration 1301:
Training Loss: -6.626603126525879
Reconstruction Loss: -11.084117889404297
Iteration 1311:
Training Loss: -6.626945495605469
Reconstruction Loss: -11.106603622436523
Iteration 1321:
Training Loss: -6.793333053588867
Reconstruction Loss: -11.11691665649414
Iteration 1331:
Training Loss: -7.010550022125244
Reconstruction Loss: -11.105104446411133
Iteration 1341:
Training Loss: -7.727323532104492
Reconstruction Loss: -11.110156059265137
Iteration 1351:
Training Loss: -6.740671157836914
Reconstruction Loss: -11.115289688110352
Iteration 1361:
Training Loss: -6.655783176422119
Reconstruction Loss: -11.12501335144043
Iteration 1371:
Training Loss: -6.729259967803955
Reconstruction Loss: -11.119935035705566
Iteration 1381:
Training Loss: -6.8662614822387695
Reconstruction Loss: -11.119665145874023
Iteration 1391:
Training Loss: -6.901582717895508
Reconstruction Loss: -11.134997367858887
Iteration 1401:
Training Loss: -6.8345232009887695
Reconstruction Loss: -11.161661148071289
Iteration 1411:
Training Loss: -7.634657859802246
Reconstruction Loss: -11.131936073303223
Iteration 1421:
Training Loss: -7.029874324798584
Reconstruction Loss: -11.139006614685059
Iteration 1431:
Training Loss: -7.13142204284668
Reconstruction Loss: -11.147454261779785
Iteration 1441:
Training Loss: -6.78662109375
Reconstruction Loss: -11.16855525970459
Iteration 1451:
Training Loss: -6.979000568389893
Reconstruction Loss: -11.167442321777344
Iteration 1461:
Training Loss: -6.6384148597717285
Reconstruction Loss: -11.153037071228027
Iteration 1471:
Training Loss: -6.849911689758301
Reconstruction Loss: -11.157086372375488
Iteration 1481:
Training Loss: -6.777629375457764
Reconstruction Loss: -11.180846214294434
Iteration 1491:
Training Loss: -6.77167272567749
Reconstruction Loss: -11.17123031616211
