5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.692763805389404
Reconstruction Loss: -0.44504663348197937
Iteration 51:
Training Loss: 5.617203235626221
Reconstruction Loss: -0.44547635316848755
Iteration 101:
Training Loss: 5.724819183349609
Reconstruction Loss: -0.44659432768821716
Iteration 151:
Training Loss: 5.596340179443359
Reconstruction Loss: -0.45546048879623413
Iteration 201:
Training Loss: 5.091533660888672
Reconstruction Loss: -0.5325945615768433
Iteration 251:
Training Loss: 4.53565788269043
Reconstruction Loss: -0.6322034001350403
Iteration 301:
Training Loss: 3.873746156692505
Reconstruction Loss: -0.7970283627510071
Iteration 351:
Training Loss: 3.8722400665283203
Reconstruction Loss: -0.7746536135673523
Iteration 401:
Training Loss: 3.7936794757843018
Reconstruction Loss: -0.8139251470565796
Iteration 451:
Training Loss: 3.3129751682281494
Reconstruction Loss: -1.041001796722412
Iteration 501:
Training Loss: 3.2175331115722656
Reconstruction Loss: -1.1854933500289917
Iteration 551:
Training Loss: 3.237278699874878
Reconstruction Loss: -1.2784035205841064
Iteration 601:
Training Loss: 3.0758769512176514
Reconstruction Loss: -1.3276602029800415
Iteration 651:
Training Loss: 3.0925159454345703
Reconstruction Loss: -1.3866159915924072
Iteration 701:
Training Loss: 2.6825592517852783
Reconstruction Loss: -1.555858850479126
Iteration 751:
Training Loss: 1.7015025615692139
Reconstruction Loss: -2.06605863571167
Iteration 801:
Training Loss: 1.1555355787277222
Reconstruction Loss: -2.6557023525238037
Iteration 851:
Training Loss: 0.3163260817527771
Reconstruction Loss: -3.234710216522217
Iteration 901:
Training Loss: -0.19783125817775726
Reconstruction Loss: -3.8180394172668457
Iteration 951:
Training Loss: -0.8071836233139038
Reconstruction Loss: -4.416214942932129
Iteration 1001:
Training Loss: -1.466321349143982
Reconstruction Loss: -5.027657508850098
Iteration 1051:
Training Loss: -2.1585805416107178
Reconstruction Loss: -5.651326656341553
Iteration 1101:
Training Loss: -2.7179410457611084
Reconstruction Loss: -6.279996395111084
Iteration 1151:
Training Loss: -3.5896341800689697
Reconstruction Loss: -6.915220737457275
Iteration 1201:
Training Loss: -3.9753611087799072
Reconstruction Loss: -7.5388569831848145
Iteration 1251:
Training Loss: -4.659084320068359
Reconstruction Loss: -8.160283088684082
Iteration 1301:
Training Loss: -5.2472357749938965
Reconstruction Loss: -8.775550842285156
Iteration 1351:
Training Loss: -5.793405055999756
Reconstruction Loss: -9.374804496765137
Iteration 1401:
Training Loss: -6.377557754516602
Reconstruction Loss: -9.962211608886719
Iteration 1451:
Training Loss: -6.875702381134033
Reconstruction Loss: -10.529886245727539
Iteration 1501:
Training Loss: -7.337917804718018
Reconstruction Loss: -11.059368133544922
Iteration 1551:
Training Loss: -7.680136680603027
Reconstruction Loss: -11.534737586975098
Iteration 1601:
Training Loss: -8.114989280700684
Reconstruction Loss: -11.933091163635254
Iteration 1651:
Training Loss: -8.27481460571289
Reconstruction Loss: -12.248906135559082
Iteration 1701:
Training Loss: -8.425532341003418
Reconstruction Loss: -12.47057056427002
Iteration 1751:
Training Loss: -8.610084533691406
Reconstruction Loss: -12.617977142333984
Iteration 1801:
Training Loss: -8.625139236450195
Reconstruction Loss: -12.70443344116211
Iteration 1851:
Training Loss: -8.487464904785156
Reconstruction Loss: -12.75291633605957
Iteration 1901:
Training Loss: -8.615097999572754
Reconstruction Loss: -12.779932022094727
Iteration 1951:
Training Loss: -8.679930686950684
Reconstruction Loss: -12.801318168640137
Iteration 2001:
Training Loss: -8.650897026062012
Reconstruction Loss: -12.804474830627441
Iteration 2051:
Training Loss: -8.794879913330078
Reconstruction Loss: -12.811483383178711
Iteration 2101:
Training Loss: -8.61784839630127
Reconstruction Loss: -12.807660102844238
Iteration 2151:
Training Loss: -8.737841606140137
Reconstruction Loss: -12.81263542175293
Iteration 2201:
Training Loss: -8.623869895935059
Reconstruction Loss: -12.816717147827148
Iteration 2251:
Training Loss: -8.777599334716797
Reconstruction Loss: -12.817692756652832
Iteration 2301:
Training Loss: -8.63271713256836
Reconstruction Loss: -12.819197654724121
Iteration 2351:
Training Loss: -8.557212829589844
Reconstruction Loss: -12.813756942749023
Iteration 2401:
Training Loss: -8.7863130569458
Reconstruction Loss: -12.819283485412598
Iteration 2451:
Training Loss: -8.826282501220703
Reconstruction Loss: -12.820723533630371
Iteration 2501:
Training Loss: -8.748346328735352
Reconstruction Loss: -12.826437950134277
Iteration 2551:
Training Loss: -8.792844772338867
Reconstruction Loss: -12.82249641418457
Iteration 2601:
Training Loss: -8.676790237426758
Reconstruction Loss: -12.824786186218262
Iteration 2651:
Training Loss: -8.680342674255371
Reconstruction Loss: -12.820927619934082
Iteration 2701:
Training Loss: -8.753774642944336
Reconstruction Loss: -12.829289436340332
Iteration 2751:
Training Loss: -8.763781547546387
Reconstruction Loss: -12.82789421081543
Iteration 2801:
Training Loss: -8.601053237915039
Reconstruction Loss: -12.835619926452637
Iteration 2851:
Training Loss: -8.635574340820312
Reconstruction Loss: -12.835083961486816
Iteration 2901:
Training Loss: -8.683557510375977
Reconstruction Loss: -12.833993911743164
Iteration 2951:
Training Loss: -8.813526153564453
Reconstruction Loss: -12.839624404907227
Iteration 3001:
Training Loss: -8.733830451965332
Reconstruction Loss: -12.8409423828125
Iteration 3051:
Training Loss: -8.567991256713867
Reconstruction Loss: -12.83770751953125
Iteration 3101:
Training Loss: -8.668153762817383
Reconstruction Loss: -12.835614204406738
Iteration 3151:
Training Loss: -8.61401081085205
Reconstruction Loss: -12.84156608581543
Iteration 3201:
Training Loss: -8.658453941345215
Reconstruction Loss: -12.842844009399414
Iteration 3251:
Training Loss: -8.683956146240234
Reconstruction Loss: -12.845273971557617
Iteration 3301:
Training Loss: -8.61274528503418
Reconstruction Loss: -12.84581184387207
Iteration 3351:
Training Loss: -8.748673439025879
Reconstruction Loss: -12.848650932312012
Iteration 3401:
Training Loss: -8.743115425109863
Reconstruction Loss: -12.848982810974121
Iteration 3451:
Training Loss: -8.68295669555664
Reconstruction Loss: -12.846259117126465
Iteration 3501:
Training Loss: -8.84917163848877
Reconstruction Loss: -12.852590560913086
Iteration 3551:
Training Loss: -8.726612091064453
Reconstruction Loss: -12.850789070129395
Iteration 3601:
Training Loss: -8.727862358093262
Reconstruction Loss: -12.853821754455566
Iteration 3651:
Training Loss: -8.739250183105469
Reconstruction Loss: -12.853438377380371
Iteration 3701:
Training Loss: -8.689727783203125
Reconstruction Loss: -12.857547760009766
Iteration 3751:
Training Loss: -8.78511905670166
Reconstruction Loss: -12.8613862991333
Iteration 3801:
Training Loss: -8.751619338989258
Reconstruction Loss: -12.856029510498047
Iteration 3851:
Training Loss: -8.651723861694336
Reconstruction Loss: -12.860182762145996
Iteration 3901:
Training Loss: -8.658729553222656
Reconstruction Loss: -12.856703758239746
Iteration 3951:
Training Loss: -8.729207038879395
Reconstruction Loss: -12.860455513000488
Iteration 4001:
Training Loss: -8.710661888122559
Reconstruction Loss: -12.864251136779785
Iteration 4051:
Training Loss: -8.854273796081543
Reconstruction Loss: -12.86129379272461
Iteration 4101:
Training Loss: -8.800246238708496
Reconstruction Loss: -12.869950294494629
Iteration 4151:
Training Loss: -8.804008483886719
Reconstruction Loss: -12.87186336517334
Iteration 4201:
Training Loss: -8.812216758728027
Reconstruction Loss: -12.868683815002441
Iteration 4251:
Training Loss: -8.74465560913086
Reconstruction Loss: -12.871230125427246
Iteration 4301:
Training Loss: -8.722343444824219
Reconstruction Loss: -12.873133659362793
Iteration 4351:
Training Loss: -8.80307674407959
Reconstruction Loss: -12.873470306396484
Iteration 4401:
Training Loss: -8.740154266357422
Reconstruction Loss: -12.875398635864258
Iteration 4451:
Training Loss: -8.752862930297852
Reconstruction Loss: -12.871525764465332
Iteration 4501:
Training Loss: -8.739142417907715
Reconstruction Loss: -12.878593444824219
Iteration 4551:
Training Loss: -8.735136985778809
Reconstruction Loss: -12.88168716430664
Iteration 4601:
Training Loss: -8.806148529052734
Reconstruction Loss: -12.88060188293457
Iteration 4651:
Training Loss: -8.743620872497559
Reconstruction Loss: -12.879060745239258
Iteration 4701:
Training Loss: -8.833427429199219
Reconstruction Loss: -12.87904167175293
Iteration 4751:
Training Loss: -8.739293098449707
Reconstruction Loss: -12.88860034942627
Iteration 4801:
Training Loss: -8.78421688079834
Reconstruction Loss: -12.888137817382812
Iteration 4851:
Training Loss: -8.829075813293457
Reconstruction Loss: -12.890144348144531
Iteration 4901:
Training Loss: -8.750645637512207
Reconstruction Loss: -12.886117935180664
Iteration 4951:
Training Loss: -8.75415325164795
Reconstruction Loss: -12.886519432067871
Iteration 5001:
Training Loss: -8.807509422302246
Reconstruction Loss: -12.887813568115234
Iteration 5051:
Training Loss: -8.783365249633789
Reconstruction Loss: -12.894166946411133
Iteration 5101:
Training Loss: -8.821710586547852
Reconstruction Loss: -12.893427848815918
Iteration 5151:
Training Loss: -8.632833480834961
Reconstruction Loss: -12.894793510437012
Iteration 5201:
Training Loss: -8.614143371582031
Reconstruction Loss: -12.895864486694336
Iteration 5251:
Training Loss: -8.790084838867188
Reconstruction Loss: -12.898019790649414
Iteration 5301:
Training Loss: -8.827882766723633
Reconstruction Loss: -12.899913787841797
Iteration 5351:
Training Loss: -8.7525053024292
Reconstruction Loss: -12.902336120605469
Iteration 5401:
Training Loss: -8.74472713470459
Reconstruction Loss: -12.90084457397461
Iteration 5451:
Training Loss: -8.695145606994629
Reconstruction Loss: -12.899967193603516
Iteration 5501:
Training Loss: -8.79647159576416
Reconstruction Loss: -12.904263496398926
Iteration 5551:
Training Loss: -8.981481552124023
Reconstruction Loss: -12.901250839233398
Iteration 5601:
Training Loss: -8.926652908325195
Reconstruction Loss: -12.903716087341309
Iteration 5651:
Training Loss: -8.873653411865234
Reconstruction Loss: -12.906876564025879
Iteration 5701:
Training Loss: -8.799659729003906
Reconstruction Loss: -12.914179801940918
Iteration 5751:
Training Loss: -8.8053617477417
Reconstruction Loss: -12.905869483947754
Iteration 5801:
Training Loss: -8.8506498336792
Reconstruction Loss: -12.914021492004395
Iteration 5851:
Training Loss: -8.726146697998047
Reconstruction Loss: -12.909210205078125
Iteration 5901:
Training Loss: -8.790140151977539
Reconstruction Loss: -12.918288230895996
Iteration 5951:
Training Loss: -8.884284973144531
Reconstruction Loss: -12.918835639953613
Iteration 6001:
Training Loss: -8.702897071838379
Reconstruction Loss: -12.920502662658691
Iteration 6051:
Training Loss: -8.929262161254883
Reconstruction Loss: -12.9170560836792
Iteration 6101:
Training Loss: -8.777891159057617
Reconstruction Loss: -12.918566703796387
Iteration 6151:
Training Loss: -8.64568042755127
Reconstruction Loss: -12.920400619506836
Iteration 6201:
Training Loss: -8.84174919128418
Reconstruction Loss: -12.9236421585083
Iteration 6251:
Training Loss: -8.829771995544434
Reconstruction Loss: -12.921480178833008
Iteration 6301:
Training Loss: -8.806182861328125
Reconstruction Loss: -12.925691604614258
Iteration 6351:
Training Loss: -8.862945556640625
Reconstruction Loss: -12.9309720993042
Iteration 6401:
Training Loss: -8.916481018066406
Reconstruction Loss: -12.928701400756836
Iteration 6451:
Training Loss: -8.850807189941406
Reconstruction Loss: -12.9244384765625
Iteration 6501:
Training Loss: -8.742847442626953
Reconstruction Loss: -12.933318138122559
Iteration 6551:
Training Loss: -8.88861083984375
Reconstruction Loss: -12.931998252868652
Iteration 6601:
Training Loss: -8.866166114807129
Reconstruction Loss: -12.935172080993652
Iteration 6651:
Training Loss: -8.772540092468262
Reconstruction Loss: -12.94078254699707
Iteration 6701:
Training Loss: -8.794260025024414
Reconstruction Loss: -12.933743476867676
Iteration 6751:
Training Loss: -8.81705093383789
Reconstruction Loss: -12.936845779418945
Iteration 6801:
Training Loss: -8.88526725769043
Reconstruction Loss: -12.931767463684082
Iteration 6851:
Training Loss: -8.756836891174316
Reconstruction Loss: -12.940784454345703
Iteration 6901:
Training Loss: -8.863901138305664
Reconstruction Loss: -12.937259674072266
Iteration 6951:
Training Loss: -8.770323753356934
Reconstruction Loss: -12.938260078430176
Iteration 7001:
Training Loss: -8.762748718261719
Reconstruction Loss: -12.941121101379395
Iteration 7051:
Training Loss: -8.810323715209961
Reconstruction Loss: -12.946203231811523
Iteration 7101:
Training Loss: -8.843759536743164
Reconstruction Loss: -12.941961288452148
Iteration 7151:
Training Loss: -8.825434684753418
Reconstruction Loss: -12.951163291931152
Iteration 7201:
Training Loss: -8.894804000854492
Reconstruction Loss: -12.947332382202148
Iteration 7251:
Training Loss: -8.834680557250977
Reconstruction Loss: -12.945409774780273
Iteration 7301:
Training Loss: -8.699309349060059
Reconstruction Loss: -12.950737953186035
Iteration 7351:
Training Loss: -8.865534782409668
Reconstruction Loss: -12.953327178955078
Iteration 7401:
Training Loss: -8.72602367401123
Reconstruction Loss: -12.955358505249023
Iteration 7451:
Training Loss: -8.68667984008789
Reconstruction Loss: -12.958481788635254
