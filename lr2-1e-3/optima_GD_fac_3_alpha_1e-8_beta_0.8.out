5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 101:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 201:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 301:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 401:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 501:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 601:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 701:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 801:
Training Loss: 5.458161354064941
Reconstruction Loss: -0.5819788575172424
Iteration 901:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819790363311768
Iteration 1001:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819790363311768
Iteration 1101:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819790363311768
Iteration 1201:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 1301:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 1401:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 1501:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 1601:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 1701:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 1801:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 1901:
Training Loss: 5.458160877227783
Reconstruction Loss: -0.5819791555404663
Iteration 2001:
Training Loss: 5.458160400390625
Reconstruction Loss: -0.5819791555404663
Iteration 2101:
Training Loss: 5.458160400390625
Reconstruction Loss: -0.5819791555404663
Iteration 2201:
Training Loss: 5.458160400390625
Reconstruction Loss: -0.5819792747497559
Iteration 2301:
Training Loss: 5.458160400390625
Reconstruction Loss: -0.5819792747497559
Iteration 2401:
Training Loss: 5.458160400390625
Reconstruction Loss: -0.5819793939590454
Iteration 2501:
Training Loss: 5.458160400390625
Reconstruction Loss: -0.5819793939590454
Iteration 2601:
Training Loss: 5.458160400390625
Reconstruction Loss: -0.5819793939590454
Iteration 2701:
Training Loss: 5.458159923553467
Reconstruction Loss: -0.5819795727729797
Iteration 2801:
Training Loss: 5.458159923553467
Reconstruction Loss: -0.5819795727729797
Iteration 2901:
Training Loss: 5.458159923553467
Reconstruction Loss: -0.5819796919822693
Iteration 3001:
Training Loss: 5.458159446716309
Reconstruction Loss: -0.5819798111915588
Iteration 3101:
Training Loss: 5.458159446716309
Reconstruction Loss: -0.5819798707962036
Iteration 3201:
Training Loss: 5.45815896987915
Reconstruction Loss: -0.5819798707962036
Iteration 3301:
Training Loss: 5.45815896987915
Reconstruction Loss: -0.5819801092147827
Iteration 3401:
Training Loss: 5.458158493041992
Reconstruction Loss: -0.5819802284240723
Iteration 3501:
Training Loss: 5.458158016204834
Reconstruction Loss: -0.5819805264472961
Iteration 3601:
Training Loss: 5.458157539367676
Reconstruction Loss: -0.5819807648658752
Iteration 3701:
Training Loss: 5.458157062530518
Reconstruction Loss: -0.5819808840751648
Iteration 3801:
Training Loss: 5.458156585693359
Reconstruction Loss: -0.5819813013076782
Iteration 3901:
Training Loss: 5.458155632019043
Reconstruction Loss: -0.5819818377494812
Iteration 4001:
Training Loss: 5.458154678344727
Reconstruction Loss: -0.5819823741912842
Iteration 4101:
Training Loss: 5.458153247833252
Reconstruction Loss: -0.5819829106330872
Iteration 4201:
Training Loss: 5.458151817321777
Reconstruction Loss: -0.5819839239120483
Iteration 4301:
Training Loss: 5.458149433135986
Reconstruction Loss: -0.5819852352142334
Iteration 4401:
Training Loss: 5.458146095275879
Reconstruction Loss: -0.5819869637489319
Iteration 4501:
Training Loss: 5.458141326904297
Reconstruction Loss: -0.581989586353302
Iteration 4601:
Training Loss: 5.458133697509766
Reconstruction Loss: -0.5819934606552124
Iteration 4701:
Training Loss: 5.4581217765808105
Reconstruction Loss: -0.5820000767707825
Iteration 4801:
Training Loss: 5.458099842071533
Reconstruction Loss: -0.5820116996765137
Iteration 4901:
Training Loss: 5.458056449890137
Reconstruction Loss: -0.582034707069397
Iteration 5001:
Training Loss: 5.4579548835754395
Reconstruction Loss: -0.5820887088775635
Iteration 5101:
Training Loss: 5.45764684677124
Reconstruction Loss: -0.5822525024414062
Iteration 5201:
Training Loss: 5.456145286560059
Reconstruction Loss: -0.5830490589141846
Iteration 5301:
Training Loss: 5.431150436401367
Reconstruction Loss: -0.596115231513977
Iteration 5401:
Training Loss: 4.9749321937561035
Reconstruction Loss: -0.6907181739807129
Iteration 5501:
Training Loss: 4.950991153717041
Reconstruction Loss: -0.6704843640327454
Iteration 5601:
Training Loss: 4.944326877593994
Reconstruction Loss: -0.6609218120574951
Iteration 5701:
Training Loss: 4.942368984222412
Reconstruction Loss: -0.6589107513427734
Iteration 5801:
Training Loss: 4.941770553588867
Reconstruction Loss: -0.6600948572158813
Iteration 5901:
Training Loss: 4.941479682922363
Reconstruction Loss: -0.6620298027992249
Iteration 6001:
Training Loss: 4.941190719604492
Reconstruction Loss: -0.6639503836631775
Iteration 6101:
Training Loss: 4.940666198730469
Reconstruction Loss: -0.6657938957214355
Iteration 6201:
Training Loss: 4.939119338989258
Reconstruction Loss: -0.6680222749710083
Iteration 6301:
Training Loss: 4.930698394775391
Reconstruction Loss: -0.6740432381629944
Iteration 6401:
Training Loss: 4.773012638092041
Reconstruction Loss: -0.7586528658866882
Iteration 6501:
Training Loss: 4.484715938568115
Reconstruction Loss: -0.8466410636901855
Iteration 6601:
Training Loss: 4.460349082946777
Reconstruction Loss: -0.8268959522247314
Iteration 6701:
Training Loss: 4.450612545013428
Reconstruction Loss: -0.8156284689903259
Iteration 6801:
Training Loss: 4.4440412521362305
Reconstruction Loss: -0.8088505268096924
Iteration 6901:
Training Loss: 4.43875789642334
Reconstruction Loss: -0.8043549060821533
Iteration 7001:
Training Loss: 4.434211254119873
Reconstruction Loss: -0.801803469657898
Iteration 7101:
Training Loss: 4.429782867431641
Reconstruction Loss: -0.8013896346092224
Iteration 7201:
Training Loss: 4.423530101776123
Reconstruction Loss: -0.8039186596870422
Iteration 7301:
Training Loss: 4.402862071990967
Reconstruction Loss: -0.816166877746582
Iteration 7401:
Training Loss: 4.157989025115967
Reconstruction Loss: -0.9484742879867554
Iteration 7501:
Training Loss: 3.7475175857543945
Reconstruction Loss: -1.1448527574539185
Iteration 7601:
Training Loss: 3.6808040142059326
Reconstruction Loss: -1.1527596712112427
Iteration 7701:
Training Loss: 3.6592423915863037
Reconstruction Loss: -1.1415026187896729
Iteration 7801:
Training Loss: 3.6508073806762695
Reconstruction Loss: -1.1314934492111206
Iteration 7901:
Training Loss: 3.646374464035034
Reconstruction Loss: -1.1249595880508423
Iteration 8001:
Training Loss: 3.6430563926696777
Reconstruction Loss: -1.1209439039230347
Iteration 8101:
Training Loss: 3.6393206119537354
Reconstruction Loss: -1.1188228130340576
Iteration 8201:
Training Loss: 3.632678747177124
Reconstruction Loss: -1.1193273067474365
Iteration 8301:
Training Loss: 3.6136326789855957
Reconstruction Loss: -1.1277613639831543
Iteration 8401:
Training Loss: 3.518521308898926
Reconstruction Loss: -1.1797453165054321
Iteration 8501:
Training Loss: 3.012151002883911
Reconstruction Loss: -1.4696472883224487
Iteration 8601:
Training Loss: 2.718975067138672
Reconstruction Loss: -1.719951868057251
Iteration 8701:
Training Loss: 2.63559627532959
Reconstruction Loss: -1.8312631845474243
Iteration 8801:
Training Loss: 2.6039867401123047
Reconstruction Loss: -1.8906731605529785
Iteration 8901:
Training Loss: 2.590357542037964
Reconstruction Loss: -1.9235752820968628
Iteration 9001:
Training Loss: 2.583390712738037
Reconstruction Loss: -1.942637324333191
Iteration 9101:
Training Loss: 2.5792133808135986
Reconstruction Loss: -1.9541445970535278
Iteration 9201:
Training Loss: 2.576409101486206
Reconstruction Loss: -1.9612743854522705
Iteration 9301:
Training Loss: 2.574380397796631
Reconstruction Loss: -1.965728998184204
Iteration 9401:
Training Loss: 2.5728342533111572
Reconstruction Loss: -1.9684728384017944
Iteration 9501:
Training Loss: 2.571608781814575
Reconstruction Loss: -1.9700740575790405
Iteration 9601:
Training Loss: 2.5706069469451904
Reconstruction Loss: -1.970879316329956
Iteration 9701:
Training Loss: 2.5697665214538574
Reconstruction Loss: -1.9711085557937622
Iteration 9801:
Training Loss: 2.569047212600708
Reconstruction Loss: -1.9709056615829468
Iteration 9901:
Training Loss: 2.56842041015625
Reconstruction Loss: -1.9703707695007324
Iteration 10001:
Training Loss: 2.5678672790527344
Reconstruction Loss: -1.9695756435394287
Iteration 10101:
Training Loss: 2.5673723220825195
Reconstruction Loss: -1.9685744047164917
Iteration 10201:
Training Loss: 2.5669260025024414
Reconstruction Loss: -1.9674092531204224
Iteration 10301:
Training Loss: 2.566519260406494
Reconstruction Loss: -1.9661134481430054
Iteration 10401:
Training Loss: 2.5661461353302
Reconstruction Loss: -1.964714765548706
Iteration 10501:
Training Loss: 2.5658013820648193
Reconstruction Loss: -1.9632349014282227
Iteration 10601:
Training Loss: 2.5654807090759277
Reconstruction Loss: -1.961691975593567
Iteration 10701:
Training Loss: 2.565180778503418
Reconstruction Loss: -1.960100769996643
Iteration 10801:
Training Loss: 2.56489896774292
Reconstruction Loss: -1.9584734439849854
Iteration 10901:
Training Loss: 2.5646324157714844
Reconstruction Loss: -1.9568191766738892
Iteration 11001:
Training Loss: 2.5643792152404785
Reconstruction Loss: -1.955146074295044
Iteration 11101:
Training Loss: 2.5641374588012695
Reconstruction Loss: -1.95345938205719
Iteration 11201:
Training Loss: 2.5639054775238037
Reconstruction Loss: -1.9517643451690674
Iteration 11301:
Training Loss: 2.5636820793151855
Reconstruction Loss: -1.9500641822814941
Iteration 11401:
Training Loss: 2.563464879989624
Reconstruction Loss: -1.9483613967895508
Iteration 11501:
Training Loss: 2.563253879547119
Reconstruction Loss: -1.946657657623291
Iteration 11601:
Training Loss: 2.563046932220459
Reconstruction Loss: -1.9449543952941895
Iteration 11701:
Training Loss: 2.5628433227539062
Reconstruction Loss: -1.9432518482208252
Iteration 11801:
Training Loss: 2.5626413822174072
Reconstruction Loss: -1.9415507316589355
Iteration 11901:
Training Loss: 2.5624401569366455
Reconstruction Loss: -1.9398508071899414
Iteration 12001:
Training Loss: 2.562237501144409
Reconstruction Loss: -1.9381520748138428
Iteration 12101:
Training Loss: 2.562032461166382
Reconstruction Loss: -1.9364546537399292
Iteration 12201:
Training Loss: 2.5618231296539307
Reconstruction Loss: -1.934757947921753
Iteration 12301:
Training Loss: 2.5616068840026855
Reconstruction Loss: -1.9330623149871826
Iteration 12401:
Training Loss: 2.561380624771118
Reconstruction Loss: -1.9313679933547974
Iteration 12501:
Training Loss: 2.5611412525177
Reconstruction Loss: -1.9296760559082031
Iteration 12601:
Training Loss: 2.5608835220336914
Reconstruction Loss: -1.9279879331588745
Iteration 12701:
Training Loss: 2.560601234436035
Reconstruction Loss: -1.9263067245483398
Iteration 12801:
Training Loss: 2.5602853298187256
Reconstruction Loss: -1.9246374368667603
Iteration 12901:
Training Loss: 2.5599231719970703
Reconstruction Loss: -1.9229872226715088
Iteration 13001:
Training Loss: 2.5594964027404785
Reconstruction Loss: -1.92136812210083
Iteration 13101:
Training Loss: 2.558976650238037
Reconstruction Loss: -1.919798493385315
Iteration 13201:
Training Loss: 2.558321714401245
Reconstruction Loss: -1.9183088541030884
Iteration 13301:
Training Loss: 2.5574610233306885
Reconstruction Loss: -1.9169485569000244
Iteration 13401:
Training Loss: 2.5562779903411865
Reconstruction Loss: -1.9158021211624146
Iteration 13501:
Training Loss: 2.5545642375946045
Reconstruction Loss: -1.9150182008743286
Iteration 13601:
Training Loss: 2.551929235458374
Reconstruction Loss: -1.9148766994476318
Iteration 13701:
Training Loss: 2.547578811645508
Reconstruction Loss: -1.915939211845398
Iteration 13801:
Training Loss: 2.5397448539733887
Reconstruction Loss: -1.919439673423767
Iteration 13901:
Training Loss: 2.5239923000335693
Reconstruction Loss: -1.9284179210662842
Iteration 14001:
Training Loss: 2.487393379211426
Reconstruction Loss: -1.9515422582626343
Iteration 14101:
Training Loss: 2.3853020668029785
Reconstruction Loss: -2.0180130004882812
Iteration 14201:
Training Loss: 2.058540105819702
Reconstruction Loss: -2.230720043182373
Iteration 14301:
Training Loss: 1.3109092712402344
Reconstruction Loss: -2.743093252182007
Iteration 14401:
Training Loss: 0.5261372923851013
Reconstruction Loss: -3.355229139328003
Iteration 14501:
Training Loss: -0.13686710596084595
Reconstruction Loss: -3.915785551071167
Iteration 14601:
Training Loss: -0.7341765761375427
Reconstruction Loss: -4.440171718597412
Iteration 14701:
Training Loss: -1.2924069166183472
Reconstruction Loss: -4.940817356109619
Iteration 14801:
Training Loss: -1.8219459056854248
Reconstruction Loss: -5.422633647918701
Iteration 14901:
Training Loss: -2.327805280685425
Reconstruction Loss: -5.888157844543457
