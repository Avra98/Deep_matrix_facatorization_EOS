5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.315743923187256
Reconstruction Loss: -0.6467143893241882
Iteration 51:
Training Loss: 5.3600263595581055
Reconstruction Loss: -0.6467145085334778
Iteration 101:
Training Loss: 5.301283836364746
Reconstruction Loss: -0.6467145085334778
Iteration 151:
Training Loss: 5.235812664031982
Reconstruction Loss: -0.6467145085334778
Iteration 201:
Training Loss: 5.395595073699951
Reconstruction Loss: -0.6467145085334778
Iteration 251:
Training Loss: 5.17713737487793
Reconstruction Loss: -0.6467146277427673
Iteration 301:
Training Loss: 5.384601593017578
Reconstruction Loss: -0.6467146277427673
Iteration 351:
Training Loss: 5.269100666046143
Reconstruction Loss: -0.6467146277427673
Iteration 401:
Training Loss: 5.2951483726501465
Reconstruction Loss: -0.6467147469520569
Iteration 451:
Training Loss: 5.371965408325195
Reconstruction Loss: -0.6467147469520569
Iteration 501:
Training Loss: 5.236295223236084
Reconstruction Loss: -0.6467147469520569
Iteration 551:
Training Loss: 5.355937957763672
Reconstruction Loss: -0.6467147469520569
Iteration 601:
Training Loss: 5.293614387512207
Reconstruction Loss: -0.6467148661613464
Iteration 651:
Training Loss: 5.3988423347473145
Reconstruction Loss: -0.6467148661613464
Iteration 701:
Training Loss: 5.351151466369629
Reconstruction Loss: -0.6467148661613464
Iteration 751:
Training Loss: 5.347468376159668
Reconstruction Loss: -0.6467148661613464
Iteration 801:
Training Loss: 5.300414562225342
Reconstruction Loss: -0.6467150449752808
Iteration 851:
Training Loss: 5.365277290344238
Reconstruction Loss: -0.6467150449752808
Iteration 901:
Training Loss: 5.311112880706787
Reconstruction Loss: -0.6467150449752808
Iteration 951:
Training Loss: 5.371853828430176
Reconstruction Loss: -0.6467150449752808
Iteration 1001:
Training Loss: 5.313187599182129
Reconstruction Loss: -0.6467151641845703
Iteration 1051:
Training Loss: 5.3356733322143555
Reconstruction Loss: -0.6467151641845703
Iteration 1101:
Training Loss: 5.420233249664307
Reconstruction Loss: -0.6467150449752808
Iteration 1151:
Training Loss: 5.294338226318359
Reconstruction Loss: -0.6467151641845703
Iteration 1201:
Training Loss: 5.29640007019043
Reconstruction Loss: -0.6467152833938599
Iteration 1251:
Training Loss: 5.249305248260498
Reconstruction Loss: -0.6467152833938599
Iteration 1301:
Training Loss: 5.2445855140686035
Reconstruction Loss: -0.6467154026031494
Iteration 1351:
Training Loss: 5.28985595703125
Reconstruction Loss: -0.6467154026031494
Iteration 1401:
Training Loss: 5.343804836273193
Reconstruction Loss: -0.6467154026031494
Iteration 1451:
Training Loss: 5.229862213134766
Reconstruction Loss: -0.646715521812439
Iteration 1501:
Training Loss: 5.199771881103516
Reconstruction Loss: -0.646715521812439
Iteration 1551:
Training Loss: 5.393772125244141
Reconstruction Loss: -0.6467156410217285
Iteration 1601:
Training Loss: 5.386039733886719
Reconstruction Loss: -0.6467156410217285
Iteration 1651:
Training Loss: 5.2751545906066895
Reconstruction Loss: -0.6467156410217285
Iteration 1701:
Training Loss: 5.165740489959717
Reconstruction Loss: -0.6467157602310181
Iteration 1751:
Training Loss: 5.216839790344238
Reconstruction Loss: -0.6467157602310181
Iteration 1801:
Training Loss: 5.346765518188477
Reconstruction Loss: -0.6467158794403076
Iteration 1851:
Training Loss: 5.24327278137207
Reconstruction Loss: -0.6467159986495972
Iteration 1901:
Training Loss: 5.355734348297119
Reconstruction Loss: -0.6467159986495972
Iteration 1951:
Training Loss: 5.240400791168213
Reconstruction Loss: -0.6467161178588867
Iteration 2001:
Training Loss: 5.359899520874023
Reconstruction Loss: -0.6467161178588867
Iteration 2051:
Training Loss: 5.263638019561768
Reconstruction Loss: -0.6467162370681763
Iteration 2101:
Training Loss: 5.318417072296143
Reconstruction Loss: -0.6467162370681763
Iteration 2151:
Training Loss: 5.172386169433594
Reconstruction Loss: -0.6467162370681763
Iteration 2201:
Training Loss: 5.274989128112793
Reconstruction Loss: -0.6467162370681763
Iteration 2251:
Training Loss: 5.513598442077637
Reconstruction Loss: -0.6467163562774658
Iteration 2301:
Training Loss: 5.380789756774902
Reconstruction Loss: -0.6467164158821106
Iteration 2351:
Training Loss: 5.371016979217529
Reconstruction Loss: -0.6467164158821106
Iteration 2401:
Training Loss: 5.305146217346191
Reconstruction Loss: -0.6467164158821106
Iteration 2451:
Training Loss: 5.256677150726318
Reconstruction Loss: -0.6467166543006897
Iteration 2501:
Training Loss: 5.317234992980957
Reconstruction Loss: -0.6467167735099792
Iteration 2551:
Training Loss: 5.275808334350586
Reconstruction Loss: -0.6467167735099792
Iteration 2601:
Training Loss: 5.430952072143555
Reconstruction Loss: -0.6467168927192688
Iteration 2651:
Training Loss: 5.295039176940918
Reconstruction Loss: -0.6467170119285583
Iteration 2701:
Training Loss: 5.355286121368408
Reconstruction Loss: -0.6467170119285583
Iteration 2751:
Training Loss: 5.297651290893555
Reconstruction Loss: -0.6467170119285583
Iteration 2801:
Training Loss: 5.31834077835083
Reconstruction Loss: -0.6467172503471375
Iteration 2851:
Training Loss: 5.284431457519531
Reconstruction Loss: -0.646717369556427
Iteration 2901:
Training Loss: 5.2552947998046875
Reconstruction Loss: -0.646717369556427
Iteration 2951:
Training Loss: 5.245453834533691
Reconstruction Loss: -0.6467174887657166
Iteration 3001:
Training Loss: 5.297094345092773
Reconstruction Loss: -0.6467176675796509
Iteration 3051:
Training Loss: 5.280677795410156
Reconstruction Loss: -0.6467177867889404
Iteration 3101:
Training Loss: 5.296623229980469
Reconstruction Loss: -0.64671790599823
Iteration 3151:
Training Loss: 5.37135124206543
Reconstruction Loss: -0.6467180252075195
Iteration 3201:
Training Loss: 5.195517063140869
Reconstruction Loss: -0.6467180252075195
Iteration 3251:
Training Loss: 5.36594295501709
Reconstruction Loss: -0.6467181444168091
Iteration 3301:
Training Loss: 5.2188720703125
Reconstruction Loss: -0.6467183828353882
Iteration 3351:
Training Loss: 5.242457389831543
Reconstruction Loss: -0.6467185020446777
Iteration 3401:
Training Loss: 5.135217189788818
Reconstruction Loss: -0.6467186212539673
Iteration 3451:
Training Loss: 5.388332366943359
Reconstruction Loss: -0.6467187404632568
Iteration 3501:
Training Loss: 5.285370826721191
Reconstruction Loss: -0.6467188596725464
Iteration 3551:
Training Loss: 5.344504356384277
Reconstruction Loss: -0.6467190384864807
Iteration 3601:
Training Loss: 5.416813850402832
Reconstruction Loss: -0.6467192769050598
Iteration 3651:
Training Loss: 5.2867045402526855
Reconstruction Loss: -0.6467193961143494
Iteration 3701:
Training Loss: 5.277998924255371
Reconstruction Loss: -0.6467195153236389
Iteration 3751:
Training Loss: 5.256843090057373
Reconstruction Loss: -0.646719753742218
Iteration 3801:
Training Loss: 5.140066146850586
Reconstruction Loss: -0.6467198729515076
Iteration 3851:
Training Loss: 5.185708999633789
Reconstruction Loss: -0.6467200517654419
Iteration 3901:
Training Loss: 5.322462558746338
Reconstruction Loss: -0.6467204093933105
Iteration 3951:
Training Loss: 5.118747234344482
Reconstruction Loss: -0.6467205286026001
Iteration 4001:
Training Loss: 5.348787307739258
Reconstruction Loss: -0.6467208862304688
Iteration 4051:
Training Loss: 5.364890098571777
Reconstruction Loss: -0.6467211246490479
Iteration 4101:
Training Loss: 5.298483848571777
Reconstruction Loss: -0.646721363067627
Iteration 4151:
Training Loss: 5.249007225036621
Reconstruction Loss: -0.6467216610908508
Iteration 4201:
Training Loss: 5.36957311630249
Reconstruction Loss: -0.6467218995094299
Iteration 4251:
Training Loss: 5.320580959320068
Reconstruction Loss: -0.6467223763465881
Iteration 4301:
Training Loss: 5.285935401916504
Reconstruction Loss: -0.646722674369812
Iteration 4351:
Training Loss: 5.2262797355651855
Reconstruction Loss: -0.6467229127883911
Iteration 4401:
Training Loss: 5.322643280029297
Reconstruction Loss: -0.6467235088348389
Iteration 4451:
Training Loss: 5.376029968261719
Reconstruction Loss: -0.6467239260673523
Iteration 4501:
Training Loss: 5.281025409698486
Reconstruction Loss: -0.646724283695221
Iteration 4551:
Training Loss: 5.275511741638184
Reconstruction Loss: -0.6467248797416687
Iteration 4601:
Training Loss: 5.3969316482543945
Reconstruction Loss: -0.6467254161834717
Iteration 4651:
Training Loss: 5.296858310699463
Reconstruction Loss: -0.6467260122299194
Iteration 4701:
Training Loss: 5.361232757568359
Reconstruction Loss: -0.646726667881012
Iteration 4751:
Training Loss: 5.322798252105713
Reconstruction Loss: -0.6467273831367493
Iteration 4801:
Training Loss: 5.196192741394043
Reconstruction Loss: -0.6467280387878418
Iteration 4851:
Training Loss: 5.351833820343018
Reconstruction Loss: -0.6467289328575134
Iteration 4901:
Training Loss: 5.1878814697265625
Reconstruction Loss: -0.6467300057411194
Iteration 4951:
Training Loss: 5.308004379272461
Reconstruction Loss: -0.646730899810791
Iteration 5001:
Training Loss: 5.256638526916504
Reconstruction Loss: -0.6467320322990417
Iteration 5051:
Training Loss: 5.393637657165527
Reconstruction Loss: -0.6467334032058716
Iteration 5101:
Training Loss: 5.2266764640808105
Reconstruction Loss: -0.6467346549034119
Iteration 5151:
Training Loss: 5.473633766174316
Reconstruction Loss: -0.6467362642288208
Iteration 5201:
Training Loss: 5.339188575744629
Reconstruction Loss: -0.6467380523681641
Iteration 5251:
Training Loss: 5.273183822631836
Reconstruction Loss: -0.646740198135376
Iteration 5301:
Training Loss: 5.324150085449219
Reconstruction Loss: -0.646742582321167
Iteration 5351:
Training Loss: 5.288632392883301
Reconstruction Loss: -0.6467454433441162
Iteration 5401:
Training Loss: 5.231393814086914
Reconstruction Loss: -0.6467487812042236
Iteration 5451:
Training Loss: 5.388611316680908
Reconstruction Loss: -0.646752655506134
Iteration 5501:
Training Loss: 5.338026523590088
Reconstruction Loss: -0.6467573046684265
Iteration 5551:
Training Loss: 5.349129676818848
Reconstruction Loss: -0.6467630863189697
Iteration 5601:
Training Loss: 5.507565975189209
Reconstruction Loss: -0.6467702388763428
Iteration 5651:
Training Loss: 5.389996528625488
Reconstruction Loss: -0.6467793583869934
Iteration 5701:
Training Loss: 5.388123035430908
Reconstruction Loss: -0.6467913389205933
Iteration 5751:
Training Loss: 5.263116359710693
Reconstruction Loss: -0.6468073725700378
Iteration 5801:
Training Loss: 5.261723041534424
Reconstruction Loss: -0.6468299031257629
Iteration 5851:
Training Loss: 5.449621677398682
Reconstruction Loss: -0.6468631625175476
Iteration 5901:
Training Loss: 5.277924060821533
Reconstruction Loss: -0.64691561460495
Iteration 5951:
Training Loss: 5.3815836906433105
Reconstruction Loss: -0.6470072269439697
Iteration 6001:
Training Loss: 5.244571208953857
Reconstruction Loss: -0.6471932530403137
Iteration 6051:
Training Loss: 5.29436731338501
Reconstruction Loss: -0.6476855874061584
Iteration 6101:
Training Loss: 5.302577972412109
Reconstruction Loss: -0.6501544713973999
Iteration 6151:
Training Loss: 4.806121826171875
Reconstruction Loss: -0.7483935356140137
Iteration 6201:
Training Loss: 4.740298271179199
Reconstruction Loss: -0.7038905620574951
Iteration 6251:
Training Loss: 4.6338934898376465
Reconstruction Loss: -0.7296063899993896
Iteration 6301:
Training Loss: 4.722715377807617
Reconstruction Loss: -0.7310848236083984
Iteration 6351:
Training Loss: 4.559023380279541
Reconstruction Loss: -0.7610779404640198
Iteration 6401:
Training Loss: 4.622946262359619
Reconstruction Loss: -0.7496244311332703
Iteration 6451:
Training Loss: 4.747802257537842
Reconstruction Loss: -0.758270263671875
Iteration 6501:
Training Loss: 4.638202667236328
Reconstruction Loss: -0.7589441537857056
Iteration 6551:
Training Loss: 4.760213375091553
Reconstruction Loss: -0.744927704334259
Iteration 6601:
Training Loss: 4.67324161529541
Reconstruction Loss: -0.7680215239524841
Iteration 6651:
Training Loss: 4.769024848937988
Reconstruction Loss: -0.7429212331771851
Iteration 6701:
Training Loss: 4.8123345375061035
Reconstruction Loss: -0.7675037384033203
Iteration 6751:
Training Loss: 4.693046569824219
Reconstruction Loss: -0.7521960139274597
Iteration 6801:
Training Loss: 4.80877161026001
Reconstruction Loss: -0.770431637763977
Iteration 6851:
Training Loss: 4.5833964347839355
Reconstruction Loss: -0.7649640440940857
Iteration 6901:
Training Loss: 4.773560047149658
Reconstruction Loss: -0.7701624631881714
Iteration 6951:
Training Loss: 4.852156639099121
Reconstruction Loss: -0.7602910399436951
Iteration 7001:
Training Loss: 4.685590744018555
Reconstruction Loss: -0.7549359202384949
Iteration 7051:
Training Loss: 4.6480889320373535
Reconstruction Loss: -0.757976770401001
Iteration 7101:
Training Loss: 4.603082656860352
Reconstruction Loss: -0.7583479881286621
Iteration 7151:
Training Loss: 4.828574180603027
Reconstruction Loss: -0.7751353979110718
Iteration 7201:
Training Loss: 4.471250534057617
Reconstruction Loss: -0.7669152021408081
Iteration 7251:
Training Loss: 4.607378005981445
Reconstruction Loss: -0.7611578702926636
Iteration 7301:
Training Loss: 4.7877607345581055
Reconstruction Loss: -0.7639716267585754
Iteration 7351:
Training Loss: 4.757171154022217
Reconstruction Loss: -0.7724066376686096
Iteration 7401:
Training Loss: 4.6727190017700195
Reconstruction Loss: -0.754479169845581
Iteration 7451:
Training Loss: 4.684420585632324
Reconstruction Loss: -0.7781777381896973
Iteration 7501:
Training Loss: 4.734279155731201
Reconstruction Loss: -0.7715623378753662
Iteration 7551:
Training Loss: 4.766088485717773
Reconstruction Loss: -0.7731675505638123
Iteration 7601:
Training Loss: 4.699901580810547
Reconstruction Loss: -0.765417218208313
Iteration 7651:
Training Loss: 4.615898132324219
Reconstruction Loss: -0.7642918825149536
Iteration 7701:
Training Loss: 4.640972137451172
Reconstruction Loss: -0.7498974800109863
Iteration 7751:
Training Loss: 4.669127464294434
Reconstruction Loss: -0.7586895227432251
Iteration 7801:
Training Loss: 4.60887336730957
Reconstruction Loss: -0.7529439926147461
Iteration 7851:
Training Loss: 4.865024089813232
Reconstruction Loss: -0.7713941335678101
Iteration 7901:
Training Loss: 4.776616096496582
Reconstruction Loss: -0.7707545757293701
Iteration 7951:
Training Loss: 4.610999584197998
Reconstruction Loss: -0.7649914026260376
Iteration 8001:
Training Loss: 4.7563395500183105
Reconstruction Loss: -0.7670800685882568
Iteration 8051:
Training Loss: 4.776977062225342
Reconstruction Loss: -0.7626689672470093
Iteration 8101:
Training Loss: 4.799191474914551
Reconstruction Loss: -0.7690708637237549
Iteration 8151:
Training Loss: 4.690765857696533
Reconstruction Loss: -0.7668739557266235
Iteration 8201:
Training Loss: 4.661683559417725
Reconstruction Loss: -0.7684633731842041
Iteration 8251:
Training Loss: 4.67549991607666
Reconstruction Loss: -0.7547134757041931
Iteration 8301:
Training Loss: 4.615536689758301
Reconstruction Loss: -0.772598922252655
Iteration 8351:
Training Loss: 4.340432643890381
Reconstruction Loss: -0.9014254808425903
Iteration 8401:
Training Loss: 4.095778942108154
Reconstruction Loss: -0.9191746115684509
Iteration 8451:
Training Loss: 4.202001094818115
Reconstruction Loss: -0.8682448863983154
Iteration 8501:
Training Loss: 4.053069114685059
Reconstruction Loss: -0.8512086272239685
Iteration 8551:
Training Loss: 4.13089656829834
Reconstruction Loss: -0.8174440264701843
Iteration 8601:
Training Loss: 4.056169033050537
Reconstruction Loss: -0.840898871421814
Iteration 8651:
Training Loss: 4.200336933135986
Reconstruction Loss: -0.8270053863525391
Iteration 8701:
Training Loss: 4.004843235015869
Reconstruction Loss: -0.816431999206543
Iteration 8751:
Training Loss: 4.0971174240112305
Reconstruction Loss: -0.8046876192092896
Iteration 8801:
Training Loss: 4.167735576629639
Reconstruction Loss: -0.8160558938980103
Iteration 8851:
Training Loss: 4.080556392669678
Reconstruction Loss: -0.819796085357666
Iteration 8901:
Training Loss: 3.9560508728027344
Reconstruction Loss: -0.8138468265533447
Iteration 8951:
Training Loss: 4.190577983856201
Reconstruction Loss: -0.8155159950256348
Iteration 9001:
Training Loss: 4.207726955413818
Reconstruction Loss: -0.8073953986167908
Iteration 9051:
Training Loss: 4.126509189605713
Reconstruction Loss: -0.8159425854682922
Iteration 9101:
Training Loss: 4.189563274383545
Reconstruction Loss: -0.8180326223373413
Iteration 9151:
Training Loss: 3.9895060062408447
Reconstruction Loss: -0.8026785254478455
Iteration 9201:
Training Loss: 4.040844917297363
Reconstruction Loss: -0.8231825828552246
Iteration 9251:
Training Loss: 4.104122638702393
Reconstruction Loss: -0.810171902179718
Iteration 9301:
Training Loss: 4.179377555847168
Reconstruction Loss: -0.8016253113746643
Iteration 9351:
Training Loss: 4.0586395263671875
Reconstruction Loss: -0.8126684427261353
Iteration 9401:
Training Loss: 4.068877220153809
Reconstruction Loss: -0.8165656328201294
Iteration 9451:
Training Loss: 4.097420692443848
Reconstruction Loss: -0.8004574179649353
Iteration 9501:
Training Loss: 4.096652030944824
Reconstruction Loss: -0.812889575958252
Iteration 9551:
Training Loss: 4.016623020172119
Reconstruction Loss: -0.8075458407402039
Iteration 9601:
Training Loss: 4.14412260055542
Reconstruction Loss: -0.8226950168609619
Iteration 9651:
Training Loss: 4.036617279052734
Reconstruction Loss: -0.8029253482818604
Iteration 9701:
Training Loss: 3.9378480911254883
Reconstruction Loss: -0.7996293306350708
Iteration 9751:
Training Loss: 4.024188995361328
Reconstruction Loss: -0.8069114685058594
Iteration 9801:
Training Loss: 4.057229518890381
Reconstruction Loss: -0.8140518069267273
Iteration 9851:
Training Loss: 4.091273784637451
Reconstruction Loss: -0.8138722777366638
Iteration 9901:
Training Loss: 4.074765682220459
Reconstruction Loss: -0.8094286322593689
Iteration 9951:
Training Loss: 3.9969124794006348
Reconstruction Loss: -0.828426718711853
Iteration 10001:
Training Loss: 4.110990047454834
Reconstruction Loss: -0.8229402303695679
Iteration 10051:
Training Loss: 4.123024940490723
Reconstruction Loss: -0.8256185054779053
Iteration 10101:
Training Loss: 4.145610332489014
Reconstruction Loss: -0.7996252775192261
Iteration 10151:
Training Loss: 4.075637340545654
Reconstruction Loss: -0.8065070509910583
Iteration 10201:
Training Loss: 4.085521697998047
Reconstruction Loss: -0.8047759532928467
Iteration 10251:
Training Loss: 4.066587448120117
Reconstruction Loss: -0.8008065223693848
Iteration 10301:
Training Loss: 4.040014743804932
Reconstruction Loss: -0.8084366321563721
Iteration 10351:
Training Loss: 4.076767444610596
Reconstruction Loss: -0.8236892223358154
Iteration 10401:
Training Loss: 4.147267818450928
Reconstruction Loss: -0.8157995939254761
Iteration 10451:
Training Loss: 4.111373424530029
Reconstruction Loss: -0.7913070917129517
Iteration 10501:
Training Loss: 4.078979015350342
Reconstruction Loss: -0.8127574324607849
Iteration 10551:
Training Loss: 4.090963840484619
Reconstruction Loss: -0.8208693265914917
Iteration 10601:
Training Loss: 4.061859130859375
Reconstruction Loss: -0.8121909499168396
Iteration 10651:
Training Loss: 4.172309398651123
Reconstruction Loss: -0.8196194171905518
Iteration 10701:
Training Loss: 4.0612688064575195
Reconstruction Loss: -0.8110302686691284
Iteration 10751:
Training Loss: 4.0781049728393555
Reconstruction Loss: -0.8114577531814575
Iteration 10801:
Training Loss: 4.100515365600586
Reconstruction Loss: -0.8198531270027161
Iteration 10851:
Training Loss: 4.0927886962890625
Reconstruction Loss: -0.8077718615531921
Iteration 10901:
Training Loss: 4.123848915100098
Reconstruction Loss: -0.822695791721344
Iteration 10951:
Training Loss: 4.056976318359375
Reconstruction Loss: -0.8124215602874756
Iteration 11001:
Training Loss: 3.9659197330474854
Reconstruction Loss: -0.81024169921875
Iteration 11051:
Training Loss: 4.1785736083984375
Reconstruction Loss: -0.8249094486236572
Iteration 11101:
Training Loss: 4.039266586303711
Reconstruction Loss: -0.8094416856765747
Iteration 11151:
Training Loss: 4.093016147613525
Reconstruction Loss: -0.8237322568893433
Iteration 11201:
Training Loss: 4.112200736999512
Reconstruction Loss: -0.7932009100914001
Iteration 11251:
Training Loss: 4.0506463050842285
Reconstruction Loss: -0.8143423795700073
Iteration 11301:
Training Loss: 4.122664928436279
Reconstruction Loss: -0.8068339824676514
Iteration 11351:
Training Loss: 4.107219696044922
Reconstruction Loss: -0.8144662976264954
Iteration 11401:
Training Loss: 3.9827702045440674
Reconstruction Loss: -0.8194776177406311
Iteration 11451:
Training Loss: 4.082493305206299
Reconstruction Loss: -0.8165121078491211
Iteration 11501:
Training Loss: 3.950126886367798
Reconstruction Loss: -0.8128999471664429
Iteration 11551:
Training Loss: 4.08528995513916
Reconstruction Loss: -0.8227651119232178
Iteration 11601:
Training Loss: 4.02215051651001
Reconstruction Loss: -0.8113614320755005
Iteration 11651:
Training Loss: 3.9934158325195312
Reconstruction Loss: -0.8079448342323303
Iteration 11701:
Training Loss: 3.950878381729126
Reconstruction Loss: -0.8110811710357666
Iteration 11751:
Training Loss: 4.000644207000732
Reconstruction Loss: -0.8272348642349243
Iteration 11801:
Training Loss: 4.019755840301514
Reconstruction Loss: -0.8190447092056274
Iteration 11851:
Training Loss: 4.140478134155273
Reconstruction Loss: -0.8024008274078369
Iteration 11901:
Training Loss: 4.166663646697998
Reconstruction Loss: -0.8108562231063843
Iteration 11951:
Training Loss: 4.158298492431641
Reconstruction Loss: -0.80857253074646
Iteration 12001:
Training Loss: 4.106329441070557
Reconstruction Loss: -0.8053815960884094
Iteration 12051:
Training Loss: 4.108482837677002
Reconstruction Loss: -0.8074290156364441
Iteration 12101:
Training Loss: 4.070055961608887
Reconstruction Loss: -0.8105952739715576
Iteration 12151:
Training Loss: 4.154128074645996
Reconstruction Loss: -0.8092706203460693
Iteration 12201:
Training Loss: 4.016511917114258
Reconstruction Loss: -0.8116718530654907
Iteration 12251:
Training Loss: 4.034354209899902
Reconstruction Loss: -0.8031275868415833
Iteration 12301:
Training Loss: 4.086118221282959
Reconstruction Loss: -0.7991399168968201
Iteration 12351:
Training Loss: 4.036192417144775
Reconstruction Loss: -0.8083807229995728
Iteration 12401:
Training Loss: 3.9601669311523438
Reconstruction Loss: -0.7931142449378967
Iteration 12451:
Training Loss: 4.120939254760742
Reconstruction Loss: -0.8047293424606323
Iteration 12501:
Training Loss: 4.0833353996276855
Reconstruction Loss: -0.8117339015007019
Iteration 12551:
Training Loss: 4.076380252838135
Reconstruction Loss: -0.8132205605506897
Iteration 12601:
Training Loss: 4.156624794006348
Reconstruction Loss: -0.8234774470329285
Iteration 12651:
Training Loss: 4.1585373878479
Reconstruction Loss: -0.8036667108535767
Iteration 12701:
Training Loss: 4.007069110870361
Reconstruction Loss: -0.8232555389404297
Iteration 12751:
Training Loss: 4.066201210021973
Reconstruction Loss: -0.8097957968711853
Iteration 12801:
Training Loss: 3.9076123237609863
Reconstruction Loss: -0.8162840604782104
Iteration 12851:
Training Loss: 4.120886325836182
Reconstruction Loss: -0.7990820407867432
Iteration 12901:
Training Loss: 4.10481595993042
Reconstruction Loss: -0.8201359510421753
Iteration 12951:
Training Loss: 4.091076850891113
Reconstruction Loss: -0.8132107257843018
Iteration 13001:
Training Loss: 4.157597541809082
Reconstruction Loss: -0.8152137994766235
Iteration 13051:
Training Loss: 3.9058117866516113
Reconstruction Loss: -0.8247661590576172
Iteration 13101:
Training Loss: 4.232959270477295
Reconstruction Loss: -0.8134768605232239
Iteration 13151:
Training Loss: 4.022904872894287
Reconstruction Loss: -0.7977958917617798
Iteration 13201:
Training Loss: 4.060291767120361
Reconstruction Loss: -0.8054702281951904
Iteration 13251:
Training Loss: 4.176346778869629
Reconstruction Loss: -0.7979899048805237
Iteration 13301:
Training Loss: 4.020064830780029
Reconstruction Loss: -0.8208193182945251
Iteration 13351:
Training Loss: 4.053418159484863
Reconstruction Loss: -0.8215745687484741
Iteration 13401:
Training Loss: 4.062297344207764
Reconstruction Loss: -0.7905011773109436
Iteration 13451:
Training Loss: 3.951841354370117
Reconstruction Loss: -0.8043580055236816
Iteration 13501:
Training Loss: 4.141507625579834
Reconstruction Loss: -0.8283506035804749
Iteration 13551:
Training Loss: 4.126700401306152
Reconstruction Loss: -0.8052630424499512
Iteration 13601:
Training Loss: 4.115922927856445
Reconstruction Loss: -0.8177587985992432
Iteration 13651:
Training Loss: 4.099337577819824
Reconstruction Loss: -0.811686635017395
Iteration 13701:
Training Loss: 4.079325199127197
Reconstruction Loss: -0.8220492601394653
Iteration 13751:
Training Loss: 4.031121730804443
Reconstruction Loss: -0.7991882562637329
Iteration 13801:
Training Loss: 4.041301250457764
Reconstruction Loss: -0.8183507323265076
Iteration 13851:
Training Loss: 4.128025531768799
Reconstruction Loss: -0.8021173477172852
Iteration 13901:
Training Loss: 4.166903972625732
Reconstruction Loss: -0.8211765885353088
Iteration 13951:
Training Loss: 4.1720685958862305
Reconstruction Loss: -0.8443909287452698
Iteration 14001:
Training Loss: 4.124802112579346
Reconstruction Loss: -0.8191326260566711
Iteration 14051:
Training Loss: 4.189599514007568
Reconstruction Loss: -0.8137999176979065
Iteration 14101:
Training Loss: 4.06699800491333
Reconstruction Loss: -0.8255826234817505
Iteration 14151:
Training Loss: 4.063691139221191
Reconstruction Loss: -0.8167505860328674
Iteration 14201:
Training Loss: 3.923341751098633
Reconstruction Loss: -0.8939485549926758
Iteration 14251:
Training Loss: 3.8174448013305664
Reconstruction Loss: -1.0386149883270264
Iteration 14301:
Training Loss: 3.7069895267486572
Reconstruction Loss: -1.0799247026443481
Iteration 14351:
Training Loss: 3.763801097869873
Reconstruction Loss: -1.091963291168213
Iteration 14401:
Training Loss: 3.705859899520874
Reconstruction Loss: -1.0734859704971313
Iteration 14451:
Training Loss: 3.6981661319732666
Reconstruction Loss: -1.0688369274139404
Iteration 14501:
Training Loss: 3.678914785385132
Reconstruction Loss: -1.0651602745056152
Iteration 14551:
Training Loss: 3.5302894115448
Reconstruction Loss: -1.0626460313796997
Iteration 14601:
Training Loss: 3.6072871685028076
Reconstruction Loss: -1.069571614265442
Iteration 14651:
Training Loss: 3.575000286102295
Reconstruction Loss: -1.093326210975647
Iteration 14701:
Training Loss: 3.481666088104248
Reconstruction Loss: -1.1020207405090332
Iteration 14751:
Training Loss: 3.4054312705993652
Reconstruction Loss: -1.106902837753296
Iteration 14801:
Training Loss: 3.5312259197235107
Reconstruction Loss: -1.1154229640960693
Iteration 14851:
Training Loss: 3.612579107284546
Reconstruction Loss: -1.1057813167572021
Iteration 14901:
Training Loss: 3.538331985473633
Reconstruction Loss: -1.0909574031829834
Iteration 14951:
Training Loss: 3.429248094558716
Reconstruction Loss: -1.088035225868225
Iteration 15001:
Training Loss: 3.48284649848938
Reconstruction Loss: -1.0747184753417969
Iteration 15051:
Training Loss: 3.4898219108581543
Reconstruction Loss: -1.0791808366775513
Iteration 15101:
Training Loss: 3.493391752243042
Reconstruction Loss: -1.0689009428024292
Iteration 15151:
Training Loss: 3.368232488632202
Reconstruction Loss: -1.065138578414917
Iteration 15201:
Training Loss: 3.467994451522827
Reconstruction Loss: -1.0462462902069092
Iteration 15251:
Training Loss: 3.4163033962249756
Reconstruction Loss: -1.0478615760803223
Iteration 15301:
Training Loss: 3.573822021484375
Reconstruction Loss: -1.0489704608917236
Iteration 15351:
Training Loss: 3.5085480213165283
Reconstruction Loss: -1.03017258644104
Iteration 15401:
Training Loss: 3.5529110431671143
Reconstruction Loss: -1.0267770290374756
Iteration 15451:
Training Loss: 3.4124860763549805
Reconstruction Loss: -1.0281083583831787
Iteration 15501:
Training Loss: 3.537916660308838
Reconstruction Loss: -1.0168077945709229
Iteration 15551:
Training Loss: 3.5466156005859375
Reconstruction Loss: -1.0268100500106812
Iteration 15601:
Training Loss: 3.393056869506836
Reconstruction Loss: -1.0072827339172363
Iteration 15651:
Training Loss: 3.4429166316986084
Reconstruction Loss: -1.0087817907333374
Iteration 15701:
Training Loss: 3.5671234130859375
Reconstruction Loss: -1.0159339904785156
Iteration 15751:
Training Loss: 3.581204891204834
Reconstruction Loss: -1.0110843181610107
Iteration 15801:
Training Loss: 3.4631400108337402
Reconstruction Loss: -0.9953199625015259
Iteration 15851:
Training Loss: 3.4085960388183594
Reconstruction Loss: -1.002779483795166
Iteration 15901:
Training Loss: 3.367471218109131
Reconstruction Loss: -1.0083417892456055
Iteration 15951:
Training Loss: 3.5294582843780518
Reconstruction Loss: -0.9928963780403137
Iteration 16001:
Training Loss: 3.4800283908843994
Reconstruction Loss: -1.0002120733261108
Iteration 16051:
Training Loss: 3.372913360595703
Reconstruction Loss: -0.9959113001823425
Iteration 16101:
Training Loss: 3.516818046569824
Reconstruction Loss: -0.9953099489212036
Iteration 16151:
Training Loss: 3.501755475997925
Reconstruction Loss: -1.0070316791534424
Iteration 16201:
Training Loss: 3.369384288787842
Reconstruction Loss: -0.9841609597206116
Iteration 16251:
Training Loss: 3.5330758094787598
Reconstruction Loss: -0.9994145631790161
Iteration 16301:
Training Loss: 3.479358673095703
Reconstruction Loss: -0.9954530000686646
Iteration 16351:
Training Loss: 3.449312925338745
Reconstruction Loss: -0.99845290184021
Iteration 16401:
Training Loss: 3.6242589950561523
Reconstruction Loss: -1.0010700225830078
Iteration 16451:
Training Loss: 3.395477533340454
Reconstruction Loss: -0.9962080121040344
Iteration 16501:
Training Loss: 3.3396694660186768
Reconstruction Loss: -0.9915575981140137
Iteration 16551:
Training Loss: 3.4160573482513428
Reconstruction Loss: -0.9882491230964661
Iteration 16601:
Training Loss: 3.547997236251831
Reconstruction Loss: -0.9819362163543701
Iteration 16651:
Training Loss: 3.4396278858184814
Reconstruction Loss: -0.9903164505958557
Iteration 16701:
Training Loss: 3.3922243118286133
Reconstruction Loss: -0.9891872406005859
Iteration 16751:
Training Loss: 3.555893898010254
Reconstruction Loss: -0.9924966096878052
Iteration 16801:
Training Loss: 3.379060983657837
Reconstruction Loss: -0.9759905338287354
Iteration 16851:
Training Loss: 3.374720811843872
Reconstruction Loss: -0.9765496850013733
Iteration 16901:
Training Loss: 3.4310555458068848
Reconstruction Loss: -0.9734663963317871
Iteration 16951:
Training Loss: 3.559293270111084
Reconstruction Loss: -0.9864620566368103
Iteration 17001:
Training Loss: 3.5168497562408447
Reconstruction Loss: -0.961532711982727
Iteration 17051:
Training Loss: 3.4503848552703857
Reconstruction Loss: -0.9781622886657715
Iteration 17101:
Training Loss: 3.4185779094696045
Reconstruction Loss: -0.9812285900115967
Iteration 17151:
Training Loss: 3.4232122898101807
Reconstruction Loss: -0.994338870048523
Iteration 17201:
Training Loss: 3.4073472023010254
Reconstruction Loss: -0.9859395027160645
Iteration 17251:
Training Loss: 3.5184268951416016
Reconstruction Loss: -0.9819080829620361
Iteration 17301:
Training Loss: 3.555948495864868
Reconstruction Loss: -0.983999490737915
Iteration 17351:
Training Loss: 3.4510698318481445
Reconstruction Loss: -0.9823508858680725
Iteration 17401:
Training Loss: 3.4982194900512695
Reconstruction Loss: -0.9782596230506897
Iteration 17451:
Training Loss: 3.450620174407959
Reconstruction Loss: -0.9681471586227417
Iteration 17501:
Training Loss: 3.516777276992798
Reconstruction Loss: -0.9725069999694824
Iteration 17551:
Training Loss: 3.5100505352020264
Reconstruction Loss: -0.9641855955123901
Iteration 17601:
Training Loss: 3.5711686611175537
Reconstruction Loss: -0.9848533868789673
Iteration 17651:
Training Loss: 3.4591867923736572
Reconstruction Loss: -0.9809185862541199
Iteration 17701:
Training Loss: 3.415658473968506
Reconstruction Loss: -0.9698788523674011
Iteration 17751:
Training Loss: 3.4330570697784424
Reconstruction Loss: -0.9657237529754639
Iteration 17801:
Training Loss: 3.323197841644287
Reconstruction Loss: -0.9761269092559814
Iteration 17851:
Training Loss: 3.4274590015411377
Reconstruction Loss: -0.9737986326217651
Iteration 17901:
Training Loss: 3.442639112472534
Reconstruction Loss: -0.977095365524292
Iteration 17951:
Training Loss: 3.427992343902588
Reconstruction Loss: -0.9777183532714844
Iteration 18001:
Training Loss: 3.486811399459839
Reconstruction Loss: -0.9870131015777588
Iteration 18051:
Training Loss: 3.477896213531494
Reconstruction Loss: -0.9894317388534546
Iteration 18101:
Training Loss: 3.4120936393737793
Reconstruction Loss: -0.9798661470413208
Iteration 18151:
Training Loss: 3.4865524768829346
Reconstruction Loss: -0.9729239344596863
Iteration 18201:
Training Loss: 3.552122116088867
Reconstruction Loss: -0.969951331615448
Iteration 18251:
Training Loss: 3.4752883911132812
Reconstruction Loss: -0.9831777215003967
Iteration 18301:
Training Loss: 3.483602523803711
Reconstruction Loss: -0.9814451932907104
Iteration 18351:
Training Loss: 3.4357919692993164
Reconstruction Loss: -0.9752362966537476
Iteration 18401:
Training Loss: 3.4323413372039795
Reconstruction Loss: -0.9880163669586182
Iteration 18451:
Training Loss: 3.365983486175537
Reconstruction Loss: -0.9813154935836792
Iteration 18501:
Training Loss: 3.4823684692382812
Reconstruction Loss: -0.9690040349960327
Iteration 18551:
Training Loss: 3.5425198078155518
Reconstruction Loss: -0.9732726812362671
Iteration 18601:
Training Loss: 3.404874324798584
Reconstruction Loss: -0.9725161790847778
Iteration 18651:
Training Loss: 3.3206207752227783
Reconstruction Loss: -0.9850819706916809
Iteration 18701:
Training Loss: 3.4998867511749268
Reconstruction Loss: -0.9663662910461426
Iteration 18751:
Training Loss: 3.451373338699341
Reconstruction Loss: -0.9720504283905029
Iteration 18801:
Training Loss: 3.517681360244751
Reconstruction Loss: -0.9781498908996582
Iteration 18851:
Training Loss: 3.464714765548706
Reconstruction Loss: -0.9707540273666382
Iteration 18901:
Training Loss: 3.450950860977173
Reconstruction Loss: -0.9791961312294006
Iteration 18951:
Training Loss: 3.5710556507110596
Reconstruction Loss: -0.9606777429580688
Iteration 19001:
Training Loss: 3.3168785572052
Reconstruction Loss: -0.9779415726661682
Iteration 19051:
Training Loss: 3.3420231342315674
Reconstruction Loss: -0.9676869511604309
Iteration 19101:
Training Loss: 3.457784652709961
Reconstruction Loss: -0.983786940574646
Iteration 19151:
Training Loss: 3.3147501945495605
Reconstruction Loss: -0.9655474424362183
Iteration 19201:
Training Loss: 3.515641450881958
Reconstruction Loss: -0.9641120433807373
Iteration 19251:
Training Loss: 3.442021369934082
Reconstruction Loss: -0.967195987701416
Iteration 19301:
Training Loss: 3.4840335845947266
Reconstruction Loss: -0.980453372001648
Iteration 19351:
Training Loss: 3.5246973037719727
Reconstruction Loss: -0.9666202068328857
Iteration 19401:
Training Loss: 3.40869402885437
Reconstruction Loss: -0.9648711681365967
Iteration 19451:
Training Loss: 3.346498489379883
Reconstruction Loss: -0.9824995994567871
Iteration 19501:
Training Loss: 3.423783779144287
Reconstruction Loss: -0.9771076440811157
Iteration 19551:
Training Loss: 3.522336721420288
Reconstruction Loss: -0.970930814743042
Iteration 19601:
Training Loss: 3.3594133853912354
Reconstruction Loss: -0.9748401045799255
Iteration 19651:
Training Loss: 3.574187755584717
Reconstruction Loss: -0.974459707736969
Iteration 19701:
Training Loss: 3.434323310852051
Reconstruction Loss: -0.9728757739067078
Iteration 19751:
Training Loss: 3.4546146392822266
Reconstruction Loss: -0.9594688415527344
Iteration 19801:
Training Loss: 3.551438093185425
Reconstruction Loss: -0.9755998253822327
Iteration 19851:
Training Loss: 3.363537311553955
Reconstruction Loss: -0.9783707857131958
Iteration 19901:
Training Loss: 3.4950780868530273
Reconstruction Loss: -0.9789707660675049
Iteration 19951:
Training Loss: 3.5444562435150146
Reconstruction Loss: -0.9775000810623169
Iteration 20001:
Training Loss: 3.4165422916412354
Reconstruction Loss: -0.9739529490470886
Iteration 20051:
Training Loss: 3.478506326675415
Reconstruction Loss: -0.9717746376991272
Iteration 20101:
Training Loss: 3.419454574584961
Reconstruction Loss: -0.9711700677871704
Iteration 20151:
Training Loss: 3.3476674556732178
Reconstruction Loss: -0.9648767113685608
Iteration 20201:
Training Loss: 3.4294848442077637
Reconstruction Loss: -0.9709513187408447
Iteration 20251:
Training Loss: 3.394071578979492
Reconstruction Loss: -0.9633620977401733
Iteration 20301:
Training Loss: 3.4407870769500732
Reconstruction Loss: -0.975963830947876
Iteration 20351:
Training Loss: 3.439579963684082
Reconstruction Loss: -0.9766920804977417
Iteration 20401:
Training Loss: 3.321354389190674
Reconstruction Loss: -0.964491605758667
Iteration 20451:
Training Loss: 3.3148059844970703
Reconstruction Loss: -0.9759882688522339
Iteration 20501:
Training Loss: 3.361952781677246
Reconstruction Loss: -0.9782998561859131
Iteration 20551:
Training Loss: 3.504168748855591
Reconstruction Loss: -0.9675006866455078
Iteration 20601:
Training Loss: 3.4328737258911133
Reconstruction Loss: -0.9716547727584839
Iteration 20651:
Training Loss: 3.4386940002441406
Reconstruction Loss: -0.961887001991272
Iteration 20701:
Training Loss: 3.290511131286621
Reconstruction Loss: -0.9742946624755859
Iteration 20751:
Training Loss: 3.4225258827209473
Reconstruction Loss: -0.9613990783691406
Iteration 20801:
Training Loss: 3.268833875656128
Reconstruction Loss: -0.9783284664154053
Iteration 20851:
Training Loss: 3.4679009914398193
Reconstruction Loss: -0.9679490923881531
Iteration 20901:
Training Loss: 3.530794382095337
Reconstruction Loss: -0.9769555330276489
Iteration 20951:
Training Loss: 3.580386161804199
Reconstruction Loss: -0.9695720672607422
Iteration 21001:
Training Loss: 3.397129535675049
Reconstruction Loss: -0.9893689155578613
Iteration 21051:
Training Loss: 3.483895778656006
Reconstruction Loss: -0.9964933395385742
Iteration 21101:
Training Loss: 3.415241003036499
Reconstruction Loss: -0.9691430330276489
Iteration 21151:
Training Loss: 3.38088059425354
Reconstruction Loss: -0.9597399234771729
Iteration 21201:
Training Loss: 3.3835349082946777
Reconstruction Loss: -0.9690921306610107
Iteration 21251:
Training Loss: 3.4593729972839355
Reconstruction Loss: -0.9678735733032227
Iteration 21301:
Training Loss: 3.208367109298706
Reconstruction Loss: -0.9655709266662598
Iteration 21351:
Training Loss: 3.492166042327881
Reconstruction Loss: -0.9622654914855957
Iteration 21401:
Training Loss: 3.4315361976623535
Reconstruction Loss: -0.9868757724761963
Iteration 21451:
Training Loss: 3.551093816757202
Reconstruction Loss: -0.9715755581855774
Iteration 21501:
Training Loss: 3.441185474395752
Reconstruction Loss: -0.9808327555656433
Iteration 21551:
Training Loss: 3.4244608879089355
Reconstruction Loss: -0.9594208598136902
Iteration 21601:
Training Loss: 3.342724561691284
Reconstruction Loss: -0.9730283617973328
Iteration 21651:
Training Loss: 3.4636423587799072
Reconstruction Loss: -0.9734914302825928
Iteration 21701:
Training Loss: 3.4801225662231445
Reconstruction Loss: -0.9774391651153564
Iteration 21751:
Training Loss: 3.4132070541381836
Reconstruction Loss: -0.9779683351516724
Iteration 21801:
Training Loss: 3.503490924835205
Reconstruction Loss: -0.9720730781555176
Iteration 21851:
Training Loss: 3.404430389404297
Reconstruction Loss: -0.9774537682533264
Iteration 21901:
Training Loss: 3.4955790042877197
Reconstruction Loss: -0.9710121750831604
Iteration 21951:
Training Loss: 3.37555193901062
Reconstruction Loss: -0.9693137407302856
Iteration 22001:
Training Loss: 3.4555375576019287
Reconstruction Loss: -0.9880276918411255
Iteration 22051:
Training Loss: 3.4647879600524902
Reconstruction Loss: -0.9665021896362305
Iteration 22101:
Training Loss: 3.5007545948028564
Reconstruction Loss: -0.9770193099975586
Iteration 22151:
Training Loss: 3.5704078674316406
Reconstruction Loss: -0.9746501445770264
Iteration 22201:
Training Loss: 3.3245456218719482
Reconstruction Loss: -0.9762388467788696
Iteration 22251:
Training Loss: 3.5278220176696777
Reconstruction Loss: -0.9755003452301025
Iteration 22301:
Training Loss: 3.50015926361084
Reconstruction Loss: -0.9710439443588257
Iteration 22351:
Training Loss: 3.4380297660827637
Reconstruction Loss: -0.9875208735466003
Iteration 22401:
Training Loss: 3.1938347816467285
Reconstruction Loss: -0.9763780832290649
Iteration 22451:
Training Loss: 3.4692537784576416
Reconstruction Loss: -0.9762041568756104
Iteration 22501:
Training Loss: 3.354963779449463
Reconstruction Loss: -0.9767482280731201
Iteration 22551:
Training Loss: 3.4662561416625977
Reconstruction Loss: -0.9908362627029419
Iteration 22601:
Training Loss: 3.4096639156341553
Reconstruction Loss: -0.9789031744003296
Iteration 22651:
Training Loss: 3.4531054496765137
Reconstruction Loss: -0.9763290882110596
Iteration 22701:
Training Loss: 3.2298810482025146
Reconstruction Loss: -0.9712686538696289
Iteration 22751:
Training Loss: 3.3659889698028564
Reconstruction Loss: -0.9838993549346924
Iteration 22801:
Training Loss: 3.3778278827667236
Reconstruction Loss: -0.9676259160041809
Iteration 22851:
Training Loss: 3.4118409156799316
Reconstruction Loss: -0.9784505367279053
Iteration 22901:
Training Loss: 3.478379011154175
Reconstruction Loss: -0.9769766330718994
Iteration 22951:
Training Loss: 3.327070951461792
Reconstruction Loss: -0.973831295967102
Iteration 23001:
Training Loss: 3.3001487255096436
Reconstruction Loss: -0.9728592038154602
Iteration 23051:
Training Loss: 3.357956647872925
Reconstruction Loss: -0.9736489653587341
Iteration 23101:
Training Loss: 3.3552863597869873
Reconstruction Loss: -0.9722495675086975
Iteration 23151:
Training Loss: 3.3965632915496826
Reconstruction Loss: -0.9600013494491577
Iteration 23201:
Training Loss: 3.4097912311553955
Reconstruction Loss: -0.9761803150177002
Iteration 23251:
Training Loss: 3.4474310874938965
Reconstruction Loss: -0.9706605672836304
Iteration 23301:
Training Loss: 3.543097734451294
Reconstruction Loss: -0.9735028743743896
Iteration 23351:
Training Loss: 3.462244987487793
Reconstruction Loss: -0.9687904119491577
Iteration 23401:
Training Loss: 3.3867900371551514
Reconstruction Loss: -0.9615259170532227
Iteration 23451:
Training Loss: 3.4392807483673096
Reconstruction Loss: -0.9650421142578125
Iteration 23501:
Training Loss: 3.4515044689178467
Reconstruction Loss: -0.9746673107147217
Iteration 23551:
Training Loss: 3.4976274967193604
Reconstruction Loss: -0.9685109257698059
Iteration 23601:
Training Loss: 3.5781772136688232
Reconstruction Loss: -0.9567714929580688
Iteration 23651:
Training Loss: 3.5152053833007812
Reconstruction Loss: -0.9581005573272705
Iteration 23701:
Training Loss: 3.453399181365967
Reconstruction Loss: -0.9768574833869934
Iteration 23751:
Training Loss: 3.299389123916626
Reconstruction Loss: -0.9801998138427734
Iteration 23801:
Training Loss: 3.450319766998291
Reconstruction Loss: -0.9766685962677002
Iteration 23851:
Training Loss: 3.465463638305664
Reconstruction Loss: -0.979850709438324
Iteration 23901:
Training Loss: 3.414048910140991
Reconstruction Loss: -0.9715161323547363
Iteration 23951:
Training Loss: 3.3984313011169434
Reconstruction Loss: -0.9708755016326904
Iteration 24001:
Training Loss: 3.426485538482666
Reconstruction Loss: -0.9715275764465332
Iteration 24051:
Training Loss: 3.442065477371216
Reconstruction Loss: -0.974865198135376
Iteration 24101:
Training Loss: 3.5453121662139893
Reconstruction Loss: -0.9808272123336792
Iteration 24151:
Training Loss: 3.5286977291107178
Reconstruction Loss: -0.9876601696014404
Iteration 24201:
Training Loss: 3.417423725128174
Reconstruction Loss: -0.9671365022659302
Iteration 24251:
Training Loss: 3.408133029937744
Reconstruction Loss: -0.9673656225204468
Iteration 24301:
Training Loss: 3.538400650024414
Reconstruction Loss: -0.9661502838134766
Iteration 24351:
Training Loss: 3.4925994873046875
Reconstruction Loss: -0.9673305153846741
Iteration 24401:
Training Loss: 3.3847713470458984
Reconstruction Loss: -0.9816560745239258
Iteration 24451:
Training Loss: 3.452439069747925
Reconstruction Loss: -0.9675562381744385
Iteration 24501:
Training Loss: 3.519625186920166
Reconstruction Loss: -0.9715890288352966
Iteration 24551:
Training Loss: 3.421461582183838
Reconstruction Loss: -0.9842618703842163
Iteration 24601:
Training Loss: 3.5414345264434814
Reconstruction Loss: -0.9696531295776367
Iteration 24651:
Training Loss: 3.4082984924316406
Reconstruction Loss: -0.9736539125442505
Iteration 24701:
Training Loss: 3.4574859142303467
Reconstruction Loss: -0.9803436398506165
Iteration 24751:
Training Loss: 3.520488739013672
Reconstruction Loss: -0.9713790416717529
Iteration 24801:
Training Loss: 3.249044895172119
Reconstruction Loss: -0.9776853322982788
Iteration 24851:
Training Loss: 3.444819927215576
Reconstruction Loss: -0.9700391292572021
Iteration 24901:
Training Loss: 3.524590015411377
Reconstruction Loss: -0.9737091064453125
Iteration 24951:
Training Loss: 3.4004335403442383
Reconstruction Loss: -0.9677309989929199
