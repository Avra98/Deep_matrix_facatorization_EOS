5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.840018272399902
Reconstruction Loss: -0.4257946014404297
Iteration 21:
Training Loss: 3.195258140563965
Reconstruction Loss: -1.0850262641906738
Iteration 41:
Training Loss: 2.3593199253082275
Reconstruction Loss: -1.524881362915039
Iteration 61:
Training Loss: 1.2907410860061646
Reconstruction Loss: -1.9924516677856445
Iteration 81:
Training Loss: 0.7392441034317017
Reconstruction Loss: -2.397994041442871
Iteration 101:
Training Loss: 0.46544235944747925
Reconstruction Loss: -2.7405309677124023
Iteration 121:
Training Loss: 0.03409716114401817
Reconstruction Loss: -3.017765522003174
Iteration 141:
Training Loss: -0.42662450671195984
Reconstruction Loss: -3.242418050765991
Iteration 161:
Training Loss: -0.6292940378189087
Reconstruction Loss: -3.4238176345825195
Iteration 181:
Training Loss: -1.0208436250686646
Reconstruction Loss: -3.580018997192383
Iteration 201:
Training Loss: -1.4330323934555054
Reconstruction Loss: -3.704362630844116
Iteration 221:
Training Loss: -1.4257025718688965
Reconstruction Loss: -3.8109781742095947
Iteration 241:
Training Loss: -1.4826381206512451
Reconstruction Loss: -3.901939868927002
Iteration 261:
Training Loss: -1.6950962543487549
Reconstruction Loss: -3.9851300716400146
Iteration 281:
Training Loss: -1.88126540184021
Reconstruction Loss: -4.059841632843018
Iteration 301:
Training Loss: -1.8629261255264282
Reconstruction Loss: -4.122860908508301
Iteration 321:
Training Loss: -1.9162182807922363
Reconstruction Loss: -4.179095268249512
Iteration 341:
Training Loss: -2.1304521560668945
Reconstruction Loss: -4.235114097595215
Iteration 361:
Training Loss: -2.088406562805176
Reconstruction Loss: -4.282876014709473
Iteration 381:
Training Loss: -2.209667682647705
Reconstruction Loss: -4.329936981201172
Iteration 401:
Training Loss: -2.6403398513793945
Reconstruction Loss: -4.373121738433838
Iteration 421:
Training Loss: -2.497246265411377
Reconstruction Loss: -4.413336753845215
Iteration 441:
Training Loss: -2.649418354034424
Reconstruction Loss: -4.450747489929199
Iteration 461:
Training Loss: -2.6729986667633057
Reconstruction Loss: -4.485647201538086
Iteration 481:
Training Loss: -2.5377326011657715
Reconstruction Loss: -4.5190629959106445
Iteration 501:
Training Loss: -2.6677098274230957
Reconstruction Loss: -4.550783634185791
Iteration 521:
Training Loss: -2.7929439544677734
Reconstruction Loss: -4.583928108215332
Iteration 541:
Training Loss: -2.890261173248291
Reconstruction Loss: -4.610647678375244
Iteration 561:
Training Loss: -2.8808906078338623
Reconstruction Loss: -4.639634132385254
Iteration 581:
Training Loss: -2.9113473892211914
Reconstruction Loss: -4.667483329772949
Iteration 601:
Training Loss: -2.756810188293457
Reconstruction Loss: -4.692330837249756
Iteration 621:
Training Loss: -2.9938275814056396
Reconstruction Loss: -4.715104579925537
Iteration 641:
Training Loss: -3.218372344970703
Reconstruction Loss: -4.740492820739746
Iteration 661:
Training Loss: -2.919912338256836
Reconstruction Loss: -4.761234760284424
Iteration 681:
Training Loss: -3.1190273761749268
Reconstruction Loss: -4.784389495849609
Iteration 701:
Training Loss: -3.176896572113037
Reconstruction Loss: -4.804508686065674
Iteration 721:
Training Loss: -3.098361015319824
Reconstruction Loss: -4.824234485626221
Iteration 741:
Training Loss: -3.2601468563079834
Reconstruction Loss: -4.846299648284912
Iteration 761:
Training Loss: -3.1866304874420166
Reconstruction Loss: -4.863458633422852
Iteration 781:
Training Loss: -3.1453680992126465
Reconstruction Loss: -4.882312774658203
Iteration 801:
Training Loss: -3.402064323425293
Reconstruction Loss: -4.8990797996521
Iteration 821:
Training Loss: -3.474409580230713
Reconstruction Loss: -4.918895721435547
Iteration 841:
Training Loss: -3.6173181533813477
Reconstruction Loss: -4.932684898376465
Iteration 861:
Training Loss: -3.342461347579956
Reconstruction Loss: -4.94809627532959
Iteration 881:
Training Loss: -3.603623151779175
Reconstruction Loss: -4.966746807098389
Iteration 901:
Training Loss: -3.6665687561035156
Reconstruction Loss: -4.980920314788818
Iteration 921:
Training Loss: -3.507079839706421
Reconstruction Loss: -4.9957804679870605
Iteration 941:
Training Loss: -3.494549036026001
Reconstruction Loss: -5.009007453918457
Iteration 961:
Training Loss: -3.7385413646698
Reconstruction Loss: -5.023655891418457
Iteration 981:
Training Loss: -3.5448153018951416
Reconstruction Loss: -5.0366926193237305
Iteration 1001:
Training Loss: -3.69000244140625
Reconstruction Loss: -5.050816059112549
Iteration 1021:
Training Loss: -3.786588191986084
Reconstruction Loss: -5.061930179595947
Iteration 1041:
Training Loss: -3.8113198280334473
Reconstruction Loss: -5.076348781585693
Iteration 1061:
Training Loss: -3.973726272583008
Reconstruction Loss: -5.086824417114258
Iteration 1081:
Training Loss: -3.757797956466675
Reconstruction Loss: -5.099803447723389
Iteration 1101:
Training Loss: -3.61740779876709
Reconstruction Loss: -5.112297058105469
Iteration 1121:
Training Loss: -4.090124130249023
Reconstruction Loss: -5.124543190002441
Iteration 1141:
Training Loss: -4.040581703186035
Reconstruction Loss: -5.136394500732422
Iteration 1161:
Training Loss: -3.7270405292510986
Reconstruction Loss: -5.144968032836914
Iteration 1181:
Training Loss: -3.798532009124756
Reconstruction Loss: -5.1558451652526855
Iteration 1201:
Training Loss: -3.9038491249084473
Reconstruction Loss: -5.166877746582031
Iteration 1221:
Training Loss: -4.3387908935546875
Reconstruction Loss: -5.178016662597656
Iteration 1241:
Training Loss: -4.044231414794922
Reconstruction Loss: -5.186800956726074
Iteration 1261:
Training Loss: -3.841970682144165
Reconstruction Loss: -5.195559024810791
Iteration 1281:
Training Loss: -3.9864277839660645
Reconstruction Loss: -5.206826210021973
Iteration 1301:
Training Loss: -4.0983781814575195
Reconstruction Loss: -5.216783046722412
Iteration 1321:
Training Loss: -3.9066667556762695
Reconstruction Loss: -5.225775241851807
Iteration 1341:
Training Loss: -4.2481160163879395
Reconstruction Loss: -5.2326154708862305
Iteration 1361:
Training Loss: -4.297561168670654
Reconstruction Loss: -5.241089820861816
Iteration 1381:
Training Loss: -4.150327682495117
Reconstruction Loss: -5.2517805099487305
Iteration 1401:
Training Loss: -4.36641263961792
Reconstruction Loss: -5.260765075683594
Iteration 1421:
Training Loss: -4.018718719482422
Reconstruction Loss: -5.269808292388916
Iteration 1441:
Training Loss: -4.113067150115967
Reconstruction Loss: -5.277471542358398
Iteration 1461:
Training Loss: -4.2466936111450195
Reconstruction Loss: -5.285616397857666
Iteration 1481:
Training Loss: -4.2902727127075195
Reconstruction Loss: -5.291060924530029
Iteration 1501:
Training Loss: -4.249882221221924
Reconstruction Loss: -5.299140453338623
Iteration 1521:
Training Loss: -4.31804895401001
Reconstruction Loss: -5.307293891906738
Iteration 1541:
Training Loss: -4.408207893371582
Reconstruction Loss: -5.315248012542725
Iteration 1561:
Training Loss: -4.5601325035095215
Reconstruction Loss: -5.321572780609131
Iteration 1581:
Training Loss: -4.459523677825928
Reconstruction Loss: -5.329893589019775
Iteration 1601:
Training Loss: -4.550228595733643
Reconstruction Loss: -5.334955215454102
Iteration 1621:
Training Loss: -4.438267230987549
Reconstruction Loss: -5.342045307159424
Iteration 1641:
Training Loss: -4.2597246170043945
Reconstruction Loss: -5.349947929382324
Iteration 1661:
Training Loss: -4.443234443664551
Reconstruction Loss: -5.357656478881836
Iteration 1681:
Training Loss: -4.439388275146484
Reconstruction Loss: -5.363336563110352
Iteration 1701:
Training Loss: -4.334334373474121
Reconstruction Loss: -5.369984149932861
Iteration 1721:
Training Loss: -4.412245273590088
Reconstruction Loss: -5.376844882965088
Iteration 1741:
Training Loss: -4.314892768859863
Reconstruction Loss: -5.382730484008789
Iteration 1761:
Training Loss: -4.429884910583496
Reconstruction Loss: -5.388501167297363
Iteration 1781:
Training Loss: -4.564664363861084
Reconstruction Loss: -5.395996570587158
Iteration 1801:
Training Loss: -4.561802864074707
Reconstruction Loss: -5.401304721832275
Iteration 1821:
Training Loss: -4.3348612785339355
Reconstruction Loss: -5.406484127044678
Iteration 1841:
Training Loss: -4.733779430389404
Reconstruction Loss: -5.412673473358154
Iteration 1861:
Training Loss: -4.450675964355469
Reconstruction Loss: -5.4187822341918945
Iteration 1881:
Training Loss: -4.572497367858887
Reconstruction Loss: -5.424427032470703
Iteration 1901:
Training Loss: -4.532354831695557
Reconstruction Loss: -5.431241989135742
Iteration 1921:
Training Loss: -4.542708396911621
Reconstruction Loss: -5.436233997344971
Iteration 1941:
Training Loss: -4.6711225509643555
Reconstruction Loss: -5.440312385559082
Iteration 1961:
Training Loss: -4.468710422515869
Reconstruction Loss: -5.444295883178711
Iteration 1981:
Training Loss: -4.573300361633301
Reconstruction Loss: -5.450953483581543
Iteration 2001:
Training Loss: -4.567725658416748
Reconstruction Loss: -5.454943656921387
Iteration 2021:
Training Loss: -4.609126091003418
Reconstruction Loss: -5.4594526290893555
Iteration 2041:
Training Loss: -4.730813503265381
Reconstruction Loss: -5.465702056884766
Iteration 2061:
Training Loss: -4.781435966491699
Reconstruction Loss: -5.471149444580078
Iteration 2081:
Training Loss: -4.684481620788574
Reconstruction Loss: -5.475436687469482
Iteration 2101:
Training Loss: -4.799213886260986
Reconstruction Loss: -5.479129791259766
Iteration 2121:
Training Loss: -4.8916473388671875
Reconstruction Loss: -5.484324932098389
Iteration 2141:
Training Loss: -4.62689208984375
Reconstruction Loss: -5.490776062011719
Iteration 2161:
Training Loss: -4.520878791809082
Reconstruction Loss: -5.494193077087402
Iteration 2181:
Training Loss: -4.833221912384033
Reconstruction Loss: -5.4999003410339355
Iteration 2201:
Training Loss: -4.687612533569336
Reconstruction Loss: -5.502814292907715
Iteration 2221:
Training Loss: -4.863493919372559
Reconstruction Loss: -5.507205009460449
Iteration 2241:
Training Loss: -4.417787551879883
Reconstruction Loss: -5.513055324554443
Iteration 2261:
Training Loss: -5.205706596374512
Reconstruction Loss: -5.515564918518066
Iteration 2281:
Training Loss: -4.682739734649658
Reconstruction Loss: -5.52152156829834
Iteration 2301:
Training Loss: -4.999298095703125
Reconstruction Loss: -5.525786876678467
Iteration 2321:
Training Loss: -4.921925067901611
Reconstruction Loss: -5.530018329620361
Iteration 2341:
Training Loss: -4.897571086883545
Reconstruction Loss: -5.534711837768555
Iteration 2361:
Training Loss: -4.832665920257568
Reconstruction Loss: -5.538588047027588
Iteration 2381:
Training Loss: -5.0007805824279785
Reconstruction Loss: -5.5408148765563965
Iteration 2401:
Training Loss: -4.88240385055542
Reconstruction Loss: -5.545980453491211
Iteration 2421:
Training Loss: -4.884082317352295
Reconstruction Loss: -5.548937797546387
Iteration 2441:
Training Loss: -5.130013465881348
Reconstruction Loss: -5.554806709289551
Iteration 2461:
Training Loss: -5.119137287139893
Reconstruction Loss: -5.557058334350586
Iteration 2481:
Training Loss: -5.391115665435791
Reconstruction Loss: -5.56201171875
Iteration 2501:
Training Loss: -5.1799750328063965
Reconstruction Loss: -5.565946102142334
Iteration 2521:
Training Loss: -4.87545919418335
Reconstruction Loss: -5.569409370422363
Iteration 2541:
Training Loss: -4.910837650299072
Reconstruction Loss: -5.573200225830078
Iteration 2561:
Training Loss: -5.021853446960449
Reconstruction Loss: -5.5772786140441895
Iteration 2581:
Training Loss: -4.951294898986816
Reconstruction Loss: -5.580288887023926
Iteration 2601:
Training Loss: -4.805026531219482
Reconstruction Loss: -5.58285665512085
Iteration 2621:
Training Loss: -5.11322546005249
Reconstruction Loss: -5.586431980133057
Iteration 2641:
Training Loss: -5.15312385559082
Reconstruction Loss: -5.590658664703369
Iteration 2661:
Training Loss: -5.042603492736816
Reconstruction Loss: -5.593047142028809
Iteration 2681:
Training Loss: -4.8975300788879395
Reconstruction Loss: -5.59765625
Iteration 2701:
Training Loss: -5.219837188720703
Reconstruction Loss: -5.600291728973389
Iteration 2721:
Training Loss: -5.084102630615234
Reconstruction Loss: -5.603050708770752
Iteration 2741:
Training Loss: -4.94070291519165
Reconstruction Loss: -5.606941223144531
Iteration 2761:
Training Loss: -5.140258312225342
Reconstruction Loss: -5.6095871925354
Iteration 2781:
Training Loss: -5.412124156951904
Reconstruction Loss: -5.613494396209717
Iteration 2801:
Training Loss: -5.030125141143799
Reconstruction Loss: -5.61674690246582
Iteration 2821:
Training Loss: -4.864755153656006
Reconstruction Loss: -5.620028018951416
Iteration 2841:
Training Loss: -5.388702392578125
Reconstruction Loss: -5.622523307800293
Iteration 2861:
Training Loss: -5.065930366516113
Reconstruction Loss: -5.626058578491211
Iteration 2881:
Training Loss: -5.081473350524902
Reconstruction Loss: -5.628016948699951
Iteration 2901:
Training Loss: -5.029451847076416
Reconstruction Loss: -5.63310432434082
Iteration 2921:
Training Loss: -5.217713356018066
Reconstruction Loss: -5.635293006896973
Iteration 2941:
Training Loss: -5.182929992675781
Reconstruction Loss: -5.638256549835205
Iteration 2961:
Training Loss: -5.11370849609375
Reconstruction Loss: -5.640822410583496
Iteration 2981:
Training Loss: -5.426441669464111
Reconstruction Loss: -5.642911434173584
