5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.5702385902404785
Reconstruction Loss: -0.46449023485183716
Iteration 11:
Training Loss: 5.258604049682617
Reconstruction Loss: -0.47155332565307617
Iteration 21:
Training Loss: 4.746984958648682
Reconstruction Loss: -0.7164152264595032
Iteration 31:
Training Loss: 3.9590487480163574
Reconstruction Loss: -0.9931793808937073
Iteration 41:
Training Loss: 3.6219730377197266
Reconstruction Loss: -1.477646827697754
Iteration 51:
Training Loss: 3.524928092956543
Reconstruction Loss: -1.6333624124526978
Iteration 61:
Training Loss: 3.0240304470062256
Reconstruction Loss: -1.8515369892120361
Iteration 71:
Training Loss: 1.815303087234497
Reconstruction Loss: -2.6181068420410156
Iteration 81:
Training Loss: 0.6312754154205322
Reconstruction Loss: -3.599642753601074
Iteration 91:
Training Loss: -0.24352124333381653
Reconstruction Loss: -4.534670352935791
Iteration 101:
Training Loss: -1.030531883239746
Reconstruction Loss: -5.35153341293335
Iteration 111:
Training Loss: -2.4078593254089355
Reconstruction Loss: -6.097259521484375
Iteration 121:
Training Loss: -3.2131288051605225
Reconstruction Loss: -6.783817291259766
Iteration 131:
Training Loss: -3.6684207916259766
Reconstruction Loss: -7.393608570098877
Iteration 141:
Training Loss: -3.5792829990386963
Reconstruction Loss: -7.9342265129089355
Iteration 151:
Training Loss: -4.781925201416016
Reconstruction Loss: -8.427962303161621
Iteration 161:
Training Loss: -5.454836845397949
Reconstruction Loss: -8.824527740478516
Iteration 171:
Training Loss: -4.817831039428711
Reconstruction Loss: -9.119382858276367
Iteration 181:
Training Loss: -5.311167240142822
Reconstruction Loss: -9.347121238708496
Iteration 191:
Training Loss: -5.3260884284973145
Reconstruction Loss: -9.507777214050293
Iteration 201:
Training Loss: -5.287487506866455
Reconstruction Loss: -9.652112007141113
Iteration 211:
Training Loss: -4.889341831207275
Reconstruction Loss: -9.704174995422363
Iteration 221:
Training Loss: -5.57629919052124
Reconstruction Loss: -9.769775390625
Iteration 231:
Training Loss: -5.682074069976807
Reconstruction Loss: -9.808733940124512
Iteration 241:
Training Loss: -5.655641078948975
Reconstruction Loss: -9.866711616516113
Iteration 251:
Training Loss: -5.716488838195801
Reconstruction Loss: -9.895584106445312
Iteration 261:
Training Loss: -5.556748390197754
Reconstruction Loss: -9.900077819824219
Iteration 271:
Training Loss: -5.436522006988525
Reconstruction Loss: -9.918011665344238
Iteration 281:
Training Loss: -5.210021495819092
Reconstruction Loss: -9.945829391479492
Iteration 291:
Training Loss: -5.108658790588379
Reconstruction Loss: -9.95987319946289
Iteration 301:
Training Loss: -5.285762786865234
Reconstruction Loss: -10.003501892089844
Iteration 311:
Training Loss: -5.495885848999023
Reconstruction Loss: -10.013964653015137
Iteration 321:
Training Loss: -5.895757675170898
Reconstruction Loss: -10.014486312866211
Iteration 331:
Training Loss: -5.58350133895874
Reconstruction Loss: -10.023077011108398
Iteration 341:
Training Loss: -5.411847114562988
Reconstruction Loss: -10.008808135986328
Iteration 351:
Training Loss: -5.622525215148926
Reconstruction Loss: -10.033069610595703
Iteration 361:
Training Loss: -5.3726115226745605
Reconstruction Loss: -10.026687622070312
Iteration 371:
Training Loss: -5.674984931945801
Reconstruction Loss: -10.072922706604004
Iteration 381:
Training Loss: -5.810506820678711
Reconstruction Loss: -10.099725723266602
Iteration 391:
Training Loss: -5.6876983642578125
Reconstruction Loss: -10.073026657104492
Iteration 401:
Training Loss: -5.61321496963501
Reconstruction Loss: -10.094036102294922
Iteration 411:
Training Loss: -6.145410060882568
Reconstruction Loss: -10.109807968139648
Iteration 421:
Training Loss: -5.425060272216797
Reconstruction Loss: -10.128036499023438
Iteration 431:
Training Loss: -5.5207133293151855
Reconstruction Loss: -10.107271194458008
Iteration 441:
Training Loss: -5.687970161437988
Reconstruction Loss: -10.142844200134277
Iteration 451:
Training Loss: -5.429265022277832
Reconstruction Loss: -10.165899276733398
Iteration 461:
Training Loss: -5.600101947784424
Reconstruction Loss: -10.144914627075195
Iteration 471:
Training Loss: -5.977717876434326
Reconstruction Loss: -10.173235893249512
Iteration 481:
Training Loss: -5.3438262939453125
Reconstruction Loss: -10.135921478271484
Iteration 491:
Training Loss: -5.736897945404053
Reconstruction Loss: -10.194725036621094
Iteration 501:
Training Loss: -5.807878494262695
Reconstruction Loss: -10.210375785827637
Iteration 511:
Training Loss: -5.751611232757568
Reconstruction Loss: -10.22313404083252
Iteration 521:
Training Loss: -5.851127624511719
Reconstruction Loss: -10.227712631225586
Iteration 531:
Training Loss: -6.170934200286865
Reconstruction Loss: -10.245710372924805
Iteration 541:
Training Loss: -6.0081024169921875
Reconstruction Loss: -10.284270286560059
Iteration 551:
Training Loss: -6.441938877105713
Reconstruction Loss: -10.230135917663574
Iteration 561:
Training Loss: -6.2418036460876465
Reconstruction Loss: -10.263541221618652
Iteration 571:
Training Loss: -5.609639644622803
Reconstruction Loss: -10.294262886047363
Iteration 581:
Training Loss: -5.8755340576171875
Reconstruction Loss: -10.263656616210938
Iteration 591:
Training Loss: -5.890631198883057
Reconstruction Loss: -10.316242218017578
Iteration 601:
Training Loss: -6.021162986755371
Reconstruction Loss: -10.288283348083496
Iteration 611:
Training Loss: -6.108759880065918
Reconstruction Loss: -10.32659912109375
Iteration 621:
Training Loss: -5.970706939697266
Reconstruction Loss: -10.284997940063477
Iteration 631:
Training Loss: -6.069430351257324
Reconstruction Loss: -10.33484935760498
Iteration 641:
Training Loss: -5.8655266761779785
Reconstruction Loss: -10.336116790771484
Iteration 651:
Training Loss: -5.8078718185424805
Reconstruction Loss: -10.333932876586914
Iteration 661:
Training Loss: -6.193340301513672
Reconstruction Loss: -10.34496021270752
Iteration 671:
Training Loss: -6.308757305145264
Reconstruction Loss: -10.363080024719238
Iteration 681:
Training Loss: -5.858258247375488
Reconstruction Loss: -10.395379066467285
Iteration 691:
Training Loss: -5.690672874450684
Reconstruction Loss: -10.38900089263916
Iteration 701:
Training Loss: -6.626374244689941
Reconstruction Loss: -10.385698318481445
Iteration 711:
Training Loss: -6.157353401184082
Reconstruction Loss: -10.401055335998535
Iteration 721:
Training Loss: -6.197403430938721
Reconstruction Loss: -10.40013313293457
Iteration 731:
Training Loss: -6.127817153930664
Reconstruction Loss: -10.394766807556152
Iteration 741:
Training Loss: -5.995053291320801
Reconstruction Loss: -10.428833961486816
Iteration 751:
Training Loss: -6.258216857910156
Reconstruction Loss: -10.435041427612305
Iteration 761:
Training Loss: -5.959737300872803
Reconstruction Loss: -10.456978797912598
Iteration 771:
Training Loss: -5.977326393127441
Reconstruction Loss: -10.458390235900879
Iteration 781:
Training Loss: -6.124634742736816
Reconstruction Loss: -10.464004516601562
Iteration 791:
Training Loss: -5.9735493659973145
Reconstruction Loss: -10.441608428955078
Iteration 801:
Training Loss: -5.951111316680908
Reconstruction Loss: -10.448596954345703
Iteration 811:
Training Loss: -5.730673789978027
Reconstruction Loss: -10.477266311645508
Iteration 821:
Training Loss: -6.183343887329102
Reconstruction Loss: -10.474503517150879
Iteration 831:
Training Loss: -6.094902038574219
Reconstruction Loss: -10.516331672668457
Iteration 841:
Training Loss: -6.171555995941162
Reconstruction Loss: -10.46619987487793
Iteration 851:
Training Loss: -6.139208793640137
Reconstruction Loss: -10.514747619628906
Iteration 861:
Training Loss: -6.467808723449707
Reconstruction Loss: -10.529781341552734
Iteration 871:
Training Loss: -6.018921375274658
Reconstruction Loss: -10.502922058105469
Iteration 881:
Training Loss: -6.078760147094727
Reconstruction Loss: -10.524563789367676
Iteration 891:
Training Loss: -6.287237644195557
Reconstruction Loss: -10.513572692871094
Iteration 901:
Training Loss: -5.995075702667236
Reconstruction Loss: -10.546648025512695
Iteration 911:
Training Loss: -6.084902286529541
Reconstruction Loss: -10.53633975982666
Iteration 921:
Training Loss: -6.343345642089844
Reconstruction Loss: -10.57263469696045
Iteration 931:
Training Loss: -6.501497268676758
Reconstruction Loss: -10.569677352905273
Iteration 941:
Training Loss: -6.0913214683532715
Reconstruction Loss: -10.576386451721191
Iteration 951:
Training Loss: -6.13529109954834
Reconstruction Loss: -10.605504989624023
Iteration 961:
Training Loss: -6.089615821838379
Reconstruction Loss: -10.600427627563477
Iteration 971:
Training Loss: -5.969624042510986
Reconstruction Loss: -10.587972640991211
Iteration 981:
Training Loss: -6.302669525146484
Reconstruction Loss: -10.584894180297852
Iteration 991:
Training Loss: -6.191346168518066
Reconstruction Loss: -10.572067260742188
Iteration 1001:
Training Loss: -6.253448486328125
Reconstruction Loss: -10.653078079223633
Iteration 1011:
Training Loss: -6.037459373474121
Reconstruction Loss: -10.589675903320312
Iteration 1021:
Training Loss: -5.858397960662842
Reconstruction Loss: -10.642667770385742
Iteration 1031:
Training Loss: -6.4955644607543945
Reconstruction Loss: -10.637455940246582
Iteration 1041:
Training Loss: -6.17702054977417
Reconstruction Loss: -10.63488483428955
Iteration 1051:
Training Loss: -6.44232702255249
Reconstruction Loss: -10.669560432434082
Iteration 1061:
Training Loss: -6.093394756317139
Reconstruction Loss: -10.682229042053223
Iteration 1071:
Training Loss: -6.114688873291016
Reconstruction Loss: -10.619942665100098
Iteration 1081:
Training Loss: -6.501810550689697
Reconstruction Loss: -10.682991981506348
Iteration 1091:
Training Loss: -5.990433692932129
Reconstruction Loss: -10.677457809448242
Iteration 1101:
Training Loss: -6.417760848999023
Reconstruction Loss: -10.67111587524414
Iteration 1111:
Training Loss: -6.427308559417725
Reconstruction Loss: -10.670694351196289
Iteration 1121:
Training Loss: -6.170712947845459
Reconstruction Loss: -10.674196243286133
Iteration 1131:
Training Loss: -6.058952808380127
Reconstruction Loss: -10.69870376586914
Iteration 1141:
Training Loss: -6.518291473388672
Reconstruction Loss: -10.729125022888184
Iteration 1151:
Training Loss: -6.581076622009277
Reconstruction Loss: -10.733080863952637
Iteration 1161:
Training Loss: -6.885663986206055
Reconstruction Loss: -10.717154502868652
Iteration 1171:
Training Loss: -6.409195423126221
Reconstruction Loss: -10.718718528747559
Iteration 1181:
Training Loss: -6.081640243530273
Reconstruction Loss: -10.745772361755371
Iteration 1191:
Training Loss: -6.525671482086182
Reconstruction Loss: -10.763040542602539
Iteration 1201:
Training Loss: -6.400550365447998
Reconstruction Loss: -10.786824226379395
Iteration 1211:
Training Loss: -6.861353397369385
Reconstruction Loss: -10.756031036376953
Iteration 1221:
Training Loss: -6.028135299682617
Reconstruction Loss: -10.727044105529785
Iteration 1231:
Training Loss: -6.526093482971191
Reconstruction Loss: -10.766861915588379
Iteration 1241:
Training Loss: -6.102956295013428
Reconstruction Loss: -10.751337051391602
Iteration 1251:
Training Loss: -6.606759071350098
Reconstruction Loss: -10.786989212036133
Iteration 1261:
Training Loss: -6.592973232269287
Reconstruction Loss: -10.758393287658691
Iteration 1271:
Training Loss: -7.099460601806641
Reconstruction Loss: -10.799246788024902
Iteration 1281:
Training Loss: -6.8026041984558105
Reconstruction Loss: -10.790172576904297
Iteration 1291:
Training Loss: -6.574640274047852
Reconstruction Loss: -10.788637161254883
Iteration 1301:
Training Loss: -6.832815647125244
Reconstruction Loss: -10.816413879394531
Iteration 1311:
Training Loss: -6.887423515319824
Reconstruction Loss: -10.82761287689209
Iteration 1321:
Training Loss: -7.099380016326904
Reconstruction Loss: -10.80062198638916
Iteration 1331:
Training Loss: -7.088162899017334
Reconstruction Loss: -10.814719200134277
Iteration 1341:
Training Loss: -6.520045280456543
Reconstruction Loss: -10.79619312286377
Iteration 1351:
Training Loss: -6.382149696350098
Reconstruction Loss: -10.802705764770508
Iteration 1361:
Training Loss: -6.4481401443481445
Reconstruction Loss: -10.823490142822266
Iteration 1371:
Training Loss: -6.771332263946533
Reconstruction Loss: -10.78883171081543
Iteration 1381:
Training Loss: -6.626372337341309
Reconstruction Loss: -10.830632209777832
Iteration 1391:
Training Loss: -6.573441028594971
Reconstruction Loss: -10.838748931884766
Iteration 1401:
Training Loss: -6.738992214202881
Reconstruction Loss: -10.837569236755371
Iteration 1411:
Training Loss: -6.721247673034668
Reconstruction Loss: -10.865937232971191
Iteration 1421:
Training Loss: -6.4514970779418945
Reconstruction Loss: -10.856799125671387
Iteration 1431:
Training Loss: -6.810575485229492
Reconstruction Loss: -10.910162925720215
Iteration 1441:
Training Loss: -7.1062798500061035
Reconstruction Loss: -10.886896133422852
Iteration 1451:
Training Loss: -6.573505401611328
Reconstruction Loss: -10.866971015930176
Iteration 1461:
Training Loss: -6.429046154022217
Reconstruction Loss: -10.885651588439941
Iteration 1471:
Training Loss: -6.549360275268555
Reconstruction Loss: -10.876896858215332
Iteration 1481:
Training Loss: -6.401564121246338
Reconstruction Loss: -10.881661415100098
Iteration 1491:
Training Loss: -6.474289894104004
Reconstruction Loss: -10.921236038208008
Iteration 1501:
Training Loss: -6.900974273681641
Reconstruction Loss: -10.889330863952637
Iteration 1511:
Training Loss: -6.569552421569824
Reconstruction Loss: -10.892679214477539
Iteration 1521:
Training Loss: -6.36461877822876
Reconstruction Loss: -10.893733978271484
Iteration 1531:
Training Loss: -6.5845537185668945
Reconstruction Loss: -10.934849739074707
Iteration 1541:
Training Loss: -6.966948509216309
Reconstruction Loss: -10.935722351074219
Iteration 1551:
Training Loss: -6.721491813659668
Reconstruction Loss: -10.955545425415039
Iteration 1561:
Training Loss: -6.66898775100708
Reconstruction Loss: -10.9435396194458
Iteration 1571:
Training Loss: -6.87295389175415
Reconstruction Loss: -10.9425687789917
Iteration 1581:
Training Loss: -6.5590338706970215
Reconstruction Loss: -10.947463989257812
Iteration 1591:
Training Loss: -6.84194278717041
Reconstruction Loss: -10.97629451751709
Iteration 1601:
Training Loss: -7.076675891876221
Reconstruction Loss: -10.92501449584961
Iteration 1611:
Training Loss: -6.800724983215332
Reconstruction Loss: -10.970649719238281
Iteration 1621:
Training Loss: -6.779509544372559
Reconstruction Loss: -10.974970817565918
Iteration 1631:
Training Loss: -6.525780200958252
Reconstruction Loss: -10.98336410522461
Iteration 1641:
Training Loss: -7.082715034484863
Reconstruction Loss: -10.974455833435059
Iteration 1651:
Training Loss: -6.895193099975586
Reconstruction Loss: -10.992042541503906
Iteration 1661:
Training Loss: -6.709343433380127
Reconstruction Loss: -11.009963035583496
Iteration 1671:
Training Loss: -6.936851501464844
Reconstruction Loss: -10.970000267028809
Iteration 1681:
Training Loss: -6.994211196899414
Reconstruction Loss: -10.977459907531738
Iteration 1691:
Training Loss: -6.770530700683594
Reconstruction Loss: -10.979653358459473
Iteration 1701:
Training Loss: -7.174230575561523
Reconstruction Loss: -11.00712776184082
Iteration 1711:
Training Loss: -6.545418739318848
Reconstruction Loss: -10.987892150878906
Iteration 1721:
Training Loss: -6.631474494934082
Reconstruction Loss: -11.000434875488281
Iteration 1731:
Training Loss: -6.47844934463501
Reconstruction Loss: -11.018744468688965
Iteration 1741:
Training Loss: -6.711636543273926
Reconstruction Loss: -11.014177322387695
Iteration 1751:
Training Loss: -6.435621738433838
Reconstruction Loss: -11.009995460510254
Iteration 1761:
Training Loss: -6.884782791137695
Reconstruction Loss: -11.00542163848877
Iteration 1771:
Training Loss: -6.771811485290527
Reconstruction Loss: -11.037036895751953
Iteration 1781:
Training Loss: -6.854519844055176
Reconstruction Loss: -11.03487491607666
Iteration 1791:
Training Loss: -6.502206802368164
Reconstruction Loss: -11.0451078414917
Iteration 1801:
Training Loss: -6.805843353271484
Reconstruction Loss: -11.035310745239258
Iteration 1811:
Training Loss: -6.675380706787109
Reconstruction Loss: -11.032947540283203
Iteration 1821:
Training Loss: -6.744554042816162
Reconstruction Loss: -11.058395385742188
Iteration 1831:
Training Loss: -6.933866024017334
Reconstruction Loss: -11.040892601013184
Iteration 1841:
Training Loss: -6.3813958168029785
Reconstruction Loss: -11.03756046295166
Iteration 1851:
Training Loss: -6.54141092300415
Reconstruction Loss: -11.094508171081543
Iteration 1861:
Training Loss: -6.841263771057129
Reconstruction Loss: -11.061137199401855
Iteration 1871:
Training Loss: -6.948099136352539
Reconstruction Loss: -11.094944953918457
Iteration 1881:
Training Loss: -6.937853813171387
Reconstruction Loss: -11.05807113647461
Iteration 1891:
Training Loss: -6.788516998291016
Reconstruction Loss: -11.067808151245117
Iteration 1901:
Training Loss: -7.306166648864746
Reconstruction Loss: -11.083108901977539
Iteration 1911:
Training Loss: -6.765567302703857
Reconstruction Loss: -11.124313354492188
Iteration 1921:
Training Loss: -6.787801265716553
Reconstruction Loss: -11.08703327178955
Iteration 1931:
Training Loss: -7.345352649688721
Reconstruction Loss: -11.103850364685059
Iteration 1941:
Training Loss: -6.786776065826416
Reconstruction Loss: -11.110990524291992
Iteration 1951:
Training Loss: -6.994500637054443
Reconstruction Loss: -11.116024017333984
Iteration 1961:
Training Loss: -6.994482040405273
Reconstruction Loss: -11.109262466430664
Iteration 1971:
Training Loss: -6.814645767211914
Reconstruction Loss: -11.108614921569824
Iteration 1981:
Training Loss: -6.667655944824219
Reconstruction Loss: -11.131808280944824
Iteration 1991:
Training Loss: -6.9982218742370605
Reconstruction Loss: -11.145088195800781
Iteration 2001:
Training Loss: -6.6826863288879395
Reconstruction Loss: -11.142104148864746
Iteration 2011:
Training Loss: -7.054014205932617
Reconstruction Loss: -11.126718521118164
Iteration 2021:
Training Loss: -7.133619785308838
Reconstruction Loss: -11.130924224853516
Iteration 2031:
Training Loss: -6.954463481903076
Reconstruction Loss: -11.138031959533691
Iteration 2041:
Training Loss: -6.988691329956055
Reconstruction Loss: -11.111769676208496
Iteration 2051:
Training Loss: -7.143856048583984
Reconstruction Loss: -11.153355598449707
Iteration 2061:
Training Loss: -6.969062805175781
Reconstruction Loss: -11.155319213867188
Iteration 2071:
Training Loss: -7.079229354858398
Reconstruction Loss: -11.157242774963379
Iteration 2081:
Training Loss: -6.675727367401123
Reconstruction Loss: -11.150100708007812
Iteration 2091:
Training Loss: -6.637020587921143
Reconstruction Loss: -11.1658935546875
Iteration 2101:
Training Loss: -7.27208948135376
Reconstruction Loss: -11.181511878967285
Iteration 2111:
Training Loss: -6.694797515869141
Reconstruction Loss: -11.182201385498047
Iteration 2121:
Training Loss: -6.685718536376953
Reconstruction Loss: -11.175838470458984
Iteration 2131:
Training Loss: -6.633618354797363
Reconstruction Loss: -11.18397331237793
Iteration 2141:
Training Loss: -7.361123561859131
Reconstruction Loss: -11.187591552734375
Iteration 2151:
Training Loss: -7.0140380859375
Reconstruction Loss: -11.184937477111816
Iteration 2161:
Training Loss: -7.287022590637207
Reconstruction Loss: -11.218506813049316
Iteration 2171:
Training Loss: -7.323668956756592
Reconstruction Loss: -11.178431510925293
Iteration 2181:
Training Loss: -7.336468696594238
Reconstruction Loss: -11.196666717529297
Iteration 2191:
Training Loss: -7.454264163970947
Reconstruction Loss: -11.20041275024414
Iteration 2201:
Training Loss: -7.095925807952881
Reconstruction Loss: -11.184725761413574
Iteration 2211:
Training Loss: -6.850830554962158
Reconstruction Loss: -11.162612915039062
Iteration 2221:
Training Loss: -7.161462783813477
Reconstruction Loss: -11.22523307800293
Iteration 2231:
Training Loss: -7.033406734466553
Reconstruction Loss: -11.199784278869629
Iteration 2241:
Training Loss: -7.254386901855469
Reconstruction Loss: -11.215310096740723
Iteration 2251:
Training Loss: -7.131518840789795
Reconstruction Loss: -11.209251403808594
Iteration 2261:
Training Loss: -7.296816349029541
Reconstruction Loss: -11.218656539916992
Iteration 2271:
Training Loss: -7.130146026611328
Reconstruction Loss: -11.219505310058594
Iteration 2281:
Training Loss: -6.467672824859619
Reconstruction Loss: -11.212750434875488
Iteration 2291:
Training Loss: -7.261403560638428
Reconstruction Loss: -11.218101501464844
Iteration 2301:
Training Loss: -6.905284881591797
Reconstruction Loss: -11.220701217651367
Iteration 2311:
Training Loss: -7.237051010131836
Reconstruction Loss: -11.231494903564453
Iteration 2321:
Training Loss: -7.135757923126221
Reconstruction Loss: -11.246633529663086
Iteration 2331:
Training Loss: -7.243711948394775
Reconstruction Loss: -11.232850074768066
Iteration 2341:
Training Loss: -7.498107433319092
Reconstruction Loss: -11.23752498626709
Iteration 2351:
Training Loss: -6.962588787078857
Reconstruction Loss: -11.247115135192871
Iteration 2361:
Training Loss: -7.5905632972717285
Reconstruction Loss: -11.260554313659668
Iteration 2371:
Training Loss: -6.807504177093506
Reconstruction Loss: -11.275879859924316
Iteration 2381:
Training Loss: -6.976293087005615
Reconstruction Loss: -11.253929138183594
Iteration 2391:
Training Loss: -6.772266387939453
Reconstruction Loss: -11.27209758758545
Iteration 2401:
Training Loss: -7.5832719802856445
Reconstruction Loss: -11.268352508544922
Iteration 2411:
Training Loss: -6.866811752319336
Reconstruction Loss: -11.24073314666748
Iteration 2421:
Training Loss: -7.412477970123291
Reconstruction Loss: -11.273674011230469
Iteration 2431:
Training Loss: -6.997707366943359
Reconstruction Loss: -11.290839195251465
Iteration 2441:
Training Loss: -6.946460247039795
Reconstruction Loss: -11.285701751708984
Iteration 2451:
Training Loss: -7.694289207458496
Reconstruction Loss: -11.286713600158691
Iteration 2461:
Training Loss: -7.381688117980957
Reconstruction Loss: -11.286303520202637
Iteration 2471:
Training Loss: -7.381558418273926
Reconstruction Loss: -11.306267738342285
Iteration 2481:
Training Loss: -7.17027473449707
Reconstruction Loss: -11.301724433898926
Iteration 2491:
Training Loss: -6.949636936187744
Reconstruction Loss: -11.283227920532227
Iteration 2501:
Training Loss: -7.081532001495361
Reconstruction Loss: -11.308984756469727
Iteration 2511:
Training Loss: -7.278797149658203
Reconstruction Loss: -11.268280029296875
Iteration 2521:
Training Loss: -6.953031539916992
Reconstruction Loss: -11.289703369140625
Iteration 2531:
Training Loss: -7.7704339027404785
Reconstruction Loss: -11.332476615905762
Iteration 2541:
Training Loss: -7.274421691894531
Reconstruction Loss: -11.30471134185791
Iteration 2551:
Training Loss: -6.851238250732422
Reconstruction Loss: -11.321263313293457
Iteration 2561:
Training Loss: -7.199425220489502
Reconstruction Loss: -11.34139633178711
Iteration 2571:
Training Loss: -7.0694780349731445
Reconstruction Loss: -11.315474510192871
Iteration 2581:
Training Loss: -7.102972507476807
Reconstruction Loss: -11.32813549041748
Iteration 2591:
Training Loss: -7.131889820098877
Reconstruction Loss: -11.332915306091309
Iteration 2601:
Training Loss: -7.27264928817749
Reconstruction Loss: -11.339188575744629
Iteration 2611:
Training Loss: -6.868927955627441
Reconstruction Loss: -11.319854736328125
Iteration 2621:
Training Loss: -7.505642890930176
Reconstruction Loss: -11.34091854095459
Iteration 2631:
Training Loss: -6.703397750854492
Reconstruction Loss: -11.346806526184082
Iteration 2641:
Training Loss: -7.436723709106445
Reconstruction Loss: -11.331048965454102
Iteration 2651:
Training Loss: -7.016890525817871
Reconstruction Loss: -11.341100692749023
Iteration 2661:
Training Loss: -7.105368614196777
Reconstruction Loss: -11.365219116210938
Iteration 2671:
Training Loss: -7.070180892944336
Reconstruction Loss: -11.363661766052246
Iteration 2681:
Training Loss: -7.227628707885742
Reconstruction Loss: -11.33166790008545
Iteration 2691:
Training Loss: -7.182987689971924
Reconstruction Loss: -11.377681732177734
Iteration 2701:
Training Loss: -7.041646957397461
Reconstruction Loss: -11.37656307220459
Iteration 2711:
Training Loss: -7.236215114593506
Reconstruction Loss: -11.380087852478027
Iteration 2721:
Training Loss: -7.045830249786377
Reconstruction Loss: -11.370603561401367
Iteration 2731:
Training Loss: -7.279434680938721
Reconstruction Loss: -11.354615211486816
Iteration 2741:
Training Loss: -7.340200424194336
Reconstruction Loss: -11.38003158569336
Iteration 2751:
Training Loss: -7.078039169311523
Reconstruction Loss: -11.375356674194336
Iteration 2761:
Training Loss: -7.664288520812988
Reconstruction Loss: -11.373433113098145
Iteration 2771:
Training Loss: -7.22934627532959
Reconstruction Loss: -11.38728141784668
Iteration 2781:
Training Loss: -7.340947151184082
Reconstruction Loss: -11.407987594604492
Iteration 2791:
Training Loss: -7.213433265686035
Reconstruction Loss: -11.371904373168945
Iteration 2801:
Training Loss: -7.496546745300293
Reconstruction Loss: -11.39372730255127
Iteration 2811:
Training Loss: -7.148842811584473
Reconstruction Loss: -11.408475875854492
Iteration 2821:
Training Loss: -7.258078575134277
Reconstruction Loss: -11.394124031066895
Iteration 2831:
Training Loss: -7.137744903564453
Reconstruction Loss: -11.389837265014648
Iteration 2841:
Training Loss: -7.487945556640625
Reconstruction Loss: -11.448305130004883
Iteration 2851:
Training Loss: -7.445967197418213
Reconstruction Loss: -11.40098762512207
Iteration 2861:
Training Loss: -7.406373500823975
Reconstruction Loss: -11.4116792678833
Iteration 2871:
Training Loss: -7.540121078491211
Reconstruction Loss: -11.41010570526123
Iteration 2881:
Training Loss: -7.060258865356445
Reconstruction Loss: -11.414283752441406
Iteration 2891:
Training Loss: -7.172305583953857
Reconstruction Loss: -11.398719787597656
Iteration 2901:
Training Loss: -6.958518028259277
Reconstruction Loss: -11.41312026977539
Iteration 2911:
Training Loss: -7.504728317260742
Reconstruction Loss: -11.425896644592285
Iteration 2921:
Training Loss: -6.896345615386963
Reconstruction Loss: -11.435171127319336
Iteration 2931:
Training Loss: -7.172506332397461
Reconstruction Loss: -11.408549308776855
Iteration 2941:
Training Loss: -7.410093784332275
Reconstruction Loss: -11.435569763183594
Iteration 2951:
Training Loss: -7.462082386016846
Reconstruction Loss: -11.443532943725586
Iteration 2961:
Training Loss: -7.668789386749268
Reconstruction Loss: -11.44497013092041
Iteration 2971:
Training Loss: -7.198683261871338
Reconstruction Loss: -11.464856147766113
Iteration 2981:
Training Loss: -7.137210369110107
Reconstruction Loss: -11.419090270996094
Iteration 2991:
Training Loss: -7.291335582733154
Reconstruction Loss: -11.449955940246582
Iteration 3001:
Training Loss: -7.107251167297363
Reconstruction Loss: -11.41781997680664
Iteration 3011:
Training Loss: -7.500628471374512
Reconstruction Loss: -11.427260398864746
Iteration 3021:
Training Loss: -7.424391746520996
Reconstruction Loss: -11.464357376098633
Iteration 3031:
Training Loss: -7.414137363433838
Reconstruction Loss: -11.455801963806152
Iteration 3041:
Training Loss: -7.577089309692383
Reconstruction Loss: -11.449453353881836
Iteration 3051:
Training Loss: -7.24029016494751
Reconstruction Loss: -11.43940544128418
Iteration 3061:
Training Loss: -7.216121673583984
Reconstruction Loss: -11.452032089233398
Iteration 3071:
Training Loss: -7.231780052185059
Reconstruction Loss: -11.49405574798584
Iteration 3081:
Training Loss: -7.565122604370117
Reconstruction Loss: -11.473973274230957
Iteration 3091:
Training Loss: -7.665724277496338
Reconstruction Loss: -11.456501960754395
Iteration 3101:
Training Loss: -7.471898078918457
Reconstruction Loss: -11.488030433654785
Iteration 3111:
Training Loss: -7.594181060791016
Reconstruction Loss: -11.484049797058105
Iteration 3121:
Training Loss: -7.507493019104004
Reconstruction Loss: -11.46254825592041
Iteration 3131:
Training Loss: -7.2236433029174805
Reconstruction Loss: -11.477898597717285
Iteration 3141:
Training Loss: -6.933368682861328
Reconstruction Loss: -11.48429012298584
Iteration 3151:
Training Loss: -7.0348663330078125
Reconstruction Loss: -11.496313095092773
Iteration 3161:
Training Loss: -7.293184280395508
Reconstruction Loss: -11.487356185913086
Iteration 3171:
Training Loss: -7.549529552459717
Reconstruction Loss: -11.501286506652832
Iteration 3181:
Training Loss: -7.505530834197998
Reconstruction Loss: -11.48647689819336
Iteration 3191:
Training Loss: -7.61454963684082
Reconstruction Loss: -11.460289001464844
Iteration 3201:
Training Loss: -7.6680450439453125
Reconstruction Loss: -11.520975112915039
Iteration 3211:
Training Loss: -7.3451738357543945
Reconstruction Loss: -11.494897842407227
Iteration 3221:
Training Loss: -7.151181697845459
Reconstruction Loss: -11.496087074279785
Iteration 3231:
Training Loss: -7.136378765106201
Reconstruction Loss: -11.513468742370605
Iteration 3241:
Training Loss: -7.419963359832764
Reconstruction Loss: -11.509712219238281
Iteration 3251:
Training Loss: -7.481200218200684
Reconstruction Loss: -11.504037857055664
Iteration 3261:
Training Loss: -7.444745063781738
Reconstruction Loss: -11.513541221618652
Iteration 3271:
Training Loss: -7.383888244628906
Reconstruction Loss: -11.49184799194336
Iteration 3281:
Training Loss: -7.656698226928711
Reconstruction Loss: -11.500238418579102
Iteration 3291:
Training Loss: -7.233588695526123
Reconstruction Loss: -11.525147438049316
Iteration 3301:
Training Loss: -7.465200424194336
Reconstruction Loss: -11.51506233215332
Iteration 3311:
Training Loss: -7.334963798522949
Reconstruction Loss: -11.506993293762207
Iteration 3321:
Training Loss: -7.785041809082031
Reconstruction Loss: -11.522151947021484
Iteration 3331:
Training Loss: -7.649436950683594
Reconstruction Loss: -11.524666786193848
Iteration 3341:
Training Loss: -8.120277404785156
Reconstruction Loss: -11.550079345703125
Iteration 3351:
Training Loss: -7.0465168952941895
Reconstruction Loss: -11.533588409423828
Iteration 3361:
Training Loss: -7.5414581298828125
Reconstruction Loss: -11.55859375
Iteration 3371:
Training Loss: -8.039087295532227
Reconstruction Loss: -11.532609939575195
Iteration 3381:
Training Loss: -8.034322738647461
Reconstruction Loss: -11.548142433166504
Iteration 3391:
Training Loss: -7.693401336669922
Reconstruction Loss: -11.5595064163208
Iteration 3401:
Training Loss: -7.836472511291504
Reconstruction Loss: -11.530991554260254
Iteration 3411:
Training Loss: -7.276008129119873
Reconstruction Loss: -11.552313804626465
Iteration 3421:
Training Loss: -7.371499061584473
Reconstruction Loss: -11.568252563476562
Iteration 3431:
Training Loss: -7.439408779144287
Reconstruction Loss: -11.558548927307129
Iteration 3441:
Training Loss: -7.821272850036621
Reconstruction Loss: -11.5519437789917
Iteration 3451:
Training Loss: -7.542424201965332
Reconstruction Loss: -11.555011749267578
Iteration 3461:
Training Loss: -7.660658836364746
Reconstruction Loss: -11.575325012207031
Iteration 3471:
Training Loss: -7.191120624542236
Reconstruction Loss: -11.56410026550293
Iteration 3481:
Training Loss: -7.468425273895264
Reconstruction Loss: -11.550053596496582
Iteration 3491:
Training Loss: -7.262875080108643
Reconstruction Loss: -11.549337387084961
Iteration 3501:
Training Loss: -7.483679294586182
Reconstruction Loss: -11.57589340209961
Iteration 3511:
Training Loss: -7.4743523597717285
Reconstruction Loss: -11.560977935791016
Iteration 3521:
Training Loss: -7.326372146606445
Reconstruction Loss: -11.53897476196289
Iteration 3531:
Training Loss: -7.375217437744141
Reconstruction Loss: -11.561941146850586
Iteration 3541:
Training Loss: -7.1722493171691895
Reconstruction Loss: -11.589333534240723
Iteration 3551:
Training Loss: -7.6098761558532715
Reconstruction Loss: -11.55959415435791
Iteration 3561:
Training Loss: -7.588729381561279
Reconstruction Loss: -11.61583137512207
Iteration 3571:
Training Loss: -7.753904342651367
Reconstruction Loss: -11.578351020812988
Iteration 3581:
Training Loss: -7.720996379852295
Reconstruction Loss: -11.59776496887207
Iteration 3591:
Training Loss: -7.413297176361084
Reconstruction Loss: -11.586870193481445
Iteration 3601:
Training Loss: -7.60683536529541
Reconstruction Loss: -11.591912269592285
Iteration 3611:
Training Loss: -7.299875736236572
Reconstruction Loss: -11.594954490661621
Iteration 3621:
Training Loss: -7.589339256286621
Reconstruction Loss: -11.601119041442871
Iteration 3631:
Training Loss: -7.451201438903809
Reconstruction Loss: -11.605466842651367
Iteration 3641:
Training Loss: -7.473394870758057
Reconstruction Loss: -11.605502128601074
Iteration 3651:
Training Loss: -7.772212505340576
Reconstruction Loss: -11.60940933227539
Iteration 3661:
Training Loss: -7.599383354187012
Reconstruction Loss: -11.598485946655273
Iteration 3671:
Training Loss: -7.44848108291626
Reconstruction Loss: -11.631366729736328
Iteration 3681:
Training Loss: -7.638874053955078
Reconstruction Loss: -11.594189643859863
Iteration 3691:
Training Loss: -7.535062313079834
Reconstruction Loss: -11.623106002807617
Iteration 3701:
Training Loss: -7.30142068862915
Reconstruction Loss: -11.595688819885254
Iteration 3711:
Training Loss: -7.445528507232666
Reconstruction Loss: -11.59839916229248
Iteration 3721:
Training Loss: -7.473761558532715
Reconstruction Loss: -11.638240814208984
Iteration 3731:
Training Loss: -7.531338214874268
Reconstruction Loss: -11.605884552001953
Iteration 3741:
Training Loss: -7.5501179695129395
Reconstruction Loss: -11.644133567810059
Iteration 3751:
Training Loss: -7.468203544616699
Reconstruction Loss: -11.62356185913086
Iteration 3761:
Training Loss: -7.271462440490723
Reconstruction Loss: -11.640609741210938
Iteration 3771:
Training Loss: -7.70513916015625
Reconstruction Loss: -11.647208213806152
Iteration 3781:
Training Loss: -7.325801849365234
Reconstruction Loss: -11.637247085571289
Iteration 3791:
Training Loss: -7.685638427734375
Reconstruction Loss: -11.62919807434082
Iteration 3801:
Training Loss: -7.8152689933776855
Reconstruction Loss: -11.620813369750977
Iteration 3811:
Training Loss: -7.519772052764893
Reconstruction Loss: -11.64190673828125
Iteration 3821:
Training Loss: -7.320508003234863
Reconstruction Loss: -11.646631240844727
Iteration 3831:
Training Loss: -7.33652400970459
Reconstruction Loss: -11.619507789611816
Iteration 3841:
Training Loss: -7.439123153686523
Reconstruction Loss: -11.61909008026123
Iteration 3851:
Training Loss: -7.441990852355957
Reconstruction Loss: -11.66434097290039
Iteration 3861:
Training Loss: -7.662738800048828
Reconstruction Loss: -11.641103744506836
Iteration 3871:
Training Loss: -7.07884407043457
Reconstruction Loss: -11.680472373962402
Iteration 3881:
Training Loss: -7.376776218414307
Reconstruction Loss: -11.653169631958008
Iteration 3891:
Training Loss: -7.422689914703369
Reconstruction Loss: -11.649785041809082
Iteration 3901:
Training Loss: -7.566993236541748
Reconstruction Loss: -11.669013023376465
Iteration 3911:
Training Loss: -7.440892696380615
Reconstruction Loss: -11.638161659240723
Iteration 3921:
Training Loss: -7.387511253356934
Reconstruction Loss: -11.655102729797363
Iteration 3931:
Training Loss: -7.323966026306152
Reconstruction Loss: -11.649372100830078
Iteration 3941:
Training Loss: -7.547512054443359
Reconstruction Loss: -11.633625030517578
Iteration 3951:
Training Loss: -7.143164157867432
Reconstruction Loss: -11.641599655151367
Iteration 3961:
Training Loss: -8.146903038024902
Reconstruction Loss: -11.688045501708984
Iteration 3971:
Training Loss: -7.873673439025879
Reconstruction Loss: -11.651985168457031
Iteration 3981:
Training Loss: -8.122961044311523
Reconstruction Loss: -11.68665885925293
Iteration 3991:
Training Loss: -7.665256023406982
Reconstruction Loss: -11.657516479492188
Iteration 4001:
Training Loss: -7.742145538330078
Reconstruction Loss: -11.654364585876465
Iteration 4011:
Training Loss: -7.477643966674805
Reconstruction Loss: -11.683537483215332
Iteration 4021:
Training Loss: -7.4760942459106445
Reconstruction Loss: -11.677959442138672
Iteration 4031:
Training Loss: -7.665218830108643
Reconstruction Loss: -11.66966724395752
Iteration 4041:
Training Loss: -7.278140068054199
Reconstruction Loss: -11.683432579040527
Iteration 4051:
Training Loss: -7.358310222625732
Reconstruction Loss: -11.696083068847656
Iteration 4061:
Training Loss: -7.718422889709473
Reconstruction Loss: -11.703144073486328
Iteration 4071:
Training Loss: -7.80249547958374
Reconstruction Loss: -11.653477668762207
Iteration 4081:
Training Loss: -7.647392272949219
Reconstruction Loss: -11.686853408813477
Iteration 4091:
Training Loss: -7.8389763832092285
Reconstruction Loss: -11.699200630187988
Iteration 4101:
Training Loss: -7.8229756355285645
Reconstruction Loss: -11.695112228393555
Iteration 4111:
Training Loss: -7.835220813751221
Reconstruction Loss: -11.68913459777832
Iteration 4121:
Training Loss: -7.508315563201904
Reconstruction Loss: -11.700303077697754
Iteration 4131:
Training Loss: -7.528757095336914
Reconstruction Loss: -11.669501304626465
Iteration 4141:
Training Loss: -7.593076229095459
Reconstruction Loss: -11.69090747833252
Iteration 4151:
Training Loss: -7.624472141265869
Reconstruction Loss: -11.710883140563965
Iteration 4161:
Training Loss: -7.612033843994141
Reconstruction Loss: -11.710375785827637
Iteration 4171:
Training Loss: -7.725624084472656
Reconstruction Loss: -11.703722953796387
Iteration 4181:
Training Loss: -7.8871002197265625
Reconstruction Loss: -11.716511726379395
Iteration 4191:
Training Loss: -8.32696533203125
Reconstruction Loss: -11.723370552062988
Iteration 4201:
Training Loss: -7.4856367111206055
Reconstruction Loss: -11.73006534576416
Iteration 4211:
Training Loss: -7.244059085845947
Reconstruction Loss: -11.73092269897461
Iteration 4221:
Training Loss: -7.618102073669434
Reconstruction Loss: -11.731877326965332
Iteration 4231:
Training Loss: -7.462599754333496
Reconstruction Loss: -11.722046852111816
Iteration 4241:
Training Loss: -7.31300163269043
Reconstruction Loss: -11.741971015930176
Iteration 4251:
Training Loss: -7.4004316329956055
Reconstruction Loss: -11.712179183959961
Iteration 4261:
Training Loss: -7.932131290435791
Reconstruction Loss: -11.726316452026367
Iteration 4271:
Training Loss: -7.4157280921936035
Reconstruction Loss: -11.735254287719727
Iteration 4281:
Training Loss: -8.033903121948242
Reconstruction Loss: -11.731316566467285
Iteration 4291:
Training Loss: -7.4824347496032715
Reconstruction Loss: -11.721223831176758
Iteration 4301:
Training Loss: -7.783227920532227
Reconstruction Loss: -11.738068580627441
Iteration 4311:
Training Loss: -7.536290168762207
Reconstruction Loss: -11.738194465637207
Iteration 4321:
Training Loss: -8.226692199707031
Reconstruction Loss: -11.745335578918457
Iteration 4331:
Training Loss: -8.143314361572266
Reconstruction Loss: -11.759247779846191
Iteration 4341:
Training Loss: -7.413810729980469
Reconstruction Loss: -11.739133834838867
Iteration 4351:
Training Loss: -7.425443172454834
Reconstruction Loss: -11.756169319152832
Iteration 4361:
Training Loss: -8.11823558807373
Reconstruction Loss: -11.75367546081543
Iteration 4371:
Training Loss: -7.355243682861328
Reconstruction Loss: -11.728597640991211
Iteration 4381:
Training Loss: -7.605294227600098
Reconstruction Loss: -11.751462936401367
Iteration 4391:
Training Loss: -7.875422477722168
Reconstruction Loss: -11.739789009094238
Iteration 4401:
Training Loss: -8.12479305267334
Reconstruction Loss: -11.749305725097656
Iteration 4411:
Training Loss: -8.178622245788574
Reconstruction Loss: -11.764087677001953
Iteration 4421:
Training Loss: -7.565113544464111
Reconstruction Loss: -11.767834663391113
Iteration 4431:
Training Loss: -7.93875789642334
Reconstruction Loss: -11.788975715637207
Iteration 4441:
Training Loss: -7.8173699378967285
Reconstruction Loss: -11.760817527770996
Iteration 4451:
Training Loss: -7.9167375564575195
Reconstruction Loss: -11.762017250061035
Iteration 4461:
Training Loss: -7.540003299713135
Reconstruction Loss: -11.761373519897461
Iteration 4471:
Training Loss: -7.562062740325928
Reconstruction Loss: -11.769142150878906
Iteration 4481:
Training Loss: -8.051575660705566
Reconstruction Loss: -11.781858444213867
Iteration 4491:
Training Loss: -7.624624729156494
Reconstruction Loss: -11.757027626037598
Iteration 4501:
Training Loss: -7.701352119445801
Reconstruction Loss: -11.770186424255371
Iteration 4511:
Training Loss: -7.7368083000183105
Reconstruction Loss: -11.79505443572998
Iteration 4521:
Training Loss: -7.538169860839844
Reconstruction Loss: -11.768404960632324
Iteration 4531:
Training Loss: -7.5179877281188965
Reconstruction Loss: -11.772141456604004
Iteration 4541:
Training Loss: -7.767111301422119
Reconstruction Loss: -11.804316520690918
Iteration 4551:
Training Loss: -8.171262741088867
Reconstruction Loss: -11.783926963806152
Iteration 4561:
Training Loss: -7.326765060424805
Reconstruction Loss: -11.813206672668457
Iteration 4571:
Training Loss: -7.848258972167969
Reconstruction Loss: -11.7637939453125
Iteration 4581:
Training Loss: -7.807464122772217
Reconstruction Loss: -11.784601211547852
Iteration 4591:
Training Loss: -7.641796588897705
Reconstruction Loss: -11.772797584533691
Iteration 4601:
Training Loss: -7.419529914855957
Reconstruction Loss: -11.79097843170166
Iteration 4611:
Training Loss: -7.94581413269043
Reconstruction Loss: -11.79247760772705
Iteration 4621:
Training Loss: -7.943827152252197
Reconstruction Loss: -11.784236907958984
Iteration 4631:
Training Loss: -7.804686546325684
Reconstruction Loss: -11.793571472167969
Iteration 4641:
Training Loss: -7.684421062469482
Reconstruction Loss: -11.78052043914795
Iteration 4651:
Training Loss: -8.114517211914062
Reconstruction Loss: -11.778100967407227
Iteration 4661:
Training Loss: -7.439374923706055
Reconstruction Loss: -11.813863754272461
Iteration 4671:
Training Loss: -7.402138710021973
Reconstruction Loss: -11.800190925598145
Iteration 4681:
Training Loss: -7.660092353820801
Reconstruction Loss: -11.801675796508789
Iteration 4691:
Training Loss: -7.843813896179199
Reconstruction Loss: -11.815275192260742
Iteration 4701:
Training Loss: -7.408289432525635
Reconstruction Loss: -11.813624382019043
Iteration 4711:
Training Loss: -7.83304500579834
Reconstruction Loss: -11.800315856933594
Iteration 4721:
Training Loss: -7.52806282043457
Reconstruction Loss: -11.82542610168457
Iteration 4731:
Training Loss: -7.827274799346924
Reconstruction Loss: -11.810166358947754
Iteration 4741:
Training Loss: -8.364866256713867
Reconstruction Loss: -11.814204216003418
Iteration 4751:
Training Loss: -7.472192287445068
Reconstruction Loss: -11.824363708496094
Iteration 4761:
Training Loss: -8.385010719299316
Reconstruction Loss: -11.807795524597168
Iteration 4771:
Training Loss: -7.529348850250244
Reconstruction Loss: -11.833534240722656
Iteration 4781:
Training Loss: -7.868165493011475
Reconstruction Loss: -11.834537506103516
Iteration 4791:
Training Loss: -7.786023139953613
Reconstruction Loss: -11.817651748657227
Iteration 4801:
Training Loss: -8.071242332458496
Reconstruction Loss: -11.810425758361816
Iteration 4811:
Training Loss: -8.214594841003418
Reconstruction Loss: -11.809991836547852
Iteration 4821:
Training Loss: -8.174748420715332
Reconstruction Loss: -11.794401168823242
Iteration 4831:
Training Loss: -7.382452487945557
Reconstruction Loss: -11.817736625671387
Iteration 4841:
Training Loss: -7.82952880859375
Reconstruction Loss: -11.841727256774902
Iteration 4851:
Training Loss: -7.301262378692627
Reconstruction Loss: -11.811532974243164
Iteration 4861:
Training Loss: -7.771397590637207
Reconstruction Loss: -11.826915740966797
Iteration 4871:
Training Loss: -7.855884075164795
Reconstruction Loss: -11.819498062133789
Iteration 4881:
Training Loss: -8.354302406311035
Reconstruction Loss: -11.850929260253906
Iteration 4891:
Training Loss: -7.769110202789307
Reconstruction Loss: -11.85283088684082
Iteration 4901:
Training Loss: -7.945338249206543
Reconstruction Loss: -11.85188102722168
Iteration 4911:
Training Loss: -8.049907684326172
Reconstruction Loss: -11.85197925567627
Iteration 4921:
Training Loss: -7.787959575653076
Reconstruction Loss: -11.844454765319824
Iteration 4931:
Training Loss: -7.591484069824219
Reconstruction Loss: -11.845760345458984
Iteration 4941:
Training Loss: -7.703498363494873
Reconstruction Loss: -11.849573135375977
Iteration 4951:
Training Loss: -7.691104412078857
Reconstruction Loss: -11.841777801513672
Iteration 4961:
Training Loss: -8.0169038772583
Reconstruction Loss: -11.827619552612305
Iteration 4971:
Training Loss: -8.394835472106934
Reconstruction Loss: -11.854166030883789
Iteration 4981:
Training Loss: -8.339280128479004
Reconstruction Loss: -11.841019630432129
Iteration 4991:
Training Loss: -7.660065650939941
Reconstruction Loss: -11.849326133728027
