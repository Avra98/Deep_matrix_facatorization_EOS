5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
    (3): Parameter containing: [torch.float32 of size 20x20]
    (4): Parameter containing: [torch.float32 of size 20x20]
)
Iteration 1:
Training Loss: 5.560806751251221
Reconstruction Loss: -0.3713933825492859
Iteration 101:
Training Loss: 2.8680734634399414
Reconstruction Loss: -1.2107124328613281
Iteration 201:
Training Loss: 1.6590977907180786
Reconstruction Loss: -1.5847326517105103
Iteration 301:
Training Loss: 0.8434177041053772
Reconstruction Loss: -1.939348578453064
Iteration 401:
Training Loss: 0.28293469548225403
Reconstruction Loss: -2.244662284851074
Iteration 501:
Training Loss: -0.14678333699703217
Reconstruction Loss: -2.5046868324279785
Iteration 601:
Training Loss: -0.4965623617172241
Reconstruction Loss: -2.725935697555542
Iteration 701:
Training Loss: -0.7920364141464233
Reconstruction Loss: -2.915210485458374
Iteration 801:
Training Loss: -1.0484051704406738
Reconstruction Loss: -3.078366756439209
Iteration 901:
Training Loss: -1.2748478651046753
Reconstruction Loss: -3.2200560569763184
Iteration 1001:
Training Loss: -1.4772130250930786
Reconstruction Loss: -3.3439507484436035
Iteration 1101:
Training Loss: -1.6595773696899414
Reconstruction Loss: -3.4529976844787598
Iteration 1201:
Training Loss: -1.8250210285186768
Reconstruction Loss: -3.5495896339416504
Iteration 1301:
Training Loss: -1.9760026931762695
Reconstruction Loss: -3.6356887817382812
Iteration 1401:
Training Loss: -2.11452579498291
Reconstruction Loss: -3.712904930114746
Iteration 1501:
Training Loss: -2.2422447204589844
Reconstruction Loss: -3.7825634479522705
Iteration 1601:
Training Loss: -2.3605363368988037
Reconstruction Loss: -3.845757007598877
Iteration 1701:
Training Loss: -2.4705445766448975
Reconstruction Loss: -3.903388500213623
Iteration 1801:
Training Loss: -2.573235273361206
Reconstruction Loss: -3.9562079906463623
Iteration 1901:
Training Loss: -2.669417381286621
Reconstruction Loss: -4.0048394203186035
Iteration 2001:
Training Loss: -2.7597837448120117
Reconstruction Loss: -4.049805641174316
Iteration 2101:
Training Loss: -2.844923496246338
Reconstruction Loss: -4.091547012329102
Iteration 2201:
Training Loss: -2.925344467163086
Reconstruction Loss: -4.130439281463623
Iteration 2301:
Training Loss: -3.001485824584961
Reconstruction Loss: -4.166797637939453
Iteration 2401:
Training Loss: -3.073741912841797
Reconstruction Loss: -4.200895309448242
Iteration 2501:
Training Loss: -3.142441749572754
Reconstruction Loss: -4.232966423034668
Iteration 2601:
Training Loss: -3.207895517349243
Reconstruction Loss: -4.2632155418396
Iteration 2701:
Training Loss: -3.2703628540039062
Reconstruction Loss: -4.291816234588623
Iteration 2801:
Training Loss: -3.330085515975952
Reconstruction Loss: -4.318922996520996
Iteration 2901:
Training Loss: -3.3872780799865723
Reconstruction Loss: -4.344669818878174
Iteration 3001:
Training Loss: -3.4421284198760986
Reconstruction Loss: -4.369176864624023
Iteration 3101:
Training Loss: -3.494817018508911
Reconstruction Loss: -4.392545700073242
Iteration 3201:
Training Loss: -3.545496940612793
Reconstruction Loss: -4.414869785308838
Iteration 3301:
Training Loss: -3.594310760498047
Reconstruction Loss: -4.436232089996338
Iteration 3401:
Training Loss: -3.6413886547088623
Reconstruction Loss: -4.456705570220947
Iteration 3501:
Training Loss: -3.686849594116211
Reconstruction Loss: -4.476357936859131
Iteration 3601:
Training Loss: -3.7307984828948975
Reconstruction Loss: -4.495243072509766
Iteration 3701:
Training Loss: -3.773340940475464
Reconstruction Loss: -4.5134172439575195
Iteration 3801:
Training Loss: -3.8145558834075928
Reconstruction Loss: -4.530927658081055
Iteration 3901:
Training Loss: -3.854534864425659
Reconstruction Loss: -4.547818183898926
Iteration 4001:
Training Loss: -3.8933515548706055
Reconstruction Loss: -4.5641255378723145
Iteration 4101:
Training Loss: -3.931077003479004
Reconstruction Loss: -4.579889297485352
Iteration 4201:
Training Loss: -3.9677722454071045
Reconstruction Loss: -4.595139503479004
Iteration 4301:
Training Loss: -4.003493309020996
Reconstruction Loss: -4.6099042892456055
Iteration 4401:
Training Loss: -4.038304805755615
Reconstruction Loss: -4.624213218688965
Iteration 4501:
Training Loss: -4.072249889373779
Reconstruction Loss: -4.63809061050415
Iteration 4601:
Training Loss: -4.105373859405518
Reconstruction Loss: -4.651559352874756
Iteration 4701:
Training Loss: -4.1377272605896
Reconstruction Loss: -4.664642333984375
Iteration 4801:
Training Loss: -4.169342994689941
Reconstruction Loss: -4.677356243133545
Iteration 4901:
Training Loss: -4.200259685516357
Reconstruction Loss: -4.6897196769714355
Iteration 5001:
Training Loss: -4.2305169105529785
Reconstruction Loss: -4.701751232147217
Iteration 5101:
Training Loss: -4.2601399421691895
Reconstruction Loss: -4.713463306427002
Iteration 5201:
Training Loss: -4.2891621589660645
Reconstruction Loss: -4.7248735427856445
Iteration 5301:
Training Loss: -4.3176116943359375
Reconstruction Loss: -4.735994338989258
Iteration 5401:
Training Loss: -4.345508098602295
Reconstruction Loss: -4.746838092803955
Iteration 5501:
Training Loss: -4.372888565063477
Reconstruction Loss: -4.75741720199585
Iteration 5601:
Training Loss: -4.399760723114014
Reconstruction Loss: -4.767743110656738
Iteration 5701:
Training Loss: -4.426159858703613
Reconstruction Loss: -4.777825832366943
Iteration 5801:
Training Loss: -4.4520955085754395
Reconstruction Loss: -4.787671089172363
Iteration 5901:
Training Loss: -4.477592945098877
Reconstruction Loss: -4.797292232513428
Iteration 6001:
Training Loss: -4.5026702880859375
Reconstruction Loss: -4.806698322296143
Iteration 6101:
Training Loss: -4.527336120605469
Reconstruction Loss: -4.815895080566406
Iteration 6201:
Training Loss: -4.551609039306641
Reconstruction Loss: -4.82489013671875
Iteration 6301:
Training Loss: -4.575503349304199
Reconstruction Loss: -4.833693027496338
Iteration 6401:
Training Loss: -4.599040508270264
Reconstruction Loss: -4.84230899810791
Iteration 6501:
Training Loss: -4.62222146987915
Reconstruction Loss: -4.850744247436523
Iteration 6601:
Training Loss: -4.645071029663086
Reconstruction Loss: -4.859008312225342
Iteration 6701:
Training Loss: -4.6675920486450195
Reconstruction Loss: -4.867101669311523
Iteration 6801:
Training Loss: -4.689804553985596
Reconstruction Loss: -4.875032901763916
Iteration 6901:
Training Loss: -4.711702346801758
Reconstruction Loss: -4.882805824279785
Iteration 7001:
Training Loss: -4.7333083152771
Reconstruction Loss: -4.890427589416504
Iteration 7101:
Training Loss: -4.754637718200684
Reconstruction Loss: -4.8979034423828125
Iteration 7201:
Training Loss: -4.775681495666504
Reconstruction Loss: -4.905234336853027
Iteration 7301:
Training Loss: -4.796454906463623
Reconstruction Loss: -4.912426948547363
Iteration 7401:
Training Loss: -4.816969871520996
Reconstruction Loss: -4.919484615325928
Iteration 7501:
Training Loss: -4.837237358093262
Reconstruction Loss: -4.926413536071777
Iteration 7601:
Training Loss: -4.8572540283203125
Reconstruction Loss: -4.9332146644592285
Iteration 7701:
Training Loss: -4.877030372619629
Reconstruction Loss: -4.939892768859863
Iteration 7801:
Training Loss: -4.89658260345459
Reconstruction Loss: -4.946455478668213
Iteration 7901:
Training Loss: -4.915904521942139
Reconstruction Loss: -4.952897548675537
Iteration 8001:
Training Loss: -4.935006141662598
Reconstruction Loss: -4.959226131439209
Iteration 8101:
Training Loss: -4.953894138336182
Reconstruction Loss: -4.965444564819336
Iteration 8201:
Training Loss: -4.972578525543213
Reconstruction Loss: -4.971560001373291
Iteration 8301:
Training Loss: -4.991058349609375
Reconstruction Loss: -4.977571964263916
Iteration 8401:
Training Loss: -5.00933313369751
Reconstruction Loss: -4.9834794998168945
Iteration 8501:
Training Loss: -5.027421951293945
Reconstruction Loss: -4.98928689956665
Iteration 8601:
Training Loss: -5.0453200340271
Reconstruction Loss: -4.994998931884766
Iteration 8701:
Training Loss: -5.063032150268555
Reconstruction Loss: -5.000616073608398
Iteration 8801:
Training Loss: -5.080566883087158
Reconstruction Loss: -5.0061421394348145
Iteration 8901:
Training Loss: -5.097923278808594
Reconstruction Loss: -5.011582374572754
Iteration 9001:
Training Loss: -5.11511754989624
Reconstruction Loss: -5.016934871673584
Iteration 9101:
Training Loss: -5.132137298583984
Reconstruction Loss: -5.02220344543457
Iteration 9201:
Training Loss: -5.148986339569092
Reconstruction Loss: -5.027388095855713
Iteration 9301:
Training Loss: -5.165678977966309
Reconstruction Loss: -5.032492637634277
Iteration 9401:
Training Loss: -5.182220458984375
Reconstruction Loss: -5.037515640258789
Iteration 9501:
Training Loss: -5.198600769042969
Reconstruction Loss: -5.042466640472412
Iteration 9601:
Training Loss: -5.214832782745361
Reconstruction Loss: -5.047343730926514
Iteration 9701:
Training Loss: -5.230914115905762
Reconstruction Loss: -5.052145957946777
Iteration 9801:
Training Loss: -5.246857166290283
Reconstruction Loss: -5.056873798370361
Iteration 9901:
Training Loss: -5.262655735015869
Reconstruction Loss: -5.061530113220215
Iteration 10001:
Training Loss: -5.278314590454102
Reconstruction Loss: -5.066115856170654
Iteration 10101:
Training Loss: -5.293824672698975
Reconstruction Loss: -5.070635795593262
Iteration 10201:
Training Loss: -5.309213638305664
Reconstruction Loss: -5.075089931488037
Iteration 10301:
Training Loss: -5.324473857879639
Reconstruction Loss: -5.079485893249512
Iteration 10401:
Training Loss: -5.339596748352051
Reconstruction Loss: -5.083818435668945
Iteration 10501:
Training Loss: -5.354588031768799
Reconstruction Loss: -5.088091850280762
Iteration 10601:
Training Loss: -5.36945915222168
Reconstruction Loss: -5.092301368713379
Iteration 10701:
Training Loss: -5.384204864501953
Reconstruction Loss: -5.096452236175537
Iteration 10801:
Training Loss: -5.398838520050049
Reconstruction Loss: -5.100546836853027
Iteration 10901:
Training Loss: -5.413353443145752
Reconstruction Loss: -5.104584693908691
Iteration 11001:
Training Loss: -5.4277520179748535
Reconstruction Loss: -5.108566761016846
Iteration 11101:
Training Loss: -5.442025661468506
Reconstruction Loss: -5.1124958992004395
Iteration 11201:
Training Loss: -5.456199645996094
Reconstruction Loss: -5.116370677947998
Iteration 11301:
Training Loss: -5.470249176025391
Reconstruction Loss: -5.120196342468262
Iteration 11401:
Training Loss: -5.484188556671143
Reconstruction Loss: -5.123970031738281
Iteration 11501:
Training Loss: -5.498037815093994
Reconstruction Loss: -5.127696514129639
Iteration 11601:
Training Loss: -5.511767387390137
Reconstruction Loss: -5.131375312805176
Iteration 11701:
Training Loss: -5.525403022766113
Reconstruction Loss: -5.135004043579102
Iteration 11801:
Training Loss: -5.538936614990234
Reconstruction Loss: -5.138583660125732
Iteration 11901:
Training Loss: -5.552363395690918
Reconstruction Loss: -5.142117977142334
Iteration 12001:
Training Loss: -5.5656962394714355
Reconstruction Loss: -5.145609378814697
Iteration 12101:
Training Loss: -5.578927516937256
Reconstruction Loss: -5.149057388305664
Iteration 12201:
Training Loss: -5.592068195343018
Reconstruction Loss: -5.152458667755127
Iteration 12301:
Training Loss: -5.605104923248291
Reconstruction Loss: -5.1558146476745605
Iteration 12401:
Training Loss: -5.618045806884766
Reconstruction Loss: -5.159129619598389
Iteration 12501:
Training Loss: -5.630909442901611
Reconstruction Loss: -5.162402153015137
Iteration 12601:
Training Loss: -5.643670082092285
Reconstruction Loss: -5.165637493133545
Iteration 12701:
Training Loss: -5.656354904174805
Reconstruction Loss: -5.168834209442139
Iteration 12801:
Training Loss: -5.668938159942627
Reconstruction Loss: -5.1719889640808105
Iteration 12901:
Training Loss: -5.681443214416504
Reconstruction Loss: -5.175107955932617
Iteration 13001:
Training Loss: -5.693859577178955
Reconstruction Loss: -5.178194046020508
Iteration 13101:
Training Loss: -5.70619010925293
Reconstruction Loss: -5.18124532699585
Iteration 13201:
Training Loss: -5.7184343338012695
Reconstruction Loss: -5.184259414672852
Iteration 13301:
Training Loss: -5.73059606552124
Reconstruction Loss: -5.187236309051514
Iteration 13401:
Training Loss: -5.742694854736328
Reconstruction Loss: -5.190176963806152
Iteration 13501:
Training Loss: -5.754704475402832
Reconstruction Loss: -5.193085193634033
Iteration 13601:
Training Loss: -5.766637802124023
Reconstruction Loss: -5.195958614349365
Iteration 13701:
Training Loss: -5.778487682342529
Reconstruction Loss: -5.198801517486572
Iteration 13801:
Training Loss: -5.790261268615723
Reconstruction Loss: -5.2016119956970215
Iteration 13901:
Training Loss: -5.8019585609436035
Reconstruction Loss: -5.2043914794921875
Iteration 14001:
Training Loss: -5.813582897186279
Reconstruction Loss: -5.207137107849121
Iteration 14101:
Training Loss: -5.825125694274902
Reconstruction Loss: -5.209850788116455
Iteration 14201:
Training Loss: -5.836609363555908
Reconstruction Loss: -5.212535858154297
Iteration 14301:
Training Loss: -5.848014831542969
Reconstruction Loss: -5.215190887451172
Iteration 14401:
Training Loss: -5.859353065490723
Reconstruction Loss: -5.217816352844238
Iteration 14501:
Training Loss: -5.870616436004639
Reconstruction Loss: -5.220414638519287
Iteration 14601:
Training Loss: -5.881809234619141
Reconstruction Loss: -5.222985744476318
Iteration 14701:
Training Loss: -5.892942428588867
Reconstruction Loss: -5.225530624389648
Iteration 14801:
Training Loss: -5.9040069580078125
Reconstruction Loss: -5.22804594039917
Iteration 14901:
Training Loss: -5.914998531341553
Reconstruction Loss: -5.230533599853516
Iteration 15001:
Training Loss: -5.925927639007568
Reconstruction Loss: -5.232995986938477
Iteration 15101:
Training Loss: -5.93679141998291
Reconstruction Loss: -5.235433101654053
Iteration 15201:
Training Loss: -5.947589874267578
Reconstruction Loss: -5.237843036651611
Iteration 15301:
Training Loss: -5.958324909210205
Reconstruction Loss: -5.240229606628418
Iteration 15401:
Training Loss: -5.9690070152282715
Reconstruction Loss: -5.242591381072998
Iteration 15501:
Training Loss: -5.979633331298828
Reconstruction Loss: -5.244926452636719
Iteration 15601:
Training Loss: -5.990184783935547
Reconstruction Loss: -5.247237205505371
Iteration 15701:
Training Loss: -6.000683307647705
Reconstruction Loss: -5.249525547027588
Iteration 15801:
Training Loss: -6.011120319366455
Reconstruction Loss: -5.251792907714844
Iteration 15901:
Training Loss: -6.0214996337890625
Reconstruction Loss: -5.254037380218506
Iteration 16001:
Training Loss: -6.031835079193115
Reconstruction Loss: -5.25626277923584
Iteration 16101:
Training Loss: -6.042109966278076
Reconstruction Loss: -5.258467197418213
Iteration 16201:
Training Loss: -6.052310466766357
Reconstruction Loss: -5.2606520652771
Iteration 16301:
Training Loss: -6.062481880187988
Reconstruction Loss: -5.262813091278076
Iteration 16401:
Training Loss: -6.072564125061035
Reconstruction Loss: -5.264952182769775
Iteration 16501:
Training Loss: -6.082601547241211
Reconstruction Loss: -5.2670674324035645
Iteration 16601:
Training Loss: -6.0925798416137695
Reconstruction Loss: -5.269162178039551
Iteration 16701:
Training Loss: -6.102517604827881
Reconstruction Loss: -5.271238803863525
Iteration 16801:
Training Loss: -6.112396240234375
Reconstruction Loss: -5.273294448852539
Iteration 16901:
Training Loss: -6.122226715087891
Reconstruction Loss: -5.275328636169434
Iteration 17001:
Training Loss: -6.132011890411377
Reconstruction Loss: -5.27734375
Iteration 17101:
Training Loss: -6.141739368438721
Reconstruction Loss: -5.279340744018555
Iteration 17201:
Training Loss: -6.151415824890137
Reconstruction Loss: -5.281320095062256
Iteration 17301:
Training Loss: -6.161064147949219
Reconstruction Loss: -5.283282279968262
Iteration 17401:
Training Loss: -6.170642852783203
Reconstruction Loss: -5.285226345062256
Iteration 17501:
Training Loss: -6.18015718460083
Reconstruction Loss: -5.287148475646973
Iteration 17601:
Training Loss: -6.189640998840332
Reconstruction Loss: -5.2890543937683105
Iteration 17701:
Training Loss: -6.199068069458008
Reconstruction Loss: -5.290944576263428
Iteration 17801:
Training Loss: -6.208459854125977
Reconstruction Loss: -5.29281759262085
Iteration 17901:
Training Loss: -6.2178144454956055
Reconstruction Loss: -5.2946696281433105
Iteration 18001:
Training Loss: -6.22709846496582
Reconstruction Loss: -5.29650354385376
Iteration 18101:
Training Loss: -6.23634672164917
Reconstruction Loss: -5.298321723937988
Iteration 18201:
Training Loss: -6.245545864105225
Reconstruction Loss: -5.3001251220703125
Iteration 18301:
Training Loss: -6.254705429077148
Reconstruction Loss: -5.301912307739258
Iteration 18401:
Training Loss: -6.263820648193359
Reconstruction Loss: -5.303683757781982
Iteration 18501:
Training Loss: -6.2729034423828125
Reconstruction Loss: -5.305441856384277
Iteration 18601:
Training Loss: -6.2819414138793945
Reconstruction Loss: -5.307186603546143
Iteration 18701:
Training Loss: -6.290928840637207
Reconstruction Loss: -5.308915138244629
Iteration 18801:
Training Loss: -6.299881458282471
Reconstruction Loss: -5.3106255531311035
Iteration 18901:
Training Loss: -6.308772087097168
Reconstruction Loss: -5.312318801879883
Iteration 19001:
Training Loss: -6.317631244659424
Reconstruction Loss: -5.313997268676758
Iteration 19101:
Training Loss: -6.32645320892334
Reconstruction Loss: -5.3156633377075195
Iteration 19201:
Training Loss: -6.335216045379639
Reconstruction Loss: -5.3173136711120605
Iteration 19301:
Training Loss: -6.343949794769287
Reconstruction Loss: -5.318949222564697
Iteration 19401:
Training Loss: -6.3526458740234375
Reconstruction Loss: -5.320571422576904
Iteration 19501:
Training Loss: -6.361313343048096
Reconstruction Loss: -5.322180271148682
Iteration 19601:
Training Loss: -6.369935035705566
Reconstruction Loss: -5.323776721954346
Iteration 19701:
Training Loss: -6.378515720367432
Reconstruction Loss: -5.3253583908081055
Iteration 19801:
Training Loss: -6.387063503265381
Reconstruction Loss: -5.326927185058594
Iteration 19901:
Training Loss: -6.395576477050781
Reconstruction Loss: -5.328484058380127
Iteration 20001:
Training Loss: -6.404051303863525
Reconstruction Loss: -5.330028533935547
Iteration 20101:
Training Loss: -6.412485122680664
Reconstruction Loss: -5.331560134887695
Iteration 20201:
Training Loss: -6.420886993408203
Reconstruction Loss: -5.333076000213623
Iteration 20301:
Training Loss: -6.4292402267456055
Reconstruction Loss: -5.334578990936279
Iteration 20401:
Training Loss: -6.437578201293945
Reconstruction Loss: -5.336073875427246
Iteration 20501:
Training Loss: -6.44585657119751
Reconstruction Loss: -5.337557315826416
Iteration 20601:
Training Loss: -6.45411491394043
Reconstruction Loss: -5.339028358459473
Iteration 20701:
Training Loss: -6.462335586547852
Reconstruction Loss: -5.340487480163574
Iteration 20801:
Training Loss: -6.470521450042725
Reconstruction Loss: -5.341935157775879
Iteration 20901:
Training Loss: -6.478670120239258
Reconstruction Loss: -5.3433709144592285
Iteration 21001:
Training Loss: -6.486780643463135
Reconstruction Loss: -5.344796657562256
Iteration 21101:
Training Loss: -6.4948601722717285
Reconstruction Loss: -5.346212387084961
Iteration 21201:
Training Loss: -6.502910614013672
Reconstruction Loss: -5.3476152420043945
Iteration 21301:
Training Loss: -6.51093053817749
Reconstruction Loss: -5.349006175994873
Iteration 21401:
Training Loss: -6.518921852111816
Reconstruction Loss: -5.350385665893555
Iteration 21501:
Training Loss: -6.526885509490967
Reconstruction Loss: -5.351754188537598
Iteration 21601:
Training Loss: -6.5348052978515625
Reconstruction Loss: -5.353111743927002
Iteration 21701:
Training Loss: -6.542701244354248
Reconstruction Loss: -5.354459762573242
Iteration 21801:
Training Loss: -6.5505571365356445
Reconstruction Loss: -5.355799674987793
Iteration 21901:
Training Loss: -6.558393478393555
Reconstruction Loss: -5.35713005065918
Iteration 22001:
Training Loss: -6.566203594207764
Reconstruction Loss: -5.358450412750244
Iteration 22101:
Training Loss: -6.573965549468994
Reconstruction Loss: -5.359760761260986
Iteration 22201:
Training Loss: -6.581699371337891
Reconstruction Loss: -5.361062049865723
Iteration 22301:
Training Loss: -6.589415550231934
Reconstruction Loss: -5.3623528480529785
Iteration 22401:
Training Loss: -6.597075939178467
Reconstruction Loss: -5.3636322021484375
Iteration 22501:
Training Loss: -6.6047258377075195
Reconstruction Loss: -5.364901542663574
Iteration 22601:
Training Loss: -6.612333297729492
Reconstruction Loss: -5.366161346435547
Iteration 22701:
Training Loss: -6.61992073059082
Reconstruction Loss: -5.367413520812988
Iteration 22801:
Training Loss: -6.6274919509887695
Reconstruction Loss: -5.368656158447266
Iteration 22901:
Training Loss: -6.635025501251221
Reconstruction Loss: -5.369888782501221
Iteration 23001:
Training Loss: -6.642529010772705
Reconstruction Loss: -5.371112823486328
Iteration 23101:
Training Loss: -6.650002479553223
Reconstruction Loss: -5.372325897216797
Iteration 23201:
Training Loss: -6.657458782196045
Reconstruction Loss: -5.373529434204102
Iteration 23301:
Training Loss: -6.664880752563477
Reconstruction Loss: -5.374722957611084
Iteration 23401:
Training Loss: -6.672277927398682
Reconstruction Loss: -5.375907897949219
Iteration 23501:
Training Loss: -6.679632663726807
Reconstruction Loss: -5.3770833015441895
Iteration 23601:
Training Loss: -6.6869730949401855
Reconstruction Loss: -5.378250598907471
Iteration 23701:
Training Loss: -6.694285869598389
Reconstruction Loss: -5.379408836364746
Iteration 23801:
Training Loss: -6.7015814781188965
Reconstruction Loss: -5.380560398101807
Iteration 23901:
Training Loss: -6.708845615386963
Reconstruction Loss: -5.381705284118652
Iteration 24001:
Training Loss: -6.716090202331543
Reconstruction Loss: -5.382840633392334
Iteration 24101:
Training Loss: -6.723289489746094
Reconstruction Loss: -5.383968353271484
Iteration 24201:
Training Loss: -6.7304816246032715
Reconstruction Loss: -5.385090351104736
Iteration 24301:
Training Loss: -6.737640857696533
Reconstruction Loss: -5.386204242706299
Iteration 24401:
Training Loss: -6.744775295257568
Reconstruction Loss: -5.3873114585876465
Iteration 24501:
Training Loss: -6.751903057098389
Reconstruction Loss: -5.3884100914001465
Iteration 24601:
Training Loss: -6.758993148803711
Reconstruction Loss: -5.389501571655273
Iteration 24701:
Training Loss: -6.766056060791016
Reconstruction Loss: -5.390585422515869
Iteration 24801:
Training Loss: -6.7731122970581055
Reconstruction Loss: -5.391662120819092
Iteration 24901:
Training Loss: -6.780126571655273
Reconstruction Loss: -5.39272928237915
Iteration 25001:
Training Loss: -6.787120819091797
Reconstruction Loss: -5.393786907196045
Iteration 25101:
Training Loss: -6.79409122467041
Reconstruction Loss: -5.394835948944092
Iteration 25201:
Training Loss: -6.80104398727417
Reconstruction Loss: -5.395877838134766
Iteration 25301:
Training Loss: -6.807967185974121
Reconstruction Loss: -5.396912574768066
Iteration 25401:
Training Loss: -6.814867973327637
Reconstruction Loss: -5.397938251495361
Iteration 25501:
Training Loss: -6.821742534637451
Reconstruction Loss: -5.398960113525391
Iteration 25601:
Training Loss: -6.828605651855469
Reconstruction Loss: -5.39997673034668
Iteration 25701:
Training Loss: -6.835437297821045
Reconstruction Loss: -5.400988578796387
Iteration 25801:
Training Loss: -6.842263221740723
Reconstruction Loss: -5.401994705200195
Iteration 25901:
Training Loss: -6.849061965942383
Reconstruction Loss: -5.402994632720947
Iteration 26001:
Training Loss: -6.855833053588867
Reconstruction Loss: -5.403988361358643
Iteration 26101:
Training Loss: -6.862588882446289
Reconstruction Loss: -5.404974460601807
Iteration 26201:
Training Loss: -6.869318008422852
Reconstruction Loss: -5.405953884124756
Iteration 26301:
Training Loss: -6.8760223388671875
Reconstruction Loss: -5.406926155090332
Iteration 26401:
Training Loss: -6.882711410522461
Reconstruction Loss: -5.407891273498535
Iteration 26501:
Training Loss: -6.889375686645508
Reconstruction Loss: -5.408850193023682
Iteration 26601:
Training Loss: -6.896036624908447
Reconstruction Loss: -5.409802436828613
Iteration 26701:
Training Loss: -6.902659893035889
Reconstruction Loss: -5.41074800491333
Iteration 26801:
Training Loss: -6.9092936515808105
Reconstruction Loss: -5.411686420440674
Iteration 26901:
Training Loss: -6.915882587432861
Reconstruction Loss: -5.4126176834106445
Iteration 27001:
Training Loss: -6.922464847564697
Reconstruction Loss: -5.413541316986084
Iteration 27101:
Training Loss: -6.929014682769775
Reconstruction Loss: -5.414460182189941
Iteration 27201:
Training Loss: -6.935549736022949
Reconstruction Loss: -5.415374755859375
Iteration 27301:
Training Loss: -6.942063331604004
Reconstruction Loss: -5.416285037994385
Iteration 27401:
Training Loss: -6.948581695556641
Reconstruction Loss: -5.417191028594971
Iteration 27501:
Training Loss: -6.955035209655762
Reconstruction Loss: -5.4180908203125
Iteration 27601:
Training Loss: -6.961499214172363
Reconstruction Loss: -5.4189839363098145
Iteration 27701:
Training Loss: -6.967940330505371
Reconstruction Loss: -5.419870853424072
Iteration 27801:
Training Loss: -6.974339008331299
Reconstruction Loss: -5.420752048492432
Iteration 27901:
Training Loss: -6.980757236480713
Reconstruction Loss: -5.421627998352051
Iteration 28001:
Training Loss: -6.987137794494629
Reconstruction Loss: -5.422497749328613
Iteration 28101:
Training Loss: -6.993507385253906
Reconstruction Loss: -5.423363208770752
Iteration 28201:
Training Loss: -6.999850273132324
Reconstruction Loss: -5.424222946166992
Iteration 28301:
Training Loss: -7.006187915802002
Reconstruction Loss: -5.425077438354492
Iteration 28401:
Training Loss: -7.012491703033447
Reconstruction Loss: -5.425928592681885
Iteration 28501:
Training Loss: -7.018794059753418
Reconstruction Loss: -5.426774024963379
Iteration 28601:
Training Loss: -7.025071144104004
Reconstruction Loss: -5.427613735198975
Iteration 28701:
Training Loss: -7.031329154968262
Reconstruction Loss: -5.428449630737305
Iteration 28801:
Training Loss: -7.037586688995361
Reconstruction Loss: -5.42927885055542
Iteration 28901:
Training Loss: -7.0438151359558105
Reconstruction Loss: -5.430102825164795
Iteration 29001:
Training Loss: -7.050027370452881
Reconstruction Loss: -5.4309210777282715
Iteration 29101:
Training Loss: -7.05623197555542
Reconstruction Loss: -5.431734085083008
Iteration 29201:
Training Loss: -7.062414169311523
Reconstruction Loss: -5.432542324066162
Iteration 29301:
Training Loss: -7.06857442855835
Reconstruction Loss: -5.433347702026367
Iteration 29401:
Training Loss: -7.074737548828125
Reconstruction Loss: -5.43414831161499
Iteration 29501:
Training Loss: -7.0808587074279785
Reconstruction Loss: -5.434945106506348
Iteration 29601:
Training Loss: -7.08697509765625
Reconstruction Loss: -5.435735702514648
Iteration 29701:
Training Loss: -7.093051910400391
Reconstruction Loss: -5.436520099639893
Iteration 29801:
Training Loss: -7.0991530418396
Reconstruction Loss: -5.437297821044922
Iteration 29901:
Training Loss: -7.105205535888672
Reconstruction Loss: -5.438069820404053
Iteration 30001:
Training Loss: -7.111251354217529
Reconstruction Loss: -5.43883752822876
Iteration 30101:
Training Loss: -7.117283821105957
Reconstruction Loss: -5.439600944519043
Iteration 30201:
Training Loss: -7.123305797576904
Reconstruction Loss: -5.440359592437744
Iteration 30301:
Training Loss: -7.1293134689331055
Reconstruction Loss: -5.441112995147705
Iteration 30401:
Training Loss: -7.135308265686035
Reconstruction Loss: -5.441862106323242
Iteration 30501:
Training Loss: -7.141293048858643
Reconstruction Loss: -5.442607879638672
Iteration 30601:
Training Loss: -7.147255897521973
Reconstruction Loss: -5.443348407745361
Iteration 30701:
Training Loss: -7.153201580047607
Reconstruction Loss: -5.444085597991943
Iteration 30801:
Training Loss: -7.159144401550293
Reconstruction Loss: -5.444818019866943
Iteration 30901:
Training Loss: -7.165070533752441
Reconstruction Loss: -5.4455461502075195
Iteration 31001:
Training Loss: -7.170983791351318
Reconstruction Loss: -5.446268558502197
Iteration 31101:
Training Loss: -7.176865100860596
Reconstruction Loss: -5.446987152099609
Iteration 31201:
Training Loss: -7.18274450302124
Reconstruction Loss: -5.447702407836914
Iteration 31301:
Training Loss: -7.188609600067139
Reconstruction Loss: -5.448414325714111
Iteration 31401:
Training Loss: -7.194437026977539
Reconstruction Loss: -5.449122905731201
Iteration 31501:
Training Loss: -7.200291156768799
Reconstruction Loss: -5.449827194213867
Iteration 31601:
Training Loss: -7.20609712600708
Reconstruction Loss: -5.450526237487793
Iteration 31701:
Training Loss: -7.211910247802734
Reconstruction Loss: -5.451220989227295
Iteration 31801:
Training Loss: -7.217703819274902
Reconstruction Loss: -5.451911926269531
Iteration 31901:
Training Loss: -7.2234787940979
Reconstruction Loss: -5.452598571777344
Iteration 32001:
Training Loss: -7.229250431060791
Reconstruction Loss: -5.453281879425049
Iteration 32101:
Training Loss: -7.234999656677246
Reconstruction Loss: -5.4539618492126465
Iteration 32201:
Training Loss: -7.240755558013916
Reconstruction Loss: -5.4546380043029785
Iteration 32301:
Training Loss: -7.24647331237793
Reconstruction Loss: -5.455311298370361
Iteration 32401:
Training Loss: -7.252199172973633
Reconstruction Loss: -5.45598030090332
Iteration 32501:
Training Loss: -7.2578959465026855
Reconstruction Loss: -5.4566450119018555
Iteration 32601:
Training Loss: -7.263608455657959
Reconstruction Loss: -5.457304954528809
Iteration 32701:
Training Loss: -7.269284248352051
Reconstruction Loss: -5.457960605621338
Iteration 32801:
Training Loss: -7.274966239929199
Reconstruction Loss: -5.458611965179443
Iteration 32901:
Training Loss: -7.2806315422058105
Reconstruction Loss: -5.459259510040283
Iteration 33001:
Training Loss: -7.286271572113037
Reconstruction Loss: -5.459902763366699
Iteration 33101:
Training Loss: -7.29191255569458
Reconstruction Loss: -5.46054220199585
Iteration 33201:
Training Loss: -7.297517776489258
Reconstruction Loss: -5.461178302764893
Iteration 33301:
Training Loss: -7.30313777923584
Reconstruction Loss: -5.461811065673828
Iteration 33401:
Training Loss: -7.3087286949157715
Reconstruction Loss: -5.462440013885498
Iteration 33501:
Training Loss: -7.314310073852539
Reconstruction Loss: -5.463065147399902
Iteration 33601:
Training Loss: -7.319888114929199
Reconstruction Loss: -5.463686466217041
Iteration 33701:
Training Loss: -7.325448989868164
Reconstruction Loss: -5.464303970336914
Iteration 33801:
Training Loss: -7.331002235412598
Reconstruction Loss: -5.464918613433838
Iteration 33901:
Training Loss: -7.336520671844482
Reconstruction Loss: -5.46552848815918
Iteration 34001:
Training Loss: -7.342037200927734
Reconstruction Loss: -5.466134548187256
Iteration 34101:
Training Loss: -7.34755277633667
Reconstruction Loss: -5.466737270355225
Iteration 34201:
Training Loss: -7.353061676025391
Reconstruction Loss: -5.467337608337402
Iteration 34301:
Training Loss: -7.358536243438721
Reconstruction Loss: -5.467933654785156
Iteration 34401:
Training Loss: -7.364025115966797
Reconstruction Loss: -5.468526840209961
Iteration 34501:
Training Loss: -7.369494438171387
Reconstruction Loss: -5.469116687774658
Iteration 34601:
Training Loss: -7.3749308586120605
Reconstruction Loss: -5.46970272064209
Iteration 34701:
Training Loss: -7.3804030418396
Reconstruction Loss: -5.470286846160889
Iteration 34801:
Training Loss: -7.385833740234375
Reconstruction Loss: -5.47086763381958
Iteration 34901:
Training Loss: -7.391268253326416
Reconstruction Loss: -5.4714460372924805
Iteration 35001:
Training Loss: -7.396678447723389
Reconstruction Loss: -5.472021102905273
Iteration 35101:
Training Loss: -7.402100563049316
Reconstruction Loss: -5.472594261169434
Iteration 35201:
Training Loss: -7.407495975494385
Reconstruction Loss: -5.473163604736328
Iteration 35301:
Training Loss: -7.412868022918701
Reconstruction Loss: -5.473731517791748
Iteration 35401:
Training Loss: -7.4182538986206055
Reconstruction Loss: -5.474295616149902
Iteration 35501:
Training Loss: -7.42361307144165
Reconstruction Loss: -5.474856853485107
Iteration 35601:
Training Loss: -7.428975582122803
Reconstruction Loss: -5.475414752960205
Iteration 35701:
Training Loss: -7.434307098388672
Reconstruction Loss: -5.475967884063721
Iteration 35801:
Training Loss: -7.439632415771484
Reconstruction Loss: -5.476517677307129
Iteration 35901:
Training Loss: -7.444952011108398
Reconstruction Loss: -5.47706413269043
Iteration 36001:
Training Loss: -7.4502692222595215
Reconstruction Loss: -5.4776082038879395
Iteration 36101:
Training Loss: -7.455551624298096
Reconstruction Loss: -5.4781494140625
Iteration 36201:
Training Loss: -7.4608283042907715
Reconstruction Loss: -5.478686332702637
Iteration 36301:
Training Loss: -7.466127395629883
Reconstruction Loss: -5.479221343994141
Iteration 36401:
Training Loss: -7.471381187438965
Reconstruction Loss: -5.4797515869140625
Iteration 36501:
Training Loss: -7.476644992828369
Reconstruction Loss: -5.480280876159668
Iteration 36601:
Training Loss: -7.48190975189209
Reconstruction Loss: -5.48080587387085
Iteration 36701:
Training Loss: -7.48714017868042
Reconstruction Loss: -5.481328010559082
Iteration 36801:
Training Loss: -7.4923601150512695
Reconstruction Loss: -5.481847286224365
Iteration 36901:
Training Loss: -7.497584342956543
Reconstruction Loss: -5.482363224029541
Iteration 37001:
Training Loss: -7.502823352813721
Reconstruction Loss: -5.482876300811768
Iteration 37101:
Training Loss: -7.507997989654541
Reconstruction Loss: -5.483386993408203
Iteration 37201:
Training Loss: -7.513197422027588
Reconstruction Loss: -5.483894348144531
Iteration 37301:
Training Loss: -7.518388748168945
Reconstruction Loss: -5.484400272369385
Iteration 37401:
Training Loss: -7.52357816696167
Reconstruction Loss: -5.484903812408447
Iteration 37501:
Training Loss: -7.528751850128174
Reconstruction Loss: -5.485405445098877
Iteration 37601:
Training Loss: -7.533907413482666
Reconstruction Loss: -5.485904693603516
Iteration 37701:
Training Loss: -7.5390496253967285
Reconstruction Loss: -5.486401081085205
Iteration 37801:
Training Loss: -7.544197082519531
Reconstruction Loss: -5.4868950843811035
Iteration 37901:
Training Loss: -7.5493292808532715
Reconstruction Loss: -5.487387657165527
Iteration 38001:
Training Loss: -7.554440021514893
Reconstruction Loss: -5.4878764152526855
Iteration 38101:
Training Loss: -7.5595550537109375
Reconstruction Loss: -5.488363742828369
Iteration 38201:
Training Loss: -7.5646653175354
Reconstruction Loss: -5.488847732543945
Iteration 38301:
Training Loss: -7.569760322570801
Reconstruction Loss: -5.489329814910889
Iteration 38401:
Training Loss: -7.5748419761657715
Reconstruction Loss: -5.489809989929199
Iteration 38501:
Training Loss: -7.5799431800842285
Reconstruction Loss: -5.490288257598877
Iteration 38601:
Training Loss: -7.585005760192871
Reconstruction Loss: -5.490764141082764
Iteration 38701:
Training Loss: -7.590066432952881
Reconstruction Loss: -5.491237163543701
Iteration 38801:
Training Loss: -7.595120429992676
Reconstruction Loss: -5.491707801818848
Iteration 38901:
Training Loss: -7.600164890289307
Reconstruction Loss: -5.492176055908203
Iteration 39001:
Training Loss: -7.605224132537842
Reconstruction Loss: -5.492642402648926
Iteration 39101:
Training Loss: -7.610240936279297
Reconstruction Loss: -5.493105888366699
Iteration 39201:
Training Loss: -7.615290641784668
Reconstruction Loss: -5.49356746673584
Iteration 39301:
Training Loss: -7.620290279388428
Reconstruction Loss: -5.494027137756348
Iteration 39401:
Training Loss: -7.625300884246826
Reconstruction Loss: -5.494483947753906
Iteration 39501:
Training Loss: -7.630314826965332
Reconstruction Loss: -5.49493932723999
Iteration 39601:
Training Loss: -7.6353044509887695
Reconstruction Loss: -5.495391845703125
Iteration 39701:
Training Loss: -7.640292167663574
Reconstruction Loss: -5.495842933654785
Iteration 39801:
Training Loss: -7.645284652709961
Reconstruction Loss: -5.496290683746338
Iteration 39901:
Training Loss: -7.6502580642700195
Reconstruction Loss: -5.496737003326416
Iteration 40001:
Training Loss: -7.655229091644287
Reconstruction Loss: -5.497179985046387
Iteration 40101:
Training Loss: -7.660172939300537
Reconstruction Loss: -5.49761962890625
Iteration 40201:
Training Loss: -7.665122032165527
Reconstruction Loss: -5.498056411743164
Iteration 40301:
Training Loss: -7.67006254196167
Reconstruction Loss: -5.498489856719971
Iteration 40401:
Training Loss: -7.6750054359436035
Reconstruction Loss: -5.49891996383667
Iteration 40501:
Training Loss: -7.679932594299316
Reconstruction Loss: -5.499347686767578
Iteration 40601:
Training Loss: -7.684819221496582
Reconstruction Loss: -5.499772548675537
Iteration 40701:
Training Loss: -7.689734935760498
Reconstruction Loss: -5.500194549560547
Iteration 40801:
Training Loss: -7.694629669189453
Reconstruction Loss: -5.500613212585449
Iteration 40901:
Training Loss: -7.699528217315674
Reconstruction Loss: -5.501030921936035
Iteration 41001:
Training Loss: -7.704401016235352
Reconstruction Loss: -5.501445770263672
Iteration 41101:
Training Loss: -7.70928430557251
Reconstruction Loss: -5.501859188079834
Iteration 41201:
Training Loss: -7.714162826538086
Reconstruction Loss: -5.502270221710205
Iteration 41301:
Training Loss: -7.719021320343018
Reconstruction Loss: -5.50268030166626
Iteration 41401:
Training Loss: -7.723882675170898
Reconstruction Loss: -5.503087997436523
Iteration 41501:
Training Loss: -7.72874641418457
Reconstruction Loss: -5.5034942626953125
Iteration 41601:
Training Loss: -7.733596324920654
Reconstruction Loss: -5.5038981437683105
Iteration 41701:
Training Loss: -7.73842191696167
Reconstruction Loss: -5.504300594329834
Iteration 41801:
Training Loss: -7.743247985839844
Reconstruction Loss: -5.504700183868408
Iteration 41901:
Training Loss: -7.748068809509277
Reconstruction Loss: -5.505098819732666
Iteration 42001:
Training Loss: -7.752892971038818
Reconstruction Loss: -5.505496025085449
Iteration 42101:
Training Loss: -7.7577104568481445
Reconstruction Loss: -5.5058913230896
Iteration 42201:
Training Loss: -7.762514591217041
Reconstruction Loss: -5.506285667419434
Iteration 42301:
Training Loss: -7.767305850982666
Reconstruction Loss: -5.506678581237793
Iteration 42401:
Training Loss: -7.772098064422607
Reconstruction Loss: -5.507070064544678
Iteration 42501:
Training Loss: -7.776891708374023
Reconstruction Loss: -5.50745964050293
Iteration 42601:
Training Loss: -7.781665325164795
Reconstruction Loss: -5.507848262786865
Iteration 42701:
Training Loss: -7.786440849304199
Reconstruction Loss: -5.508234977722168
Iteration 42801:
Training Loss: -7.791208267211914
Reconstruction Loss: -5.508620738983154
Iteration 42901:
Training Loss: -7.795993328094482
Reconstruction Loss: -5.50900411605835
Iteration 43001:
Training Loss: -7.800746917724609
Reconstruction Loss: -5.5093865394592285
Iteration 43101:
Training Loss: -7.805502891540527
Reconstruction Loss: -5.509766101837158
Iteration 43201:
Training Loss: -7.810230255126953
Reconstruction Loss: -5.5101447105407715
Iteration 43301:
Training Loss: -7.81497859954834
Reconstruction Loss: -5.510519981384277
Iteration 43401:
Training Loss: -7.8197150230407715
Reconstruction Loss: -5.510894298553467
Iteration 43501:
Training Loss: -7.8244242668151855
Reconstruction Loss: -5.511266231536865
Iteration 43601:
Training Loss: -7.8291521072387695
Reconstruction Loss: -5.511634826660156
Iteration 43701:
Training Loss: -7.8338541984558105
Reconstruction Loss: -5.5120015144348145
Iteration 43801:
Training Loss: -7.838583469390869
Reconstruction Loss: -5.51236629486084
Iteration 43901:
Training Loss: -7.843299865722656
Reconstruction Loss: -5.512728214263916
Iteration 44001:
Training Loss: -7.847961902618408
Reconstruction Loss: -5.513089179992676
Iteration 44101:
Training Loss: -7.852655410766602
Reconstruction Loss: -5.5134477615356445
Iteration 44201:
Training Loss: -7.857333660125732
Reconstruction Loss: -5.513803958892822
Iteration 44301:
Training Loss: -7.862031936645508
Reconstruction Loss: -5.514158248901367
Iteration 44401:
Training Loss: -7.866671562194824
Reconstruction Loss: -5.514510154724121
Iteration 44501:
Training Loss: -7.871346950531006
Reconstruction Loss: -5.5148606300354
Iteration 44601:
Training Loss: -7.875999450683594
Reconstruction Loss: -5.515209197998047
Iteration 44701:
Training Loss: -7.8806610107421875
Reconstruction Loss: -5.515556335449219
Iteration 44801:
Training Loss: -7.885301113128662
Reconstruction Loss: -5.515902519226074
Iteration 44901:
Training Loss: -7.88993501663208
Reconstruction Loss: -5.516247272491455
Iteration 45001:
Training Loss: -7.89457368850708
Reconstruction Loss: -5.516590118408203
Iteration 45101:
Training Loss: -7.899204730987549
Reconstruction Loss: -5.516932487487793
Iteration 45201:
Training Loss: -7.9038567543029785
Reconstruction Loss: -5.51727294921875
Iteration 45301:
Training Loss: -7.9084696769714355
Reconstruction Loss: -5.517612457275391
Iteration 45401:
Training Loss: -7.913081645965576
Reconstruction Loss: -5.517951011657715
Iteration 45501:
Training Loss: -7.917700290679932
Reconstruction Loss: -5.518287181854248
Iteration 45601:
Training Loss: -7.9223127365112305
Reconstruction Loss: -5.518621444702148
Iteration 45701:
Training Loss: -7.926914215087891
Reconstruction Loss: -5.518954753875732
Iteration 45801:
Training Loss: -7.931517601013184
Reconstruction Loss: -5.519286155700684
Iteration 45901:
Training Loss: -7.936100006103516
Reconstruction Loss: -5.519616603851318
Iteration 46001:
Training Loss: -7.940661430358887
Reconstruction Loss: -5.5199456214904785
Iteration 46101:
Training Loss: -7.94527530670166
Reconstruction Loss: -5.520272254943848
Iteration 46201:
Training Loss: -7.949841022491455
Reconstruction Loss: -5.5205979347229
Iteration 46301:
Training Loss: -7.9544243812561035
Reconstruction Loss: -5.5209221839904785
Iteration 46401:
Training Loss: -7.958967685699463
Reconstruction Loss: -5.521245002746582
Iteration 46501:
Training Loss: -7.963542461395264
Reconstruction Loss: -5.521565914154053
Iteration 46601:
Training Loss: -7.968102931976318
Reconstruction Loss: -5.521885395050049
Iteration 46701:
Training Loss: -7.972671985626221
Reconstruction Loss: -5.52220344543457
Iteration 46801:
Training Loss: -7.977212905883789
Reconstruction Loss: -5.522520065307617
Iteration 46901:
Training Loss: -7.981746673583984
Reconstruction Loss: -5.522834777832031
Iteration 47001:
Training Loss: -7.986292362213135
Reconstruction Loss: -5.523146152496338
Iteration 47101:
Training Loss: -7.99081563949585
Reconstruction Loss: -5.523457050323486
Iteration 47201:
Training Loss: -7.99533748626709
Reconstruction Loss: -5.523766040802002
Iteration 47301:
Training Loss: -7.999843120574951
Reconstruction Loss: -5.524073600769043
Iteration 47401:
Training Loss: -8.004372596740723
Reconstruction Loss: -5.524379730224609
Iteration 47501:
Training Loss: -8.008879661560059
Reconstruction Loss: -5.524683952331543
Iteration 47601:
Training Loss: -8.013383865356445
Reconstruction Loss: -5.524986267089844
Iteration 47701:
Training Loss: -8.017892837524414
Reconstruction Loss: -5.525287628173828
Iteration 47801:
Training Loss: -8.022397994995117
Reconstruction Loss: -5.525587558746338
Iteration 47901:
Training Loss: -8.026869773864746
Reconstruction Loss: -5.525886058807373
Iteration 48001:
Training Loss: -8.031343460083008
Reconstruction Loss: -5.526182651519775
Iteration 48101:
Training Loss: -8.035839080810547
Reconstruction Loss: -5.526479244232178
Iteration 48201:
Training Loss: -8.040343284606934
Reconstruction Loss: -5.526773929595947
Iteration 48301:
Training Loss: -8.044817924499512
Reconstruction Loss: -5.5270676612854
Iteration 48401:
Training Loss: -8.049286842346191
Reconstruction Loss: -5.527360439300537
Iteration 48501:
Training Loss: -8.053747177124023
Reconstruction Loss: -5.527651309967041
Iteration 48601:
Training Loss: -8.05823802947998
Reconstruction Loss: -5.52794075012207
Iteration 48701:
Training Loss: -8.062682151794434
Reconstruction Loss: -5.528229236602783
Iteration 48801:
Training Loss: -8.067150115966797
Reconstruction Loss: -5.528514862060547
Iteration 48901:
Training Loss: -8.0715970993042
Reconstruction Loss: -5.528799057006836
Iteration 49001:
Training Loss: -8.07605266571045
Reconstruction Loss: -5.529082775115967
Iteration 49101:
Training Loss: -8.080520629882812
Reconstruction Loss: -5.529364109039307
Iteration 49201:
Training Loss: -8.084919929504395
Reconstruction Loss: -5.529643535614014
Iteration 49301:
Training Loss: -8.089349746704102
Reconstruction Loss: -5.5299224853515625
Iteration 49401:
Training Loss: -8.093749046325684
Reconstruction Loss: -5.5301995277404785
Iteration 49501:
Training Loss: -8.098180770874023
Reconstruction Loss: -5.53047513961792
Iteration 49601:
Training Loss: -8.102590560913086
Reconstruction Loss: -5.530750751495361
Iteration 49701:
Training Loss: -8.106989860534668
Reconstruction Loss: -5.531023979187012
Iteration 49801:
Training Loss: -8.111371994018555
Reconstruction Loss: -5.531296253204346
Iteration 49901:
Training Loss: -8.11577033996582
Reconstruction Loss: -5.531567573547363
