5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.715385913848877
Reconstruction Loss: -0.4932844042778015
Iteration 51:
Training Loss: 5.710454940795898
Reconstruction Loss: -0.4934452772140503
Iteration 101:
Training Loss: 5.628198146820068
Reconstruction Loss: -0.493854820728302
Iteration 151:
Training Loss: 5.676551342010498
Reconstruction Loss: -0.49641793966293335
Iteration 201:
Training Loss: 5.469504356384277
Reconstruction Loss: -0.6711996793746948
Iteration 251:
Training Loss: 4.767537593841553
Reconstruction Loss: -0.8286895155906677
Iteration 301:
Training Loss: 4.460688591003418
Reconstruction Loss: -0.8391529321670532
Iteration 351:
Training Loss: 4.483517169952393
Reconstruction Loss: -0.8419241905212402
Iteration 401:
Training Loss: 4.28579568862915
Reconstruction Loss: -0.989716112613678
Iteration 451:
Training Loss: 3.7853453159332275
Reconstruction Loss: -1.1631834506988525
Iteration 501:
Training Loss: 3.6449005603790283
Reconstruction Loss: -1.1441482305526733
Iteration 551:
Training Loss: 3.5563130378723145
Reconstruction Loss: -1.1136982440948486
Iteration 601:
Training Loss: 3.4489264488220215
Reconstruction Loss: -1.112028956413269
Iteration 651:
Training Loss: 3.253237247467041
Reconstruction Loss: -1.2724560499191284
Iteration 701:
Training Loss: 2.7785637378692627
Reconstruction Loss: -1.5439163446426392
Iteration 751:
Training Loss: 2.5097949504852295
Reconstruction Loss: -1.6581743955612183
Iteration 801:
Training Loss: 2.6627674102783203
Reconstruction Loss: -1.7242038249969482
Iteration 851:
Training Loss: 2.543369770050049
Reconstruction Loss: -1.8013250827789307
Iteration 901:
Training Loss: 2.033755302429199
Reconstruction Loss: -1.9766440391540527
Iteration 951:
Training Loss: 1.4776973724365234
Reconstruction Loss: -2.385568857192993
Iteration 1001:
Training Loss: 0.8503627181053162
Reconstruction Loss: -2.8973629474639893
Iteration 1051:
Training Loss: 0.2229611873626709
Reconstruction Loss: -3.3905344009399414
Iteration 1101:
Training Loss: -0.2575259506702423
Reconstruction Loss: -3.8648486137390137
Iteration 1151:
Training Loss: -0.758986234664917
Reconstruction Loss: -4.323153018951416
Iteration 1201:
Training Loss: -1.3916096687316895
Reconstruction Loss: -4.766810894012451
Iteration 1251:
Training Loss: -1.7339251041412354
Reconstruction Loss: -5.194370269775391
Iteration 1301:
Training Loss: -2.236018419265747
Reconstruction Loss: -5.607129096984863
Iteration 1351:
Training Loss: -2.579530954360962
Reconstruction Loss: -6.007424354553223
Iteration 1401:
Training Loss: -3.1097824573516846
Reconstruction Loss: -6.395290851593018
Iteration 1451:
Training Loss: -3.54504132270813
Reconstruction Loss: -6.771152973175049
Iteration 1501:
Training Loss: -3.9274346828460693
Reconstruction Loss: -7.137907981872559
Iteration 1551:
Training Loss: -4.2338080406188965
Reconstruction Loss: -7.495685577392578
Iteration 1601:
Training Loss: -4.717575550079346
Reconstruction Loss: -7.8434953689575195
Iteration 1651:
Training Loss: -4.893632411956787
Reconstruction Loss: -8.182876586914062
Iteration 1701:
Training Loss: -5.346475601196289
Reconstruction Loss: -8.514520645141602
Iteration 1751:
Training Loss: -5.706141948699951
Reconstruction Loss: -8.83304500579834
Iteration 1801:
Training Loss: -5.783450603485107
Reconstruction Loss: -9.142725944519043
Iteration 1851:
Training Loss: -6.201905250549316
Reconstruction Loss: -9.437437057495117
Iteration 1901:
Training Loss: -6.488230228424072
Reconstruction Loss: -9.720754623413086
Iteration 1951:
Training Loss: -6.73817253112793
Reconstruction Loss: -9.98622989654541
Iteration 2001:
Training Loss: -7.105401515960693
Reconstruction Loss: -10.234444618225098
Iteration 2051:
Training Loss: -7.109984874725342
Reconstruction Loss: -10.460774421691895
Iteration 2101:
Training Loss: -7.1429619789123535
Reconstruction Loss: -10.664345741271973
Iteration 2151:
Training Loss: -7.553652763366699
Reconstruction Loss: -10.84749984741211
Iteration 2201:
Training Loss: -7.449979782104492
Reconstruction Loss: -11.005309104919434
Iteration 2251:
Training Loss: -7.506342887878418
Reconstruction Loss: -11.144107818603516
Iteration 2301:
Training Loss: -7.574441432952881
Reconstruction Loss: -11.260844230651855
Iteration 2351:
Training Loss: -7.711474418640137
Reconstruction Loss: -11.357949256896973
Iteration 2401:
Training Loss: -7.66498327255249
Reconstruction Loss: -11.438643455505371
Iteration 2451:
Training Loss: -7.83715295791626
Reconstruction Loss: -11.505301475524902
Iteration 2501:
Training Loss: -7.801191806793213
Reconstruction Loss: -11.559329986572266
Iteration 2551:
Training Loss: -7.63965368270874
Reconstruction Loss: -11.604094505310059
Iteration 2601:
Training Loss: -7.704431056976318
Reconstruction Loss: -11.641210556030273
Iteration 2651:
Training Loss: -7.834260940551758
Reconstruction Loss: -11.673379898071289
Iteration 2701:
Training Loss: -7.735274791717529
Reconstruction Loss: -11.697616577148438
Iteration 2751:
Training Loss: -7.913415908813477
Reconstruction Loss: -11.71717643737793
Iteration 2801:
Training Loss: -7.857032775878906
Reconstruction Loss: -11.73572063446045
Iteration 2851:
Training Loss: -7.742293357849121
Reconstruction Loss: -11.750064849853516
Iteration 2901:
Training Loss: -7.78200101852417
Reconstruction Loss: -11.761825561523438
Iteration 2951:
Training Loss: -7.990880012512207
Reconstruction Loss: -11.772186279296875
Iteration 3001:
Training Loss: -7.703173637390137
Reconstruction Loss: -11.779767036437988
Iteration 3051:
Training Loss: -7.710634708404541
Reconstruction Loss: -11.788479804992676
Iteration 3101:
Training Loss: -7.813960075378418
Reconstruction Loss: -11.79777717590332
Iteration 3151:
Training Loss: -7.8282670974731445
Reconstruction Loss: -11.80573844909668
Iteration 3201:
Training Loss: -8.009442329406738
Reconstruction Loss: -11.810925483703613
Iteration 3251:
Training Loss: -7.920586585998535
Reconstruction Loss: -11.815185546875
Iteration 3301:
Training Loss: -8.121437072753906
Reconstruction Loss: -11.820701599121094
Iteration 3351:
Training Loss: -8.101234436035156
Reconstruction Loss: -11.826619148254395
Iteration 3401:
Training Loss: -7.9571003913879395
Reconstruction Loss: -11.83189868927002
Iteration 3451:
Training Loss: -7.886178493499756
Reconstruction Loss: -11.837152481079102
Iteration 3501:
Training Loss: -7.983268737792969
Reconstruction Loss: -11.839242935180664
Iteration 3551:
Training Loss: -7.987062454223633
Reconstruction Loss: -11.844525337219238
Iteration 3601:
Training Loss: -7.7858052253723145
Reconstruction Loss: -11.850024223327637
Iteration 3651:
Training Loss: -7.844557285308838
Reconstruction Loss: -11.8538236618042
Iteration 3701:
Training Loss: -7.9517621994018555
Reconstruction Loss: -11.854869842529297
Iteration 3751:
Training Loss: -7.884083271026611
Reconstruction Loss: -11.86131477355957
Iteration 3801:
Training Loss: -7.880218029022217
Reconstruction Loss: -11.86366081237793
Iteration 3851:
Training Loss: -8.072265625
Reconstruction Loss: -11.86673355102539
Iteration 3901:
Training Loss: -7.998727798461914
Reconstruction Loss: -11.871349334716797
Iteration 3951:
Training Loss: -7.987120151519775
Reconstruction Loss: -11.875812530517578
Iteration 4001:
Training Loss: -7.930222988128662
Reconstruction Loss: -11.879194259643555
Iteration 4051:
Training Loss: -8.01508903503418
Reconstruction Loss: -11.882389068603516
Iteration 4101:
Training Loss: -7.826087474822998
Reconstruction Loss: -11.88801383972168
Iteration 4151:
Training Loss: -7.750410556793213
Reconstruction Loss: -11.889473915100098
Iteration 4201:
Training Loss: -8.018061637878418
Reconstruction Loss: -11.891867637634277
Iteration 4251:
Training Loss: -7.901824951171875
Reconstruction Loss: -11.895591735839844
Iteration 4301:
Training Loss: -7.9797868728637695
Reconstruction Loss: -11.898333549499512
Iteration 4351:
Training Loss: -7.86260461807251
Reconstruction Loss: -11.902790069580078
Iteration 4401:
Training Loss: -7.932108402252197
Reconstruction Loss: -11.90569019317627
Iteration 4451:
Training Loss: -7.793635845184326
Reconstruction Loss: -11.909107208251953
Iteration 4501:
Training Loss: -7.969734191894531
Reconstruction Loss: -11.914432525634766
Iteration 4551:
Training Loss: -8.022119522094727
Reconstruction Loss: -11.915940284729004
Iteration 4601:
Training Loss: -7.956365585327148
Reconstruction Loss: -11.91979694366455
Iteration 4651:
Training Loss: -7.995068550109863
Reconstruction Loss: -11.924223899841309
Iteration 4701:
Training Loss: -7.907276153564453
Reconstruction Loss: -11.925031661987305
Iteration 4751:
Training Loss: -7.984124183654785
Reconstruction Loss: -11.927952766418457
Iteration 4801:
Training Loss: -7.918280124664307
Reconstruction Loss: -11.9317626953125
Iteration 4851:
Training Loss: -8.015848159790039
Reconstruction Loss: -11.936423301696777
Iteration 4901:
Training Loss: -7.923984050750732
Reconstruction Loss: -11.938323974609375
Iteration 4951:
Training Loss: -8.033773422241211
Reconstruction Loss: -11.942724227905273
Iteration 5001:
Training Loss: -7.980131149291992
Reconstruction Loss: -11.945184707641602
Iteration 5051:
Training Loss: -7.921133518218994
Reconstruction Loss: -11.949052810668945
Iteration 5101:
Training Loss: -8.0117826461792
Reconstruction Loss: -11.952258110046387
Iteration 5151:
Training Loss: -8.00549602508545
Reconstruction Loss: -11.95556354522705
Iteration 5201:
Training Loss: -8.000991821289062
Reconstruction Loss: -11.957708358764648
Iteration 5251:
Training Loss: -7.98807430267334
Reconstruction Loss: -11.960258483886719
Iteration 5301:
Training Loss: -8.071930885314941
Reconstruction Loss: -11.962203979492188
Iteration 5351:
Training Loss: -8.007187843322754
Reconstruction Loss: -11.966390609741211
Iteration 5401:
Training Loss: -8.254251480102539
Reconstruction Loss: -11.969937324523926
Iteration 5451:
Training Loss: -8.244644165039062
Reconstruction Loss: -11.971510887145996
Iteration 5501:
Training Loss: -8.06924057006836
Reconstruction Loss: -11.977445602416992
Iteration 5551:
Training Loss: -8.015853881835938
Reconstruction Loss: -11.979743957519531
Iteration 5601:
Training Loss: -8.069425582885742
Reconstruction Loss: -11.983710289001465
Iteration 5651:
Training Loss: -8.147355079650879
Reconstruction Loss: -11.984299659729004
Iteration 5701:
Training Loss: -7.977144718170166
Reconstruction Loss: -11.987424850463867
Iteration 5751:
Training Loss: -8.040452003479004
Reconstruction Loss: -11.990357398986816
Iteration 5801:
Training Loss: -8.049681663513184
Reconstruction Loss: -11.994231224060059
Iteration 5851:
Training Loss: -8.042322158813477
Reconstruction Loss: -11.997211456298828
Iteration 5901:
Training Loss: -8.300268173217773
Reconstruction Loss: -11.999693870544434
Iteration 5951:
Training Loss: -8.07686996459961
Reconstruction Loss: -12.000946044921875
Iteration 6001:
Training Loss: -7.9670233726501465
Reconstruction Loss: -12.006768226623535
Iteration 6051:
Training Loss: -8.054376602172852
Reconstruction Loss: -12.00816535949707
Iteration 6101:
Training Loss: -8.155131340026855
Reconstruction Loss: -12.012893676757812
Iteration 6151:
Training Loss: -7.9921464920043945
Reconstruction Loss: -12.013904571533203
Iteration 6201:
Training Loss: -8.065746307373047
Reconstruction Loss: -12.017871856689453
Iteration 6251:
Training Loss: -8.089468002319336
Reconstruction Loss: -12.019364356994629
Iteration 6301:
Training Loss: -8.121237754821777
Reconstruction Loss: -12.024465560913086
Iteration 6351:
Training Loss: -8.096073150634766
Reconstruction Loss: -12.02646255493164
Iteration 6401:
Training Loss: -8.205769538879395
Reconstruction Loss: -12.029986381530762
Iteration 6451:
Training Loss: -8.085027694702148
Reconstruction Loss: -12.033551216125488
Iteration 6501:
Training Loss: -8.111891746520996
Reconstruction Loss: -12.035400390625
Iteration 6551:
Training Loss: -8.169719696044922
Reconstruction Loss: -12.039166450500488
Iteration 6601:
Training Loss: -8.060715675354004
Reconstruction Loss: -12.04411506652832
Iteration 6651:
Training Loss: -8.216413497924805
Reconstruction Loss: -12.043124198913574
Iteration 6701:
Training Loss: -8.137389183044434
Reconstruction Loss: -12.047430992126465
Iteration 6751:
Training Loss: -8.21998119354248
Reconstruction Loss: -12.049555778503418
Iteration 6801:
Training Loss: -8.134077072143555
Reconstruction Loss: -12.052752494812012
Iteration 6851:
Training Loss: -8.245630264282227
Reconstruction Loss: -12.054232597351074
Iteration 6901:
Training Loss: -8.180375099182129
Reconstruction Loss: -12.057794570922852
Iteration 6951:
Training Loss: -8.059479713439941
Reconstruction Loss: -12.060867309570312
Iteration 7001:
Training Loss: -8.196383476257324
Reconstruction Loss: -12.063196182250977
Iteration 7051:
Training Loss: -8.233960151672363
Reconstruction Loss: -12.065349578857422
Iteration 7101:
Training Loss: -8.062043190002441
Reconstruction Loss: -12.070388793945312
Iteration 7151:
Training Loss: -8.058110237121582
Reconstruction Loss: -12.073945999145508
Iteration 7201:
Training Loss: -8.256010055541992
Reconstruction Loss: -12.073129653930664
Iteration 7251:
Training Loss: -8.123184204101562
Reconstruction Loss: -12.077570915222168
Iteration 7301:
Training Loss: -8.360264778137207
Reconstruction Loss: -12.080589294433594
Iteration 7351:
Training Loss: -8.258069038391113
Reconstruction Loss: -12.083234786987305
Iteration 7401:
Training Loss: -8.179874420166016
Reconstruction Loss: -12.084127426147461
Iteration 7451:
Training Loss: -8.214887619018555
Reconstruction Loss: -12.087963104248047
