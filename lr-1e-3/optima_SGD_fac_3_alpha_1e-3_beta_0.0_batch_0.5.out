5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.576006889343262
Reconstruction Loss: -0.4176020324230194
Iteration 51:
Training Loss: 5.204162120819092
Reconstruction Loss: -0.6400884985923767
Iteration 101:
Training Loss: 3.866150379180908
Reconstruction Loss: -1.213807225227356
Iteration 151:
Training Loss: 2.7937276363372803
Reconstruction Loss: -1.801973581314087
Iteration 201:
Training Loss: 1.6728044748306274
Reconstruction Loss: -2.509103536605835
Iteration 251:
Training Loss: 0.7953568696975708
Reconstruction Loss: -3.17303466796875
Iteration 301:
Training Loss: -0.08749869465827942
Reconstruction Loss: -3.709273099899292
Iteration 351:
Training Loss: -0.4535069465637207
Reconstruction Loss: -4.127322673797607
Iteration 401:
Training Loss: -1.0097233057022095
Reconstruction Loss: -4.451035499572754
Iteration 451:
Training Loss: -1.3459193706512451
Reconstruction Loss: -4.704377174377441
Iteration 501:
Training Loss: -1.582463026046753
Reconstruction Loss: -4.906386375427246
Iteration 551:
Training Loss: -1.720747709274292
Reconstruction Loss: -5.068912506103516
Iteration 601:
Training Loss: -1.9035367965698242
Reconstruction Loss: -5.204817771911621
Iteration 651:
Training Loss: -2.0603220462799072
Reconstruction Loss: -5.31632137298584
Iteration 701:
Training Loss: -2.3462178707122803
Reconstruction Loss: -5.416467189788818
Iteration 751:
Training Loss: -2.392312526702881
Reconstruction Loss: -5.501160621643066
Iteration 801:
Training Loss: -2.347686529159546
Reconstruction Loss: -5.576578140258789
Iteration 851:
Training Loss: -2.596121311187744
Reconstruction Loss: -5.643990993499756
Iteration 901:
Training Loss: -2.746305465698242
Reconstruction Loss: -5.706303596496582
Iteration 951:
Training Loss: -2.772562265396118
Reconstruction Loss: -5.763125896453857
Iteration 1001:
Training Loss: -2.8908698558807373
Reconstruction Loss: -5.815333843231201
Iteration 1051:
Training Loss: -3.014294147491455
Reconstruction Loss: -5.863913536071777
Iteration 1101:
Training Loss: -2.9172823429107666
Reconstruction Loss: -5.910477638244629
Iteration 1151:
Training Loss: -2.9425137042999268
Reconstruction Loss: -5.9529852867126465
Iteration 1201:
Training Loss: -3.0300745964050293
Reconstruction Loss: -5.990715026855469
Iteration 1251:
Training Loss: -3.072812795639038
Reconstruction Loss: -6.031200408935547
Iteration 1301:
Training Loss: -3.021352529525757
Reconstruction Loss: -6.0658416748046875
Iteration 1351:
Training Loss: -3.2138469219207764
Reconstruction Loss: -6.1007771492004395
Iteration 1401:
Training Loss: -3.204162120819092
Reconstruction Loss: -6.132359027862549
Iteration 1451:
Training Loss: -3.4773342609405518
Reconstruction Loss: -6.165196418762207
Iteration 1501:
Training Loss: -3.2936854362487793
Reconstruction Loss: -6.194364547729492
Iteration 1551:
Training Loss: -3.5217020511627197
Reconstruction Loss: -6.223057746887207
Iteration 1601:
Training Loss: -3.6166024208068848
Reconstruction Loss: -6.25006103515625
Iteration 1651:
Training Loss: -3.6430857181549072
Reconstruction Loss: -6.277312755584717
Iteration 1701:
Training Loss: -3.740351438522339
Reconstruction Loss: -6.301962375640869
Iteration 1751:
Training Loss: -3.528373956680298
Reconstruction Loss: -6.32716178894043
Iteration 1801:
Training Loss: -3.643254041671753
Reconstruction Loss: -6.350656509399414
Iteration 1851:
Training Loss: -3.666229248046875
Reconstruction Loss: -6.373993396759033
Iteration 1901:
Training Loss: -3.6548118591308594
Reconstruction Loss: -6.396111965179443
Iteration 1951:
Training Loss: -3.9449057579040527
Reconstruction Loss: -6.417677402496338
Iteration 2001:
Training Loss: -3.629228115081787
Reconstruction Loss: -6.438016414642334
Iteration 2051:
Training Loss: -3.7129650115966797
Reconstruction Loss: -6.459222316741943
Iteration 2101:
Training Loss: -3.8092763423919678
Reconstruction Loss: -6.478551387786865
Iteration 2151:
Training Loss: -3.783400058746338
Reconstruction Loss: -6.49797248840332
Iteration 2201:
Training Loss: -3.8133158683776855
Reconstruction Loss: -6.517193794250488
Iteration 2251:
Training Loss: -3.895979881286621
Reconstruction Loss: -6.535141468048096
Iteration 2301:
Training Loss: -3.9194436073303223
Reconstruction Loss: -6.552931308746338
Iteration 2351:
Training Loss: -3.9199936389923096
Reconstruction Loss: -6.570494651794434
Iteration 2401:
Training Loss: -3.9219746589660645
Reconstruction Loss: -6.587121963500977
Iteration 2451:
Training Loss: -4.0733418464660645
Reconstruction Loss: -6.602769374847412
Iteration 2501:
Training Loss: -4.131850242614746
Reconstruction Loss: -6.619602203369141
Iteration 2551:
Training Loss: -3.9998958110809326
Reconstruction Loss: -6.6356987953186035
Iteration 2601:
Training Loss: -4.2691450119018555
Reconstruction Loss: -6.650166988372803
Iteration 2651:
Training Loss: -4.001827239990234
Reconstruction Loss: -6.666236400604248
Iteration 2701:
Training Loss: -4.114911079406738
Reconstruction Loss: -6.680102825164795
Iteration 2751:
Training Loss: -4.332840919494629
Reconstruction Loss: -6.695384979248047
Iteration 2801:
Training Loss: -4.304825305938721
Reconstruction Loss: -6.707289695739746
Iteration 2851:
Training Loss: -4.328945636749268
Reconstruction Loss: -6.72159481048584
Iteration 2901:
Training Loss: -4.194754123687744
Reconstruction Loss: -6.735788345336914
Iteration 2951:
Training Loss: -4.289409637451172
Reconstruction Loss: -6.7485551834106445
Iteration 3001:
Training Loss: -4.47192907333374
Reconstruction Loss: -6.760860443115234
Iteration 3051:
Training Loss: -4.385642051696777
Reconstruction Loss: -6.774222373962402
Iteration 3101:
Training Loss: -4.256889820098877
Reconstruction Loss: -6.787133693695068
Iteration 3151:
Training Loss: -4.32312536239624
Reconstruction Loss: -6.799905776977539
Iteration 3201:
Training Loss: -4.45475959777832
Reconstruction Loss: -6.811251640319824
Iteration 3251:
Training Loss: -4.332798480987549
Reconstruction Loss: -6.823790073394775
Iteration 3301:
Training Loss: -4.396531105041504
Reconstruction Loss: -6.835333347320557
Iteration 3351:
Training Loss: -4.463441848754883
Reconstruction Loss: -6.8469438552856445
Iteration 3401:
Training Loss: -4.44809103012085
Reconstruction Loss: -6.857799053192139
Iteration 3451:
Training Loss: -4.500921726226807
Reconstruction Loss: -6.868593215942383
Iteration 3501:
Training Loss: -4.551970481872559
Reconstruction Loss: -6.879593372344971
Iteration 3551:
Training Loss: -4.529369354248047
Reconstruction Loss: -6.889770984649658
Iteration 3601:
Training Loss: -4.449995040893555
Reconstruction Loss: -6.900256156921387
Iteration 3651:
Training Loss: -4.545174598693848
Reconstruction Loss: -6.910948276519775
Iteration 3701:
Training Loss: -4.72886323928833
Reconstruction Loss: -6.920724868774414
Iteration 3751:
Training Loss: -4.6131696701049805
Reconstruction Loss: -6.930658340454102
Iteration 3801:
Training Loss: -4.670544147491455
Reconstruction Loss: -6.940780162811279
Iteration 3851:
Training Loss: -4.5696611404418945
Reconstruction Loss: -6.949499130249023
Iteration 3901:
Training Loss: -4.6573991775512695
Reconstruction Loss: -6.958491802215576
Iteration 3951:
Training Loss: -4.812195301055908
Reconstruction Loss: -6.968785285949707
Iteration 4001:
Training Loss: -4.839459419250488
Reconstruction Loss: -6.977950096130371
Iteration 4051:
Training Loss: -4.800983428955078
Reconstruction Loss: -6.987372398376465
Iteration 4101:
Training Loss: -4.718148708343506
Reconstruction Loss: -6.995512008666992
Iteration 4151:
Training Loss: -4.782259464263916
Reconstruction Loss: -7.005069732666016
Iteration 4201:
Training Loss: -4.8734660148620605
Reconstruction Loss: -7.0138115882873535
Iteration 4251:
Training Loss: -4.73396635055542
Reconstruction Loss: -7.021681785583496
Iteration 4301:
Training Loss: -4.83524227142334
Reconstruction Loss: -7.030245304107666
Iteration 4351:
Training Loss: -4.892330646514893
Reconstruction Loss: -7.039083003997803
Iteration 4401:
Training Loss: -4.8659162521362305
Reconstruction Loss: -7.04655122756958
Iteration 4451:
Training Loss: -5.00459098815918
Reconstruction Loss: -7.055051326751709
Iteration 4501:
Training Loss: -4.989289283752441
Reconstruction Loss: -7.063121318817139
Iteration 4551:
Training Loss: -4.859943866729736
Reconstruction Loss: -7.070520401000977
Iteration 4601:
Training Loss: -4.866324424743652
Reconstruction Loss: -7.078973293304443
Iteration 4651:
Training Loss: -5.10329532623291
Reconstruction Loss: -7.0860795974731445
Iteration 4701:
Training Loss: -4.9940338134765625
Reconstruction Loss: -7.09405517578125
Iteration 4751:
Training Loss: -4.921433925628662
Reconstruction Loss: -7.102210998535156
Iteration 4801:
Training Loss: -4.880483627319336
Reconstruction Loss: -7.108302593231201
Iteration 4851:
Training Loss: -5.125940322875977
Reconstruction Loss: -7.116700649261475
Iteration 4901:
Training Loss: -5.005183219909668
Reconstruction Loss: -7.123621940612793
Iteration 4951:
Training Loss: -5.104122638702393
Reconstruction Loss: -7.130744457244873
Iteration 5001:
Training Loss: -5.0372090339660645
Reconstruction Loss: -7.137398719787598
Iteration 5051:
Training Loss: -4.874521255493164
Reconstruction Loss: -7.144861698150635
Iteration 5101:
Training Loss: -5.139148712158203
Reconstruction Loss: -7.150995254516602
Iteration 5151:
Training Loss: -5.063130855560303
Reconstruction Loss: -7.158056259155273
Iteration 5201:
Training Loss: -5.107816219329834
Reconstruction Loss: -7.164950370788574
Iteration 5251:
Training Loss: -4.993645668029785
Reconstruction Loss: -7.171585559844971
Iteration 5301:
Training Loss: -5.0549821853637695
Reconstruction Loss: -7.177981853485107
Iteration 5351:
Training Loss: -5.092047214508057
Reconstruction Loss: -7.183990955352783
Iteration 5401:
Training Loss: -5.326526641845703
Reconstruction Loss: -7.1906914710998535
Iteration 5451:
Training Loss: -5.195912837982178
Reconstruction Loss: -7.196874618530273
Iteration 5501:
Training Loss: -5.133228778839111
Reconstruction Loss: -7.203475475311279
Iteration 5551:
Training Loss: -5.234745025634766
Reconstruction Loss: -7.209537982940674
Iteration 5601:
Training Loss: -5.163802146911621
Reconstruction Loss: -7.215898513793945
Iteration 5651:
Training Loss: -5.278793811798096
Reconstruction Loss: -7.221720218658447
Iteration 5701:
Training Loss: -5.293266773223877
Reconstruction Loss: -7.228113174438477
Iteration 5751:
Training Loss: -5.381180286407471
Reconstruction Loss: -7.234156608581543
Iteration 5801:
Training Loss: -5.175329685211182
Reconstruction Loss: -7.2393903732299805
Iteration 5851:
Training Loss: -5.22198486328125
Reconstruction Loss: -7.24644660949707
Iteration 5901:
Training Loss: -5.268306255340576
Reconstruction Loss: -7.251136779785156
Iteration 5951:
Training Loss: -5.416735649108887
Reconstruction Loss: -7.2571821212768555
Iteration 6001:
Training Loss: -5.373732566833496
Reconstruction Loss: -7.263360500335693
Iteration 6051:
Training Loss: -5.2096099853515625
Reconstruction Loss: -7.267858028411865
Iteration 6101:
Training Loss: -5.389291763305664
Reconstruction Loss: -7.275117874145508
Iteration 6151:
Training Loss: -5.327975749969482
Reconstruction Loss: -7.27950382232666
Iteration 6201:
Training Loss: -5.2510857582092285
Reconstruction Loss: -7.284780502319336
Iteration 6251:
Training Loss: -5.33723783493042
Reconstruction Loss: -7.289689540863037
Iteration 6301:
Training Loss: -5.383087635040283
Reconstruction Loss: -7.296035289764404
Iteration 6351:
Training Loss: -5.474798202514648
Reconstruction Loss: -7.299661636352539
Iteration 6401:
Training Loss: -5.455840110778809
Reconstruction Loss: -7.306390285491943
Iteration 6451:
Training Loss: -5.436617851257324
Reconstruction Loss: -7.311783790588379
Iteration 6501:
Training Loss: -5.448078155517578
Reconstruction Loss: -7.315540790557861
Iteration 6551:
Training Loss: -5.365719318389893
Reconstruction Loss: -7.320865154266357
Iteration 6601:
Training Loss: -5.634207725524902
Reconstruction Loss: -7.325779438018799
Iteration 6651:
Training Loss: -5.465054035186768
Reconstruction Loss: -7.331218719482422
Iteration 6701:
Training Loss: -5.487527847290039
Reconstruction Loss: -7.335315704345703
Iteration 6751:
Training Loss: -5.6046295166015625
Reconstruction Loss: -7.340906143188477
Iteration 6801:
Training Loss: -5.520815372467041
Reconstruction Loss: -7.3460187911987305
Iteration 6851:
Training Loss: -5.570135593414307
Reconstruction Loss: -7.3508429527282715
Iteration 6901:
Training Loss: -5.53251314163208
Reconstruction Loss: -7.354843616485596
Iteration 6951:
Training Loss: -5.672502040863037
Reconstruction Loss: -7.360088348388672
Iteration 7001:
Training Loss: -5.532814025878906
Reconstruction Loss: -7.364039421081543
Iteration 7051:
Training Loss: -5.640610694885254
Reconstruction Loss: -7.369244575500488
Iteration 7101:
Training Loss: -5.5408196449279785
Reconstruction Loss: -7.373264312744141
Iteration 7151:
Training Loss: -5.513366222381592
Reconstruction Loss: -7.378650188446045
Iteration 7201:
Training Loss: -5.549396514892578
Reconstruction Loss: -7.382927417755127
Iteration 7251:
Training Loss: -5.590529441833496
Reconstruction Loss: -7.3871331214904785
Iteration 7301:
Training Loss: -5.637257099151611
Reconstruction Loss: -7.391153812408447
Iteration 7351:
Training Loss: -5.626116752624512
Reconstruction Loss: -7.3949408531188965
Iteration 7401:
Training Loss: -5.706141471862793
Reconstruction Loss: -7.399599552154541
Iteration 7451:
Training Loss: -5.573093414306641
Reconstruction Loss: -7.404787540435791
