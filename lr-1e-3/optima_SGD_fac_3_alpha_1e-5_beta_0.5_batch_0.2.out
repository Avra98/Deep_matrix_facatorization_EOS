5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.1901421546936035
Reconstruction Loss: -0.6384909152984619
Iteration 21:
Training Loss: 5.258368492126465
Reconstruction Loss: -0.6385844945907593
Iteration 41:
Training Loss: 5.500084400177002
Reconstruction Loss: -0.6387477517127991
Iteration 61:
Training Loss: 5.350189685821533
Reconstruction Loss: -0.6392083168029785
Iteration 81:
Training Loss: 5.341948509216309
Reconstruction Loss: -0.6415581107139587
Iteration 101:
Training Loss: 5.299555778503418
Reconstruction Loss: -0.7017545700073242
Iteration 121:
Training Loss: 4.233689785003662
Reconstruction Loss: -0.8684859871864319
Iteration 141:
Training Loss: 4.216782093048096
Reconstruction Loss: -0.9683969020843506
Iteration 161:
Training Loss: 3.8216240406036377
Reconstruction Loss: -1.027280330657959
Iteration 181:
Training Loss: 3.777416944503784
Reconstruction Loss: -1.0827651023864746
Iteration 201:
Training Loss: 3.7681820392608643
Reconstruction Loss: -1.1227118968963623
Iteration 221:
Training Loss: 3.7069618701934814
Reconstruction Loss: -1.1477867364883423
Iteration 241:
Training Loss: 3.4816818237304688
Reconstruction Loss: -1.1685854196548462
Iteration 261:
Training Loss: 3.6986446380615234
Reconstruction Loss: -1.1819126605987549
Iteration 281:
Training Loss: 3.3332996368408203
Reconstruction Loss: -1.2061541080474854
Iteration 301:
Training Loss: 3.541822910308838
Reconstruction Loss: -1.2742539644241333
Iteration 321:
Training Loss: 3.056518077850342
Reconstruction Loss: -1.4782094955444336
Iteration 341:
Training Loss: 2.783317804336548
Reconstruction Loss: -1.6785991191864014
Iteration 361:
Training Loss: 2.8808960914611816
Reconstruction Loss: -1.7829740047454834
Iteration 381:
Training Loss: 2.324237108230591
Reconstruction Loss: -1.9057645797729492
Iteration 401:
Training Loss: 1.9487015008926392
Reconstruction Loss: -2.1934263706207275
Iteration 421:
Training Loss: 1.4060331583023071
Reconstruction Loss: -2.635941982269287
Iteration 441:
Training Loss: 0.6546204686164856
Reconstruction Loss: -3.0815892219543457
Iteration 461:
Training Loss: 0.16518712043762207
Reconstruction Loss: -3.5085418224334717
Iteration 481:
Training Loss: -0.05612635612487793
Reconstruction Loss: -3.924933910369873
Iteration 501:
Training Loss: -0.4968690574169159
Reconstruction Loss: -4.332645416259766
Iteration 521:
Training Loss: -1.0798814296722412
Reconstruction Loss: -4.7291259765625
Iteration 541:
Training Loss: -1.8010730743408203
Reconstruction Loss: -5.124384880065918
Iteration 561:
Training Loss: -1.9562870264053345
Reconstruction Loss: -5.505587100982666
Iteration 581:
Training Loss: -2.463395357131958
Reconstruction Loss: -5.88026237487793
Iteration 601:
Training Loss: -3.0405712127685547
Reconstruction Loss: -6.249083518981934
Iteration 621:
Training Loss: -3.3116087913513184
Reconstruction Loss: -6.607201099395752
Iteration 641:
Training Loss: -3.6415088176727295
Reconstruction Loss: -6.962408542633057
Iteration 661:
Training Loss: -4.030311107635498
Reconstruction Loss: -7.30903959274292
Iteration 681:
Training Loss: -4.467182159423828
Reconstruction Loss: -7.645881652832031
Iteration 701:
Training Loss: -4.468145847320557
Reconstruction Loss: -7.976016521453857
Iteration 721:
Training Loss: -5.315736770629883
Reconstruction Loss: -8.300911903381348
Iteration 741:
Training Loss: -5.2435455322265625
Reconstruction Loss: -8.614822387695312
Iteration 761:
Training Loss: -5.549861907958984
Reconstruction Loss: -8.9190092086792
Iteration 781:
Training Loss: -5.775392532348633
Reconstruction Loss: -9.21219539642334
Iteration 801:
Training Loss: -6.1395087242126465
Reconstruction Loss: -9.494619369506836
Iteration 821:
Training Loss: -6.430395126342773
Reconstruction Loss: -9.75627326965332
Iteration 841:
Training Loss: -6.8158345222473145
Reconstruction Loss: -10.004939079284668
Iteration 861:
Training Loss: -6.77714729309082
Reconstruction Loss: -10.235599517822266
Iteration 881:
Training Loss: -7.3884077072143555
Reconstruction Loss: -10.441678047180176
Iteration 901:
Training Loss: -7.355988025665283
Reconstruction Loss: -10.629816055297852
Iteration 921:
Training Loss: -7.318708896636963
Reconstruction Loss: -10.792557716369629
Iteration 941:
Training Loss: -7.429585933685303
Reconstruction Loss: -10.93217658996582
Iteration 961:
Training Loss: -7.514786243438721
Reconstruction Loss: -11.05517864227295
Iteration 981:
Training Loss: -7.4505157470703125
Reconstruction Loss: -11.154401779174805
Iteration 1001:
Training Loss: -7.482794761657715
Reconstruction Loss: -11.244694709777832
Iteration 1021:
Training Loss: -7.55898904800415
Reconstruction Loss: -11.311308860778809
Iteration 1041:
Training Loss: -7.664644241333008
Reconstruction Loss: -11.363410949707031
Iteration 1061:
Training Loss: -7.734299659729004
Reconstruction Loss: -11.412710189819336
Iteration 1081:
Training Loss: -7.867480278015137
Reconstruction Loss: -11.453226089477539
Iteration 1101:
Training Loss: -7.73903226852417
Reconstruction Loss: -11.484549522399902
Iteration 1121:
Training Loss: -7.574023246765137
Reconstruction Loss: -11.508173942565918
Iteration 1141:
Training Loss: -8.081719398498535
Reconstruction Loss: -11.52539348602295
Iteration 1161:
Training Loss: -7.703077793121338
Reconstruction Loss: -11.547260284423828
Iteration 1181:
Training Loss: -7.732715606689453
Reconstruction Loss: -11.560615539550781
Iteration 1201:
Training Loss: -7.513281345367432
Reconstruction Loss: -11.574067115783691
Iteration 1221:
Training Loss: -7.676901817321777
Reconstruction Loss: -11.585894584655762
Iteration 1241:
Training Loss: -7.499159812927246
Reconstruction Loss: -11.589106559753418
Iteration 1261:
Training Loss: -7.892676830291748
Reconstruction Loss: -11.602121353149414
Iteration 1281:
Training Loss: -7.803843021392822
Reconstruction Loss: -11.609101295471191
Iteration 1301:
Training Loss: -7.648116588592529
Reconstruction Loss: -11.617050170898438
Iteration 1321:
Training Loss: -7.744574069976807
Reconstruction Loss: -11.61935806274414
Iteration 1341:
Training Loss: -7.727941513061523
Reconstruction Loss: -11.62936782836914
Iteration 1361:
Training Loss: -7.686913967132568
Reconstruction Loss: -11.6337251663208
Iteration 1381:
Training Loss: -7.7097601890563965
Reconstruction Loss: -11.628585815429688
Iteration 1401:
Training Loss: -7.731319427490234
Reconstruction Loss: -11.63985824584961
Iteration 1421:
Training Loss: -7.66315221786499
Reconstruction Loss: -11.645170211791992
Iteration 1441:
Training Loss: -7.635787010192871
Reconstruction Loss: -11.65125560760498
Iteration 1461:
Training Loss: -7.841145992279053
Reconstruction Loss: -11.64793586730957
Iteration 1481:
Training Loss: -7.666020393371582
Reconstruction Loss: -11.654667854309082
Iteration 1501:
Training Loss: -8.09851360321045
Reconstruction Loss: -11.65863037109375
Iteration 1521:
Training Loss: -7.71602725982666
Reconstruction Loss: -11.661137580871582
Iteration 1541:
Training Loss: -8.081633567810059
Reconstruction Loss: -11.663661003112793
Iteration 1561:
Training Loss: -7.921591758728027
Reconstruction Loss: -11.666769027709961
Iteration 1581:
Training Loss: -8.054122924804688
Reconstruction Loss: -11.668938636779785
Iteration 1601:
Training Loss: -7.376049995422363
Reconstruction Loss: -11.675786972045898
Iteration 1621:
Training Loss: -7.737651824951172
Reconstruction Loss: -11.678525924682617
Iteration 1641:
Training Loss: -8.215790748596191
Reconstruction Loss: -11.677115440368652
Iteration 1661:
Training Loss: -7.61851692199707
Reconstruction Loss: -11.683427810668945
Iteration 1681:
Training Loss: -8.109121322631836
Reconstruction Loss: -11.6869535446167
Iteration 1701:
Training Loss: -7.933545112609863
Reconstruction Loss: -11.684863090515137
Iteration 1721:
Training Loss: -7.846840858459473
Reconstruction Loss: -11.692063331604004
Iteration 1741:
Training Loss: -7.798247814178467
Reconstruction Loss: -11.68973159790039
Iteration 1761:
Training Loss: -7.916863441467285
Reconstruction Loss: -11.69459056854248
Iteration 1781:
Training Loss: -7.826814651489258
Reconstruction Loss: -11.70004653930664
Iteration 1801:
Training Loss: -7.830966949462891
Reconstruction Loss: -11.701272964477539
Iteration 1821:
Training Loss: -7.769176006317139
Reconstruction Loss: -11.705550193786621
Iteration 1841:
Training Loss: -7.706356048583984
Reconstruction Loss: -11.703567504882812
Iteration 1861:
Training Loss: -7.777459144592285
Reconstruction Loss: -11.709667205810547
Iteration 1881:
Training Loss: -7.793353080749512
Reconstruction Loss: -11.713509559631348
Iteration 1901:
Training Loss: -7.768489360809326
Reconstruction Loss: -11.713552474975586
Iteration 1921:
Training Loss: -7.470835208892822
Reconstruction Loss: -11.722275733947754
Iteration 1941:
Training Loss: -8.358352661132812
Reconstruction Loss: -11.717000007629395
Iteration 1961:
Training Loss: -7.836361408233643
Reconstruction Loss: -11.722187042236328
Iteration 1981:
Training Loss: -8.226163864135742
Reconstruction Loss: -11.722678184509277
Iteration 2001:
Training Loss: -7.872824668884277
Reconstruction Loss: -11.726286888122559
Iteration 2021:
Training Loss: -7.7015156745910645
Reconstruction Loss: -11.729907035827637
Iteration 2041:
Training Loss: -8.106399536132812
Reconstruction Loss: -11.731521606445312
Iteration 2061:
Training Loss: -8.01359748840332
Reconstruction Loss: -11.733963966369629
Iteration 2081:
Training Loss: -7.748971939086914
Reconstruction Loss: -11.738204956054688
Iteration 2101:
Training Loss: -7.917308807373047
Reconstruction Loss: -11.740756034851074
Iteration 2121:
Training Loss: -7.79425048828125
Reconstruction Loss: -11.742847442626953
Iteration 2141:
Training Loss: -7.936828136444092
Reconstruction Loss: -11.741193771362305
Iteration 2161:
Training Loss: -7.7773895263671875
Reconstruction Loss: -11.746609687805176
Iteration 2181:
Training Loss: -7.550153732299805
Reconstruction Loss: -11.748624801635742
Iteration 2201:
Training Loss: -7.57360315322876
Reconstruction Loss: -11.752510070800781
Iteration 2221:
Training Loss: -8.128641128540039
Reconstruction Loss: -11.754019737243652
Iteration 2241:
Training Loss: -8.095598220825195
Reconstruction Loss: -11.757848739624023
Iteration 2261:
Training Loss: -8.047636032104492
Reconstruction Loss: -11.760687828063965
Iteration 2281:
Training Loss: -7.705902099609375
Reconstruction Loss: -11.758683204650879
Iteration 2301:
Training Loss: -7.811014175415039
Reconstruction Loss: -11.762757301330566
Iteration 2321:
Training Loss: -7.869904041290283
Reconstruction Loss: -11.764005661010742
Iteration 2341:
Training Loss: -8.000663757324219
Reconstruction Loss: -11.766568183898926
Iteration 2361:
Training Loss: -7.733420372009277
Reconstruction Loss: -11.77302360534668
Iteration 2381:
Training Loss: -7.99272346496582
Reconstruction Loss: -11.776863098144531
Iteration 2401:
Training Loss: -7.889163017272949
Reconstruction Loss: -11.775455474853516
Iteration 2421:
Training Loss: -7.9977312088012695
Reconstruction Loss: -11.775630950927734
Iteration 2441:
Training Loss: -8.20302963256836
Reconstruction Loss: -11.779511451721191
Iteration 2461:
Training Loss: -7.954262733459473
Reconstruction Loss: -11.785445213317871
Iteration 2481:
Training Loss: -7.914590358734131
Reconstruction Loss: -11.788959503173828
Iteration 2501:
Training Loss: -8.34734058380127
Reconstruction Loss: -11.789868354797363
Iteration 2521:
Training Loss: -8.042144775390625
Reconstruction Loss: -11.786774635314941
Iteration 2541:
Training Loss: -7.730069637298584
Reconstruction Loss: -11.79349136352539
Iteration 2561:
Training Loss: -8.158731460571289
Reconstruction Loss: -11.795437812805176
Iteration 2581:
Training Loss: -7.718224048614502
Reconstruction Loss: -11.800302505493164
Iteration 2601:
Training Loss: -7.724466323852539
Reconstruction Loss: -11.798504829406738
Iteration 2621:
Training Loss: -8.043679237365723
Reconstruction Loss: -11.80654239654541
Iteration 2641:
Training Loss: -7.97416877746582
Reconstruction Loss: -11.803803443908691
Iteration 2661:
Training Loss: -8.27739143371582
Reconstruction Loss: -11.803136825561523
Iteration 2681:
Training Loss: -8.059008598327637
Reconstruction Loss: -11.806501388549805
Iteration 2701:
Training Loss: -8.34736156463623
Reconstruction Loss: -11.81081485748291
Iteration 2721:
Training Loss: -7.920679569244385
Reconstruction Loss: -11.813026428222656
Iteration 2741:
Training Loss: -7.797297954559326
Reconstruction Loss: -11.817054748535156
Iteration 2761:
Training Loss: -7.971182823181152
Reconstruction Loss: -11.816617965698242
Iteration 2781:
Training Loss: -7.785516738891602
Reconstruction Loss: -11.819597244262695
Iteration 2801:
Training Loss: -8.056391716003418
Reconstruction Loss: -11.817014694213867
Iteration 2821:
Training Loss: -7.74483060836792
Reconstruction Loss: -11.830449104309082
Iteration 2841:
Training Loss: -7.819221496582031
Reconstruction Loss: -11.829069137573242
Iteration 2861:
Training Loss: -8.032227516174316
Reconstruction Loss: -11.831766128540039
Iteration 2881:
Training Loss: -8.013399124145508
Reconstruction Loss: -11.833139419555664
Iteration 2901:
Training Loss: -8.038372039794922
Reconstruction Loss: -11.833558082580566
Iteration 2921:
Training Loss: -8.269682884216309
Reconstruction Loss: -11.834609031677246
Iteration 2941:
Training Loss: -8.091696739196777
Reconstruction Loss: -11.83818531036377
Iteration 2961:
Training Loss: -8.027494430541992
Reconstruction Loss: -11.8413667678833
Iteration 2981:
Training Loss: -8.182515144348145
Reconstruction Loss: -11.84124755859375
