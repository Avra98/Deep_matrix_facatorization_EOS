5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.498221397399902
Reconstruction Loss: -0.3632635176181793
Iteration 11:
Training Loss: 5.5976386070251465
Reconstruction Loss: -0.3633890748023987
Iteration 21:
Training Loss: 5.872936725616455
Reconstruction Loss: -0.36360669136047363
Iteration 31:
Training Loss: 5.973441123962402
Reconstruction Loss: -0.36422738432884216
Iteration 41:
Training Loss: 5.729301929473877
Reconstruction Loss: -0.3684276342391968
Iteration 51:
Training Loss: 4.926252365112305
Reconstruction Loss: -0.4995243549346924
Iteration 61:
Training Loss: 4.8257670402526855
Reconstruction Loss: -0.5938788652420044
Iteration 71:
Training Loss: 4.6249918937683105
Reconstruction Loss: -0.9821745157241821
Iteration 81:
Training Loss: 3.552690267562866
Reconstruction Loss: -1.2815732955932617
Iteration 91:
Training Loss: 2.9781675338745117
Reconstruction Loss: -1.6388208866119385
Iteration 101:
Training Loss: 3.1519908905029297
Reconstruction Loss: -1.7415013313293457
Iteration 111:
Training Loss: 2.600491523742676
Reconstruction Loss: -1.7720288038253784
Iteration 121:
Training Loss: 3.36606502532959
Reconstruction Loss: -1.7734047174453735
Iteration 131:
Training Loss: 2.6273040771484375
Reconstruction Loss: -1.8385646343231201
Iteration 141:
Training Loss: 2.3454766273498535
Reconstruction Loss: -2.1163527965545654
Iteration 151:
Training Loss: 1.5905139446258545
Reconstruction Loss: -2.824183464050293
Iteration 161:
Training Loss: 0.9603649377822876
Reconstruction Loss: -3.6695940494537354
Iteration 171:
Training Loss: -0.14579740166664124
Reconstruction Loss: -4.455089569091797
Iteration 181:
Training Loss: -0.6562527418136597
Reconstruction Loss: -5.154191493988037
Iteration 191:
Training Loss: -2.257953643798828
Reconstruction Loss: -5.798529148101807
Iteration 201:
Training Loss: -2.562084197998047
Reconstruction Loss: -6.3853440284729
Iteration 211:
Training Loss: -3.2662506103515625
Reconstruction Loss: -6.921014308929443
Iteration 221:
Training Loss: -3.416994094848633
Reconstruction Loss: -7.405500888824463
Iteration 231:
Training Loss: -4.243396759033203
Reconstruction Loss: -7.8419108390808105
Iteration 241:
Training Loss: -4.239168643951416
Reconstruction Loss: -8.239558219909668
Iteration 251:
Training Loss: -4.58209228515625
Reconstruction Loss: -8.58119010925293
Iteration 261:
Training Loss: -5.191036701202393
Reconstruction Loss: -8.876493453979492
Iteration 271:
Training Loss: -5.024606704711914
Reconstruction Loss: -9.112349510192871
Iteration 281:
Training Loss: -4.887476444244385
Reconstruction Loss: -9.286493301391602
Iteration 291:
Training Loss: -5.309585094451904
Reconstruction Loss: -9.432463645935059
Iteration 301:
Training Loss: -5.363239765167236
Reconstruction Loss: -9.537023544311523
Iteration 311:
Training Loss: -5.664581775665283
Reconstruction Loss: -9.605788230895996
Iteration 321:
Training Loss: -5.57631254196167
Reconstruction Loss: -9.670397758483887
Iteration 331:
Training Loss: -6.011349201202393
Reconstruction Loss: -9.717649459838867
Iteration 341:
Training Loss: -5.416170120239258
Reconstruction Loss: -9.759049415588379
Iteration 351:
Training Loss: -5.61937952041626
Reconstruction Loss: -9.781481742858887
Iteration 361:
Training Loss: -5.391282558441162
Reconstruction Loss: -9.807746887207031
Iteration 371:
Training Loss: -5.888592720031738
Reconstruction Loss: -9.834471702575684
Iteration 381:
Training Loss: -5.2195820808410645
Reconstruction Loss: -9.848710060119629
Iteration 391:
Training Loss: -5.810311794281006
Reconstruction Loss: -9.867430686950684
Iteration 401:
Training Loss: -5.598423004150391
Reconstruction Loss: -9.892680168151855
Iteration 411:
Training Loss: -5.5387749671936035
Reconstruction Loss: -9.900720596313477
Iteration 421:
Training Loss: -5.98921012878418
Reconstruction Loss: -9.915724754333496
Iteration 431:
Training Loss: -5.556335926055908
Reconstruction Loss: -9.944451332092285
Iteration 441:
Training Loss: -6.065330505371094
Reconstruction Loss: -9.958451271057129
Iteration 451:
Training Loss: -5.608936309814453
Reconstruction Loss: -9.96825122833252
Iteration 461:
Training Loss: -5.291549205780029
Reconstruction Loss: -9.98644733428955
Iteration 471:
Training Loss: -5.770981788635254
Reconstruction Loss: -9.999375343322754
Iteration 481:
Training Loss: -5.7798051834106445
Reconstruction Loss: -10.020383834838867
Iteration 491:
Training Loss: -5.503398418426514
Reconstruction Loss: -10.033527374267578
Iteration 501:
Training Loss: -6.453018665313721
Reconstruction Loss: -10.0526123046875
Iteration 511:
Training Loss: -6.1465654373168945
Reconstruction Loss: -10.071435928344727
Iteration 521:
Training Loss: -5.586928367614746
Reconstruction Loss: -10.086469650268555
Iteration 531:
Training Loss: -6.029996395111084
Reconstruction Loss: -10.101737022399902
Iteration 541:
Training Loss: -5.6287102699279785
Reconstruction Loss: -10.116878509521484
Iteration 551:
Training Loss: -6.017597675323486
Reconstruction Loss: -10.123863220214844
Iteration 561:
Training Loss: -5.711082458496094
Reconstruction Loss: -10.138816833496094
Iteration 571:
Training Loss: -6.197479724884033
Reconstruction Loss: -10.157926559448242
Iteration 581:
Training Loss: -5.611317157745361
Reconstruction Loss: -10.172978401184082
Iteration 591:
Training Loss: -5.780281066894531
Reconstruction Loss: -10.188477516174316
Iteration 601:
Training Loss: -6.252636432647705
Reconstruction Loss: -10.204105377197266
Iteration 611:
Training Loss: -5.992544174194336
Reconstruction Loss: -10.210906982421875
Iteration 621:
Training Loss: -6.15774393081665
Reconstruction Loss: -10.227201461791992
Iteration 631:
Training Loss: -5.754591941833496
Reconstruction Loss: -10.239896774291992
Iteration 641:
Training Loss: -5.688714027404785
Reconstruction Loss: -10.266616821289062
Iteration 651:
Training Loss: -6.153863906860352
Reconstruction Loss: -10.273521423339844
Iteration 661:
Training Loss: -6.0641608238220215
Reconstruction Loss: -10.281291961669922
Iteration 671:
Training Loss: -5.5539751052856445
Reconstruction Loss: -10.286174774169922
Iteration 681:
Training Loss: -6.241479396820068
Reconstruction Loss: -10.308365821838379
Iteration 691:
Training Loss: -6.246871471405029
Reconstruction Loss: -10.322369575500488
Iteration 701:
Training Loss: -6.283470153808594
Reconstruction Loss: -10.336915969848633
Iteration 711:
Training Loss: -6.183825492858887
Reconstruction Loss: -10.358755111694336
Iteration 721:
Training Loss: -6.137016296386719
Reconstruction Loss: -10.363225936889648
Iteration 731:
Training Loss: -6.199779510498047
Reconstruction Loss: -10.374783515930176
Iteration 741:
Training Loss: -6.13446044921875
Reconstruction Loss: -10.395018577575684
Iteration 751:
Training Loss: -6.046542167663574
Reconstruction Loss: -10.407562255859375
Iteration 761:
Training Loss: -6.398860931396484
Reconstruction Loss: -10.401000022888184
Iteration 771:
Training Loss: -5.864435195922852
Reconstruction Loss: -10.421463012695312
Iteration 781:
Training Loss: -6.124477863311768
Reconstruction Loss: -10.440078735351562
Iteration 791:
Training Loss: -6.34052848815918
Reconstruction Loss: -10.454636573791504
Iteration 801:
Training Loss: -6.229835510253906
Reconstruction Loss: -10.468344688415527
Iteration 811:
Training Loss: -6.0532402992248535
Reconstruction Loss: -10.480835914611816
Iteration 821:
Training Loss: -6.253442287445068
Reconstruction Loss: -10.488181114196777
Iteration 831:
Training Loss: -6.479156970977783
Reconstruction Loss: -10.501208305358887
Iteration 841:
Training Loss: -6.319479942321777
Reconstruction Loss: -10.509132385253906
Iteration 851:
Training Loss: -6.047637939453125
Reconstruction Loss: -10.515576362609863
Iteration 861:
Training Loss: -6.054687023162842
Reconstruction Loss: -10.523868560791016
Iteration 871:
Training Loss: -6.066168308258057
Reconstruction Loss: -10.540055274963379
Iteration 881:
Training Loss: -6.318367004394531
Reconstruction Loss: -10.557352066040039
Iteration 891:
Training Loss: -6.307037353515625
Reconstruction Loss: -10.564959526062012
Iteration 901:
Training Loss: -6.245394706726074
Reconstruction Loss: -10.57496166229248
Iteration 911:
Training Loss: -6.194921493530273
Reconstruction Loss: -10.589714050292969
Iteration 921:
Training Loss: -6.1318559646606445
Reconstruction Loss: -10.602479934692383
Iteration 931:
Training Loss: -6.143375396728516
Reconstruction Loss: -10.60253620147705
Iteration 941:
Training Loss: -5.9956135749816895
Reconstruction Loss: -10.620345115661621
Iteration 951:
Training Loss: -6.216409683227539
Reconstruction Loss: -10.631674766540527
Iteration 961:
Training Loss: -6.370721340179443
Reconstruction Loss: -10.637718200683594
Iteration 971:
Training Loss: -6.824816703796387
Reconstruction Loss: -10.64324951171875
Iteration 981:
Training Loss: -6.2254557609558105
Reconstruction Loss: -10.659741401672363
Iteration 991:
Training Loss: -6.68630313873291
Reconstruction Loss: -10.67764949798584
Iteration 1001:
Training Loss: -6.29793643951416
Reconstruction Loss: -10.674176216125488
Iteration 1011:
Training Loss: -6.451532363891602
Reconstruction Loss: -10.689775466918945
Iteration 1021:
Training Loss: -6.446162223815918
Reconstruction Loss: -10.702927589416504
Iteration 1031:
Training Loss: -6.472733020782471
Reconstruction Loss: -10.714162826538086
Iteration 1041:
Training Loss: -6.635553359985352
Reconstruction Loss: -10.718279838562012
Iteration 1051:
Training Loss: -6.49146032333374
Reconstruction Loss: -10.73208999633789
Iteration 1061:
Training Loss: -6.475074291229248
Reconstruction Loss: -10.743614196777344
Iteration 1071:
Training Loss: -6.801109790802002
Reconstruction Loss: -10.747336387634277
Iteration 1081:
Training Loss: -6.423433303833008
Reconstruction Loss: -10.752535820007324
Iteration 1091:
Training Loss: -6.673309326171875
Reconstruction Loss: -10.768800735473633
Iteration 1101:
Training Loss: -6.658073425292969
Reconstruction Loss: -10.779464721679688
Iteration 1111:
Training Loss: -6.757238388061523
Reconstruction Loss: -10.780049324035645
Iteration 1121:
Training Loss: -6.965223789215088
Reconstruction Loss: -10.788628578186035
Iteration 1131:
Training Loss: -6.610427379608154
Reconstruction Loss: -10.804327011108398
Iteration 1141:
Training Loss: -6.534969806671143
Reconstruction Loss: -10.819647789001465
Iteration 1151:
Training Loss: -6.520643711090088
Reconstruction Loss: -10.817540168762207
Iteration 1161:
Training Loss: -7.330471992492676
Reconstruction Loss: -10.831825256347656
Iteration 1171:
Training Loss: -6.470597743988037
Reconstruction Loss: -10.831214904785156
Iteration 1181:
Training Loss: -6.905544757843018
Reconstruction Loss: -10.839693069458008
Iteration 1191:
Training Loss: -6.423287391662598
Reconstruction Loss: -10.86180305480957
Iteration 1201:
Training Loss: -6.5699849128723145
Reconstruction Loss: -10.861343383789062
Iteration 1211:
Training Loss: -6.745632171630859
Reconstruction Loss: -10.87010669708252
Iteration 1221:
Training Loss: -7.151075839996338
Reconstruction Loss: -10.88378620147705
Iteration 1231:
Training Loss: -6.503117561340332
Reconstruction Loss: -10.890422821044922
Iteration 1241:
Training Loss: -6.81804895401001
Reconstruction Loss: -10.901178359985352
Iteration 1251:
Training Loss: -6.516693115234375
Reconstruction Loss: -10.901830673217773
Iteration 1261:
Training Loss: -6.688979625701904
Reconstruction Loss: -10.9205904006958
Iteration 1271:
Training Loss: -6.31583309173584
Reconstruction Loss: -10.91782283782959
Iteration 1281:
Training Loss: -6.429433822631836
Reconstruction Loss: -10.935881614685059
Iteration 1291:
Training Loss: -6.87481164932251
Reconstruction Loss: -10.933228492736816
Iteration 1301:
Training Loss: -6.8441243171691895
Reconstruction Loss: -10.952970504760742
Iteration 1311:
Training Loss: -6.918368339538574
Reconstruction Loss: -10.957158088684082
Iteration 1321:
Training Loss: -7.071654796600342
Reconstruction Loss: -10.963628768920898
Iteration 1331:
Training Loss: -6.685696601867676
Reconstruction Loss: -10.974913597106934
Iteration 1341:
Training Loss: -6.759298324584961
Reconstruction Loss: -10.987231254577637
Iteration 1351:
Training Loss: -7.020419120788574
Reconstruction Loss: -10.988524436950684
Iteration 1361:
Training Loss: -6.680456638336182
Reconstruction Loss: -11.00815486907959
Iteration 1371:
Training Loss: -6.91237211227417
Reconstruction Loss: -11.011825561523438
Iteration 1381:
Training Loss: -7.061163902282715
Reconstruction Loss: -11.02065658569336
Iteration 1391:
Training Loss: -7.275944232940674
Reconstruction Loss: -11.023151397705078
Iteration 1401:
Training Loss: -7.032841682434082
Reconstruction Loss: -11.023844718933105
Iteration 1411:
Training Loss: -6.642960548400879
Reconstruction Loss: -11.03980827331543
Iteration 1421:
Training Loss: -7.311388969421387
Reconstruction Loss: -11.052072525024414
Iteration 1431:
Training Loss: -6.996188640594482
Reconstruction Loss: -11.057194709777832
Iteration 1441:
Training Loss: -6.627089500427246
Reconstruction Loss: -11.051498413085938
Iteration 1451:
Training Loss: -6.951069355010986
Reconstruction Loss: -11.060924530029297
Iteration 1461:
Training Loss: -6.807347297668457
Reconstruction Loss: -11.071039199829102
Iteration 1471:
Training Loss: -6.504863262176514
Reconstruction Loss: -11.08321475982666
Iteration 1481:
Training Loss: -6.509942054748535
Reconstruction Loss: -11.081473350524902
Iteration 1491:
Training Loss: -6.777603626251221
Reconstruction Loss: -11.09374713897705
