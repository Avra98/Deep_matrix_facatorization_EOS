5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.749045372009277
Reconstruction Loss: -0.4109147787094116
Iteration 51:
Training Loss: 5.68585205078125
Reconstruction Loss: -0.4109147787094116
Iteration 101:
Training Loss: 5.709641456604004
Reconstruction Loss: -0.4109147787094116
Iteration 151:
Training Loss: 5.657934188842773
Reconstruction Loss: -0.41091468930244446
Iteration 201:
Training Loss: 5.726725101470947
Reconstruction Loss: -0.4109147787094116
Iteration 251:
Training Loss: 5.753674507141113
Reconstruction Loss: -0.4109147787094116
Iteration 301:
Training Loss: 5.706413745880127
Reconstruction Loss: -0.4109147787094116
Iteration 351:
Training Loss: 5.575501441955566
Reconstruction Loss: -0.4109147787094116
Iteration 401:
Training Loss: 5.736999034881592
Reconstruction Loss: -0.4109148681163788
Iteration 451:
Training Loss: 5.4784088134765625
Reconstruction Loss: -0.4109148681163788
Iteration 501:
Training Loss: 5.6791768074035645
Reconstruction Loss: -0.4109148681163788
Iteration 551:
Training Loss: 5.667006015777588
Reconstruction Loss: -0.4109148681163788
Iteration 601:
Training Loss: 5.809676170349121
Reconstruction Loss: -0.4109148681163788
Iteration 651:
Training Loss: 5.646714687347412
Reconstruction Loss: -0.4109148681163788
Iteration 701:
Training Loss: 5.742647171020508
Reconstruction Loss: -0.4109148681163788
Iteration 751:
Training Loss: 5.831459045410156
Reconstruction Loss: -0.4109148681163788
Iteration 801:
Training Loss: 5.679866313934326
Reconstruction Loss: -0.4109150469303131
Iteration 851:
Training Loss: 5.588648796081543
Reconstruction Loss: -0.4109150469303131
Iteration 901:
Training Loss: 5.75126314163208
Reconstruction Loss: -0.4109150469303131
Iteration 951:
Training Loss: 5.589227676391602
Reconstruction Loss: -0.4109151363372803
Iteration 1001:
Training Loss: 5.652012825012207
Reconstruction Loss: -0.4109151363372803
Iteration 1051:
Training Loss: 5.720655918121338
Reconstruction Loss: -0.4109151363372803
Iteration 1101:
Training Loss: 5.626522541046143
Reconstruction Loss: -0.4109151363372803
Iteration 1151:
Training Loss: 5.689540863037109
Reconstruction Loss: -0.41091522574424744
Iteration 1201:
Training Loss: 5.447988510131836
Reconstruction Loss: -0.41091522574424744
Iteration 1251:
Training Loss: 5.582710266113281
Reconstruction Loss: -0.41091522574424744
Iteration 1301:
Training Loss: 5.804738998413086
Reconstruction Loss: -0.4109153151512146
Iteration 1351:
Training Loss: 5.762441635131836
Reconstruction Loss: -0.4109154939651489
Iteration 1401:
Training Loss: 5.762076377868652
Reconstruction Loss: -0.4109154939651489
Iteration 1451:
Training Loss: 5.644482135772705
Reconstruction Loss: -0.4109155833721161
Iteration 1501:
Training Loss: 5.770174026489258
Reconstruction Loss: -0.4109155833721161
Iteration 1551:
Training Loss: 5.616884708404541
Reconstruction Loss: -0.4109157621860504
Iteration 1601:
Training Loss: 5.607339382171631
Reconstruction Loss: -0.41091594099998474
Iteration 1651:
Training Loss: 5.640390872955322
Reconstruction Loss: -0.41091611981391907
Iteration 1701:
Training Loss: 5.587821960449219
Reconstruction Loss: -0.4109162986278534
Iteration 1751:
Training Loss: 5.505890846252441
Reconstruction Loss: -0.4109164774417877
Iteration 1801:
Training Loss: 5.59553861618042
Reconstruction Loss: -0.41091692447662354
Iteration 1851:
Training Loss: 5.533306121826172
Reconstruction Loss: -0.41091737151145935
Iteration 1901:
Training Loss: 5.698121070861816
Reconstruction Loss: -0.41091811656951904
Iteration 1951:
Training Loss: 5.698489665985107
Reconstruction Loss: -0.4109190106391907
Iteration 2001:
Training Loss: 5.718793869018555
Reconstruction Loss: -0.4109204411506653
Iteration 2051:
Training Loss: 5.731215000152588
Reconstruction Loss: -0.4109225869178772
Iteration 2101:
Training Loss: 5.649511814117432
Reconstruction Loss: -0.4109261929988861
Iteration 2151:
Training Loss: 5.590416431427002
Reconstruction Loss: -0.41093239188194275
Iteration 2201:
Training Loss: 5.590800762176514
Reconstruction Loss: -0.41094523668289185
Iteration 2251:
Training Loss: 5.65671968460083
Reconstruction Loss: -0.41097527742385864
Iteration 2301:
Training Loss: 5.680943965911865
Reconstruction Loss: -0.4110668897628784
Iteration 2351:
Training Loss: 5.741976737976074
Reconstruction Loss: -0.41150546073913574
Iteration 2401:
Training Loss: 5.623979568481445
Reconstruction Loss: -0.41839003562927246
Iteration 2451:
Training Loss: 5.078802108764648
Reconstruction Loss: -0.4611290395259857
Iteration 2501:
Training Loss: 5.094655990600586
Reconstruction Loss: -0.45323699712753296
Iteration 2551:
Training Loss: 5.175981521606445
Reconstruction Loss: -0.47020992636680603
Iteration 2601:
Training Loss: 5.013975143432617
Reconstruction Loss: -0.48704686760902405
Iteration 2651:
Training Loss: 5.027612209320068
Reconstruction Loss: -0.5112876892089844
Iteration 2701:
Training Loss: 5.009343147277832
Reconstruction Loss: -0.5285081267356873
Iteration 2751:
Training Loss: 4.991628170013428
Reconstruction Loss: -0.5447095632553101
Iteration 2801:
Training Loss: 4.9264235496521
Reconstruction Loss: -0.5606237649917603
Iteration 2851:
Training Loss: 5.0896735191345215
Reconstruction Loss: -0.5660660266876221
Iteration 2901:
Training Loss: 4.964412689208984
Reconstruction Loss: -0.5707192420959473
Iteration 2951:
Training Loss: 4.925227642059326
Reconstruction Loss: -0.5738290548324585
Iteration 3001:
Training Loss: 4.991136074066162
Reconstruction Loss: -0.5726433992385864
Iteration 3051:
Training Loss: 4.985989570617676
Reconstruction Loss: -0.584247350692749
Iteration 3101:
Training Loss: 5.061018943786621
Reconstruction Loss: -0.605878472328186
Iteration 3151:
Training Loss: 4.5486931800842285
Reconstruction Loss: -0.8527646660804749
Iteration 3201:
Training Loss: 4.32573127746582
Reconstruction Loss: -0.8495124578475952
Iteration 3251:
Training Loss: 4.264248371124268
Reconstruction Loss: -0.8501818776130676
Iteration 3301:
Training Loss: 4.311794281005859
Reconstruction Loss: -0.8536655306816101
Iteration 3351:
Training Loss: 4.37224006652832
Reconstruction Loss: -0.8646866679191589
Iteration 3401:
Training Loss: 4.252577781677246
Reconstruction Loss: -0.8637714385986328
Iteration 3451:
Training Loss: 4.096432209014893
Reconstruction Loss: -0.8680546283721924
Iteration 3501:
Training Loss: 4.240052223205566
Reconstruction Loss: -0.8662800788879395
Iteration 3551:
Training Loss: 4.366243362426758
Reconstruction Loss: -0.8609983325004578
Iteration 3601:
Training Loss: 4.279321670532227
Reconstruction Loss: -0.8663479685783386
Iteration 3651:
Training Loss: 4.4457106590271
Reconstruction Loss: -0.8717235326766968
Iteration 3701:
Training Loss: 4.317616939544678
Reconstruction Loss: -0.8669814467430115
Iteration 3751:
Training Loss: 4.30267858505249
Reconstruction Loss: -0.8649606704711914
Iteration 3801:
Training Loss: 4.219254016876221
Reconstruction Loss: -0.8667028546333313
Iteration 3851:
Training Loss: 4.2051215171813965
Reconstruction Loss: -0.8662372827529907
Iteration 3901:
Training Loss: 4.331244945526123
Reconstruction Loss: -0.8619741201400757
Iteration 3951:
Training Loss: 4.101276874542236
Reconstruction Loss: -0.862960696220398
Iteration 4001:
Training Loss: 4.261459827423096
Reconstruction Loss: -0.8820286989212036
Iteration 4051:
Training Loss: 3.9909510612487793
Reconstruction Loss: -0.9997338056564331
Iteration 4101:
Training Loss: 3.8621861934661865
Reconstruction Loss: -1.1102585792541504
Iteration 4151:
Training Loss: 3.7711000442504883
Reconstruction Loss: -1.141574740409851
Iteration 4201:
Training Loss: 3.6461825370788574
Reconstruction Loss: -1.1345508098602295
Iteration 4251:
Training Loss: 3.6693997383117676
Reconstruction Loss: -1.1267032623291016
Iteration 4301:
Training Loss: 3.6875290870666504
Reconstruction Loss: -1.1195553541183472
Iteration 4351:
Training Loss: 3.7905256748199463
Reconstruction Loss: -1.1075375080108643
Iteration 4401:
Training Loss: 3.7599494457244873
Reconstruction Loss: -1.1042670011520386
Iteration 4451:
Training Loss: 3.632047414779663
Reconstruction Loss: -1.1002495288848877
Iteration 4501:
Training Loss: 3.515932559967041
Reconstruction Loss: -1.1000524759292603
Iteration 4551:
Training Loss: 3.6617751121520996
Reconstruction Loss: -1.0997623205184937
Iteration 4601:
Training Loss: 3.589839220046997
Reconstruction Loss: -1.098080039024353
Iteration 4651:
Training Loss: 3.6425580978393555
Reconstruction Loss: -1.0980618000030518
Iteration 4701:
Training Loss: 3.545276403427124
Reconstruction Loss: -1.0960460901260376
Iteration 4751:
Training Loss: 3.6656124591827393
Reconstruction Loss: -1.0916725397109985
Iteration 4801:
Training Loss: 3.737581491470337
Reconstruction Loss: -1.0954890251159668
Iteration 4851:
Training Loss: 3.553929567337036
Reconstruction Loss: -1.0939387083053589
Iteration 4901:
Training Loss: 3.552371025085449
Reconstruction Loss: -1.0964012145996094
Iteration 4951:
Training Loss: 3.7338266372680664
Reconstruction Loss: -1.099104881286621
Iteration 5001:
Training Loss: 3.6062207221984863
Reconstruction Loss: -1.1103248596191406
Iteration 5051:
Training Loss: 3.6286511421203613
Reconstruction Loss: -1.1394728422164917
Iteration 5101:
Training Loss: 3.4371564388275146
Reconstruction Loss: -1.2726625204086304
Iteration 5151:
Training Loss: 2.9312386512756348
Reconstruction Loss: -1.4738972187042236
Iteration 5201:
Training Loss: 3.0539050102233887
Reconstruction Loss: -1.5746209621429443
Iteration 5251:
Training Loss: 2.826447010040283
Reconstruction Loss: -1.6352678537368774
Iteration 5301:
Training Loss: 2.755253791809082
Reconstruction Loss: -1.6909446716308594
Iteration 5351:
Training Loss: 2.7868289947509766
Reconstruction Loss: -1.7317132949829102
Iteration 5401:
Training Loss: 2.7178921699523926
Reconstruction Loss: -1.759842872619629
Iteration 5451:
Training Loss: 2.8529529571533203
Reconstruction Loss: -1.7822054624557495
Iteration 5501:
Training Loss: 2.77593994140625
Reconstruction Loss: -1.8038623332977295
Iteration 5551:
Training Loss: 2.494906425476074
Reconstruction Loss: -1.8392561674118042
Iteration 5601:
Training Loss: 2.484719753265381
Reconstruction Loss: -1.9432334899902344
Iteration 5651:
Training Loss: 1.9678021669387817
Reconstruction Loss: -2.287407875061035
Iteration 5701:
Training Loss: 0.9619837403297424
Reconstruction Loss: -2.974764108657837
Iteration 5751:
Training Loss: 0.13677048683166504
Reconstruction Loss: -3.650611639022827
Iteration 5801:
Training Loss: -0.5668372511863708
Reconstruction Loss: -4.23185920715332
Iteration 5851:
Training Loss: -1.1476716995239258
Reconstruction Loss: -4.7502336502075195
Iteration 5901:
Training Loss: -1.717347502708435
Reconstruction Loss: -5.233870506286621
Iteration 5951:
Training Loss: -2.0406739711761475
Reconstruction Loss: -5.687244415283203
Iteration 6001:
Training Loss: -2.731686592102051
Reconstruction Loss: -6.125273704528809
Iteration 6051:
Training Loss: -3.0669608116149902
Reconstruction Loss: -6.547521591186523
Iteration 6101:
Training Loss: -3.565488815307617
Reconstruction Loss: -6.957375526428223
Iteration 6151:
Training Loss: -3.9202237129211426
Reconstruction Loss: -7.354992866516113
Iteration 6201:
Training Loss: -4.271233558654785
Reconstruction Loss: -7.752084732055664
Iteration 6251:
Training Loss: -4.772489547729492
Reconstruction Loss: -8.138520240783691
Iteration 6301:
Training Loss: -5.14986515045166
Reconstruction Loss: -8.5241117477417
Iteration 6351:
Training Loss: -5.591920375823975
Reconstruction Loss: -8.903684616088867
Iteration 6401:
Training Loss: -6.012365341186523
Reconstruction Loss: -9.28240966796875
Iteration 6451:
Training Loss: -6.321203708648682
Reconstruction Loss: -9.656624794006348
Iteration 6501:
Training Loss: -6.750416278839111
Reconstruction Loss: -10.03173828125
Iteration 6551:
Training Loss: -7.210540771484375
Reconstruction Loss: -10.403460502624512
Iteration 6601:
Training Loss: -7.327325344085693
Reconstruction Loss: -10.772592544555664
Iteration 6651:
Training Loss: -7.678528308868408
Reconstruction Loss: -11.143877983093262
Iteration 6701:
Training Loss: -8.105628967285156
Reconstruction Loss: -11.509265899658203
Iteration 6751:
Training Loss: -8.550474166870117
Reconstruction Loss: -11.878323554992676
Iteration 6801:
Training Loss: -8.929967880249023
Reconstruction Loss: -12.245265007019043
Iteration 6851:
Training Loss: -9.381019592285156
Reconstruction Loss: -12.610373497009277
Iteration 6901:
Training Loss: -9.697513580322266
Reconstruction Loss: -12.977350234985352
Iteration 6951:
Training Loss: -10.075532913208008
Reconstruction Loss: -13.340971946716309
Iteration 7001:
Training Loss: -10.389565467834473
Reconstruction Loss: -13.705487251281738
Iteration 7051:
Training Loss: -10.779038429260254
Reconstruction Loss: -14.06852912902832
Iteration 7101:
Training Loss: -11.07580280303955
Reconstruction Loss: -14.430770874023438
Iteration 7151:
Training Loss: -11.555150032043457
Reconstruction Loss: -14.793750762939453
Iteration 7201:
Training Loss: -11.846678733825684
Reconstruction Loss: -15.156189918518066
Iteration 7251:
Training Loss: -12.270910263061523
Reconstruction Loss: -15.516215324401855
Iteration 7301:
Training Loss: -12.568214416503906
Reconstruction Loss: -15.878719329833984
Iteration 7351:
Training Loss: -12.906170845031738
Reconstruction Loss: -16.239002227783203
Iteration 7401:
Training Loss: -13.163325309753418
Reconstruction Loss: -16.59689712524414
Iteration 7451:
Training Loss: -13.64408016204834
Reconstruction Loss: -16.957393646240234
