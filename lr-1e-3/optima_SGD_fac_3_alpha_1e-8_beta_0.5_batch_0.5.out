5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.626515865325928
Reconstruction Loss: -0.45477649569511414
Iteration 51:
Training Loss: 5.493375778198242
Reconstruction Loss: -0.4547765851020813
Iteration 101:
Training Loss: 5.647342681884766
Reconstruction Loss: -0.4547765851020813
Iteration 151:
Training Loss: 5.597296237945557
Reconstruction Loss: -0.4547765851020813
Iteration 201:
Training Loss: 5.51793909072876
Reconstruction Loss: -0.4547765851020813
Iteration 251:
Training Loss: 5.6925787925720215
Reconstruction Loss: -0.4547765851020813
Iteration 301:
Training Loss: 5.6179938316345215
Reconstruction Loss: -0.4547765851020813
Iteration 351:
Training Loss: 5.6465911865234375
Reconstruction Loss: -0.4547765851020813
Iteration 401:
Training Loss: 5.668516159057617
Reconstruction Loss: -0.45477667450904846
Iteration 451:
Training Loss: 5.649766445159912
Reconstruction Loss: -0.45477667450904846
Iteration 501:
Training Loss: 5.442646503448486
Reconstruction Loss: -0.45477667450904846
Iteration 551:
Training Loss: 5.642176151275635
Reconstruction Loss: -0.45477667450904846
Iteration 601:
Training Loss: 5.609169006347656
Reconstruction Loss: -0.45477667450904846
Iteration 651:
Training Loss: 5.466977119445801
Reconstruction Loss: -0.45477667450904846
Iteration 701:
Training Loss: 5.680307388305664
Reconstruction Loss: -0.45477667450904846
Iteration 751:
Training Loss: 5.504144191741943
Reconstruction Loss: -0.45477667450904846
Iteration 801:
Training Loss: 5.608511447906494
Reconstruction Loss: -0.45477667450904846
Iteration 851:
Training Loss: 5.471987247467041
Reconstruction Loss: -0.4547767639160156
Iteration 901:
Training Loss: 5.730249404907227
Reconstruction Loss: -0.4547768831253052
Iteration 951:
Training Loss: 5.729632377624512
Reconstruction Loss: -0.4547768831253052
Iteration 1001:
Training Loss: 5.4753828048706055
Reconstruction Loss: -0.4547768831253052
Iteration 1051:
Training Loss: 5.8627166748046875
Reconstruction Loss: -0.45477694272994995
Iteration 1101:
Training Loss: 5.527349948883057
Reconstruction Loss: -0.45477694272994995
Iteration 1151:
Training Loss: 5.64078950881958
Reconstruction Loss: -0.45477694272994995
Iteration 1201:
Training Loss: 5.524572372436523
Reconstruction Loss: -0.45477694272994995
Iteration 1251:
Training Loss: 5.550545692443848
Reconstruction Loss: -0.45477715134620667
Iteration 1301:
Training Loss: 5.688509464263916
Reconstruction Loss: -0.45477724075317383
Iteration 1351:
Training Loss: 5.5814056396484375
Reconstruction Loss: -0.45477724075317383
Iteration 1401:
Training Loss: 5.64434289932251
Reconstruction Loss: -0.45477724075317383
Iteration 1451:
Training Loss: 5.754541397094727
Reconstruction Loss: -0.454777330160141
Iteration 1501:
Training Loss: 5.5554118156433105
Reconstruction Loss: -0.4547775387763977
Iteration 1551:
Training Loss: 5.682905673980713
Reconstruction Loss: -0.45477771759033203
Iteration 1601:
Training Loss: 5.504153251647949
Reconstruction Loss: -0.45477771759033203
Iteration 1651:
Training Loss: 5.712218761444092
Reconstruction Loss: -0.4547780752182007
Iteration 1701:
Training Loss: 5.62450647354126
Reconstruction Loss: -0.454778254032135
Iteration 1751:
Training Loss: 5.535429954528809
Reconstruction Loss: -0.4547785520553589
Iteration 1801:
Training Loss: 5.6530351638793945
Reconstruction Loss: -0.4547789394855499
Iteration 1851:
Training Loss: 5.694238662719727
Reconstruction Loss: -0.45477932691574097
Iteration 1901:
Training Loss: 5.6716508865356445
Reconstruction Loss: -0.45478004217147827
Iteration 1951:
Training Loss: 5.646491050720215
Reconstruction Loss: -0.45478081703186035
Iteration 2001:
Training Loss: 5.580841541290283
Reconstruction Loss: -0.45478203892707825
Iteration 2051:
Training Loss: 5.642656326293945
Reconstruction Loss: -0.45478373765945435
Iteration 2101:
Training Loss: 5.578588485717773
Reconstruction Loss: -0.45478618144989014
Iteration 2151:
Training Loss: 5.561461448669434
Reconstruction Loss: -0.454789936542511
Iteration 2201:
Training Loss: 5.571934223175049
Reconstruction Loss: -0.4547961950302124
Iteration 2251:
Training Loss: 5.5592498779296875
Reconstruction Loss: -0.4548074007034302
Iteration 2301:
Training Loss: 5.676110744476318
Reconstruction Loss: -0.4548305869102478
Iteration 2351:
Training Loss: 5.678831100463867
Reconstruction Loss: -0.4548858404159546
Iteration 2401:
Training Loss: 5.72540807723999
Reconstruction Loss: -0.4550609588623047
Iteration 2451:
Training Loss: 5.660367965698242
Reconstruction Loss: -0.45599403977394104
Iteration 2501:
Training Loss: 5.636092662811279
Reconstruction Loss: -0.477521687746048
Iteration 2551:
Training Loss: 5.140250205993652
Reconstruction Loss: -0.539993166923523
Iteration 2601:
Training Loss: 4.973363399505615
Reconstruction Loss: -0.5237310528755188
Iteration 2651:
Training Loss: 5.041385650634766
Reconstruction Loss: -0.512572169303894
Iteration 2701:
Training Loss: 4.955475330352783
Reconstruction Loss: -0.4937417507171631
Iteration 2751:
Training Loss: 5.182213306427002
Reconstruction Loss: -0.5105571746826172
Iteration 2801:
Training Loss: 5.011075973510742
Reconstruction Loss: -0.5055899620056152
Iteration 2851:
Training Loss: 5.124116897583008
Reconstruction Loss: -0.5057496428489685
Iteration 2901:
Training Loss: 4.9699225425720215
Reconstruction Loss: -0.5032935738563538
Iteration 2951:
Training Loss: 5.18152379989624
Reconstruction Loss: -0.49423593282699585
Iteration 3001:
Training Loss: 5.366872787475586
Reconstruction Loss: -0.5017645359039307
Iteration 3051:
Training Loss: 5.015716552734375
Reconstruction Loss: -0.5029783248901367
Iteration 3101:
Training Loss: 5.1483564376831055
Reconstruction Loss: -0.4979289174079895
Iteration 3151:
Training Loss: 5.1326069831848145
Reconstruction Loss: -0.5255820751190186
Iteration 3201:
Training Loss: 4.770565986633301
Reconstruction Loss: -0.7255216836929321
Iteration 3251:
Training Loss: 4.54988431930542
Reconstruction Loss: -0.7351900339126587
Iteration 3301:
Training Loss: 4.607056140899658
Reconstruction Loss: -0.7231582999229431
Iteration 3351:
Training Loss: 4.58696174621582
Reconstruction Loss: -0.7088748216629028
Iteration 3401:
Training Loss: 4.510315895080566
Reconstruction Loss: -0.7075547575950623
Iteration 3451:
Training Loss: 4.6352362632751465
Reconstruction Loss: -0.6996481418609619
Iteration 3501:
Training Loss: 4.523974418640137
Reconstruction Loss: -0.6985501646995544
Iteration 3551:
Training Loss: 4.407217979431152
Reconstruction Loss: -0.7914363741874695
Iteration 3601:
Training Loss: 3.7810842990875244
Reconstruction Loss: -1.0272061824798584
Iteration 3651:
Training Loss: 3.829124927520752
Reconstruction Loss: -0.9754809141159058
Iteration 3701:
Training Loss: 3.855053424835205
Reconstruction Loss: -0.9463518857955933
Iteration 3751:
Training Loss: 3.725294589996338
Reconstruction Loss: -0.9339985847473145
Iteration 3801:
Training Loss: 3.820448160171509
Reconstruction Loss: -0.9217302799224854
Iteration 3851:
Training Loss: 3.8179426193237305
Reconstruction Loss: -0.9180460572242737
Iteration 3901:
Training Loss: 3.7673020362854004
Reconstruction Loss: -0.9192279577255249
Iteration 3951:
Training Loss: 3.819392681121826
Reconstruction Loss: -0.9126629829406738
Iteration 4001:
Training Loss: 3.81042218208313
Reconstruction Loss: -0.9096471667289734
Iteration 4051:
Training Loss: 3.7338144779205322
Reconstruction Loss: -0.9101674556732178
Iteration 4101:
Training Loss: 3.6835978031158447
Reconstruction Loss: -0.905973494052887
Iteration 4151:
Training Loss: 3.868867874145508
Reconstruction Loss: -0.9088122248649597
Iteration 4201:
Training Loss: 3.8111894130706787
Reconstruction Loss: -0.9127775430679321
Iteration 4251:
Training Loss: 3.787668466567993
Reconstruction Loss: -0.9069752097129822
Iteration 4301:
Training Loss: 3.8481435775756836
Reconstruction Loss: -0.9038928747177124
Iteration 4351:
Training Loss: 3.5991737842559814
Reconstruction Loss: -0.9082080721855164
Iteration 4401:
Training Loss: 3.6507794857025146
Reconstruction Loss: -0.9027551412582397
Iteration 4451:
Training Loss: 3.887626886367798
Reconstruction Loss: -0.9071612358093262
Iteration 4501:
Training Loss: 3.871899366378784
Reconstruction Loss: -0.9095605611801147
Iteration 4551:
Training Loss: 3.9299886226654053
Reconstruction Loss: -0.9083613753318787
Iteration 4601:
Training Loss: 3.8126754760742188
Reconstruction Loss: -0.9101821184158325
Iteration 4651:
Training Loss: 3.7429378032684326
Reconstruction Loss: -0.9123288989067078
Iteration 4701:
Training Loss: 3.7415082454681396
Reconstruction Loss: -0.9443690180778503
Iteration 4751:
Training Loss: 3.2238669395446777
Reconstruction Loss: -1.1210840940475464
Iteration 4801:
Training Loss: 3.154362440109253
Reconstruction Loss: -1.354691505432129
Iteration 4851:
Training Loss: 2.9567039012908936
Reconstruction Loss: -1.4558029174804688
Iteration 4901:
Training Loss: 2.985578775405884
Reconstruction Loss: -1.5209914445877075
Iteration 4951:
Training Loss: 2.9578938484191895
Reconstruction Loss: -1.5572912693023682
Iteration 5001:
Training Loss: 2.948364734649658
Reconstruction Loss: -1.5824244022369385
Iteration 5051:
Training Loss: 2.8720502853393555
Reconstruction Loss: -1.5893131494522095
Iteration 5101:
Training Loss: 2.935004472732544
Reconstruction Loss: -1.5922582149505615
Iteration 5151:
Training Loss: 2.9081873893737793
Reconstruction Loss: -1.5945791006088257
Iteration 5201:
Training Loss: 3.0316994190216064
Reconstruction Loss: -1.6007893085479736
Iteration 5251:
Training Loss: 2.8540732860565186
Reconstruction Loss: -1.5992660522460938
Iteration 5301:
Training Loss: 2.838508129119873
Reconstruction Loss: -1.6023635864257812
Iteration 5351:
Training Loss: 2.858050584793091
Reconstruction Loss: -1.60207998752594
Iteration 5401:
Training Loss: 2.8535244464874268
Reconstruction Loss: -1.6050870418548584
Iteration 5451:
Training Loss: 2.8076491355895996
Reconstruction Loss: -1.605484127998352
Iteration 5501:
Training Loss: 2.9862775802612305
Reconstruction Loss: -1.6070072650909424
Iteration 5551:
Training Loss: 2.862705707550049
Reconstruction Loss: -1.608072280883789
Iteration 5601:
Training Loss: 2.721343755722046
Reconstruction Loss: -1.6052125692367554
Iteration 5651:
Training Loss: 2.88724422454834
Reconstruction Loss: -1.6057270765304565
Iteration 5701:
Training Loss: 2.7662734985351562
Reconstruction Loss: -1.6078784465789795
Iteration 5751:
Training Loss: 3.0498478412628174
Reconstruction Loss: -1.609545111656189
Iteration 5801:
Training Loss: 2.903167724609375
Reconstruction Loss: -1.605843424797058
Iteration 5851:
Training Loss: 2.933640480041504
Reconstruction Loss: -1.6084246635437012
Iteration 5901:
Training Loss: 3.0263450145721436
Reconstruction Loss: -1.6064726114273071
Iteration 5951:
Training Loss: 2.8580734729766846
Reconstruction Loss: -1.6085922718048096
Iteration 6001:
Training Loss: 2.7979979515075684
Reconstruction Loss: -1.6039515733718872
Iteration 6051:
Training Loss: 2.6286299228668213
Reconstruction Loss: -1.6083498001098633
Iteration 6101:
Training Loss: 2.8906333446502686
Reconstruction Loss: -1.6111574172973633
Iteration 6151:
Training Loss: 2.7785494327545166
Reconstruction Loss: -1.6071778535842896
Iteration 6201:
Training Loss: 2.5991291999816895
Reconstruction Loss: -1.6083884239196777
Iteration 6251:
Training Loss: 2.9535112380981445
Reconstruction Loss: -1.6093225479125977
Iteration 6301:
Training Loss: 3.0497186183929443
Reconstruction Loss: -1.6096938848495483
Iteration 6351:
Training Loss: 2.886892080307007
Reconstruction Loss: -1.6107110977172852
Iteration 6401:
Training Loss: 2.9930319786071777
Reconstruction Loss: -1.6109575033187866
Iteration 6451:
Training Loss: 3.0263819694519043
Reconstruction Loss: -1.6105765104293823
Iteration 6501:
Training Loss: 2.880239963531494
Reconstruction Loss: -1.611485242843628
Iteration 6551:
Training Loss: 2.9024531841278076
Reconstruction Loss: -1.6107360124588013
Iteration 6601:
Training Loss: 2.8591318130493164
Reconstruction Loss: -1.6097846031188965
Iteration 6651:
Training Loss: 2.9301059246063232
Reconstruction Loss: -1.6160022020339966
Iteration 6701:
Training Loss: 2.9071595668792725
Reconstruction Loss: -1.6172101497650146
Iteration 6751:
Training Loss: 2.935028314590454
Reconstruction Loss: -1.6225486993789673
Iteration 6801:
Training Loss: 2.8326005935668945
Reconstruction Loss: -1.6324652433395386
Iteration 6851:
Training Loss: 2.66774582862854
Reconstruction Loss: -1.667586326599121
Iteration 6901:
Training Loss: 2.598891019821167
Reconstruction Loss: -1.7898781299591064
Iteration 6951:
Training Loss: 1.8978450298309326
Reconstruction Loss: -2.246692657470703
Iteration 7001:
Training Loss: 0.9516828060150146
Reconstruction Loss: -2.9590258598327637
Iteration 7051:
Training Loss: 0.16253700852394104
Reconstruction Loss: -3.566596269607544
Iteration 7101:
Training Loss: -0.5759104490280151
Reconstruction Loss: -4.109320640563965
Iteration 7151:
Training Loss: -0.9998028874397278
Reconstruction Loss: -4.613489151000977
Iteration 7201:
Training Loss: -1.4482210874557495
Reconstruction Loss: -5.101491451263428
Iteration 7251:
Training Loss: -2.164381980895996
Reconstruction Loss: -5.5787153244018555
Iteration 7301:
Training Loss: -2.525282382965088
Reconstruction Loss: -6.045802116394043
Iteration 7351:
Training Loss: -3.0325334072113037
Reconstruction Loss: -6.510000228881836
Iteration 7401:
Training Loss: -3.566490650177002
Reconstruction Loss: -6.972926139831543
Iteration 7451:
Training Loss: -3.936495542526245
Reconstruction Loss: -7.428908348083496
