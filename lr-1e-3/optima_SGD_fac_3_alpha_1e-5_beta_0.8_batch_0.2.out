5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.475296974182129
Reconstruction Loss: -0.44581952691078186
Iteration 21:
Training Loss: 5.545271873474121
Reconstruction Loss: -0.4459041357040405
Iteration 41:
Training Loss: 5.137085914611816
Reconstruction Loss: -0.4460297226905823
Iteration 61:
Training Loss: 5.188225746154785
Reconstruction Loss: -0.4463033378124237
Iteration 81:
Training Loss: 5.138559341430664
Reconstruction Loss: -0.44721871614456177
Iteration 101:
Training Loss: 5.304321765899658
Reconstruction Loss: -0.454286128282547
Iteration 121:
Training Loss: 4.973830699920654
Reconstruction Loss: -0.6145120859146118
Iteration 141:
Training Loss: 4.432636260986328
Reconstruction Loss: -0.8449517488479614
Iteration 161:
Training Loss: 4.6307477951049805
Reconstruction Loss: -0.8377830982208252
Iteration 181:
Training Loss: 4.314001083374023
Reconstruction Loss: -0.8728702664375305
Iteration 201:
Training Loss: 4.021097660064697
Reconstruction Loss: -1.128182291984558
Iteration 221:
Training Loss: 4.06026029586792
Reconstruction Loss: -1.2634987831115723
Iteration 241:
Training Loss: 3.6067724227905273
Reconstruction Loss: -1.3010401725769043
Iteration 261:
Training Loss: 3.1476054191589355
Reconstruction Loss: -1.4085513353347778
Iteration 281:
Training Loss: 2.893031120300293
Reconstruction Loss: -1.7005786895751953
Iteration 301:
Training Loss: 3.198268413543701
Reconstruction Loss: -1.8010053634643555
Iteration 321:
Training Loss: 2.950052261352539
Reconstruction Loss: -1.806747555732727
Iteration 341:
Training Loss: 2.694624185562134
Reconstruction Loss: -1.8057262897491455
Iteration 361:
Training Loss: 2.9444806575775146
Reconstruction Loss: -1.7893167734146118
Iteration 381:
Training Loss: 2.562126398086548
Reconstruction Loss: -1.7820155620574951
Iteration 401:
Training Loss: 2.689368724822998
Reconstruction Loss: -1.8141902685165405
Iteration 421:
Training Loss: 2.5071933269500732
Reconstruction Loss: -1.9588500261306763
Iteration 441:
Training Loss: 1.924095630645752
Reconstruction Loss: -2.3710498809814453
Iteration 461:
Training Loss: 1.0661611557006836
Reconstruction Loss: -2.949690818786621
Iteration 481:
Training Loss: 0.4216354489326477
Reconstruction Loss: -3.5373051166534424
Iteration 501:
Training Loss: -0.45260316133499146
Reconstruction Loss: -4.118719100952148
Iteration 521:
Training Loss: -0.806418776512146
Reconstruction Loss: -4.672146320343018
Iteration 541:
Training Loss: -1.5929937362670898
Reconstruction Loss: -5.219371318817139
Iteration 561:
Training Loss: -2.0435268878936768
Reconstruction Loss: -5.746226787567139
Iteration 581:
Training Loss: -2.5957589149475098
Reconstruction Loss: -6.266247272491455
Iteration 601:
Training Loss: -3.141179084777832
Reconstruction Loss: -6.770773410797119
Iteration 621:
Training Loss: -3.426326036453247
Reconstruction Loss: -7.2658491134643555
Iteration 641:
Training Loss: -4.595847129821777
Reconstruction Loss: -7.754861354827881
Iteration 661:
Training Loss: -4.583745956420898
Reconstruction Loss: -8.225739479064941
Iteration 681:
Training Loss: -4.906511306762695
Reconstruction Loss: -8.683382034301758
Iteration 701:
Training Loss: -5.56636381149292
Reconstruction Loss: -9.124844551086426
Iteration 721:
Training Loss: -5.984788417816162
Reconstruction Loss: -9.539365768432617
Iteration 741:
Training Loss: -6.032783031463623
Reconstruction Loss: -9.93153190612793
Iteration 761:
Training Loss: -6.723369598388672
Reconstruction Loss: -10.288984298706055
Iteration 781:
Training Loss: -6.771932125091553
Reconstruction Loss: -10.607673645019531
Iteration 801:
Training Loss: -7.032016754150391
Reconstruction Loss: -10.88647747039795
Iteration 821:
Training Loss: -7.040370941162109
Reconstruction Loss: -11.11513614654541
Iteration 841:
Training Loss: -7.2685041427612305
Reconstruction Loss: -11.30843448638916
Iteration 861:
Training Loss: -7.234195709228516
Reconstruction Loss: -11.45645523071289
Iteration 881:
Training Loss: -7.496565341949463
Reconstruction Loss: -11.566122055053711
Iteration 901:
Training Loss: -7.59132194519043
Reconstruction Loss: -11.650903701782227
Iteration 921:
Training Loss: -7.298892974853516
Reconstruction Loss: -11.715265274047852
Iteration 941:
Training Loss: -7.510615348815918
Reconstruction Loss: -11.758988380432129
Iteration 961:
Training Loss: -7.6100616455078125
Reconstruction Loss: -11.792067527770996
Iteration 981:
Training Loss: -7.300356864929199
Reconstruction Loss: -11.811638832092285
Iteration 1001:
Training Loss: -7.473567962646484
Reconstruction Loss: -11.83899211883545
Iteration 1021:
Training Loss: -7.684940338134766
Reconstruction Loss: -11.85326099395752
Iteration 1041:
Training Loss: -7.692497730255127
Reconstruction Loss: -11.866412162780762
Iteration 1061:
Training Loss: -7.45488166809082
Reconstruction Loss: -11.876593589782715
Iteration 1081:
Training Loss: -7.504328727722168
Reconstruction Loss: -11.883627891540527
Iteration 1101:
Training Loss: -7.540464401245117
Reconstruction Loss: -11.890850067138672
Iteration 1121:
Training Loss: -7.60252571105957
Reconstruction Loss: -11.900677680969238
Iteration 1141:
Training Loss: -7.594608783721924
Reconstruction Loss: -11.9043550491333
Iteration 1161:
Training Loss: -7.675868511199951
Reconstruction Loss: -11.912181854248047
Iteration 1181:
Training Loss: -7.622133255004883
Reconstruction Loss: -11.914761543273926
Iteration 1201:
Training Loss: -7.5636372566223145
Reconstruction Loss: -11.916894912719727
Iteration 1221:
Training Loss: -7.320066452026367
Reconstruction Loss: -11.927566528320312
Iteration 1241:
Training Loss: -7.426032543182373
Reconstruction Loss: -11.929813385009766
Iteration 1261:
Training Loss: -7.374913692474365
Reconstruction Loss: -11.931658744812012
Iteration 1281:
Training Loss: -7.567119598388672
Reconstruction Loss: -11.941535949707031
Iteration 1301:
Training Loss: -7.719925403594971
Reconstruction Loss: -11.94576644897461
Iteration 1321:
Training Loss: -7.829810619354248
Reconstruction Loss: -11.947492599487305
Iteration 1341:
Training Loss: -7.644433975219727
Reconstruction Loss: -11.95130729675293
Iteration 1361:
Training Loss: -7.510445594787598
Reconstruction Loss: -11.954257011413574
Iteration 1381:
Training Loss: -7.54794979095459
Reconstruction Loss: -11.961111068725586
Iteration 1401:
Training Loss: -7.789883136749268
Reconstruction Loss: -11.963135719299316
Iteration 1421:
Training Loss: -7.702361583709717
Reconstruction Loss: -11.964317321777344
Iteration 1441:
Training Loss: -7.393096923828125
Reconstruction Loss: -11.971930503845215
Iteration 1461:
Training Loss: -7.519314289093018
Reconstruction Loss: -11.976609230041504
Iteration 1481:
Training Loss: -7.427631855010986
Reconstruction Loss: -11.978209495544434
Iteration 1501:
Training Loss: -7.5555901527404785
Reconstruction Loss: -11.982185363769531
Iteration 1521:
Training Loss: -7.58944845199585
Reconstruction Loss: -11.986950874328613
Iteration 1541:
Training Loss: -7.487006187438965
Reconstruction Loss: -11.99180793762207
Iteration 1561:
Training Loss: -7.815564155578613
Reconstruction Loss: -11.994378089904785
Iteration 1581:
Training Loss: -7.626019477844238
Reconstruction Loss: -12.00179386138916
Iteration 1601:
Training Loss: -7.696853160858154
Reconstruction Loss: -12.007967948913574
Iteration 1621:
Training Loss: -7.861793518066406
Reconstruction Loss: -12.00752067565918
Iteration 1641:
Training Loss: -7.6639790534973145
Reconstruction Loss: -12.00929069519043
Iteration 1661:
Training Loss: -7.531938076019287
Reconstruction Loss: -12.016908645629883
Iteration 1681:
Training Loss: -7.727111339569092
Reconstruction Loss: -12.018085479736328
Iteration 1701:
Training Loss: -7.371104717254639
Reconstruction Loss: -12.02110767364502
Iteration 1721:
Training Loss: -7.814730167388916
Reconstruction Loss: -12.027515411376953
Iteration 1741:
Training Loss: -7.603404998779297
Reconstruction Loss: -12.029035568237305
Iteration 1761:
Training Loss: -7.705570220947266
Reconstruction Loss: -12.035890579223633
Iteration 1781:
Training Loss: -7.618419647216797
Reconstruction Loss: -12.038830757141113
Iteration 1801:
Training Loss: -7.754774570465088
Reconstruction Loss: -12.042607307434082
Iteration 1821:
Training Loss: -7.792694091796875
Reconstruction Loss: -12.045084953308105
Iteration 1841:
Training Loss: -7.873350620269775
Reconstruction Loss: -12.04845905303955
Iteration 1861:
Training Loss: -7.66508150100708
Reconstruction Loss: -12.050333023071289
Iteration 1881:
Training Loss: -7.621891021728516
Reconstruction Loss: -12.05794906616211
Iteration 1901:
Training Loss: -7.700174331665039
Reconstruction Loss: -12.056279182434082
Iteration 1921:
Training Loss: -7.48242712020874
Reconstruction Loss: -12.060888290405273
Iteration 1941:
Training Loss: -7.754087448120117
Reconstruction Loss: -12.066665649414062
Iteration 1961:
Training Loss: -7.551540851593018
Reconstruction Loss: -12.069953918457031
Iteration 1981:
Training Loss: -7.801576614379883
Reconstruction Loss: -12.07384204864502
Iteration 2001:
Training Loss: -7.828616619110107
Reconstruction Loss: -12.07554817199707
Iteration 2021:
Training Loss: -7.751723289489746
Reconstruction Loss: -12.078274726867676
Iteration 2041:
Training Loss: -7.6020612716674805
Reconstruction Loss: -12.087906837463379
Iteration 2061:
Training Loss: -7.624838352203369
Reconstruction Loss: -12.090235710144043
Iteration 2081:
Training Loss: -7.612473487854004
Reconstruction Loss: -12.095741271972656
Iteration 2101:
Training Loss: -7.797667980194092
Reconstruction Loss: -12.099959373474121
Iteration 2121:
Training Loss: -7.982943058013916
Reconstruction Loss: -12.106169700622559
Iteration 2141:
Training Loss: -7.749258041381836
Reconstruction Loss: -12.10330581665039
Iteration 2161:
Training Loss: -7.656097888946533
Reconstruction Loss: -12.103377342224121
Iteration 2181:
Training Loss: -7.819569110870361
Reconstruction Loss: -12.10776424407959
Iteration 2201:
Training Loss: -7.814590930938721
Reconstruction Loss: -12.113454818725586
Iteration 2221:
Training Loss: -7.733229637145996
Reconstruction Loss: -12.120959281921387
Iteration 2241:
Training Loss: -7.748307704925537
Reconstruction Loss: -12.126060485839844
Iteration 2261:
Training Loss: -7.4467644691467285
Reconstruction Loss: -12.128891944885254
Iteration 2281:
Training Loss: -7.974847316741943
Reconstruction Loss: -12.12807846069336
Iteration 2301:
Training Loss: -7.769567966461182
Reconstruction Loss: -12.135157585144043
Iteration 2321:
Training Loss: -7.800337791442871
Reconstruction Loss: -12.141587257385254
Iteration 2341:
Training Loss: -7.826116561889648
Reconstruction Loss: -12.136751174926758
Iteration 2361:
Training Loss: -8.128965377807617
Reconstruction Loss: -12.140396118164062
Iteration 2381:
Training Loss: -7.872521877288818
Reconstruction Loss: -12.149178504943848
Iteration 2401:
Training Loss: -7.585696697235107
Reconstruction Loss: -12.152571678161621
Iteration 2421:
Training Loss: -7.702409744262695
Reconstruction Loss: -12.15425968170166
Iteration 2441:
Training Loss: -7.6259636878967285
Reconstruction Loss: -12.15913200378418
Iteration 2461:
Training Loss: -7.7921462059021
Reconstruction Loss: -12.156139373779297
Iteration 2481:
Training Loss: -7.606323719024658
Reconstruction Loss: -12.16493034362793
Iteration 2501:
Training Loss: -7.951873302459717
Reconstruction Loss: -12.164400100708008
Iteration 2521:
Training Loss: -7.776391983032227
Reconstruction Loss: -12.166050910949707
Iteration 2541:
Training Loss: -8.207669258117676
Reconstruction Loss: -12.173891067504883
Iteration 2561:
Training Loss: -7.975315570831299
Reconstruction Loss: -12.175087928771973
Iteration 2581:
Training Loss: -8.067397117614746
Reconstruction Loss: -12.178135871887207
Iteration 2601:
Training Loss: -8.01246452331543
Reconstruction Loss: -12.186129570007324
Iteration 2621:
Training Loss: -7.916834354400635
Reconstruction Loss: -12.190096855163574
Iteration 2641:
Training Loss: -7.792357921600342
Reconstruction Loss: -12.188619613647461
Iteration 2661:
Training Loss: -7.553061485290527
Reconstruction Loss: -12.191611289978027
Iteration 2681:
Training Loss: -7.9861741065979
Reconstruction Loss: -12.198259353637695
Iteration 2701:
Training Loss: -7.617283344268799
Reconstruction Loss: -12.19823169708252
Iteration 2721:
Training Loss: -7.677319049835205
Reconstruction Loss: -12.199952125549316
Iteration 2741:
Training Loss: -7.872379779815674
Reconstruction Loss: -12.209064483642578
Iteration 2761:
Training Loss: -7.659901142120361
Reconstruction Loss: -12.20835018157959
Iteration 2781:
Training Loss: -7.969141483306885
Reconstruction Loss: -12.213411331176758
Iteration 2801:
Training Loss: -7.941207408905029
Reconstruction Loss: -12.21390438079834
Iteration 2821:
Training Loss: -7.904881477355957
Reconstruction Loss: -12.21890926361084
Iteration 2841:
Training Loss: -7.815341949462891
Reconstruction Loss: -12.222868919372559
Iteration 2861:
Training Loss: -7.95735502243042
Reconstruction Loss: -12.223793983459473
Iteration 2881:
Training Loss: -7.791485786437988
Reconstruction Loss: -12.231828689575195
Iteration 2901:
Training Loss: -7.890631198883057
Reconstruction Loss: -12.22995376586914
Iteration 2921:
Training Loss: -7.957097053527832
Reconstruction Loss: -12.230178833007812
Iteration 2941:
Training Loss: -7.834475994110107
Reconstruction Loss: -12.236761093139648
Iteration 2961:
Training Loss: -7.9857401847839355
Reconstruction Loss: -12.242158889770508
Iteration 2981:
Training Loss: -8.094520568847656
Reconstruction Loss: -12.239314079284668
