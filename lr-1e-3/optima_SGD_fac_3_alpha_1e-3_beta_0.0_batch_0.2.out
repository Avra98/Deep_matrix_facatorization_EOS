5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.577800273895264
Reconstruction Loss: -0.4333153963088989
Iteration 21:
Training Loss: 4.947483062744141
Reconstruction Loss: -0.7740394473075867
Iteration 41:
Training Loss: 3.423874616622925
Reconstruction Loss: -1.5075634717941284
Iteration 61:
Training Loss: 2.4241464138031006
Reconstruction Loss: -2.403254747390747
Iteration 81:
Training Loss: 0.6871312856674194
Reconstruction Loss: -3.2200210094451904
Iteration 101:
Training Loss: 0.07101154327392578
Reconstruction Loss: -3.8619866371154785
Iteration 121:
Training Loss: -0.6701859831809998
Reconstruction Loss: -4.3919453620910645
Iteration 141:
Training Loss: -0.9287054538726807
Reconstruction Loss: -4.830427646636963
Iteration 161:
Training Loss: -1.213327169418335
Reconstruction Loss: -5.187972545623779
Iteration 181:
Training Loss: -1.8717385530471802
Reconstruction Loss: -5.481023788452148
Iteration 201:
Training Loss: -1.9427118301391602
Reconstruction Loss: -5.706122398376465
Iteration 221:
Training Loss: -2.265070676803589
Reconstruction Loss: -5.895363807678223
Iteration 241:
Training Loss: -2.447143077850342
Reconstruction Loss: -6.037920951843262
Iteration 261:
Training Loss: -2.602205991744995
Reconstruction Loss: -6.151243209838867
Iteration 281:
Training Loss: -2.705775260925293
Reconstruction Loss: -6.246605396270752
Iteration 301:
Training Loss: -2.869772434234619
Reconstruction Loss: -6.3247246742248535
Iteration 321:
Training Loss: -2.6185872554779053
Reconstruction Loss: -6.389742851257324
Iteration 341:
Training Loss: -2.9012656211853027
Reconstruction Loss: -6.451700210571289
Iteration 361:
Training Loss: -3.3677361011505127
Reconstruction Loss: -6.503336429595947
Iteration 381:
Training Loss: -3.136793613433838
Reconstruction Loss: -6.546398639678955
Iteration 401:
Training Loss: -3.1503264904022217
Reconstruction Loss: -6.593437671661377
Iteration 421:
Training Loss: -3.1611485481262207
Reconstruction Loss: -6.637676239013672
Iteration 441:
Training Loss: -3.1706557273864746
Reconstruction Loss: -6.67219877243042
Iteration 461:
Training Loss: -3.3006346225738525
Reconstruction Loss: -6.702951431274414
Iteration 481:
Training Loss: -3.2874557971954346
Reconstruction Loss: -6.742213249206543
Iteration 501:
Training Loss: -3.3237111568450928
Reconstruction Loss: -6.770829200744629
Iteration 521:
Training Loss: -3.485290765762329
Reconstruction Loss: -6.798598766326904
Iteration 541:
Training Loss: -3.6234548091888428
Reconstruction Loss: -6.829362392425537
Iteration 561:
Training Loss: -3.65982985496521
Reconstruction Loss: -6.854513168334961
Iteration 581:
Training Loss: -3.595296859741211
Reconstruction Loss: -6.883060455322266
Iteration 601:
Training Loss: -3.5273334980010986
Reconstruction Loss: -6.905402660369873
Iteration 621:
Training Loss: -3.5329601764678955
Reconstruction Loss: -6.9299092292785645
Iteration 641:
Training Loss: -3.7735557556152344
Reconstruction Loss: -6.952275276184082
Iteration 661:
Training Loss: -3.8493595123291016
Reconstruction Loss: -6.9748430252075195
Iteration 681:
Training Loss: -3.8541553020477295
Reconstruction Loss: -6.996489524841309
Iteration 701:
Training Loss: -3.9787919521331787
Reconstruction Loss: -7.016417026519775
Iteration 721:
Training Loss: -4.0202202796936035
Reconstruction Loss: -7.037522315979004
Iteration 741:
Training Loss: -3.8611602783203125
Reconstruction Loss: -7.058470249176025
Iteration 761:
Training Loss: -3.749454975128174
Reconstruction Loss: -7.0727033615112305
Iteration 781:
Training Loss: -4.263576984405518
Reconstruction Loss: -7.0914130210876465
Iteration 801:
Training Loss: -4.0640459060668945
Reconstruction Loss: -7.108473300933838
Iteration 821:
Training Loss: -4.176349639892578
Reconstruction Loss: -7.126753330230713
Iteration 841:
Training Loss: -4.0257086753845215
Reconstruction Loss: -7.1399149894714355
Iteration 861:
Training Loss: -4.044444561004639
Reconstruction Loss: -7.157723903656006
Iteration 881:
Training Loss: -4.257908821105957
Reconstruction Loss: -7.172420024871826
Iteration 901:
Training Loss: -4.257564067840576
Reconstruction Loss: -7.187367916107178
Iteration 921:
Training Loss: -4.218369007110596
Reconstruction Loss: -7.201450824737549
Iteration 941:
Training Loss: -4.277414321899414
Reconstruction Loss: -7.21510648727417
Iteration 961:
Training Loss: -4.350645542144775
Reconstruction Loss: -7.229261875152588
Iteration 981:
Training Loss: -4.311504364013672
Reconstruction Loss: -7.2425432205200195
Iteration 1001:
Training Loss: -4.376462936401367
Reconstruction Loss: -7.255341529846191
Iteration 1021:
Training Loss: -4.630981922149658
Reconstruction Loss: -7.270525932312012
Iteration 1041:
Training Loss: -4.241157054901123
Reconstruction Loss: -7.280852794647217
Iteration 1061:
Training Loss: -4.369720458984375
Reconstruction Loss: -7.293612003326416
Iteration 1081:
Training Loss: -4.730743885040283
Reconstruction Loss: -7.302711009979248
Iteration 1101:
Training Loss: -4.667514801025391
Reconstruction Loss: -7.316181659698486
Iteration 1121:
Training Loss: -4.4631829261779785
Reconstruction Loss: -7.326198101043701
Iteration 1141:
Training Loss: -4.557449817657471
Reconstruction Loss: -7.338842391967773
Iteration 1161:
Training Loss: -4.397890090942383
Reconstruction Loss: -7.350170135498047
Iteration 1181:
Training Loss: -4.496349334716797
Reconstruction Loss: -7.358383655548096
Iteration 1201:
Training Loss: -4.613763332366943
Reconstruction Loss: -7.370597839355469
Iteration 1221:
Training Loss: -4.502115726470947
Reconstruction Loss: -7.380833148956299
Iteration 1241:
Training Loss: -4.650577545166016
Reconstruction Loss: -7.3888325691223145
Iteration 1261:
Training Loss: -4.901947498321533
Reconstruction Loss: -7.3997039794921875
Iteration 1281:
Training Loss: -4.737754821777344
Reconstruction Loss: -7.408796310424805
Iteration 1301:
Training Loss: -4.717939376831055
Reconstruction Loss: -7.419935703277588
Iteration 1321:
Training Loss: -4.583957672119141
Reconstruction Loss: -7.424403667449951
Iteration 1341:
Training Loss: -4.715536117553711
Reconstruction Loss: -7.435883522033691
Iteration 1361:
Training Loss: -4.68061637878418
Reconstruction Loss: -7.445176601409912
Iteration 1381:
Training Loss: -5.051558971405029
Reconstruction Loss: -7.4528937339782715
Iteration 1401:
Training Loss: -4.907143592834473
Reconstruction Loss: -7.461781978607178
Iteration 1421:
Training Loss: -4.764649868011475
Reconstruction Loss: -7.470250129699707
Iteration 1441:
Training Loss: -4.703296661376953
Reconstruction Loss: -7.478677272796631
Iteration 1461:
Training Loss: -4.767174243927002
Reconstruction Loss: -7.486781120300293
Iteration 1481:
Training Loss: -4.86830472946167
Reconstruction Loss: -7.493834495544434
Iteration 1501:
Training Loss: -4.952796459197998
Reconstruction Loss: -7.50156307220459
Iteration 1521:
Training Loss: -5.024157524108887
Reconstruction Loss: -7.509354591369629
Iteration 1541:
Training Loss: -4.8029704093933105
Reconstruction Loss: -7.512451648712158
Iteration 1561:
Training Loss: -5.176398754119873
Reconstruction Loss: -7.522314071655273
Iteration 1581:
Training Loss: -5.049337387084961
Reconstruction Loss: -7.530069351196289
Iteration 1601:
Training Loss: -5.134685516357422
Reconstruction Loss: -7.537239074707031
Iteration 1621:
Training Loss: -4.947673797607422
Reconstruction Loss: -7.540891170501709
Iteration 1641:
Training Loss: -4.9548139572143555
Reconstruction Loss: -7.549980163574219
Iteration 1661:
Training Loss: -5.187517166137695
Reconstruction Loss: -7.558999538421631
Iteration 1681:
Training Loss: -4.925385475158691
Reconstruction Loss: -7.5633344650268555
Iteration 1701:
Training Loss: -5.019156455993652
Reconstruction Loss: -7.57059383392334
Iteration 1721:
Training Loss: -5.3006486892700195
Reconstruction Loss: -7.5757737159729
Iteration 1741:
Training Loss: -5.0797600746154785
Reconstruction Loss: -7.581763744354248
Iteration 1761:
Training Loss: -5.092031002044678
Reconstruction Loss: -7.587629795074463
Iteration 1781:
Training Loss: -5.196959495544434
Reconstruction Loss: -7.594719886779785
Iteration 1801:
Training Loss: -5.180332660675049
Reconstruction Loss: -7.59977912902832
Iteration 1821:
Training Loss: -5.305018424987793
Reconstruction Loss: -7.607179164886475
Iteration 1841:
Training Loss: -5.257787704467773
Reconstruction Loss: -7.6150922775268555
Iteration 1861:
Training Loss: -5.4722466468811035
Reconstruction Loss: -7.617777347564697
Iteration 1881:
Training Loss: -5.362268924713135
Reconstruction Loss: -7.626099109649658
Iteration 1901:
Training Loss: -5.342522621154785
Reconstruction Loss: -7.631566524505615
Iteration 1921:
Training Loss: -5.49549674987793
Reconstruction Loss: -7.635677814483643
Iteration 1941:
Training Loss: -5.418270587921143
Reconstruction Loss: -7.64035177230835
Iteration 1961:
Training Loss: -5.495397090911865
Reconstruction Loss: -7.646059989929199
Iteration 1981:
Training Loss: -5.756101608276367
Reconstruction Loss: -7.650269031524658
Iteration 2001:
Training Loss: -5.23192024230957
Reconstruction Loss: -7.657599925994873
Iteration 2021:
Training Loss: -5.226109504699707
Reconstruction Loss: -7.661986827850342
Iteration 2041:
Training Loss: -5.447337627410889
Reconstruction Loss: -7.666612148284912
Iteration 2061:
Training Loss: -5.45336389541626
Reconstruction Loss: -7.672084331512451
Iteration 2081:
Training Loss: -5.275383949279785
Reconstruction Loss: -7.675142765045166
Iteration 2101:
Training Loss: -5.835000038146973
Reconstruction Loss: -7.681250095367432
Iteration 2121:
Training Loss: -5.48520040512085
Reconstruction Loss: -7.6857380867004395
Iteration 2141:
Training Loss: -5.322892189025879
Reconstruction Loss: -7.690452575683594
Iteration 2161:
Training Loss: -5.540757656097412
Reconstruction Loss: -7.697218894958496
Iteration 2181:
Training Loss: -5.565789699554443
Reconstruction Loss: -7.700578689575195
Iteration 2201:
Training Loss: -5.481561183929443
Reconstruction Loss: -7.704249858856201
Iteration 2221:
Training Loss: -5.452352523803711
Reconstruction Loss: -7.708634853363037
Iteration 2241:
Training Loss: -5.457275867462158
Reconstruction Loss: -7.713132858276367
Iteration 2261:
Training Loss: -5.656674861907959
Reconstruction Loss: -7.718092441558838
Iteration 2281:
Training Loss: -5.55037260055542
Reconstruction Loss: -7.721706390380859
Iteration 2301:
Training Loss: -5.6588568687438965
Reconstruction Loss: -7.726353645324707
Iteration 2321:
Training Loss: -5.493475437164307
Reconstruction Loss: -7.732289791107178
Iteration 2341:
Training Loss: -5.7215166091918945
Reconstruction Loss: -7.737199783325195
Iteration 2361:
Training Loss: -5.740152835845947
Reconstruction Loss: -7.7396368980407715
Iteration 2381:
Training Loss: -5.6972246170043945
Reconstruction Loss: -7.743213176727295
Iteration 2401:
Training Loss: -5.519156455993652
Reconstruction Loss: -7.747537612915039
Iteration 2421:
Training Loss: -5.56981086730957
Reconstruction Loss: -7.752641677856445
Iteration 2441:
Training Loss: -5.6802978515625
Reconstruction Loss: -7.755867958068848
Iteration 2461:
Training Loss: -5.770793437957764
Reconstruction Loss: -7.759405612945557
Iteration 2481:
Training Loss: -5.790379047393799
Reconstruction Loss: -7.763561725616455
Iteration 2501:
Training Loss: -5.410260200500488
Reconstruction Loss: -7.766448020935059
Iteration 2521:
Training Loss: -5.614520072937012
Reconstruction Loss: -7.772202014923096
Iteration 2541:
Training Loss: -5.540167331695557
Reconstruction Loss: -7.774533271789551
Iteration 2561:
Training Loss: -5.997100830078125
Reconstruction Loss: -7.778092384338379
Iteration 2581:
Training Loss: -5.653339385986328
Reconstruction Loss: -7.780640125274658
Iteration 2601:
Training Loss: -5.726747512817383
Reconstruction Loss: -7.783116817474365
Iteration 2621:
Training Loss: -5.675177097320557
Reconstruction Loss: -7.790928363800049
Iteration 2641:
Training Loss: -6.245028495788574
Reconstruction Loss: -7.793382167816162
Iteration 2661:
Training Loss: -6.10545539855957
Reconstruction Loss: -7.79635763168335
Iteration 2681:
Training Loss: -5.737776279449463
Reconstruction Loss: -7.802204608917236
Iteration 2701:
Training Loss: -5.76644229888916
Reconstruction Loss: -7.803217887878418
Iteration 2721:
Training Loss: -5.627315044403076
Reconstruction Loss: -7.806530475616455
Iteration 2741:
Training Loss: -5.582396507263184
Reconstruction Loss: -7.810448169708252
Iteration 2761:
Training Loss: -5.552071571350098
Reconstruction Loss: -7.813870429992676
Iteration 2781:
Training Loss: -5.862645626068115
Reconstruction Loss: -7.816569805145264
Iteration 2801:
Training Loss: -6.210951328277588
Reconstruction Loss: -7.820479869842529
Iteration 2821:
Training Loss: -5.921598434448242
Reconstruction Loss: -7.824425220489502
Iteration 2841:
Training Loss: -6.034291744232178
Reconstruction Loss: -7.8263115882873535
Iteration 2861:
Training Loss: -5.9431633949279785
Reconstruction Loss: -7.830190658569336
Iteration 2881:
Training Loss: -5.871350288391113
Reconstruction Loss: -7.833008289337158
Iteration 2901:
Training Loss: -5.980321884155273
Reconstruction Loss: -7.835334300994873
Iteration 2921:
Training Loss: -6.204413414001465
Reconstruction Loss: -7.840454578399658
Iteration 2941:
Training Loss: -5.800368309020996
Reconstruction Loss: -7.842052459716797
Iteration 2961:
Training Loss: -5.875015735626221
Reconstruction Loss: -7.845419406890869
Iteration 2981:
Training Loss: -5.764069080352783
Reconstruction Loss: -7.848791122436523
