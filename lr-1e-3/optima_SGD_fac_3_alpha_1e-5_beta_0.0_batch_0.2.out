5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 60
Iteration 1:
Training Loss: 5.215480804443359
Reconstruction Loss: -0.4376191794872284
Iteration 21:
Training Loss: 5.4696855545043945
Reconstruction Loss: -0.43775248527526855
Iteration 41:
Training Loss: 5.600794792175293
Reconstruction Loss: -0.43799105286598206
Iteration 61:
Training Loss: 5.38736629486084
Reconstruction Loss: -0.4388296902179718
Iteration 81:
Training Loss: 5.46080207824707
Reconstruction Loss: -0.4487913250923157
Iteration 101:
Training Loss: 5.235208511352539
Reconstruction Loss: -0.6427903175354004
Iteration 121:
Training Loss: 4.852056503295898
Reconstruction Loss: -0.6884691715240479
Iteration 141:
Training Loss: 4.413286209106445
Reconstruction Loss: -0.8568763136863708
Iteration 161:
Training Loss: 4.439671993255615
Reconstruction Loss: -0.8777627348899841
Iteration 181:
Training Loss: 4.156573295593262
Reconstruction Loss: -1.0133705139160156
Iteration 201:
Training Loss: 3.2613022327423096
Reconstruction Loss: -1.1854910850524902
Iteration 221:
Training Loss: 3.5939159393310547
Reconstruction Loss: -1.1998353004455566
Iteration 241:
Training Loss: 3.37662672996521
Reconstruction Loss: -1.1804529428482056
Iteration 261:
Training Loss: 3.2779886722564697
Reconstruction Loss: -1.1523003578186035
Iteration 281:
Training Loss: 3.2788102626800537
Reconstruction Loss: -1.1403131484985352
Iteration 301:
Training Loss: 3.257234573364258
Reconstruction Loss: -1.124220371246338
Iteration 321:
Training Loss: 3.291396141052246
Reconstruction Loss: -1.1235525608062744
Iteration 341:
Training Loss: 3.551105499267578
Reconstruction Loss: -1.136992335319519
Iteration 361:
Training Loss: 3.1447956562042236
Reconstruction Loss: -1.1674418449401855
Iteration 381:
Training Loss: 2.9942479133605957
Reconstruction Loss: -1.2780249118804932
Iteration 401:
Training Loss: 2.688631296157837
Reconstruction Loss: -1.4761693477630615
Iteration 421:
Training Loss: 2.065918207168579
Reconstruction Loss: -1.7101844549179077
Iteration 441:
Training Loss: 1.9462769031524658
Reconstruction Loss: -1.9623210430145264
Iteration 461:
Training Loss: 1.3850128650665283
Reconstruction Loss: -2.2509799003601074
Iteration 481:
Training Loss: 1.0112476348876953
Reconstruction Loss: -2.5741984844207764
Iteration 501:
Training Loss: 0.5188700556755066
Reconstruction Loss: -2.903740406036377
Iteration 521:
Training Loss: 0.04832577705383301
Reconstruction Loss: -3.2227022647857666
Iteration 541:
Training Loss: -0.28264737129211426
Reconstruction Loss: -3.5194878578186035
Iteration 561:
Training Loss: -0.5239765048027039
Reconstruction Loss: -3.7959039211273193
Iteration 581:
Training Loss: -0.772864818572998
Reconstruction Loss: -4.0463056564331055
Iteration 601:
Training Loss: -1.1770753860473633
Reconstruction Loss: -4.275244235992432
Iteration 621:
Training Loss: -1.3629463911056519
Reconstruction Loss: -4.486238479614258
Iteration 641:
Training Loss: -1.4185289144515991
Reconstruction Loss: -4.678345203399658
Iteration 661:
Training Loss: -2.0578346252441406
Reconstruction Loss: -4.858346462249756
Iteration 681:
Training Loss: -1.77894926071167
Reconstruction Loss: -5.023287296295166
Iteration 701:
Training Loss: -2.228792190551758
Reconstruction Loss: -5.175642967224121
Iteration 721:
Training Loss: -1.9643912315368652
Reconstruction Loss: -5.313714027404785
Iteration 741:
Training Loss: -2.099400043487549
Reconstruction Loss: -5.4421820640563965
Iteration 761:
Training Loss: -2.533099889755249
Reconstruction Loss: -5.569210529327393
Iteration 781:
Training Loss: -2.336320638656616
Reconstruction Loss: -5.680057525634766
Iteration 801:
Training Loss: -2.5297679901123047
Reconstruction Loss: -5.790755271911621
Iteration 821:
Training Loss: -2.983158826828003
Reconstruction Loss: -5.891716480255127
Iteration 841:
Training Loss: -2.939680337905884
Reconstruction Loss: -5.98720121383667
Iteration 861:
Training Loss: -2.838181257247925
Reconstruction Loss: -6.0747175216674805
Iteration 881:
Training Loss: -2.722679853439331
Reconstruction Loss: -6.159421920776367
Iteration 901:
Training Loss: -3.022873878479004
Reconstruction Loss: -6.242729187011719
Iteration 921:
Training Loss: -3.094963788986206
Reconstruction Loss: -6.319551944732666
Iteration 941:
Training Loss: -3.31545090675354
Reconstruction Loss: -6.393859386444092
Iteration 961:
Training Loss: -3.3776934146881104
Reconstruction Loss: -6.4655537605285645
Iteration 981:
Training Loss: -3.2065939903259277
Reconstruction Loss: -6.532194137573242
Iteration 1001:
Training Loss: -3.310098171234131
Reconstruction Loss: -6.59421968460083
Iteration 1021:
Training Loss: -3.584406852722168
Reconstruction Loss: -6.658261775970459
Iteration 1041:
Training Loss: -3.5509109497070312
Reconstruction Loss: -6.71687650680542
Iteration 1061:
Training Loss: -3.4760944843292236
Reconstruction Loss: -6.775587558746338
Iteration 1081:
Training Loss: -3.6504998207092285
Reconstruction Loss: -6.826370716094971
Iteration 1101:
Training Loss: -3.9239232540130615
Reconstruction Loss: -6.88200044631958
Iteration 1121:
Training Loss: -3.696054697036743
Reconstruction Loss: -6.934322834014893
Iteration 1141:
Training Loss: -3.5776379108428955
Reconstruction Loss: -6.985577583312988
Iteration 1161:
Training Loss: -3.719115972518921
Reconstruction Loss: -7.032372951507568
Iteration 1181:
Training Loss: -3.7780728340148926
Reconstruction Loss: -7.078886032104492
Iteration 1201:
Training Loss: -4.016589641571045
Reconstruction Loss: -7.12367582321167
Iteration 1221:
Training Loss: -4.07501745223999
Reconstruction Loss: -7.167847156524658
Iteration 1241:
Training Loss: -4.0934977531433105
Reconstruction Loss: -7.211818695068359
Iteration 1261:
Training Loss: -3.759793758392334
Reconstruction Loss: -7.251923084259033
Iteration 1281:
Training Loss: -3.8902459144592285
Reconstruction Loss: -7.290637969970703
Iteration 1301:
Training Loss: -4.466007232666016
Reconstruction Loss: -7.3320159912109375
Iteration 1321:
Training Loss: -4.3992390632629395
Reconstruction Loss: -7.369230270385742
Iteration 1341:
Training Loss: -4.215131759643555
Reconstruction Loss: -7.40488338470459
Iteration 1361:
Training Loss: -4.315589427947998
Reconstruction Loss: -7.444871425628662
Iteration 1381:
Training Loss: -4.206715106964111
Reconstruction Loss: -7.4786601066589355
Iteration 1401:
Training Loss: -4.336578369140625
Reconstruction Loss: -7.509076118469238
Iteration 1421:
Training Loss: -4.2864603996276855
Reconstruction Loss: -7.545901298522949
Iteration 1441:
Training Loss: -4.3398637771606445
Reconstruction Loss: -7.578515529632568
Iteration 1461:
Training Loss: -4.505246162414551
Reconstruction Loss: -7.607419013977051
Iteration 1481:
Training Loss: -4.439339637756348
Reconstruction Loss: -7.63936710357666
Iteration 1501:
Training Loss: -4.496131896972656
Reconstruction Loss: -7.672431945800781
Iteration 1521:
Training Loss: -4.47957181930542
Reconstruction Loss: -7.702655792236328
Iteration 1541:
Training Loss: -4.700034141540527
Reconstruction Loss: -7.732319355010986
Iteration 1561:
Training Loss: -4.76607608795166
Reconstruction Loss: -7.759665489196777
Iteration 1581:
Training Loss: -4.560369491577148
Reconstruction Loss: -7.783773899078369
Iteration 1601:
Training Loss: -4.62833309173584
Reconstruction Loss: -7.816628456115723
Iteration 1621:
Training Loss: -4.412280082702637
Reconstruction Loss: -7.840559005737305
Iteration 1641:
Training Loss: -4.741918563842773
Reconstruction Loss: -7.866917610168457
Iteration 1661:
Training Loss: -4.990883827209473
Reconstruction Loss: -7.893742561340332
Iteration 1681:
Training Loss: -4.764857769012451
Reconstruction Loss: -7.918322563171387
Iteration 1701:
Training Loss: -4.979686737060547
Reconstruction Loss: -7.945802688598633
Iteration 1721:
Training Loss: -4.658959865570068
Reconstruction Loss: -7.967868804931641
Iteration 1741:
Training Loss: -4.914255142211914
Reconstruction Loss: -7.993466854095459
Iteration 1761:
Training Loss: -4.95637321472168
Reconstruction Loss: -8.01541519165039
Iteration 1781:
Training Loss: -4.949012756347656
Reconstruction Loss: -8.0409574508667
Iteration 1801:
Training Loss: -4.884934425354004
Reconstruction Loss: -8.063963890075684
Iteration 1821:
Training Loss: -4.908377647399902
Reconstruction Loss: -8.08225154876709
Iteration 1841:
Training Loss: -5.229780197143555
Reconstruction Loss: -8.10922908782959
Iteration 1861:
Training Loss: -5.120648384094238
Reconstruction Loss: -8.129251480102539
Iteration 1881:
Training Loss: -5.164331436157227
Reconstruction Loss: -8.147905349731445
Iteration 1901:
Training Loss: -5.241227626800537
Reconstruction Loss: -8.171574592590332
Iteration 1921:
Training Loss: -5.016508102416992
Reconstruction Loss: -8.193877220153809
Iteration 1941:
Training Loss: -4.932430744171143
Reconstruction Loss: -8.210895538330078
Iteration 1961:
Training Loss: -5.171211242675781
Reconstruction Loss: -8.231021881103516
Iteration 1981:
Training Loss: -5.053201675415039
Reconstruction Loss: -8.249571800231934
Iteration 2001:
Training Loss: -5.100641250610352
Reconstruction Loss: -8.269936561584473
Iteration 2021:
Training Loss: -4.908484935760498
Reconstruction Loss: -8.288444519042969
Iteration 2041:
Training Loss: -5.329099178314209
Reconstruction Loss: -8.308882713317871
Iteration 2061:
Training Loss: -5.204127311706543
Reconstruction Loss: -8.32719898223877
Iteration 2081:
Training Loss: -4.993386745452881
Reconstruction Loss: -8.34234619140625
Iteration 2101:
Training Loss: -5.182572364807129
Reconstruction Loss: -8.365981101989746
Iteration 2121:
Training Loss: -5.495683193206787
Reconstruction Loss: -8.380427360534668
Iteration 2141:
Training Loss: -5.269599437713623
Reconstruction Loss: -8.401336669921875
Iteration 2161:
Training Loss: -5.333688735961914
Reconstruction Loss: -8.416543006896973
Iteration 2181:
Training Loss: -5.448661804199219
Reconstruction Loss: -8.4353609085083
Iteration 2201:
Training Loss: -5.057818412780762
Reconstruction Loss: -8.44884204864502
Iteration 2221:
Training Loss: -5.6223602294921875
Reconstruction Loss: -8.464208602905273
Iteration 2241:
Training Loss: -5.629078388214111
Reconstruction Loss: -8.482341766357422
Iteration 2261:
Training Loss: -5.374370098114014
Reconstruction Loss: -8.501046180725098
Iteration 2281:
Training Loss: -5.304900646209717
Reconstruction Loss: -8.51284122467041
Iteration 2301:
Training Loss: -5.305028915405273
Reconstruction Loss: -8.531305313110352
Iteration 2321:
Training Loss: -5.325855255126953
Reconstruction Loss: -8.54914665222168
Iteration 2341:
Training Loss: -5.537694931030273
Reconstruction Loss: -8.564242362976074
Iteration 2361:
Training Loss: -5.493710517883301
Reconstruction Loss: -8.577719688415527
Iteration 2381:
Training Loss: -5.513550758361816
Reconstruction Loss: -8.592850685119629
Iteration 2401:
Training Loss: -5.6218085289001465
Reconstruction Loss: -8.606650352478027
Iteration 2421:
Training Loss: -5.597001075744629
Reconstruction Loss: -8.623183250427246
Iteration 2441:
Training Loss: -5.716335773468018
Reconstruction Loss: -8.637125015258789
Iteration 2461:
Training Loss: -5.450282573699951
Reconstruction Loss: -8.64874267578125
Iteration 2481:
Training Loss: -5.592236518859863
Reconstruction Loss: -8.664252281188965
Iteration 2501:
Training Loss: -5.3834710121154785
Reconstruction Loss: -8.681678771972656
Iteration 2521:
Training Loss: -5.935033321380615
Reconstruction Loss: -8.692692756652832
Iteration 2541:
Training Loss: -5.671921730041504
Reconstruction Loss: -8.707173347473145
Iteration 2561:
Training Loss: -5.647968292236328
Reconstruction Loss: -8.722654342651367
Iteration 2581:
Training Loss: -5.697521209716797
Reconstruction Loss: -8.734596252441406
Iteration 2601:
Training Loss: -5.808176040649414
Reconstruction Loss: -8.74902057647705
Iteration 2621:
Training Loss: -5.7520246505737305
Reconstruction Loss: -8.75671672821045
Iteration 2641:
Training Loss: -5.697253227233887
Reconstruction Loss: -8.773619651794434
Iteration 2661:
Training Loss: -5.6476263999938965
Reconstruction Loss: -8.785890579223633
Iteration 2681:
Training Loss: -5.907125949859619
Reconstruction Loss: -8.800312995910645
Iteration 2701:
Training Loss: -5.992370128631592
Reconstruction Loss: -8.810443878173828
Iteration 2721:
Training Loss: -5.588690280914307
Reconstruction Loss: -8.824518203735352
Iteration 2741:
Training Loss: -5.985346794128418
Reconstruction Loss: -8.834141731262207
Iteration 2761:
Training Loss: -5.895601749420166
Reconstruction Loss: -8.849669456481934
Iteration 2781:
Training Loss: -6.0346784591674805
Reconstruction Loss: -8.85805892944336
Iteration 2801:
Training Loss: -5.725739479064941
Reconstruction Loss: -8.873929023742676
Iteration 2821:
Training Loss: -5.808479309082031
Reconstruction Loss: -8.885565757751465
Iteration 2841:
Training Loss: -5.783104419708252
Reconstruction Loss: -8.893047332763672
Iteration 2861:
Training Loss: -5.647349834442139
Reconstruction Loss: -8.904236793518066
Iteration 2881:
Training Loss: -5.773166656494141
Reconstruction Loss: -8.918171882629395
Iteration 2901:
Training Loss: -5.756298542022705
Reconstruction Loss: -8.927928924560547
Iteration 2921:
Training Loss: -5.719912528991699
Reconstruction Loss: -8.940319061279297
Iteration 2941:
Training Loss: -5.917503833770752
Reconstruction Loss: -8.952507972717285
Iteration 2961:
Training Loss: -5.613739490509033
Reconstruction Loss: -8.963398933410645
Iteration 2981:
Training Loss: -6.018568515777588
Reconstruction Loss: -8.974509239196777
