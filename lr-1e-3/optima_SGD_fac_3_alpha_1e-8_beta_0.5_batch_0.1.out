5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.791987419128418
Reconstruction Loss: -0.3980022966861725
Iteration 11:
Training Loss: 5.525209903717041
Reconstruction Loss: -0.3980022966861725
Iteration 21:
Training Loss: 6.049454689025879
Reconstruction Loss: -0.3980022966861725
Iteration 31:
Training Loss: 5.678802967071533
Reconstruction Loss: -0.3980022966861725
Iteration 41:
Training Loss: 5.881492614746094
Reconstruction Loss: -0.3980022966861725
Iteration 51:
Training Loss: 5.756271839141846
Reconstruction Loss: -0.3980022966861725
Iteration 61:
Training Loss: 5.5990495681762695
Reconstruction Loss: -0.3980022966861725
Iteration 71:
Training Loss: 5.733864784240723
Reconstruction Loss: -0.3980022966861725
Iteration 81:
Training Loss: 5.844455242156982
Reconstruction Loss: -0.398002564907074
Iteration 91:
Training Loss: 5.5399556159973145
Reconstruction Loss: -0.398002564907074
Iteration 101:
Training Loss: 5.623164176940918
Reconstruction Loss: -0.398002564907074
Iteration 111:
Training Loss: 5.789522171020508
Reconstruction Loss: -0.398002564907074
Iteration 121:
Training Loss: 6.120604515075684
Reconstruction Loss: -0.398002564907074
Iteration 131:
Training Loss: 5.767826080322266
Reconstruction Loss: -0.398002564907074
Iteration 141:
Training Loss: 5.34393310546875
Reconstruction Loss: -0.398002564907074
Iteration 151:
Training Loss: 5.959493160247803
Reconstruction Loss: -0.39800265431404114
Iteration 161:
Training Loss: 5.735972881317139
Reconstruction Loss: -0.39800265431404114
Iteration 171:
Training Loss: 6.169821262359619
Reconstruction Loss: -0.39800265431404114
Iteration 181:
Training Loss: 5.528439998626709
Reconstruction Loss: -0.39800265431404114
Iteration 191:
Training Loss: 5.369778156280518
Reconstruction Loss: -0.3980027437210083
Iteration 201:
Training Loss: 5.643280982971191
Reconstruction Loss: -0.3980027437210083
Iteration 211:
Training Loss: 5.337625980377197
Reconstruction Loss: -0.3980029225349426
Iteration 221:
Training Loss: 5.406858921051025
Reconstruction Loss: -0.3980029225349426
Iteration 231:
Training Loss: 5.9789886474609375
Reconstruction Loss: -0.3980029225349426
Iteration 241:
Training Loss: 5.72434139251709
Reconstruction Loss: -0.39800310134887695
Iteration 251:
Training Loss: 6.271030902862549
Reconstruction Loss: -0.3980032801628113
Iteration 261:
Training Loss: 5.118587970733643
Reconstruction Loss: -0.39800336956977844
Iteration 271:
Training Loss: 5.592235088348389
Reconstruction Loss: -0.3980034589767456
Iteration 281:
Training Loss: 5.595775604248047
Reconstruction Loss: -0.3980037271976471
Iteration 291:
Training Loss: 5.845865726470947
Reconstruction Loss: -0.3980039060115814
Iteration 301:
Training Loss: 5.531470775604248
Reconstruction Loss: -0.3980042636394501
Iteration 311:
Training Loss: 6.1069440841674805
Reconstruction Loss: -0.3980046212673187
Iteration 321:
Training Loss: 5.899784088134766
Reconstruction Loss: -0.39800506830215454
Iteration 331:
Training Loss: 6.305203914642334
Reconstruction Loss: -0.39800557494163513
Iteration 341:
Training Loss: 5.923740386962891
Reconstruction Loss: -0.3980063796043396
Iteration 351:
Training Loss: 5.592329978942871
Reconstruction Loss: -0.3980073630809784
Iteration 361:
Training Loss: 5.637921333312988
Reconstruction Loss: -0.39800870418548584
Iteration 371:
Training Loss: 5.280673027038574
Reconstruction Loss: -0.3980107307434082
Iteration 381:
Training Loss: 5.47520637512207
Reconstruction Loss: -0.3980136513710022
Iteration 391:
Training Loss: 4.992886066436768
Reconstruction Loss: -0.3980184495449066
Iteration 401:
Training Loss: 5.0882744789123535
Reconstruction Loss: -0.3980264365673065
Iteration 411:
Training Loss: 6.051948547363281
Reconstruction Loss: -0.3980408310890198
Iteration 421:
Training Loss: 5.928485870361328
Reconstruction Loss: -0.3980717062950134
Iteration 431:
Training Loss: 6.071134567260742
Reconstruction Loss: -0.3981517553329468
Iteration 441:
Training Loss: 5.303905010223389
Reconstruction Loss: -0.3984486758708954
Iteration 451:
Training Loss: 5.98881196975708
Reconstruction Loss: -0.40069907903671265
Iteration 461:
Training Loss: 5.141774654388428
Reconstruction Loss: -0.5571719408035278
Iteration 471:
Training Loss: 5.339381217956543
Reconstruction Loss: -0.49459534883499146
Iteration 481:
Training Loss: 5.23153018951416
Reconstruction Loss: -0.48345261812210083
Iteration 491:
Training Loss: 5.215872287750244
Reconstruction Loss: -0.48446398973464966
Iteration 501:
Training Loss: 5.700370788574219
Reconstruction Loss: -0.5153171420097351
Iteration 511:
Training Loss: 4.6076250076293945
Reconstruction Loss: -0.7008631825447083
Iteration 521:
Training Loss: 4.819138526916504
Reconstruction Loss: -0.7000144720077515
Iteration 531:
Training Loss: 4.688375949859619
Reconstruction Loss: -0.7158625721931458
Iteration 541:
Training Loss: 4.7789201736450195
Reconstruction Loss: -0.706085741519928
Iteration 551:
Training Loss: 4.995221138000488
Reconstruction Loss: -0.7089282274246216
Iteration 561:
Training Loss: 4.6927032470703125
Reconstruction Loss: -0.7355020046234131
Iteration 571:
Training Loss: 4.759758949279785
Reconstruction Loss: -0.7064787149429321
Iteration 581:
Training Loss: 4.920811653137207
Reconstruction Loss: -0.7161834239959717
Iteration 591:
Training Loss: 4.244614124298096
Reconstruction Loss: -0.7147039771080017
Iteration 601:
Training Loss: 5.039459705352783
Reconstruction Loss: -0.6922014355659485
Iteration 611:
Training Loss: 5.007073402404785
Reconstruction Loss: -0.7015255093574524
Iteration 621:
Training Loss: 4.758491516113281
Reconstruction Loss: -0.7208863496780396
Iteration 631:
Training Loss: 4.412518501281738
Reconstruction Loss: -0.709060549736023
Iteration 641:
Training Loss: 4.813803195953369
Reconstruction Loss: -0.7205149531364441
Iteration 651:
Training Loss: 5.097924709320068
Reconstruction Loss: -0.6979178786277771
Iteration 661:
Training Loss: 5.033773899078369
Reconstruction Loss: -0.7041588425636292
Iteration 671:
Training Loss: 5.073657035827637
Reconstruction Loss: -0.6822266578674316
Iteration 681:
Training Loss: 4.730875492095947
Reconstruction Loss: -0.7084677219390869
Iteration 691:
Training Loss: 4.810372829437256
Reconstruction Loss: -0.7463440895080566
Iteration 701:
Training Loss: 4.668729782104492
Reconstruction Loss: -0.7968676686286926
Iteration 711:
Training Loss: 3.946934938430786
Reconstruction Loss: -1.0173923969268799
Iteration 721:
Training Loss: 3.6409614086151123
Reconstruction Loss: -0.9663145542144775
Iteration 731:
Training Loss: 4.070791244506836
Reconstruction Loss: -0.9615361094474792
Iteration 741:
Training Loss: 3.921921968460083
Reconstruction Loss: -0.9359012246131897
Iteration 751:
Training Loss: 4.477346897125244
Reconstruction Loss: -0.9293016791343689
Iteration 761:
Training Loss: 3.9581682682037354
Reconstruction Loss: -0.9013276100158691
Iteration 771:
Training Loss: 4.5122833251953125
Reconstruction Loss: -0.9107928276062012
Iteration 781:
Training Loss: 3.7635135650634766
Reconstruction Loss: -0.8892072439193726
Iteration 791:
Training Loss: 4.066452503204346
Reconstruction Loss: -0.9138126373291016
Iteration 801:
Training Loss: 3.935800075531006
Reconstruction Loss: -0.912960410118103
Iteration 811:
Training Loss: 3.4249427318573
Reconstruction Loss: -1.1448745727539062
Iteration 821:
Training Loss: 2.8306498527526855
Reconstruction Loss: -1.3467572927474976
Iteration 831:
Training Loss: 2.326228141784668
Reconstruction Loss: -1.4068301916122437
Iteration 841:
Training Loss: 2.7985939979553223
Reconstruction Loss: -1.448511004447937
Iteration 851:
Training Loss: 2.9457175731658936
Reconstruction Loss: -1.4907387495040894
Iteration 861:
Training Loss: 3.2264456748962402
Reconstruction Loss: -1.5004329681396484
Iteration 871:
Training Loss: 2.846463680267334
Reconstruction Loss: -1.5195066928863525
Iteration 881:
Training Loss: 2.75972580909729
Reconstruction Loss: -1.5339168310165405
Iteration 891:
Training Loss: 3.1982364654541016
Reconstruction Loss: -1.5454410314559937
Iteration 901:
Training Loss: 2.679532051086426
Reconstruction Loss: -1.5375550985336304
Iteration 911:
Training Loss: 2.8120315074920654
Reconstruction Loss: -1.5403468608856201
Iteration 921:
Training Loss: 2.7393264770507812
Reconstruction Loss: -1.5410085916519165
Iteration 931:
Training Loss: 2.857389450073242
Reconstruction Loss: -1.5593774318695068
Iteration 941:
Training Loss: 3.109884738922119
Reconstruction Loss: -1.5551224946975708
Iteration 951:
Training Loss: 2.960320472717285
Reconstruction Loss: -1.53812837600708
Iteration 961:
Training Loss: 3.3837249279022217
Reconstruction Loss: -1.5572311878204346
Iteration 971:
Training Loss: 3.2496283054351807
Reconstruction Loss: -1.5610089302062988
Iteration 981:
Training Loss: 2.7314863204956055
Reconstruction Loss: -1.5575305223464966
Iteration 991:
Training Loss: 3.3044357299804688
Reconstruction Loss: -1.5542807579040527
Iteration 1001:
Training Loss: 2.3672585487365723
Reconstruction Loss: -1.5422502756118774
Iteration 1011:
Training Loss: 2.955852746963501
Reconstruction Loss: -1.5575296878814697
Iteration 1021:
Training Loss: 2.6730475425720215
Reconstruction Loss: -1.5501539707183838
Iteration 1031:
Training Loss: 2.7766408920288086
Reconstruction Loss: -1.5394763946533203
Iteration 1041:
Training Loss: 2.6634397506713867
Reconstruction Loss: -1.5432500839233398
Iteration 1051:
Training Loss: 2.933424711227417
Reconstruction Loss: -1.5533819198608398
Iteration 1061:
Training Loss: 2.81390380859375
Reconstruction Loss: -1.5524903535842896
Iteration 1071:
Training Loss: 2.864882469177246
Reconstruction Loss: -1.5503042936325073
Iteration 1081:
Training Loss: 2.234365224838257
Reconstruction Loss: -1.544960379600525
Iteration 1091:
Training Loss: 3.058340549468994
Reconstruction Loss: -1.5449217557907104
Iteration 1101:
Training Loss: 3.0551745891571045
Reconstruction Loss: -1.5376172065734863
Iteration 1111:
Training Loss: 2.811107635498047
Reconstruction Loss: -1.5484856367111206
Iteration 1121:
Training Loss: 2.887824535369873
Reconstruction Loss: -1.543728232383728
Iteration 1131:
Training Loss: 2.627060651779175
Reconstruction Loss: -1.552566409111023
Iteration 1141:
Training Loss: 2.989513874053955
Reconstruction Loss: -1.5307254791259766
Iteration 1151:
Training Loss: 2.793126106262207
Reconstruction Loss: -1.5347102880477905
Iteration 1161:
Training Loss: 2.8441171646118164
Reconstruction Loss: -1.5432393550872803
Iteration 1171:
Training Loss: 2.752610206604004
Reconstruction Loss: -1.538801908493042
Iteration 1181:
Training Loss: 2.9134469032287598
Reconstruction Loss: -1.5424747467041016
Iteration 1191:
Training Loss: 3.0290980339050293
Reconstruction Loss: -1.548252820968628
Iteration 1201:
Training Loss: 2.9283082485198975
Reconstruction Loss: -1.541875958442688
Iteration 1211:
Training Loss: 2.89418625831604
Reconstruction Loss: -1.5313327312469482
Iteration 1221:
Training Loss: 2.8903567790985107
Reconstruction Loss: -1.5275919437408447
Iteration 1231:
Training Loss: 2.7728428840637207
Reconstruction Loss: -1.5364959239959717
Iteration 1241:
Training Loss: 2.918607234954834
Reconstruction Loss: -1.5396642684936523
Iteration 1251:
Training Loss: 2.500765085220337
Reconstruction Loss: -1.538773775100708
Iteration 1261:
Training Loss: 2.6711971759796143
Reconstruction Loss: -1.5376263856887817
Iteration 1271:
Training Loss: 2.711609125137329
Reconstruction Loss: -1.5395724773406982
Iteration 1281:
Training Loss: 2.832895040512085
Reconstruction Loss: -1.5488789081573486
Iteration 1291:
Training Loss: 3.0410592555999756
Reconstruction Loss: -1.5440819263458252
Iteration 1301:
Training Loss: 2.830745220184326
Reconstruction Loss: -1.5452880859375
Iteration 1311:
Training Loss: 3.028526782989502
Reconstruction Loss: -1.5312892198562622
Iteration 1321:
Training Loss: 2.6197667121887207
Reconstruction Loss: -1.5375349521636963
Iteration 1331:
Training Loss: 2.8620033264160156
Reconstruction Loss: -1.5360757112503052
Iteration 1341:
Training Loss: 2.8752503395080566
Reconstruction Loss: -1.5374751091003418
Iteration 1351:
Training Loss: 2.805755138397217
Reconstruction Loss: -1.5331162214279175
Iteration 1361:
Training Loss: 2.784479856491089
Reconstruction Loss: -1.5404435396194458
Iteration 1371:
Training Loss: 2.8011951446533203
Reconstruction Loss: -1.5399749279022217
Iteration 1381:
Training Loss: 2.9654452800750732
Reconstruction Loss: -1.5371536016464233
Iteration 1391:
Training Loss: 3.095360040664673
Reconstruction Loss: -1.5360291004180908
Iteration 1401:
Training Loss: 2.5879623889923096
Reconstruction Loss: -1.5306322574615479
Iteration 1411:
Training Loss: 2.680575370788574
Reconstruction Loss: -1.535739541053772
Iteration 1421:
Training Loss: 2.5070013999938965
Reconstruction Loss: -1.5332239866256714
Iteration 1431:
Training Loss: 2.778345823287964
Reconstruction Loss: -1.5387649536132812
Iteration 1441:
Training Loss: 3.273024797439575
Reconstruction Loss: -1.5359981060028076
Iteration 1451:
Training Loss: 2.8653295040130615
Reconstruction Loss: -1.5351122617721558
Iteration 1461:
Training Loss: 2.8809425830841064
Reconstruction Loss: -1.5473781824111938
Iteration 1471:
Training Loss: 3.1309940814971924
Reconstruction Loss: -1.5605361461639404
Iteration 1481:
Training Loss: 2.8058667182922363
Reconstruction Loss: -1.5429869890213013
Iteration 1491:
Training Loss: 2.8347902297973633
Reconstruction Loss: -1.541738748550415
