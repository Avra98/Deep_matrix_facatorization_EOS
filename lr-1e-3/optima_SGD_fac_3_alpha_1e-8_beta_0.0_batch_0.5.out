5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.471195697784424
Reconstruction Loss: -0.47834858298301697
Iteration 51:
Training Loss: 5.667834758758545
Reconstruction Loss: -0.47834858298301697
Iteration 101:
Training Loss: 5.408456802368164
Reconstruction Loss: -0.47834858298301697
Iteration 151:
Training Loss: 5.467549800872803
Reconstruction Loss: -0.47834867238998413
Iteration 201:
Training Loss: 5.4559221267700195
Reconstruction Loss: -0.47834867238998413
Iteration 251:
Training Loss: 5.556694030761719
Reconstruction Loss: -0.47834867238998413
Iteration 301:
Training Loss: 5.410872459411621
Reconstruction Loss: -0.47834867238998413
Iteration 351:
Training Loss: 5.500605583190918
Reconstruction Loss: -0.47834867238998413
Iteration 401:
Training Loss: 5.479515552520752
Reconstruction Loss: -0.47834867238998413
Iteration 451:
Training Loss: 5.597151756286621
Reconstruction Loss: -0.47834867238998413
Iteration 501:
Training Loss: 5.506784915924072
Reconstruction Loss: -0.47834867238998413
Iteration 551:
Training Loss: 5.387324333190918
Reconstruction Loss: -0.47834867238998413
Iteration 601:
Training Loss: 5.406579494476318
Reconstruction Loss: -0.4783487617969513
Iteration 651:
Training Loss: 5.533882141113281
Reconstruction Loss: -0.4783487617969513
Iteration 701:
Training Loss: 5.62227725982666
Reconstruction Loss: -0.4783487617969513
Iteration 751:
Training Loss: 5.434219837188721
Reconstruction Loss: -0.4783487617969513
Iteration 801:
Training Loss: 5.513648509979248
Reconstruction Loss: -0.4783487617969513
Iteration 851:
Training Loss: 5.523500442504883
Reconstruction Loss: -0.47834885120391846
Iteration 901:
Training Loss: 5.478205680847168
Reconstruction Loss: -0.47834885120391846
Iteration 951:
Training Loss: 5.5974931716918945
Reconstruction Loss: -0.47834885120391846
Iteration 1001:
Training Loss: 5.441699504852295
Reconstruction Loss: -0.47834885120391846
Iteration 1051:
Training Loss: 5.439751625061035
Reconstruction Loss: -0.47834905982017517
Iteration 1101:
Training Loss: 5.527142524719238
Reconstruction Loss: -0.47834914922714233
Iteration 1151:
Training Loss: 5.319156646728516
Reconstruction Loss: -0.47834914922714233
Iteration 1201:
Training Loss: 5.543972969055176
Reconstruction Loss: -0.4783492684364319
Iteration 1251:
Training Loss: 5.552246570587158
Reconstruction Loss: -0.4783492684364319
Iteration 1301:
Training Loss: 5.451562881469727
Reconstruction Loss: -0.47834932804107666
Iteration 1351:
Training Loss: 5.48608922958374
Reconstruction Loss: -0.4783494472503662
Iteration 1401:
Training Loss: 5.472166061401367
Reconstruction Loss: -0.4783495366573334
Iteration 1451:
Training Loss: 5.511422157287598
Reconstruction Loss: -0.4783497452735901
Iteration 1501:
Training Loss: 5.543306827545166
Reconstruction Loss: -0.4783499240875244
Iteration 1551:
Training Loss: 5.623496055603027
Reconstruction Loss: -0.47835010290145874
Iteration 1601:
Training Loss: 5.431959629058838
Reconstruction Loss: -0.4783504009246826
Iteration 1651:
Training Loss: 5.4723992347717285
Reconstruction Loss: -0.47835057973861694
Iteration 1701:
Training Loss: 5.399556636810303
Reconstruction Loss: -0.47835099697113037
Iteration 1751:
Training Loss: 5.64680814743042
Reconstruction Loss: -0.4783516526222229
Iteration 1801:
Training Loss: 5.467598915100098
Reconstruction Loss: -0.47835230827331543
Iteration 1851:
Training Loss: 5.480743885040283
Reconstruction Loss: -0.4783532917499542
Iteration 1901:
Training Loss: 5.452367305755615
Reconstruction Loss: -0.4783545434474945
Iteration 1951:
Training Loss: 5.529339790344238
Reconstruction Loss: -0.47835636138916016
Iteration 2001:
Training Loss: 5.514444828033447
Reconstruction Loss: -0.4783594608306885
Iteration 2051:
Training Loss: 5.489216327667236
Reconstruction Loss: -0.4783637523651123
Iteration 2101:
Training Loss: 5.639915943145752
Reconstruction Loss: -0.47837117314338684
Iteration 2151:
Training Loss: 5.358619213104248
Reconstruction Loss: -0.4783846437931061
Iteration 2201:
Training Loss: 5.462177753448486
Reconstruction Loss: -0.478411465883255
Iteration 2251:
Training Loss: 5.473773956298828
Reconstruction Loss: -0.4784742593765259
Iteration 2301:
Training Loss: 5.6360764503479
Reconstruction Loss: -0.47866010665893555
Iteration 2351:
Training Loss: 5.626865863800049
Reconstruction Loss: -0.47951748967170715
Iteration 2401:
Training Loss: 5.342906475067139
Reconstruction Loss: -0.49146032333374023
Iteration 2451:
Training Loss: 5.137628078460693
Reconstruction Loss: -0.5930962562561035
Iteration 2501:
Training Loss: 4.898889541625977
Reconstruction Loss: -0.5423909425735474
Iteration 2551:
Training Loss: 4.974210739135742
Reconstruction Loss: -0.5298687219619751
Iteration 2601:
Training Loss: 4.885141372680664
Reconstruction Loss: -0.5235151052474976
Iteration 2651:
Training Loss: 5.012866020202637
Reconstruction Loss: -0.5249485969543457
Iteration 2701:
Training Loss: 5.042962551116943
Reconstruction Loss: -0.526055634021759
Iteration 2751:
Training Loss: 4.796474933624268
Reconstruction Loss: -0.5217946767807007
Iteration 2801:
Training Loss: 4.9349751472473145
Reconstruction Loss: -0.5206642150878906
Iteration 2851:
Training Loss: 4.965691566467285
Reconstruction Loss: -0.5238806009292603
Iteration 2901:
Training Loss: 4.873624324798584
Reconstruction Loss: -0.5272471308708191
Iteration 2951:
Training Loss: 4.9926958084106445
Reconstruction Loss: -0.5293861031532288
Iteration 3001:
Training Loss: 4.977957725524902
Reconstruction Loss: -0.5252867341041565
Iteration 3051:
Training Loss: 4.959149360656738
Reconstruction Loss: -0.5244673490524292
Iteration 3101:
Training Loss: 4.845963478088379
Reconstruction Loss: -0.5228206515312195
Iteration 3151:
Training Loss: 4.919750690460205
Reconstruction Loss: -0.5228874087333679
Iteration 3201:
Training Loss: 4.943039894104004
Reconstruction Loss: -0.5267999768257141
Iteration 3251:
Training Loss: 4.965286731719971
Reconstruction Loss: -0.5234183073043823
Iteration 3301:
Training Loss: 4.8834147453308105
Reconstruction Loss: -0.5254092812538147
Iteration 3351:
Training Loss: 4.915794849395752
Reconstruction Loss: -0.5262090563774109
Iteration 3401:
Training Loss: 4.9699320793151855
Reconstruction Loss: -0.5198290348052979
Iteration 3451:
Training Loss: 4.996084213256836
Reconstruction Loss: -0.5263324975967407
Iteration 3501:
Training Loss: 4.866113185882568
Reconstruction Loss: -0.5334096550941467
Iteration 3551:
Training Loss: 4.710036754608154
Reconstruction Loss: -0.6335965394973755
Iteration 3601:
Training Loss: 4.562580108642578
Reconstruction Loss: -0.614052414894104
Iteration 3651:
Training Loss: 4.534663200378418
Reconstruction Loss: -0.5761781930923462
Iteration 3701:
Training Loss: 4.547704219818115
Reconstruction Loss: -0.5553159117698669
Iteration 3751:
Training Loss: 4.413659572601318
Reconstruction Loss: -0.5178178548812866
Iteration 3801:
Training Loss: 4.653923511505127
Reconstruction Loss: -0.5049890875816345
Iteration 3851:
Training Loss: 4.575651168823242
Reconstruction Loss: -0.5082747936248779
Iteration 3901:
Training Loss: 4.507162570953369
Reconstruction Loss: -0.5117044448852539
Iteration 3951:
Training Loss: 4.581735610961914
Reconstruction Loss: -0.5174379944801331
Iteration 4001:
Training Loss: 4.640555381774902
Reconstruction Loss: -0.5202080011367798
Iteration 4051:
Training Loss: 4.431421756744385
Reconstruction Loss: -0.5339053869247437
Iteration 4101:
Training Loss: 4.436743259429932
Reconstruction Loss: -0.5420033931732178
Iteration 4151:
Training Loss: 4.528951168060303
Reconstruction Loss: -0.5503747463226318
Iteration 4201:
Training Loss: 4.478957653045654
Reconstruction Loss: -0.5708823800086975
Iteration 4251:
Training Loss: 4.574179649353027
Reconstruction Loss: -0.5915641784667969
Iteration 4301:
Training Loss: 4.523024082183838
Reconstruction Loss: -0.6142488718032837
Iteration 4351:
Training Loss: 4.510284900665283
Reconstruction Loss: -0.6537549495697021
Iteration 4401:
Training Loss: 4.4744791984558105
Reconstruction Loss: -0.7168872356414795
Iteration 4451:
Training Loss: 4.177254676818848
Reconstruction Loss: -0.8775229454040527
Iteration 4501:
Training Loss: 3.7184250354766846
Reconstruction Loss: -1.154848575592041
Iteration 4551:
Training Loss: 3.485804557800293
Reconstruction Loss: -1.2520573139190674
Iteration 4601:
Training Loss: 3.3266329765319824
Reconstruction Loss: -1.2803606986999512
Iteration 4651:
Training Loss: 3.3969194889068604
Reconstruction Loss: -1.2747082710266113
Iteration 4701:
Training Loss: 3.4117050170898438
Reconstruction Loss: -1.2739572525024414
Iteration 4751:
Training Loss: 3.5405406951904297
Reconstruction Loss: -1.2681556940078735
Iteration 4801:
Training Loss: 3.3461315631866455
Reconstruction Loss: -1.2650563716888428
Iteration 4851:
Training Loss: 3.3285791873931885
Reconstruction Loss: -1.2563753128051758
Iteration 4901:
Training Loss: 3.310778856277466
Reconstruction Loss: -1.2499439716339111
Iteration 4951:
Training Loss: 3.3634819984436035
Reconstruction Loss: -1.2445907592773438
Iteration 5001:
Training Loss: 3.350264549255371
Reconstruction Loss: -1.2407927513122559
Iteration 5051:
Training Loss: 3.53637957572937
Reconstruction Loss: -1.2430027723312378
Iteration 5101:
Training Loss: 3.6555440425872803
Reconstruction Loss: -1.242024302482605
Iteration 5151:
Training Loss: 3.4486827850341797
Reconstruction Loss: -1.2401363849639893
Iteration 5201:
Training Loss: 3.459113836288452
Reconstruction Loss: -1.2384570837020874
Iteration 5251:
Training Loss: 3.486135959625244
Reconstruction Loss: -1.2394821643829346
Iteration 5301:
Training Loss: 3.5704574584960938
Reconstruction Loss: -1.236865758895874
Iteration 5351:
Training Loss: 3.462026834487915
Reconstruction Loss: -1.2411975860595703
Iteration 5401:
Training Loss: 3.463916778564453
Reconstruction Loss: -1.238415002822876
Iteration 5451:
Training Loss: 3.321126699447632
Reconstruction Loss: -1.236334204673767
Iteration 5501:
Training Loss: 3.4760682582855225
Reconstruction Loss: -1.2382903099060059
Iteration 5551:
Training Loss: 3.428546190261841
Reconstruction Loss: -1.2353676557540894
Iteration 5601:
Training Loss: 3.570446252822876
Reconstruction Loss: -1.2432581186294556
Iteration 5651:
Training Loss: 3.539857864379883
Reconstruction Loss: -1.2385153770446777
Iteration 5701:
Training Loss: 3.413681745529175
Reconstruction Loss: -1.2341214418411255
Iteration 5751:
Training Loss: 3.449904680252075
Reconstruction Loss: -1.2371983528137207
Iteration 5801:
Training Loss: 3.3542306423187256
Reconstruction Loss: -1.2390241622924805
Iteration 5851:
Training Loss: 3.4848194122314453
Reconstruction Loss: -1.2406930923461914
Iteration 5901:
Training Loss: 3.525153875350952
Reconstruction Loss: -1.2419066429138184
Iteration 5951:
Training Loss: 3.43434476852417
Reconstruction Loss: -1.2499116659164429
Iteration 6001:
Training Loss: 3.2494056224823
Reconstruction Loss: -1.2935187816619873
Iteration 6051:
Training Loss: 3.091463565826416
Reconstruction Loss: -1.4594606161117554
Iteration 6101:
Training Loss: 2.9616503715515137
Reconstruction Loss: -1.6727168560028076
Iteration 6151:
Training Loss: 2.8483126163482666
Reconstruction Loss: -1.7729790210723877
Iteration 6201:
Training Loss: 2.6762990951538086
Reconstruction Loss: -1.816221833229065
Iteration 6251:
Training Loss: 2.701242208480835
Reconstruction Loss: -1.8368929624557495
Iteration 6301:
Training Loss: 2.738057851791382
Reconstruction Loss: -1.8429151773452759
Iteration 6351:
Training Loss: 2.689338207244873
Reconstruction Loss: -1.8460044860839844
Iteration 6401:
Training Loss: 2.8398287296295166
Reconstruction Loss: -1.8485668897628784
Iteration 6451:
Training Loss: 2.774665355682373
Reconstruction Loss: -1.8432923555374146
Iteration 6501:
Training Loss: 2.5126705169677734
Reconstruction Loss: -1.8464434146881104
Iteration 6551:
Training Loss: 2.7315800189971924
Reconstruction Loss: -1.8487435579299927
Iteration 6601:
Training Loss: 2.616717576980591
Reconstruction Loss: -1.8468048572540283
Iteration 6651:
Training Loss: 2.703535795211792
Reconstruction Loss: -1.844531536102295
Iteration 6701:
Training Loss: 2.568552017211914
Reconstruction Loss: -1.845538854598999
Iteration 6751:
Training Loss: 2.6127612590789795
Reconstruction Loss: -1.8458056449890137
Iteration 6801:
Training Loss: 2.8068530559539795
Reconstruction Loss: -1.85086989402771
Iteration 6851:
Training Loss: 2.734523057937622
Reconstruction Loss: -1.844462513923645
Iteration 6901:
Training Loss: 2.6683435440063477
Reconstruction Loss: -1.848441481590271
Iteration 6951:
Training Loss: 2.7018327713012695
Reconstruction Loss: -1.847188949584961
Iteration 7001:
Training Loss: 2.6612632274627686
Reconstruction Loss: -1.8497445583343506
Iteration 7051:
Training Loss: 2.662674903869629
Reconstruction Loss: -1.849865436553955
Iteration 7101:
Training Loss: 2.6430931091308594
Reconstruction Loss: -1.849215030670166
Iteration 7151:
Training Loss: 2.676779270172119
Reconstruction Loss: -1.8489208221435547
Iteration 7201:
Training Loss: 2.6867311000823975
Reconstruction Loss: -1.8473248481750488
Iteration 7251:
Training Loss: 2.6492300033569336
Reconstruction Loss: -1.8483033180236816
Iteration 7301:
Training Loss: 2.5549466609954834
Reconstruction Loss: -1.849034070968628
Iteration 7351:
Training Loss: 2.787050247192383
Reconstruction Loss: -1.845420479774475
Iteration 7401:
Training Loss: 2.521272659301758
Reconstruction Loss: -1.8460707664489746
Iteration 7451:
Training Loss: 2.727938413619995
Reconstruction Loss: -1.84769868850708
