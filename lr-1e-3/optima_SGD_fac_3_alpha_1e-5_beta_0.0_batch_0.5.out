5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.552469730377197
Reconstruction Loss: -0.4973739981651306
Iteration 51:
Training Loss: 5.507164478302002
Reconstruction Loss: -0.4974994659423828
Iteration 101:
Training Loss: 5.5115580558776855
Reconstruction Loss: -0.49772071838378906
Iteration 151:
Training Loss: 5.599057197570801
Reconstruction Loss: -0.4983368515968323
Iteration 201:
Training Loss: 5.5153656005859375
Reconstruction Loss: -0.5015746355056763
Iteration 251:
Training Loss: 5.290199279785156
Reconstruction Loss: -0.6017922163009644
Iteration 301:
Training Loss: 4.575761795043945
Reconstruction Loss: -0.8404925465583801
Iteration 351:
Training Loss: 4.290393352508545
Reconstruction Loss: -0.9011789560317993
Iteration 401:
Training Loss: 3.8871374130249023
Reconstruction Loss: -1.1487950086593628
Iteration 451:
Training Loss: 3.6449012756347656
Reconstruction Loss: -1.2563034296035767
Iteration 501:
Training Loss: 3.0858118534088135
Reconstruction Loss: -1.5945230722427368
Iteration 551:
Training Loss: 2.9140124320983887
Reconstruction Loss: -1.768669605255127
Iteration 601:
Training Loss: 2.7200515270233154
Reconstruction Loss: -1.828614354133606
Iteration 651:
Training Loss: 2.6992909908294678
Reconstruction Loss: -1.914981722831726
Iteration 701:
Training Loss: 2.311701774597168
Reconstruction Loss: -2.2363381385803223
Iteration 751:
Training Loss: 1.2681872844696045
Reconstruction Loss: -2.9060773849487305
Iteration 801:
Training Loss: 0.30693376064300537
Reconstruction Loss: -3.5778658390045166
Iteration 851:
Training Loss: -0.49855685234069824
Reconstruction Loss: -4.1844072341918945
Iteration 901:
Training Loss: -1.15084969997406
Reconstruction Loss: -4.73505973815918
Iteration 951:
Training Loss: -1.6562038660049438
Reconstruction Loss: -5.239867687225342
Iteration 1001:
Training Loss: -2.1971726417541504
Reconstruction Loss: -5.706938743591309
Iteration 1051:
Training Loss: -2.880838632583618
Reconstruction Loss: -6.1393609046936035
Iteration 1101:
Training Loss: -3.336198329925537
Reconstruction Loss: -6.546566486358643
Iteration 1151:
Training Loss: -3.659849166870117
Reconstruction Loss: -6.932040214538574
Iteration 1201:
Training Loss: -3.959367513656616
Reconstruction Loss: -7.301433563232422
Iteration 1251:
Training Loss: -4.456093788146973
Reconstruction Loss: -7.659484386444092
Iteration 1301:
Training Loss: -4.9571757316589355
Reconstruction Loss: -8.005406379699707
Iteration 1351:
Training Loss: -5.31770658493042
Reconstruction Loss: -8.34362506866455
Iteration 1401:
Training Loss: -5.582435607910156
Reconstruction Loss: -8.67284870147705
Iteration 1451:
Training Loss: -5.903681755065918
Reconstruction Loss: -8.993192672729492
Iteration 1501:
Training Loss: -6.096069812774658
Reconstruction Loss: -9.306038856506348
Iteration 1551:
Training Loss: -6.4881062507629395
Reconstruction Loss: -9.610488891601562
Iteration 1601:
Training Loss: -6.67049503326416
Reconstruction Loss: -9.903986930847168
Iteration 1651:
Training Loss: -7.2165117263793945
Reconstruction Loss: -10.186347961425781
Iteration 1701:
Training Loss: -7.333796501159668
Reconstruction Loss: -10.45497989654541
Iteration 1751:
Training Loss: -7.391791820526123
Reconstruction Loss: -10.707486152648926
Iteration 1801:
Training Loss: -7.659807205200195
Reconstruction Loss: -10.94379997253418
Iteration 1851:
Training Loss: -7.930476188659668
Reconstruction Loss: -11.161231994628906
Iteration 1901:
Training Loss: -7.9203033447265625
Reconstruction Loss: -11.358328819274902
Iteration 1951:
Training Loss: -8.173629760742188
Reconstruction Loss: -11.535036087036133
Iteration 2001:
Training Loss: -8.208394050598145
Reconstruction Loss: -11.690820693969727
Iteration 2051:
Training Loss: -8.250603675842285
Reconstruction Loss: -11.825828552246094
Iteration 2101:
Training Loss: -8.263106346130371
Reconstruction Loss: -11.938800811767578
Iteration 2151:
Training Loss: -8.285192489624023
Reconstruction Loss: -12.03795337677002
Iteration 2201:
Training Loss: -8.422554969787598
Reconstruction Loss: -12.122274398803711
Iteration 2251:
Training Loss: -8.530183792114258
Reconstruction Loss: -12.191378593444824
Iteration 2301:
Training Loss: -8.376779556274414
Reconstruction Loss: -12.248234748840332
Iteration 2351:
Training Loss: -8.529206275939941
Reconstruction Loss: -12.295997619628906
Iteration 2401:
Training Loss: -8.47255802154541
Reconstruction Loss: -12.334359169006348
Iteration 2451:
Training Loss: -8.573262214660645
Reconstruction Loss: -12.36925220489502
Iteration 2501:
Training Loss: -8.437519073486328
Reconstruction Loss: -12.398890495300293
Iteration 2551:
Training Loss: -8.522274017333984
Reconstruction Loss: -12.4210844039917
Iteration 2601:
Training Loss: -8.460090637207031
Reconstruction Loss: -12.43825626373291
Iteration 2651:
Training Loss: -8.58260440826416
Reconstruction Loss: -12.456597328186035
Iteration 2701:
Training Loss: -8.498671531677246
Reconstruction Loss: -12.471765518188477
Iteration 2751:
Training Loss: -8.421767234802246
Reconstruction Loss: -12.482995986938477
Iteration 2801:
Training Loss: -8.515443801879883
Reconstruction Loss: -12.495429992675781
Iteration 2851:
Training Loss: -8.517413139343262
Reconstruction Loss: -12.506518363952637
Iteration 2901:
Training Loss: -8.472921371459961
Reconstruction Loss: -12.51157283782959
Iteration 2951:
Training Loss: -8.495609283447266
Reconstruction Loss: -12.518488883972168
Iteration 3001:
Training Loss: -8.53137493133545
Reconstruction Loss: -12.52730941772461
Iteration 3051:
Training Loss: -8.479296684265137
Reconstruction Loss: -12.531025886535645
Iteration 3101:
Training Loss: -8.58871841430664
Reconstruction Loss: -12.539647102355957
Iteration 3151:
Training Loss: -8.628815650939941
Reconstruction Loss: -12.544889450073242
Iteration 3201:
Training Loss: -8.561041831970215
Reconstruction Loss: -12.547161102294922
Iteration 3251:
Training Loss: -8.524994850158691
Reconstruction Loss: -12.55239486694336
Iteration 3301:
Training Loss: -8.560317039489746
Reconstruction Loss: -12.55610179901123
Iteration 3351:
Training Loss: -8.597752571105957
Reconstruction Loss: -12.559638977050781
Iteration 3401:
Training Loss: -8.455774307250977
Reconstruction Loss: -12.565624237060547
Iteration 3451:
Training Loss: -8.468904495239258
Reconstruction Loss: -12.569393157958984
Iteration 3501:
Training Loss: -8.675593376159668
Reconstruction Loss: -12.57260799407959
Iteration 3551:
Training Loss: -8.490730285644531
Reconstruction Loss: -12.574416160583496
Iteration 3601:
Training Loss: -8.497037887573242
Reconstruction Loss: -12.578378677368164
Iteration 3651:
Training Loss: -8.621912002563477
Reconstruction Loss: -12.580931663513184
Iteration 3701:
Training Loss: -8.579123497009277
Reconstruction Loss: -12.586529731750488
Iteration 3751:
Training Loss: -8.529748916625977
Reconstruction Loss: -12.585066795349121
Iteration 3801:
Training Loss: -8.713738441467285
Reconstruction Loss: -12.590489387512207
Iteration 3851:
Training Loss: -8.592682838439941
Reconstruction Loss: -12.591792106628418
Iteration 3901:
Training Loss: -8.558588027954102
Reconstruction Loss: -12.595247268676758
Iteration 3951:
Training Loss: -8.531150817871094
Reconstruction Loss: -12.599053382873535
Iteration 4001:
Training Loss: -8.570104598999023
Reconstruction Loss: -12.600662231445312
Iteration 4051:
Training Loss: -8.625097274780273
Reconstruction Loss: -12.604425430297852
Iteration 4101:
Training Loss: -8.546125411987305
Reconstruction Loss: -12.608179092407227
Iteration 4151:
Training Loss: -8.652589797973633
Reconstruction Loss: -12.609195709228516
Iteration 4201:
Training Loss: -8.595478057861328
Reconstruction Loss: -12.612613677978516
Iteration 4251:
Training Loss: -8.516664505004883
Reconstruction Loss: -12.61515998840332
Iteration 4301:
Training Loss: -8.713827133178711
Reconstruction Loss: -12.618169784545898
Iteration 4351:
Training Loss: -8.635221481323242
Reconstruction Loss: -12.621748924255371
Iteration 4401:
Training Loss: -8.646751403808594
Reconstruction Loss: -12.623294830322266
Iteration 4451:
Training Loss: -8.600360870361328
Reconstruction Loss: -12.624135971069336
Iteration 4501:
Training Loss: -8.490214347839355
Reconstruction Loss: -12.627553939819336
Iteration 4551:
Training Loss: -8.609926223754883
Reconstruction Loss: -12.629677772521973
Iteration 4601:
Training Loss: -8.58405590057373
Reconstruction Loss: -12.632364273071289
Iteration 4651:
Training Loss: -8.734368324279785
Reconstruction Loss: -12.633831977844238
Iteration 4701:
Training Loss: -8.770352363586426
Reconstruction Loss: -12.63674545288086
Iteration 4751:
Training Loss: -8.693001747131348
Reconstruction Loss: -12.639266967773438
Iteration 4801:
Training Loss: -8.649609565734863
Reconstruction Loss: -12.644383430480957
Iteration 4851:
Training Loss: -8.542890548706055
Reconstruction Loss: -12.6489839553833
Iteration 4901:
Training Loss: -8.68488597869873
Reconstruction Loss: -12.64851188659668
Iteration 4951:
Training Loss: -8.647278785705566
Reconstruction Loss: -12.651901245117188
Iteration 5001:
Training Loss: -8.499176025390625
Reconstruction Loss: -12.655345916748047
Iteration 5051:
Training Loss: -8.653824806213379
Reconstruction Loss: -12.657060623168945
Iteration 5101:
Training Loss: -8.611578941345215
Reconstruction Loss: -12.661932945251465
Iteration 5151:
Training Loss: -8.69501781463623
Reconstruction Loss: -12.663281440734863
Iteration 5201:
Training Loss: -8.627463340759277
Reconstruction Loss: -12.667032241821289
Iteration 5251:
Training Loss: -8.66303825378418
Reconstruction Loss: -12.666053771972656
Iteration 5301:
Training Loss: -8.494444847106934
Reconstruction Loss: -12.670987129211426
Iteration 5351:
Training Loss: -8.625164985656738
Reconstruction Loss: -12.67136287689209
Iteration 5401:
Training Loss: -8.63134765625
Reconstruction Loss: -12.676877975463867
Iteration 5451:
Training Loss: -8.7401704788208
Reconstruction Loss: -12.676248550415039
Iteration 5501:
Training Loss: -8.72336483001709
Reconstruction Loss: -12.680460929870605
Iteration 5551:
Training Loss: -8.737627983093262
Reconstruction Loss: -12.682903289794922
Iteration 5601:
Training Loss: -8.608736991882324
Reconstruction Loss: -12.68533992767334
Iteration 5651:
Training Loss: -8.685315132141113
Reconstruction Loss: -12.688109397888184
Iteration 5701:
Training Loss: -8.723967552185059
Reconstruction Loss: -12.688450813293457
Iteration 5751:
Training Loss: -8.6074800491333
Reconstruction Loss: -12.693828582763672
Iteration 5801:
Training Loss: -8.744632720947266
Reconstruction Loss: -12.696693420410156
Iteration 5851:
Training Loss: -8.736284255981445
Reconstruction Loss: -12.698512077331543
Iteration 5901:
Training Loss: -8.74565601348877
Reconstruction Loss: -12.699958801269531
Iteration 5951:
Training Loss: -8.73593807220459
Reconstruction Loss: -12.7027587890625
Iteration 6001:
Training Loss: -8.64902114868164
Reconstruction Loss: -12.705904960632324
Iteration 6051:
Training Loss: -8.745672225952148
Reconstruction Loss: -12.706324577331543
Iteration 6101:
Training Loss: -8.77291488647461
Reconstruction Loss: -12.712052345275879
Iteration 6151:
Training Loss: -8.64021110534668
Reconstruction Loss: -12.711539268493652
Iteration 6201:
Training Loss: -8.700949668884277
Reconstruction Loss: -12.71606731414795
Iteration 6251:
Training Loss: -8.58838939666748
Reconstruction Loss: -12.71663761138916
Iteration 6301:
Training Loss: -8.591808319091797
Reconstruction Loss: -12.720909118652344
Iteration 6351:
Training Loss: -8.871920585632324
Reconstruction Loss: -12.721322059631348
Iteration 6401:
Training Loss: -8.673407554626465
Reconstruction Loss: -12.726101875305176
Iteration 6451:
Training Loss: -8.693078994750977
Reconstruction Loss: -12.725103378295898
Iteration 6501:
Training Loss: -8.727433204650879
Reconstruction Loss: -12.729351043701172
Iteration 6551:
Training Loss: -8.76973819732666
Reconstruction Loss: -12.731916427612305
Iteration 6601:
Training Loss: -8.754643440246582
Reconstruction Loss: -12.733067512512207
Iteration 6651:
Training Loss: -8.799434661865234
Reconstruction Loss: -12.737832069396973
Iteration 6701:
Training Loss: -8.68520450592041
Reconstruction Loss: -12.73717975616455
Iteration 6751:
Training Loss: -8.75466537475586
Reconstruction Loss: -12.741406440734863
Iteration 6801:
Training Loss: -8.702808380126953
Reconstruction Loss: -12.744741439819336
Iteration 6851:
Training Loss: -8.739982604980469
Reconstruction Loss: -12.747025489807129
Iteration 6901:
Training Loss: -8.676107406616211
Reconstruction Loss: -12.74822998046875
Iteration 6951:
Training Loss: -8.81846809387207
Reconstruction Loss: -12.751782417297363
Iteration 7001:
Training Loss: -8.850866317749023
Reconstruction Loss: -12.753327369689941
Iteration 7051:
Training Loss: -8.71558952331543
Reconstruction Loss: -12.75406551361084
Iteration 7101:
Training Loss: -8.732118606567383
Reconstruction Loss: -12.756034851074219
Iteration 7151:
Training Loss: -8.842851638793945
Reconstruction Loss: -12.762724876403809
Iteration 7201:
Training Loss: -8.853288650512695
Reconstruction Loss: -12.763448715209961
Iteration 7251:
Training Loss: -8.698816299438477
Reconstruction Loss: -12.76672649383545
Iteration 7301:
Training Loss: -8.675639152526855
Reconstruction Loss: -12.76558780670166
Iteration 7351:
Training Loss: -8.702847480773926
Reconstruction Loss: -12.768003463745117
Iteration 7401:
Training Loss: -8.763739585876465
Reconstruction Loss: -12.772886276245117
Iteration 7451:
Training Loss: -8.649333000183105
Reconstruction Loss: -12.773091316223145
