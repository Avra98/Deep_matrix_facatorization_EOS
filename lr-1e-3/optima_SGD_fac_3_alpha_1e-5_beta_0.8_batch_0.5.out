5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.5756049156188965
Reconstruction Loss: -0.4368225932121277
Iteration 51:
Training Loss: 5.495225429534912
Reconstruction Loss: -0.4369479715824127
Iteration 101:
Training Loss: 5.6880574226379395
Reconstruction Loss: -0.43717682361602783
Iteration 151:
Training Loss: 5.587851047515869
Reconstruction Loss: -0.43783995509147644
Iteration 201:
Training Loss: 5.619304656982422
Reconstruction Loss: -0.4412841200828552
Iteration 251:
Training Loss: 5.420913219451904
Reconstruction Loss: -0.5380983948707581
Iteration 301:
Training Loss: 4.710874557495117
Reconstruction Loss: -0.8796721696853638
Iteration 351:
Training Loss: 4.3407135009765625
Reconstruction Loss: -0.9089130163192749
Iteration 401:
Training Loss: 4.101291656494141
Reconstruction Loss: -1.0771231651306152
Iteration 451:
Training Loss: 3.9612743854522705
Reconstruction Loss: -1.136300802230835
Iteration 501:
Training Loss: 3.3758761882781982
Reconstruction Loss: -1.5052310228347778
Iteration 551:
Training Loss: 2.0268146991729736
Reconstruction Loss: -2.3187272548675537
Iteration 601:
Training Loss: 1.0852166414260864
Reconstruction Loss: -3.099574089050293
Iteration 651:
Training Loss: 0.10655418783426285
Reconstruction Loss: -3.781001329421997
Iteration 701:
Training Loss: -0.5917535424232483
Reconstruction Loss: -4.412681579589844
Iteration 751:
Training Loss: -1.221190333366394
Reconstruction Loss: -5.001124858856201
Iteration 801:
Training Loss: -2.089445114135742
Reconstruction Loss: -5.553125858306885
Iteration 851:
Training Loss: -2.4757614135742188
Reconstruction Loss: -6.068097114562988
Iteration 901:
Training Loss: -3.0469114780426025
Reconstruction Loss: -6.556717872619629
Iteration 951:
Training Loss: -3.4687302112579346
Reconstruction Loss: -7.025766849517822
Iteration 1001:
Training Loss: -4.042019844055176
Reconstruction Loss: -7.479788303375244
Iteration 1051:
Training Loss: -4.493619918823242
Reconstruction Loss: -7.919922351837158
Iteration 1101:
Training Loss: -5.060678958892822
Reconstruction Loss: -8.350255012512207
Iteration 1151:
Training Loss: -5.272213935852051
Reconstruction Loss: -8.771500587463379
Iteration 1201:
Training Loss: -5.901466369628906
Reconstruction Loss: -9.190696716308594
Iteration 1251:
Training Loss: -6.250072956085205
Reconstruction Loss: -9.59947395324707
Iteration 1301:
Training Loss: -6.640639781951904
Reconstruction Loss: -9.999168395996094
Iteration 1351:
Training Loss: -6.917940616607666
Reconstruction Loss: -10.393268585205078
Iteration 1401:
Training Loss: -7.260188102722168
Reconstruction Loss: -10.778114318847656
Iteration 1451:
Training Loss: -7.687405586242676
Reconstruction Loss: -11.153800010681152
Iteration 1501:
Training Loss: -8.028546333312988
Reconstruction Loss: -11.513296127319336
Iteration 1551:
Training Loss: -8.431429862976074
Reconstruction Loss: -11.855419158935547
Iteration 1601:
Training Loss: -8.704045295715332
Reconstruction Loss: -12.178211212158203
Iteration 1651:
Training Loss: -8.98580551147461
Reconstruction Loss: -12.474735260009766
Iteration 1701:
Training Loss: -9.087005615234375
Reconstruction Loss: -12.738713264465332
Iteration 1751:
Training Loss: -9.209980964660645
Reconstruction Loss: -12.973173141479492
Iteration 1801:
Training Loss: -9.269628524780273
Reconstruction Loss: -13.172430992126465
Iteration 1851:
Training Loss: -9.60736083984375
Reconstruction Loss: -13.336089134216309
Iteration 1901:
Training Loss: -9.454041481018066
Reconstruction Loss: -13.466891288757324
Iteration 1951:
Training Loss: -9.467000007629395
Reconstruction Loss: -13.573494911193848
Iteration 2001:
Training Loss: -9.663999557495117
Reconstruction Loss: -13.654088973999023
Iteration 2051:
Training Loss: -9.501328468322754
Reconstruction Loss: -13.717090606689453
Iteration 2101:
Training Loss: -9.627297401428223
Reconstruction Loss: -13.765141487121582
Iteration 2151:
Training Loss: -9.609893798828125
Reconstruction Loss: -13.801186561584473
Iteration 2201:
Training Loss: -9.506239891052246
Reconstruction Loss: -13.829521179199219
Iteration 2251:
Training Loss: -9.647478103637695
Reconstruction Loss: -13.850446701049805
Iteration 2301:
Training Loss: -9.574524879455566
Reconstruction Loss: -13.866637229919434
Iteration 2351:
Training Loss: -9.598736763000488
Reconstruction Loss: -13.877886772155762
Iteration 2401:
Training Loss: -9.69504165649414
Reconstruction Loss: -13.890380859375
Iteration 2451:
Training Loss: -9.602432250976562
Reconstruction Loss: -13.896971702575684
Iteration 2501:
Training Loss: -9.646568298339844
Reconstruction Loss: -13.904369354248047
Iteration 2551:
Training Loss: -9.78756046295166
Reconstruction Loss: -13.910743713378906
Iteration 2601:
Training Loss: -9.650803565979004
Reconstruction Loss: -13.916873931884766
Iteration 2651:
Training Loss: -9.80283260345459
Reconstruction Loss: -13.918946266174316
Iteration 2701:
Training Loss: -9.696493148803711
Reconstruction Loss: -13.920930862426758
Iteration 2751:
Training Loss: -9.621028900146484
Reconstruction Loss: -13.923906326293945
Iteration 2801:
Training Loss: -9.579822540283203
Reconstruction Loss: -13.9286470413208
Iteration 2851:
Training Loss: -9.708009719848633
Reconstruction Loss: -13.930520057678223
Iteration 2901:
Training Loss: -9.765719413757324
Reconstruction Loss: -13.933205604553223
Iteration 2951:
Training Loss: -9.638799667358398
Reconstruction Loss: -13.939191818237305
Iteration 3001:
Training Loss: -9.717328071594238
Reconstruction Loss: -13.939109802246094
Iteration 3051:
Training Loss: -9.673410415649414
Reconstruction Loss: -13.941255569458008
Iteration 3101:
Training Loss: -9.643457412719727
Reconstruction Loss: -13.938239097595215
Iteration 3151:
Training Loss: -9.594284057617188
Reconstruction Loss: -13.943363189697266
Iteration 3201:
Training Loss: -9.591930389404297
Reconstruction Loss: -13.942232131958008
Iteration 3251:
Training Loss: -9.74539852142334
Reconstruction Loss: -13.947168350219727
Iteration 3301:
Training Loss: -9.566781044006348
Reconstruction Loss: -13.949430465698242
Iteration 3351:
Training Loss: -9.57174015045166
Reconstruction Loss: -13.948995590209961
Iteration 3401:
Training Loss: -9.589000701904297
Reconstruction Loss: -13.949984550476074
Iteration 3451:
Training Loss: -9.840836524963379
Reconstruction Loss: -13.953783988952637
Iteration 3501:
Training Loss: -9.673687934875488
Reconstruction Loss: -13.95439338684082
Iteration 3551:
Training Loss: -9.608327865600586
Reconstruction Loss: -13.95417594909668
Iteration 3601:
Training Loss: -9.631518363952637
Reconstruction Loss: -13.954178810119629
Iteration 3651:
Training Loss: -9.667643547058105
Reconstruction Loss: -13.959393501281738
Iteration 3701:
Training Loss: -9.74710464477539
Reconstruction Loss: -13.958779335021973
Iteration 3751:
Training Loss: -9.63914966583252
Reconstruction Loss: -13.960125923156738
Iteration 3801:
Training Loss: -9.5404052734375
Reconstruction Loss: -13.96017837524414
Iteration 3851:
Training Loss: -9.737553596496582
Reconstruction Loss: -13.960012435913086
Iteration 3901:
Training Loss: -9.819633483886719
Reconstruction Loss: -13.962339401245117
Iteration 3951:
Training Loss: -9.632999420166016
Reconstruction Loss: -13.965083122253418
Iteration 4001:
Training Loss: -9.816468238830566
Reconstruction Loss: -13.96512508392334
Iteration 4051:
Training Loss: -9.614291191101074
Reconstruction Loss: -13.96825122833252
Iteration 4101:
Training Loss: -9.627082824707031
Reconstruction Loss: -13.969001770019531
Iteration 4151:
Training Loss: -9.670470237731934
Reconstruction Loss: -13.97179889678955
Iteration 4201:
Training Loss: -9.719953536987305
Reconstruction Loss: -13.971842765808105
Iteration 4251:
Training Loss: -9.671123504638672
Reconstruction Loss: -13.975314140319824
Iteration 4301:
Training Loss: -9.654526710510254
Reconstruction Loss: -13.973539352416992
Iteration 4351:
Training Loss: -9.63670825958252
Reconstruction Loss: -13.97436237335205
Iteration 4401:
Training Loss: -9.7191801071167
Reconstruction Loss: -13.977086067199707
Iteration 4451:
Training Loss: -9.641170501708984
Reconstruction Loss: -13.978515625
Iteration 4501:
Training Loss: -9.6431245803833
Reconstruction Loss: -13.97829818725586
Iteration 4551:
Training Loss: -9.72900104522705
Reconstruction Loss: -13.980733871459961
Iteration 4601:
Training Loss: -9.619468688964844
Reconstruction Loss: -13.983613014221191
Iteration 4651:
Training Loss: -9.731905937194824
Reconstruction Loss: -13.981353759765625
Iteration 4701:
Training Loss: -9.693161964416504
Reconstruction Loss: -13.983859062194824
Iteration 4751:
Training Loss: -9.680567741394043
Reconstruction Loss: -13.986800193786621
Iteration 4801:
Training Loss: -9.612526893615723
Reconstruction Loss: -13.986833572387695
Iteration 4851:
Training Loss: -9.507465362548828
Reconstruction Loss: -13.990141868591309
Iteration 4901:
Training Loss: -9.693805694580078
Reconstruction Loss: -13.988994598388672
Iteration 4951:
Training Loss: -9.671673774719238
Reconstruction Loss: -13.98989200592041
Iteration 5001:
Training Loss: -9.60487174987793
Reconstruction Loss: -13.992640495300293
Iteration 5051:
Training Loss: -9.686202049255371
Reconstruction Loss: -13.99457836151123
Iteration 5101:
Training Loss: -9.733972549438477
Reconstruction Loss: -13.994180679321289
Iteration 5151:
Training Loss: -9.62741470336914
Reconstruction Loss: -13.99467945098877
Iteration 5201:
Training Loss: -9.710688591003418
Reconstruction Loss: -13.99798583984375
Iteration 5251:
Training Loss: -9.863005638122559
Reconstruction Loss: -13.999552726745605
Iteration 5301:
Training Loss: -9.801505088806152
Reconstruction Loss: -13.99979305267334
Iteration 5351:
Training Loss: -9.760169982910156
Reconstruction Loss: -14.00178050994873
Iteration 5401:
Training Loss: -9.773237228393555
Reconstruction Loss: -14.002955436706543
Iteration 5451:
Training Loss: -9.654147148132324
Reconstruction Loss: -14.002405166625977
Iteration 5501:
Training Loss: -9.76301097869873
Reconstruction Loss: -14.003724098205566
Iteration 5551:
Training Loss: -9.740514755249023
Reconstruction Loss: -14.006365776062012
Iteration 5601:
Training Loss: -9.749772071838379
Reconstruction Loss: -14.00611400604248
Iteration 5651:
Training Loss: -9.593188285827637
Reconstruction Loss: -14.008842468261719
Iteration 5701:
Training Loss: -9.776291847229004
Reconstruction Loss: -14.008606910705566
Iteration 5751:
Training Loss: -9.680106163024902
Reconstruction Loss: -14.009573936462402
Iteration 5801:
Training Loss: -9.61188793182373
Reconstruction Loss: -14.014253616333008
Iteration 5851:
Training Loss: -9.66420841217041
Reconstruction Loss: -14.012906074523926
Iteration 5901:
Training Loss: -9.700240135192871
Reconstruction Loss: -14.014084815979004
Iteration 5951:
Training Loss: -9.7258939743042
Reconstruction Loss: -14.015824317932129
Iteration 6001:
Training Loss: -9.776208877563477
Reconstruction Loss: -14.017370223999023
Iteration 6051:
Training Loss: -9.765474319458008
Reconstruction Loss: -14.017715454101562
Iteration 6101:
Training Loss: -9.756681442260742
Reconstruction Loss: -14.019050598144531
Iteration 6151:
Training Loss: -9.80715560913086
Reconstruction Loss: -14.020073890686035
Iteration 6201:
Training Loss: -9.554671287536621
Reconstruction Loss: -14.021428108215332
Iteration 6251:
Training Loss: -9.710698127746582
Reconstruction Loss: -14.022690773010254
Iteration 6301:
Training Loss: -9.707010269165039
Reconstruction Loss: -14.023122787475586
Iteration 6351:
Training Loss: -9.840224266052246
Reconstruction Loss: -14.026643753051758
Iteration 6401:
Training Loss: -9.662605285644531
Reconstruction Loss: -14.027570724487305
Iteration 6451:
Training Loss: -9.694464683532715
Reconstruction Loss: -14.027491569519043
Iteration 6501:
Training Loss: -9.670915603637695
Reconstruction Loss: -14.031126022338867
Iteration 6551:
Training Loss: -9.79320240020752
Reconstruction Loss: -14.031831741333008
Iteration 6601:
Training Loss: -9.80948257446289
Reconstruction Loss: -14.032136917114258
Iteration 6651:
Training Loss: -9.859938621520996
Reconstruction Loss: -14.031962394714355
Iteration 6701:
Training Loss: -9.833948135375977
Reconstruction Loss: -14.031237602233887
Iteration 6751:
Training Loss: -9.785977363586426
Reconstruction Loss: -14.034416198730469
Iteration 6801:
Training Loss: -9.667449951171875
Reconstruction Loss: -14.036945343017578
Iteration 6851:
Training Loss: -9.620319366455078
Reconstruction Loss: -14.038816452026367
Iteration 6901:
Training Loss: -9.857601165771484
Reconstruction Loss: -14.038074493408203
Iteration 6951:
Training Loss: -9.7467622756958
Reconstruction Loss: -14.04124927520752
Iteration 7001:
Training Loss: -9.735824584960938
Reconstruction Loss: -14.042027473449707
Iteration 7051:
Training Loss: -9.811558723449707
Reconstruction Loss: -14.043203353881836
Iteration 7101:
Training Loss: -9.822312355041504
Reconstruction Loss: -14.044450759887695
Iteration 7151:
Training Loss: -9.81395435333252
Reconstruction Loss: -14.045137405395508
Iteration 7201:
Training Loss: -9.798728942871094
Reconstruction Loss: -14.044452667236328
Iteration 7251:
Training Loss: -9.832810401916504
Reconstruction Loss: -14.04895305633545
Iteration 7301:
Training Loss: -9.714542388916016
Reconstruction Loss: -14.047764778137207
Iteration 7351:
Training Loss: -9.657855987548828
Reconstruction Loss: -14.050524711608887
Iteration 7401:
Training Loss: -9.762107849121094
Reconstruction Loss: -14.051465034484863
Iteration 7451:
Training Loss: -9.729303359985352
Reconstruction Loss: -14.051148414611816
