5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.3388872146606445
Reconstruction Loss: -0.48463326692581177
Iteration 11:
Training Loss: 5.6962361335754395
Reconstruction Loss: -0.48463332653045654
Iteration 21:
Training Loss: 5.802863597869873
Reconstruction Loss: -0.48463332653045654
Iteration 31:
Training Loss: 5.219841957092285
Reconstruction Loss: -0.48463332653045654
Iteration 41:
Training Loss: 5.714158058166504
Reconstruction Loss: -0.48463332653045654
Iteration 51:
Training Loss: 5.803921222686768
Reconstruction Loss: -0.48463332653045654
Iteration 61:
Training Loss: 5.343324661254883
Reconstruction Loss: -0.48463332653045654
Iteration 71:
Training Loss: 5.893743991851807
Reconstruction Loss: -0.48463332653045654
Iteration 81:
Training Loss: 5.430010795593262
Reconstruction Loss: -0.48463332653045654
Iteration 91:
Training Loss: 5.723851203918457
Reconstruction Loss: -0.48463332653045654
Iteration 101:
Training Loss: 5.234659671783447
Reconstruction Loss: -0.48463332653045654
Iteration 111:
Training Loss: 5.452393531799316
Reconstruction Loss: -0.48463332653045654
Iteration 121:
Training Loss: 5.562924861907959
Reconstruction Loss: -0.48463332653045654
Iteration 131:
Training Loss: 5.361535549163818
Reconstruction Loss: -0.48463332653045654
Iteration 141:
Training Loss: 5.516557693481445
Reconstruction Loss: -0.4846334457397461
Iteration 151:
Training Loss: 5.460903167724609
Reconstruction Loss: -0.48463353514671326
Iteration 161:
Training Loss: 5.607004642486572
Reconstruction Loss: -0.48463353514671326
Iteration 171:
Training Loss: 4.999453544616699
Reconstruction Loss: -0.4846336245536804
Iteration 181:
Training Loss: 5.286409378051758
Reconstruction Loss: -0.48463374376296997
Iteration 191:
Training Loss: 5.7031121253967285
Reconstruction Loss: -0.48463374376296997
Iteration 201:
Training Loss: 5.669602394104004
Reconstruction Loss: -0.48463374376296997
Iteration 211:
Training Loss: 5.31911039352417
Reconstruction Loss: -0.48463374376296997
Iteration 221:
Training Loss: 5.82062292098999
Reconstruction Loss: -0.48463383316993713
Iteration 231:
Training Loss: 5.142340660095215
Reconstruction Loss: -0.48463383316993713
Iteration 241:
Training Loss: 5.41987943649292
Reconstruction Loss: -0.48463383316993713
Iteration 251:
Training Loss: 5.038069248199463
Reconstruction Loss: -0.48463404178619385
Iteration 261:
Training Loss: 5.031533241271973
Reconstruction Loss: -0.48463404178619385
Iteration 271:
Training Loss: 5.151601791381836
Reconstruction Loss: -0.4846341013908386
Iteration 281:
Training Loss: 5.3546295166015625
Reconstruction Loss: -0.4846342206001282
Iteration 291:
Training Loss: 5.148639678955078
Reconstruction Loss: -0.4846343994140625
Iteration 301:
Training Loss: 5.675542831420898
Reconstruction Loss: -0.4846346080303192
Iteration 311:
Training Loss: 4.72921085357666
Reconstruction Loss: -0.4846346974372864
Iteration 321:
Training Loss: 5.852118968963623
Reconstruction Loss: -0.4846348762512207
Iteration 331:
Training Loss: 5.34791374206543
Reconstruction Loss: -0.48463529348373413
Iteration 341:
Training Loss: 5.701164722442627
Reconstruction Loss: -0.484635591506958
Iteration 351:
Training Loss: 4.997068881988525
Reconstruction Loss: -0.4846360683441162
Iteration 361:
Training Loss: 5.476700782775879
Reconstruction Loss: -0.4846366345882416
Iteration 371:
Training Loss: 5.849220275878906
Reconstruction Loss: -0.4846373200416565
Iteration 381:
Training Loss: 5.7792744636535645
Reconstruction Loss: -0.4846384823322296
Iteration 391:
Training Loss: 5.358902454376221
Reconstruction Loss: -0.4846399426460266
Iteration 401:
Training Loss: 5.560783863067627
Reconstruction Loss: -0.4846421480178833
Iteration 411:
Training Loss: 4.777098655700684
Reconstruction Loss: -0.4846455454826355
Iteration 421:
Training Loss: 5.692256450653076
Reconstruction Loss: -0.4846510589122772
Iteration 431:
Training Loss: 5.55075216293335
Reconstruction Loss: -0.4846611022949219
Iteration 441:
Training Loss: 5.700956344604492
Reconstruction Loss: -0.4846813380718231
Iteration 451:
Training Loss: 5.757931232452393
Reconstruction Loss: -0.4847286641597748
Iteration 461:
Training Loss: 5.282238960266113
Reconstruction Loss: -0.48487454652786255
Iteration 471:
Training Loss: 5.267505168914795
Reconstruction Loss: -0.4855797290802002
Iteration 481:
Training Loss: 5.2167558670043945
Reconstruction Loss: -0.4971782863140106
Iteration 491:
Training Loss: 5.334968090057373
Reconstruction Loss: -0.5850089192390442
Iteration 501:
Training Loss: 4.96375846862793
Reconstruction Loss: -0.570970356464386
Iteration 511:
Training Loss: 5.6992082595825195
Reconstruction Loss: -0.5408251881599426
Iteration 521:
Training Loss: 4.648171901702881
Reconstruction Loss: -0.5552361607551575
Iteration 531:
Training Loss: 4.648838520050049
Reconstruction Loss: -0.557744026184082
Iteration 541:
Training Loss: 5.5248589515686035
Reconstruction Loss: -0.5354376435279846
Iteration 551:
Training Loss: 4.947234153747559
Reconstruction Loss: -0.543093740940094
Iteration 561:
Training Loss: 5.079585552215576
Reconstruction Loss: -0.548398494720459
Iteration 571:
Training Loss: 4.71088981628418
Reconstruction Loss: -0.5330427289009094
Iteration 581:
Training Loss: 5.161514759063721
Reconstruction Loss: -0.5485207438468933
Iteration 591:
Training Loss: 4.991908073425293
Reconstruction Loss: -0.5134813189506531
Iteration 601:
Training Loss: 5.225625991821289
Reconstruction Loss: -0.5156724452972412
Iteration 611:
Training Loss: 5.440290927886963
Reconstruction Loss: -0.5391811728477478
Iteration 621:
Training Loss: 5.38552188873291
Reconstruction Loss: -0.5392180681228638
Iteration 631:
Training Loss: 5.170247554779053
Reconstruction Loss: -0.7383872866630554
Iteration 641:
Training Loss: 4.501285552978516
Reconstruction Loss: -0.7500843405723572
Iteration 651:
Training Loss: 4.668307304382324
Reconstruction Loss: -0.7449118494987488
Iteration 661:
Training Loss: 4.858689308166504
Reconstruction Loss: -0.7352263927459717
Iteration 671:
Training Loss: 4.364975452423096
Reconstruction Loss: -0.7615500092506409
Iteration 681:
Training Loss: 4.405409336090088
Reconstruction Loss: -0.7730831503868103
Iteration 691:
Training Loss: 4.581773281097412
Reconstruction Loss: -0.7990902662277222
Iteration 701:
Training Loss: 4.502064228057861
Reconstruction Loss: -0.8338174819946289
Iteration 711:
Training Loss: 4.221344470977783
Reconstruction Loss: -1.0329399108886719
Iteration 721:
Training Loss: 3.729501962661743
Reconstruction Loss: -1.1189589500427246
Iteration 731:
Training Loss: 3.7597758769989014
Reconstruction Loss: -1.1319007873535156
Iteration 741:
Training Loss: 3.925180196762085
Reconstruction Loss: -1.1301809549331665
Iteration 751:
Training Loss: 4.136445999145508
Reconstruction Loss: -1.1349543333053589
Iteration 761:
Training Loss: 4.162272930145264
Reconstruction Loss: -1.1069055795669556
Iteration 771:
Training Loss: 3.9908318519592285
Reconstruction Loss: -1.1104731559753418
Iteration 781:
Training Loss: 3.4117271900177
Reconstruction Loss: -1.0886183977127075
Iteration 791:
Training Loss: 3.5716850757598877
Reconstruction Loss: -1.0849673748016357
Iteration 801:
Training Loss: 3.970125913619995
Reconstruction Loss: -1.0885175466537476
Iteration 811:
Training Loss: 3.816880702972412
Reconstruction Loss: -1.0895774364471436
Iteration 821:
Training Loss: 4.065717697143555
Reconstruction Loss: -1.0863726139068604
Iteration 831:
Training Loss: 3.956921100616455
Reconstruction Loss: -1.0736738443374634
Iteration 841:
Training Loss: 4.076040267944336
Reconstruction Loss: -1.0859191417694092
Iteration 851:
Training Loss: 3.4512743949890137
Reconstruction Loss: -1.083418369293213
Iteration 861:
Training Loss: 3.6803910732269287
Reconstruction Loss: -1.08340585231781
Iteration 871:
Training Loss: 3.673807144165039
Reconstruction Loss: -1.07773756980896
Iteration 881:
Training Loss: 3.578803539276123
Reconstruction Loss: -1.0744593143463135
Iteration 891:
Training Loss: 3.7995474338531494
Reconstruction Loss: -1.0642167329788208
Iteration 901:
Training Loss: 3.7414941787719727
Reconstruction Loss: -1.0794730186462402
Iteration 911:
Training Loss: 3.9466240406036377
Reconstruction Loss: -1.0744969844818115
Iteration 921:
Training Loss: 3.9623985290527344
Reconstruction Loss: -1.060176968574524
Iteration 931:
Training Loss: 3.4003798961639404
Reconstruction Loss: -1.0774939060211182
Iteration 941:
Training Loss: 3.878627061843872
Reconstruction Loss: -1.0601342916488647
Iteration 951:
Training Loss: 4.12483024597168
Reconstruction Loss: -1.048892617225647
Iteration 961:
Training Loss: 4.158961772918701
Reconstruction Loss: -1.0567079782485962
Iteration 971:
Training Loss: 4.092810153961182
Reconstruction Loss: -1.052597165107727
Iteration 981:
Training Loss: 3.45166015625
Reconstruction Loss: -1.055579423904419
Iteration 991:
Training Loss: 3.787402391433716
Reconstruction Loss: -1.0617390871047974
Iteration 1001:
Training Loss: 3.7183127403259277
Reconstruction Loss: -1.0402766466140747
Iteration 1011:
Training Loss: 4.297418594360352
Reconstruction Loss: -1.0557142496109009
Iteration 1021:
Training Loss: 4.2051849365234375
Reconstruction Loss: -1.0472774505615234
Iteration 1031:
Training Loss: 4.1272711753845215
Reconstruction Loss: -1.0480644702911377
Iteration 1041:
Training Loss: 3.607567071914673
Reconstruction Loss: -1.0385960340499878
Iteration 1051:
Training Loss: 4.102748870849609
Reconstruction Loss: -1.0874688625335693
Iteration 1061:
Training Loss: 3.504362106323242
Reconstruction Loss: -1.3588557243347168
Iteration 1071:
Training Loss: 2.7850658893585205
Reconstruction Loss: -1.516788363456726
Iteration 1081:
Training Loss: 2.8668787479400635
Reconstruction Loss: -1.543587327003479
Iteration 1091:
Training Loss: 2.6024327278137207
Reconstruction Loss: -1.5501347780227661
Iteration 1101:
Training Loss: 3.2720890045166016
Reconstruction Loss: -1.540436029434204
Iteration 1111:
Training Loss: 3.034071683883667
Reconstruction Loss: -1.5241808891296387
Iteration 1121:
Training Loss: 2.958763360977173
Reconstruction Loss: -1.5196055173873901
Iteration 1131:
Training Loss: 2.648076057434082
Reconstruction Loss: -1.5204650163650513
Iteration 1141:
Training Loss: 2.410548210144043
Reconstruction Loss: -1.5094832181930542
Iteration 1151:
Training Loss: 2.6812169551849365
Reconstruction Loss: -1.502807378768921
Iteration 1161:
Training Loss: 2.757253408432007
Reconstruction Loss: -1.4917086362838745
Iteration 1171:
Training Loss: 3.1657662391662598
Reconstruction Loss: -1.4922168254852295
Iteration 1181:
Training Loss: 2.5905137062072754
Reconstruction Loss: -1.4912537336349487
Iteration 1191:
Training Loss: 2.9305622577667236
Reconstruction Loss: -1.4801123142242432
Iteration 1201:
Training Loss: 2.9037179946899414
Reconstruction Loss: -1.486167073249817
Iteration 1211:
Training Loss: 2.8835480213165283
Reconstruction Loss: -1.4764153957366943
Iteration 1221:
Training Loss: 3.129824638366699
Reconstruction Loss: -1.4718228578567505
Iteration 1231:
Training Loss: 2.6328020095825195
Reconstruction Loss: -1.4793369770050049
Iteration 1241:
Training Loss: 2.616283655166626
Reconstruction Loss: -1.4685388803482056
Iteration 1251:
Training Loss: 2.877261161804199
Reconstruction Loss: -1.4723119735717773
Iteration 1261:
Training Loss: 2.8982152938842773
Reconstruction Loss: -1.4657751321792603
Iteration 1271:
Training Loss: 2.827638626098633
Reconstruction Loss: -1.4596432447433472
Iteration 1281:
Training Loss: 2.9240591526031494
Reconstruction Loss: -1.4595255851745605
Iteration 1291:
Training Loss: 2.4365036487579346
Reconstruction Loss: -1.4532157182693481
Iteration 1301:
Training Loss: 2.7244091033935547
Reconstruction Loss: -1.460080623626709
Iteration 1311:
Training Loss: 2.9986560344696045
Reconstruction Loss: -1.4569735527038574
Iteration 1321:
Training Loss: 3.0570719242095947
Reconstruction Loss: -1.4582407474517822
Iteration 1331:
Training Loss: 2.573895215988159
Reconstruction Loss: -1.4385123252868652
Iteration 1341:
Training Loss: 2.7909491062164307
Reconstruction Loss: -1.4572069644927979
Iteration 1351:
Training Loss: 2.9868783950805664
Reconstruction Loss: -1.4577027559280396
Iteration 1361:
Training Loss: 2.9223761558532715
Reconstruction Loss: -1.4536750316619873
Iteration 1371:
Training Loss: 2.916149139404297
Reconstruction Loss: -1.4443864822387695
Iteration 1381:
Training Loss: 2.7523834705352783
Reconstruction Loss: -1.4514440298080444
Iteration 1391:
Training Loss: 3.0459272861480713
Reconstruction Loss: -1.441497802734375
Iteration 1401:
Training Loss: 3.074545383453369
Reconstruction Loss: -1.4523237943649292
Iteration 1411:
Training Loss: 2.9105677604675293
Reconstruction Loss: -1.4429104328155518
Iteration 1421:
Training Loss: 3.2213265895843506
Reconstruction Loss: -1.4555150270462036
Iteration 1431:
Training Loss: 2.4583897590637207
Reconstruction Loss: -1.4421288967132568
Iteration 1441:
Training Loss: 3.1367249488830566
Reconstruction Loss: -1.4483083486557007
Iteration 1451:
Training Loss: 2.657233953475952
Reconstruction Loss: -1.4355052709579468
Iteration 1461:
Training Loss: 2.7115588188171387
Reconstruction Loss: -1.4510983228683472
Iteration 1471:
Training Loss: 2.3991260528564453
Reconstruction Loss: -1.4381479024887085
Iteration 1481:
Training Loss: 2.67486572265625
Reconstruction Loss: -1.455003023147583
Iteration 1491:
Training Loss: 3.117467164993286
Reconstruction Loss: -1.4517301321029663
