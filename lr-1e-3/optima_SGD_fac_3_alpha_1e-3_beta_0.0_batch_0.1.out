5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 30
Iteration 1:
Training Loss: 5.909087181091309
Reconstruction Loss: -0.4066004753112793
Iteration 11:
Training Loss: 4.305208683013916
Reconstruction Loss: -0.9016228318214417
Iteration 21:
Training Loss: 2.9698143005371094
Reconstruction Loss: -1.6150050163269043
Iteration 31:
Training Loss: 1.4037692546844482
Reconstruction Loss: -2.2946431636810303
Iteration 41:
Training Loss: 1.185280442237854
Reconstruction Loss: -2.851954936981201
Iteration 51:
Training Loss: 0.6241258978843689
Reconstruction Loss: -3.325512170791626
Iteration 61:
Training Loss: 0.12163417786359787
Reconstruction Loss: -3.761369228363037
Iteration 71:
Training Loss: -0.3727160096168518
Reconstruction Loss: -4.155319690704346
Iteration 81:
Training Loss: -0.8644596338272095
Reconstruction Loss: -4.505199432373047
Iteration 91:
Training Loss: -1.197501301765442
Reconstruction Loss: -4.799848556518555
Iteration 101:
Training Loss: -1.401288390159607
Reconstruction Loss: -5.055342674255371
Iteration 111:
Training Loss: -2.432220697402954
Reconstruction Loss: -5.259773254394531
Iteration 121:
Training Loss: -1.8426196575164795
Reconstruction Loss: -5.438108921051025
Iteration 131:
Training Loss: -2.342411756515503
Reconstruction Loss: -5.577495098114014
Iteration 141:
Training Loss: -2.289381265640259
Reconstruction Loss: -5.701615810394287
Iteration 151:
Training Loss: -2.5102291107177734
Reconstruction Loss: -5.803518295288086
Iteration 161:
Training Loss: -2.185199022293091
Reconstruction Loss: -5.892162322998047
Iteration 171:
Training Loss: -1.9981505870819092
Reconstruction Loss: -5.966470241546631
Iteration 181:
Training Loss: -2.5994906425476074
Reconstruction Loss: -6.044585704803467
Iteration 191:
Training Loss: -2.7056994438171387
Reconstruction Loss: -6.109118461608887
Iteration 201:
Training Loss: -2.845702886581421
Reconstruction Loss: -6.166337490081787
Iteration 211:
Training Loss: -2.7182250022888184
Reconstruction Loss: -6.223607540130615
Iteration 221:
Training Loss: -2.686647891998291
Reconstruction Loss: -6.270495891571045
Iteration 231:
Training Loss: -3.531012773513794
Reconstruction Loss: -6.312183856964111
Iteration 241:
Training Loss: -2.8414947986602783
Reconstruction Loss: -6.35875940322876
Iteration 251:
Training Loss: -3.154482364654541
Reconstruction Loss: -6.397923469543457
Iteration 261:
Training Loss: -3.1015467643737793
Reconstruction Loss: -6.434389114379883
Iteration 271:
Training Loss: -2.882932186126709
Reconstruction Loss: -6.471337795257568
Iteration 281:
Training Loss: -2.7968454360961914
Reconstruction Loss: -6.502405643463135
Iteration 291:
Training Loss: -3.5599958896636963
Reconstruction Loss: -6.536839008331299
Iteration 301:
Training Loss: -3.3483901023864746
Reconstruction Loss: -6.564094066619873
Iteration 311:
Training Loss: -3.321381092071533
Reconstruction Loss: -6.599483489990234
Iteration 321:
Training Loss: -2.9406511783599854
Reconstruction Loss: -6.627237796783447
Iteration 331:
Training Loss: -3.864821434020996
Reconstruction Loss: -6.65875768661499
Iteration 341:
Training Loss: -4.075242042541504
Reconstruction Loss: -6.68543815612793
Iteration 351:
Training Loss: -3.3671939373016357
Reconstruction Loss: -6.709213733673096
Iteration 361:
Training Loss: -3.469538450241089
Reconstruction Loss: -6.739729881286621
Iteration 371:
Training Loss: -3.982822895050049
Reconstruction Loss: -6.763819694519043
Iteration 381:
Training Loss: -3.6006877422332764
Reconstruction Loss: -6.784898281097412
Iteration 391:
Training Loss: -4.1605048179626465
Reconstruction Loss: -6.80794620513916
Iteration 401:
Training Loss: -3.664853572845459
Reconstruction Loss: -6.829956531524658
Iteration 411:
Training Loss: -3.6214427947998047
Reconstruction Loss: -6.849133491516113
Iteration 421:
Training Loss: -3.837470531463623
Reconstruction Loss: -6.866922378540039
Iteration 431:
Training Loss: -3.8788838386535645
Reconstruction Loss: -6.892110347747803
Iteration 441:
Training Loss: -3.967771530151367
Reconstruction Loss: -6.910521030426025
Iteration 451:
Training Loss: -3.7692489624023438
Reconstruction Loss: -6.931199073791504
Iteration 461:
Training Loss: -4.316547870635986
Reconstruction Loss: -6.948380470275879
Iteration 471:
Training Loss: -4.388751029968262
Reconstruction Loss: -6.966241359710693
Iteration 481:
Training Loss: -3.8426873683929443
Reconstruction Loss: -6.984797477722168
Iteration 491:
Training Loss: -3.8898303508758545
Reconstruction Loss: -7.000807762145996
Iteration 501:
Training Loss: -4.23898983001709
Reconstruction Loss: -7.016008377075195
Iteration 511:
Training Loss: -3.9767696857452393
Reconstruction Loss: -7.035540580749512
Iteration 521:
Training Loss: -4.693542957305908
Reconstruction Loss: -7.0492777824401855
Iteration 531:
Training Loss: -4.212475299835205
Reconstruction Loss: -7.066589832305908
Iteration 541:
Training Loss: -4.263800144195557
Reconstruction Loss: -7.080116271972656
Iteration 551:
Training Loss: -4.153346061706543
Reconstruction Loss: -7.090965270996094
Iteration 561:
Training Loss: -4.612409591674805
Reconstruction Loss: -7.112648963928223
Iteration 571:
Training Loss: -4.226798057556152
Reconstruction Loss: -7.119116306304932
Iteration 581:
Training Loss: -4.38684606552124
Reconstruction Loss: -7.136676788330078
Iteration 591:
Training Loss: -4.27631139755249
Reconstruction Loss: -7.147824764251709
Iteration 601:
Training Loss: -3.9011826515197754
Reconstruction Loss: -7.16158390045166
Iteration 611:
Training Loss: -4.328383445739746
Reconstruction Loss: -7.177431583404541
Iteration 621:
Training Loss: -4.5649213790893555
Reconstruction Loss: -7.1892900466918945
Iteration 631:
Training Loss: -4.5711846351623535
Reconstruction Loss: -7.2013397216796875
Iteration 641:
Training Loss: -4.551768779754639
Reconstruction Loss: -7.217647552490234
Iteration 651:
Training Loss: -5.300167083740234
Reconstruction Loss: -7.224424362182617
Iteration 661:
Training Loss: -4.178027153015137
Reconstruction Loss: -7.235177993774414
Iteration 671:
Training Loss: -4.340667247772217
Reconstruction Loss: -7.2464799880981445
Iteration 681:
Training Loss: -4.40943717956543
Reconstruction Loss: -7.259549140930176
Iteration 691:
Training Loss: -4.627744197845459
Reconstruction Loss: -7.26865291595459
Iteration 701:
Training Loss: -5.019349098205566
Reconstruction Loss: -7.280432224273682
Iteration 711:
Training Loss: -4.986362457275391
Reconstruction Loss: -7.288692951202393
Iteration 721:
Training Loss: -4.857153415679932
Reconstruction Loss: -7.2974677085876465
Iteration 731:
Training Loss: -4.571552276611328
Reconstruction Loss: -7.310199737548828
Iteration 741:
Training Loss: -5.1003007888793945
Reconstruction Loss: -7.317173480987549
Iteration 751:
Training Loss: -5.222629070281982
Reconstruction Loss: -7.330530643463135
Iteration 761:
Training Loss: -4.818674087524414
Reconstruction Loss: -7.339494705200195
Iteration 771:
Training Loss: -5.389585971832275
Reconstruction Loss: -7.34967565536499
Iteration 781:
Training Loss: -4.745364665985107
Reconstruction Loss: -7.35806941986084
Iteration 791:
Training Loss: -4.637495517730713
Reconstruction Loss: -7.365917682647705
Iteration 801:
Training Loss: -4.627919673919678
Reconstruction Loss: -7.3716020584106445
Iteration 811:
Training Loss: -5.214779376983643
Reconstruction Loss: -7.385532379150391
Iteration 821:
Training Loss: -4.667990207672119
Reconstruction Loss: -7.396059513092041
Iteration 831:
Training Loss: -4.671051502227783
Reconstruction Loss: -7.398132801055908
Iteration 841:
Training Loss: -4.709367275238037
Reconstruction Loss: -7.412351131439209
Iteration 851:
Training Loss: -4.906208038330078
Reconstruction Loss: -7.4202141761779785
Iteration 861:
Training Loss: -5.103928089141846
Reconstruction Loss: -7.426405429840088
Iteration 871:
Training Loss: -5.159455299377441
Reconstruction Loss: -7.436511516571045
Iteration 881:
Training Loss: -5.02551794052124
Reconstruction Loss: -7.447959899902344
Iteration 891:
Training Loss: -5.582742691040039
Reconstruction Loss: -7.448737621307373
Iteration 901:
Training Loss: -5.079747200012207
Reconstruction Loss: -7.458526134490967
Iteration 911:
Training Loss: -5.530059814453125
Reconstruction Loss: -7.464605331420898
Iteration 921:
Training Loss: -4.654077053070068
Reconstruction Loss: -7.4731316566467285
Iteration 931:
Training Loss: -5.219889163970947
Reconstruction Loss: -7.481759071350098
Iteration 941:
Training Loss: -5.077206611633301
Reconstruction Loss: -7.4821858406066895
Iteration 951:
Training Loss: -5.239583969116211
Reconstruction Loss: -7.4955220222473145
Iteration 961:
Training Loss: -4.804996967315674
Reconstruction Loss: -7.499633312225342
Iteration 971:
Training Loss: -5.196140289306641
Reconstruction Loss: -7.507049560546875
Iteration 981:
Training Loss: -5.280334949493408
Reconstruction Loss: -7.515495300292969
Iteration 991:
Training Loss: -5.218947410583496
Reconstruction Loss: -7.519582271575928
Iteration 1001:
Training Loss: -5.172085285186768
Reconstruction Loss: -7.527758598327637
Iteration 1011:
Training Loss: -5.132390022277832
Reconstruction Loss: -7.533200740814209
Iteration 1021:
Training Loss: -5.385383129119873
Reconstruction Loss: -7.540441036224365
Iteration 1031:
Training Loss: -5.782681941986084
Reconstruction Loss: -7.544070720672607
Iteration 1041:
Training Loss: -5.234076023101807
Reconstruction Loss: -7.549607276916504
Iteration 1051:
Training Loss: -5.100203514099121
Reconstruction Loss: -7.558291435241699
Iteration 1061:
Training Loss: -5.607287406921387
Reconstruction Loss: -7.564033031463623
Iteration 1071:
Training Loss: -5.439089298248291
Reconstruction Loss: -7.568594455718994
Iteration 1081:
Training Loss: -5.562183856964111
Reconstruction Loss: -7.577217102050781
Iteration 1091:
Training Loss: -5.251577377319336
Reconstruction Loss: -7.580565452575684
Iteration 1101:
Training Loss: -5.6443071365356445
Reconstruction Loss: -7.587637424468994
Iteration 1111:
Training Loss: -5.0843119621276855
Reconstruction Loss: -7.590812683105469
Iteration 1121:
Training Loss: -5.362382888793945
Reconstruction Loss: -7.597494125366211
Iteration 1131:
Training Loss: -5.406618595123291
Reconstruction Loss: -7.600986003875732
Iteration 1141:
Training Loss: -5.684389114379883
Reconstruction Loss: -7.609047889709473
Iteration 1151:
Training Loss: -5.361226558685303
Reconstruction Loss: -7.614423751831055
Iteration 1161:
Training Loss: -5.520755290985107
Reconstruction Loss: -7.6213507652282715
Iteration 1171:
Training Loss: -5.276444435119629
Reconstruction Loss: -7.622920989990234
Iteration 1181:
Training Loss: -5.518463611602783
Reconstruction Loss: -7.6302032470703125
Iteration 1191:
Training Loss: -5.922853946685791
Reconstruction Loss: -7.63599967956543
Iteration 1201:
Training Loss: -5.599968433380127
Reconstruction Loss: -7.638012409210205
Iteration 1211:
Training Loss: -4.977252960205078
Reconstruction Loss: -7.643460273742676
Iteration 1221:
Training Loss: -5.296898365020752
Reconstruction Loss: -7.647849082946777
Iteration 1231:
Training Loss: -5.5186896324157715
Reconstruction Loss: -7.656494140625
Iteration 1241:
Training Loss: -5.714842319488525
Reconstruction Loss: -7.660982131958008
Iteration 1251:
Training Loss: -5.902201175689697
Reconstruction Loss: -7.665643692016602
Iteration 1261:
Training Loss: -5.418710231781006
Reconstruction Loss: -7.668326377868652
Iteration 1271:
Training Loss: -6.2156901359558105
Reconstruction Loss: -7.66977071762085
Iteration 1281:
Training Loss: -6.11919641494751
Reconstruction Loss: -7.677265644073486
Iteration 1291:
Training Loss: -5.6215314865112305
Reconstruction Loss: -7.6820149421691895
Iteration 1301:
Training Loss: -5.1264328956604
Reconstruction Loss: -7.687251567840576
Iteration 1311:
Training Loss: -5.91268253326416
Reconstruction Loss: -7.688028812408447
Iteration 1321:
Training Loss: -5.773824214935303
Reconstruction Loss: -7.694796562194824
Iteration 1331:
Training Loss: -5.812992572784424
Reconstruction Loss: -7.697208881378174
Iteration 1341:
Training Loss: -6.121694564819336
Reconstruction Loss: -7.701769828796387
Iteration 1351:
Training Loss: -6.261233806610107
Reconstruction Loss: -7.708552360534668
Iteration 1361:
Training Loss: -5.635005474090576
Reconstruction Loss: -7.711304187774658
Iteration 1371:
Training Loss: -6.115917205810547
Reconstruction Loss: -7.714997291564941
Iteration 1381:
Training Loss: -5.668562889099121
Reconstruction Loss: -7.719228267669678
Iteration 1391:
Training Loss: -5.4052324295043945
Reconstruction Loss: -7.723804950714111
Iteration 1401:
Training Loss: -5.897829532623291
Reconstruction Loss: -7.726893424987793
Iteration 1411:
Training Loss: -5.635202884674072
Reconstruction Loss: -7.729475975036621
Iteration 1421:
Training Loss: -5.830653667449951
Reconstruction Loss: -7.732623100280762
Iteration 1431:
Training Loss: -5.975119113922119
Reconstruction Loss: -7.741189002990723
Iteration 1441:
Training Loss: -6.003838539123535
Reconstruction Loss: -7.745224475860596
Iteration 1451:
Training Loss: -5.827150344848633
Reconstruction Loss: -7.7459001541137695
Iteration 1461:
Training Loss: -5.769679546356201
Reconstruction Loss: -7.74750280380249
Iteration 1471:
Training Loss: -6.010825157165527
Reconstruction Loss: -7.7506890296936035
Iteration 1481:
Training Loss: -6.048411846160889
Reconstruction Loss: -7.755467891693115
Iteration 1491:
Training Loss: -5.769298553466797
Reconstruction Loss: -7.762548446655273
