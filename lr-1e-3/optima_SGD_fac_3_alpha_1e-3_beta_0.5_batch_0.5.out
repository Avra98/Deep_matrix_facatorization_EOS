5
ParameterList(
    (0): Parameter containing: [torch.float32 of size 20x20]
    (1): Parameter containing: [torch.float32 of size 20x20]
    (2): Parameter containing: [torch.float32 of size 20x20]
)
300 150
Iteration 1:
Training Loss: 5.802651882171631
Reconstruction Loss: -0.38522228598594666
Iteration 51:
Training Loss: 4.999465465545654
Reconstruction Loss: -0.6919909119606018
Iteration 101:
Training Loss: 3.0164852142333984
Reconstruction Loss: -1.882184386253357
Iteration 151:
Training Loss: 1.27364981174469
Reconstruction Loss: -2.9132394790649414
Iteration 201:
Training Loss: 0.42923417687416077
Reconstruction Loss: -3.7903974056243896
Iteration 251:
Training Loss: -0.5090054273605347
Reconstruction Loss: -4.48941707611084
Iteration 301:
Training Loss: -1.1971206665039062
Reconstruction Loss: -4.99207878112793
Iteration 351:
Training Loss: -1.5569828748703003
Reconstruction Loss: -5.3447160720825195
Iteration 401:
Training Loss: -1.748069405555725
Reconstruction Loss: -5.596673965454102
Iteration 451:
Training Loss: -2.041857957839966
Reconstruction Loss: -5.7790679931640625
Iteration 501:
Training Loss: -2.1973423957824707
Reconstruction Loss: -5.918498992919922
Iteration 551:
Training Loss: -2.3516790866851807
Reconstruction Loss: -6.026857376098633
Iteration 601:
Training Loss: -2.5952415466308594
Reconstruction Loss: -6.115926742553711
Iteration 651:
Training Loss: -2.7473130226135254
Reconstruction Loss: -6.1910624504089355
Iteration 701:
Training Loss: -2.9490225315093994
Reconstruction Loss: -6.2572526931762695
Iteration 751:
Training Loss: -2.8920977115631104
Reconstruction Loss: -6.314609527587891
Iteration 801:
Training Loss: -2.9545540809631348
Reconstruction Loss: -6.3650360107421875
Iteration 851:
Training Loss: -2.9361941814422607
Reconstruction Loss: -6.412614822387695
Iteration 901:
Training Loss: -3.0067248344421387
Reconstruction Loss: -6.4552507400512695
Iteration 951:
Training Loss: -3.1650538444519043
Reconstruction Loss: -6.494854927062988
Iteration 1001:
Training Loss: -3.109750747680664
Reconstruction Loss: -6.533044338226318
Iteration 1051:
Training Loss: -3.3269805908203125
Reconstruction Loss: -6.567075252532959
Iteration 1101:
Training Loss: -3.3516933917999268
Reconstruction Loss: -6.601077556610107
Iteration 1151:
Training Loss: -3.487700939178467
Reconstruction Loss: -6.6321234703063965
Iteration 1201:
Training Loss: -3.4976985454559326
Reconstruction Loss: -6.661447048187256
Iteration 1251:
Training Loss: -3.6135482788085938
Reconstruction Loss: -6.68830680847168
Iteration 1301:
Training Loss: -3.618149995803833
Reconstruction Loss: -6.715875148773193
Iteration 1351:
Training Loss: -3.7425856590270996
Reconstruction Loss: -6.740790843963623
Iteration 1401:
Training Loss: -3.708787202835083
Reconstruction Loss: -6.764832496643066
Iteration 1451:
Training Loss: -3.6034977436065674
Reconstruction Loss: -6.787899971008301
Iteration 1501:
Training Loss: -3.721679925918579
Reconstruction Loss: -6.809080123901367
Iteration 1551:
Training Loss: -3.836205005645752
Reconstruction Loss: -6.8310956954956055
Iteration 1601:
Training Loss: -3.89841628074646
Reconstruction Loss: -6.851377010345459
Iteration 1651:
Training Loss: -3.771974563598633
Reconstruction Loss: -6.87081241607666
Iteration 1701:
Training Loss: -3.897315740585327
Reconstruction Loss: -6.891251087188721
Iteration 1751:
Training Loss: -3.9437150955200195
Reconstruction Loss: -6.9084906578063965
Iteration 1801:
Training Loss: -4.133401870727539
Reconstruction Loss: -6.926246166229248
Iteration 1851:
Training Loss: -4.101017951965332
Reconstruction Loss: -6.945018768310547
Iteration 1901:
Training Loss: -4.127684593200684
Reconstruction Loss: -6.961026668548584
Iteration 1951:
Training Loss: -4.167524337768555
Reconstruction Loss: -6.976680755615234
Iteration 2001:
Training Loss: -4.27302885055542
Reconstruction Loss: -6.99315881729126
Iteration 2051:
Training Loss: -4.233644485473633
Reconstruction Loss: -7.007565975189209
Iteration 2101:
Training Loss: -4.0922393798828125
Reconstruction Loss: -7.023414134979248
Iteration 2151:
Training Loss: -4.426353931427002
Reconstruction Loss: -7.037412643432617
Iteration 2201:
Training Loss: -4.233921527862549
Reconstruction Loss: -7.052215576171875
Iteration 2251:
Training Loss: -4.128291606903076
Reconstruction Loss: -7.066624164581299
Iteration 2301:
Training Loss: -4.249532222747803
Reconstruction Loss: -7.079421520233154
Iteration 2351:
Training Loss: -4.503406047821045
Reconstruction Loss: -7.0924458503723145
Iteration 2401:
Training Loss: -4.329719543457031
Reconstruction Loss: -7.105504035949707
Iteration 2451:
Training Loss: -4.354273319244385
Reconstruction Loss: -7.117732048034668
Iteration 2501:
Training Loss: -4.437903881072998
Reconstruction Loss: -7.128658771514893
Iteration 2551:
Training Loss: -4.4670867919921875
Reconstruction Loss: -7.141960144042969
Iteration 2601:
Training Loss: -4.480293273925781
Reconstruction Loss: -7.153882026672363
Iteration 2651:
Training Loss: -4.491810321807861
Reconstruction Loss: -7.163453102111816
Iteration 2701:
Training Loss: -4.6854376792907715
Reconstruction Loss: -7.176140308380127
Iteration 2751:
Training Loss: -4.511021614074707
Reconstruction Loss: -7.18686056137085
Iteration 2801:
Training Loss: -4.80744743347168
Reconstruction Loss: -7.1972575187683105
Iteration 2851:
Training Loss: -4.58297061920166
Reconstruction Loss: -7.2085771560668945
Iteration 2901:
Training Loss: -4.741837024688721
Reconstruction Loss: -7.219037055969238
Iteration 2951:
Training Loss: -4.680940628051758
Reconstruction Loss: -7.228541374206543
Iteration 3001:
Training Loss: -4.617229461669922
Reconstruction Loss: -7.239040851593018
Iteration 3051:
Training Loss: -4.841732025146484
Reconstruction Loss: -7.24930477142334
Iteration 3101:
Training Loss: -4.816155433654785
Reconstruction Loss: -7.257011413574219
Iteration 3151:
Training Loss: -4.873194217681885
Reconstruction Loss: -7.267480373382568
Iteration 3201:
Training Loss: -4.746196746826172
Reconstruction Loss: -7.277012348175049
Iteration 3251:
Training Loss: -4.893461227416992
Reconstruction Loss: -7.285849094390869
Iteration 3301:
Training Loss: -4.777181148529053
Reconstruction Loss: -7.295438766479492
Iteration 3351:
Training Loss: -5.012664794921875
Reconstruction Loss: -7.303058624267578
Iteration 3401:
Training Loss: -4.809996604919434
Reconstruction Loss: -7.312800884246826
Iteration 3451:
Training Loss: -4.985408782958984
Reconstruction Loss: -7.321532249450684
Iteration 3501:
Training Loss: -4.857784271240234
Reconstruction Loss: -7.32878303527832
Iteration 3551:
Training Loss: -4.921838760375977
Reconstruction Loss: -7.336783409118652
Iteration 3601:
Training Loss: -4.893111705780029
Reconstruction Loss: -7.344587802886963
Iteration 3651:
Training Loss: -5.0876898765563965
Reconstruction Loss: -7.354132175445557
Iteration 3701:
Training Loss: -4.9467453956604
Reconstruction Loss: -7.3612470626831055
Iteration 3751:
Training Loss: -5.00562858581543
Reconstruction Loss: -7.369546413421631
Iteration 3801:
Training Loss: -5.130411148071289
Reconstruction Loss: -7.3766703605651855
Iteration 3851:
Training Loss: -5.03574800491333
Reconstruction Loss: -7.3845062255859375
Iteration 3901:
Training Loss: -4.958346843719482
Reconstruction Loss: -7.3910651206970215
Iteration 3951:
Training Loss: -5.217091083526611
Reconstruction Loss: -7.399000644683838
Iteration 4001:
Training Loss: -5.122687339782715
Reconstruction Loss: -7.407141208648682
Iteration 4051:
Training Loss: -5.1243181228637695
Reconstruction Loss: -7.413002014160156
Iteration 4101:
Training Loss: -5.177168369293213
Reconstruction Loss: -7.419549942016602
Iteration 4151:
Training Loss: -5.134554862976074
Reconstruction Loss: -7.4276251792907715
Iteration 4201:
Training Loss: -5.0503740310668945
Reconstruction Loss: -7.433739185333252
Iteration 4251:
Training Loss: -5.225194454193115
Reconstruction Loss: -7.440304279327393
Iteration 4301:
Training Loss: -5.106420993804932
Reconstruction Loss: -7.447288513183594
Iteration 4351:
Training Loss: -5.235684394836426
Reconstruction Loss: -7.453948497772217
Iteration 4401:
Training Loss: -5.275657653808594
Reconstruction Loss: -7.461334705352783
Iteration 4451:
Training Loss: -5.189738750457764
Reconstruction Loss: -7.467096328735352
Iteration 4501:
Training Loss: -5.1900634765625
Reconstruction Loss: -7.472797870635986
Iteration 4551:
Training Loss: -5.375237464904785
Reconstruction Loss: -7.479272365570068
Iteration 4601:
Training Loss: -5.302432537078857
Reconstruction Loss: -7.485592842102051
Iteration 4651:
Training Loss: -5.278000831604004
Reconstruction Loss: -7.492176055908203
Iteration 4701:
Training Loss: -5.363955020904541
Reconstruction Loss: -7.4970927238464355
Iteration 4751:
Training Loss: -5.333900451660156
Reconstruction Loss: -7.503689765930176
Iteration 4801:
Training Loss: -5.441278457641602
Reconstruction Loss: -7.509618282318115
Iteration 4851:
Training Loss: -5.390824794769287
Reconstruction Loss: -7.5155415534973145
Iteration 4901:
Training Loss: -5.46588659286499
Reconstruction Loss: -7.520575046539307
Iteration 4951:
Training Loss: -5.569402694702148
Reconstruction Loss: -7.526599884033203
Iteration 5001:
Training Loss: -5.522624969482422
Reconstruction Loss: -7.532153129577637
Iteration 5051:
Training Loss: -5.3890700340271
Reconstruction Loss: -7.537931442260742
Iteration 5101:
Training Loss: -5.420236587524414
Reconstruction Loss: -7.54364013671875
Iteration 5151:
Training Loss: -5.49821138381958
Reconstruction Loss: -7.549374103546143
Iteration 5201:
Training Loss: -5.591753005981445
Reconstruction Loss: -7.555029392242432
Iteration 5251:
Training Loss: -5.654362678527832
Reconstruction Loss: -7.559054851531982
Iteration 5301:
Training Loss: -5.521700382232666
Reconstruction Loss: -7.5648932456970215
Iteration 5351:
Training Loss: -5.455547332763672
Reconstruction Loss: -7.569496154785156
Iteration 5401:
Training Loss: -5.539356708526611
Reconstruction Loss: -7.5748677253723145
Iteration 5451:
Training Loss: -5.537937641143799
Reconstruction Loss: -7.579681396484375
Iteration 5501:
Training Loss: -5.512328624725342
Reconstruction Loss: -7.585112571716309
Iteration 5551:
Training Loss: -5.530860424041748
Reconstruction Loss: -7.58999490737915
Iteration 5601:
Training Loss: -5.59622859954834
Reconstruction Loss: -7.595572471618652
Iteration 5651:
Training Loss: -5.708951950073242
Reconstruction Loss: -7.600116729736328
Iteration 5701:
Training Loss: -5.681346416473389
Reconstruction Loss: -7.604942321777344
Iteration 5751:
Training Loss: -5.543560981750488
Reconstruction Loss: -7.610160827636719
Iteration 5801:
Training Loss: -5.7678093910217285
Reconstruction Loss: -7.6141357421875
Iteration 5851:
Training Loss: -5.601945877075195
Reconstruction Loss: -7.619218349456787
Iteration 5901:
Training Loss: -5.566707134246826
Reconstruction Loss: -7.623618125915527
Iteration 5951:
Training Loss: -5.7315778732299805
Reconstruction Loss: -7.627738952636719
Iteration 6001:
Training Loss: -5.646800518035889
Reconstruction Loss: -7.632852554321289
Iteration 6051:
Training Loss: -5.743985652923584
Reconstruction Loss: -7.637299537658691
Iteration 6101:
Training Loss: -5.674703598022461
Reconstruction Loss: -7.6420464515686035
Iteration 6151:
Training Loss: -5.806628704071045
Reconstruction Loss: -7.6459503173828125
Iteration 6201:
Training Loss: -5.6918816566467285
Reconstruction Loss: -7.650452136993408
Iteration 6251:
Training Loss: -5.99247932434082
Reconstruction Loss: -7.6554179191589355
Iteration 6301:
Training Loss: -5.941180229187012
Reconstruction Loss: -7.658745765686035
Iteration 6351:
Training Loss: -5.6917266845703125
Reconstruction Loss: -7.663456439971924
Iteration 6401:
Training Loss: -5.92040491104126
Reconstruction Loss: -7.668461799621582
Iteration 6451:
Training Loss: -5.826119422912598
Reconstruction Loss: -7.671968936920166
Iteration 6501:
Training Loss: -5.8527140617370605
Reconstruction Loss: -7.6754984855651855
Iteration 6551:
Training Loss: -5.818858623504639
Reconstruction Loss: -7.679683685302734
Iteration 6601:
Training Loss: -5.784157752990723
Reconstruction Loss: -7.684334754943848
Iteration 6651:
Training Loss: -5.839006423950195
Reconstruction Loss: -7.688256740570068
Iteration 6701:
Training Loss: -5.961841106414795
Reconstruction Loss: -7.693290710449219
Iteration 6751:
Training Loss: -5.818005561828613
Reconstruction Loss: -7.69556188583374
Iteration 6801:
Training Loss: -5.930567741394043
Reconstruction Loss: -7.698795795440674
Iteration 6851:
Training Loss: -5.791343688964844
Reconstruction Loss: -7.704398155212402
Iteration 6901:
Training Loss: -5.844597816467285
Reconstruction Loss: -7.707534313201904
Iteration 6951:
Training Loss: -5.943982124328613
Reconstruction Loss: -7.711590766906738
Iteration 7001:
Training Loss: -5.912254333496094
Reconstruction Loss: -7.715221881866455
Iteration 7051:
Training Loss: -5.979889392852783
Reconstruction Loss: -7.7190327644348145
Iteration 7101:
Training Loss: -6.14945650100708
Reconstruction Loss: -7.722724437713623
Iteration 7151:
Training Loss: -6.006827354431152
Reconstruction Loss: -7.726429462432861
Iteration 7201:
Training Loss: -6.1081438064575195
Reconstruction Loss: -7.729822158813477
Iteration 7251:
Training Loss: -5.923611164093018
Reconstruction Loss: -7.734269142150879
Iteration 7301:
Training Loss: -5.917522430419922
Reconstruction Loss: -7.737643241882324
Iteration 7351:
Training Loss: -5.815964698791504
Reconstruction Loss: -7.740830898284912
Iteration 7401:
Training Loss: -6.082094669342041
Reconstruction Loss: -7.744662284851074
Iteration 7451:
Training Loss: -5.946707725524902
Reconstruction Loss: -7.747997760772705
